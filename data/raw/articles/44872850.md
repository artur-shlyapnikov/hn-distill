LLMs’ “simulated reasoning” abilities are a “brittle mirage,” researchers find - Ars Technica                                                       

[Skip to content](#main)

[Ars Technica home](https://arstechnica.com/)

Sections

[Forum](/civis/)

[Subscribe](/subscribe/)

[Search](/search/)

*   [AI](https://arstechnica.com/ai/)
*   [Biz & IT](https://arstechnica.com/information-technology/)
*   [Cars](https://arstechnica.com/cars/)
*   [Culture](https://arstechnica.com/culture/)
*   [Gaming](https://arstechnica.com/gaming/)
*   [Health](https://arstechnica.com/health/)
*   [Policy](https://arstechnica.com/tech-policy/)
*   [Science](https://arstechnica.com/science/)
*   [Security](https://arstechnica.com/security/)
*   [Space](https://arstechnica.com/space/)
*   [Tech](https://arstechnica.com/gadgets/)

*   [Feature](/features/)
*   [Reviews](/reviews/)

*   [AI](https://arstechnica.com/ai/)
*   [Biz & IT](https://arstechnica.com/information-technology/)
*   [Cars](https://arstechnica.com/cars/)
*   [Culture](https://arstechnica.com/culture/)
*   [Gaming](https://arstechnica.com/gaming/)
*   [Health](https://arstechnica.com/health/)
*   [Policy](https://arstechnica.com/tech-policy/)
*   [Science](https://arstechnica.com/science/)
*   [Security](https://arstechnica.com/security/)
*   [Space](https://arstechnica.com/space/)
*   [Tech](https://arstechnica.com/gadgets/)

[Forum](/civis/)

[Subscribe](/subscribe/)

Story text

Size Small Standard Large Width \* Standard Wide Links Standard Orange

\* Subscribers only  
  [Learn more](/store/product/subscriptions/)

Pin to story

Theme

*   HyperLight
*   Day & Night
*   Dark
*   System

Sign In

It's (not) thinking?

# LLMs’ “simulated reasoning” abilities are a “brittle mirage,” researchers find

Chain-of-thought AI "degrades significantly" when asked to generalize beyond training.

[Kyle Orland](https://arstechnica.com/author/kyle-orland/) – Aug 11, 2025 1:16 pm | [149](https://arstechnica.com/ai/2025/08/researchers-find-llms-are-bad-at-logical-inference-good-at-fluent-nonsense/#comments "149 comments")

 [![](https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-1155287117-640x512.jpg) ![](https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-1155287117-1152x648.jpg)](https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-1155287117.jpg)

How much does this puzzle resemble one the robot has seen before? Credit: Getty Images

How much does this puzzle resemble one the robot has seen before? Credit: Getty Images

Text settings

Story text

Size Small Standard Large Width \* Standard Wide Links Standard Orange

\* Subscribers only  
  [Learn more](/store/product/subscriptions/)

Minimize to nav

In recent months, the AI industry has started [moving toward](https://arstechnica.com/ai/2025/04/openai-releases-new-simulated-reasoning-models-with-full-tool-access/) so-called [simulated reasoning models](https://arstechnica.com/ai/2025/01/china-is-catching-up-with-americas-best-reasoning-ai-models/) that [use a "chain of thought" process](https://arstechnica.com/ai/2025/02/claude-3-7-sonnet-debuts-with-extended-thinking-to-tackle-complex-problems/) to work through tricky problems in multiple logical steps. At the same time, [recent](https://arstechnica.com/ai/2025/06/new-apple-study-challenges-whether-ai-models-truly-reason-through-problems/) [research](https://arstechnica.com/ai/2025/04/new-study-shows-why-simulated-reasoning-ai-models-dont-yet-live-up-to-their-billing/) has [cast doubt](https://arstechnica.com/ai/2025/04/researchers-concerned-to-find-ai-models-hiding-their-true-reasoning-processes/) on whether those models [have even a basic understanding of general logical concepts](https://arstechnica.com/ai/2025/06/with-the-launch-of-o3-pro-lets-talk-about-what-ai-reasoning-actually-does/) or an accurate grasp of their own "thought process." Similar research shows that these "reasoning" models can often produce incoherent, logically unsound answers when questions include irrelevant clauses or deviate even slightly from common templates found in their training data.

In [a recent pre-print paper](https://arxiv.org/pdf/2508.01191), researchers from the University of Arizona summarize this existing work as "suggest\[ing\] that LLMs are not principled reasoners but rather sophisticated simulators of reasoning-like text." To pull on that thread, the researchers created a carefully controlled LLM environment in an attempt to measure just how well chain-of-thought reasoning works when presented with "out of domain" logical problems that don't match the specific logical patterns found in their training data.

The results suggest that the seemingly large performance leaps made by chain-of-thought models are "largely a brittle mirage" that "become\[s\] fragile and prone to failure even under moderate distribution shifts," the researchers write. "Rather than demonstrating a true understanding of text, CoT reasoning under task transformations appears to reflect a replication of patterns learned during training."

## No one trained me for this!

To test an LLM's generalized reasoning capability in an objective, measurable way, the researchers created a specially controlled LLM training environment [called DataAlchemy](https://github.com/ChengshuaiZhao0/DataAlchemy). This setup creates small models trained on examples of two extremely simple text transformations—an [ROT cypher](https://www.dcode.fr/rot-cipher) and [cyclical shifts](https://www.computer.org/csdl/proceedings-article/icee/2010/3997d483/12OmNvo67DA)—followed by additional training that demonstrates those two functions performed in various orders and combinations.

[![](https://cdn.arstechnica.net/wp-content/uploads/2025/08/llmtraining.png)](https://cdn.arstechnica.net/wp-content/uploads/2025/08/llmtraining.png)

The researchers used test cases that fall outside of the LLM training data in task type, format, and length.

Credit: [Zhao et al](https://arxiv.org/pdf/2508.01191)

The researchers used test cases that fall outside of the LLM training data in task type, format, and length. Credit: [Zhao et al](https://arxiv.org/pdf/2508.01191)

These simplified models were then tested using a variety of tasks, some of which precisely or closely matched the function patterns in the training data and others that required function compositions that were either partially or fully "out of domain" for the training data. For instance, a model trained on data showing two cyclical shifts might be asked to perform a novel transformation involving two ROT shifts (with basic training on what a single example of either shift looks like). The final answers and reasoning steps were compared to the desired answer using [BLEU scores](https://huggingface.co/spaces/evaluate-metric/bleu) and [Levenshtein Distance](https://en.wikipedia.org/wiki/Levenshtein_distance) for an objective measure of their accuracy.

As the researchers hypothesized, these basic models started to fail catastrophically when asked to generalize novel sets of transformations that were not directly demonstrated in the training data. While the models would often try to generalize new logical rules based on similar patterns in the training data, this would quite often lead to the model laying out "correct reasoning paths, yet incorrect answer\[s\]." In other cases, the LLM would sometimes stumble onto correct answers paired with "unfaithful reasoning paths" that didn't follow logically.

"Rather than demonstrating a true understanding of text, CoT reasoning under task transformations appears to reflect a replication of patterns learned during training," the researchers write.

[![](https://cdn.arstechnica.net/wp-content/uploads/2025/08/llmood.png)](https://cdn.arstechnica.net/wp-content/uploads/2025/08/llmood.png)

As requested tasks get further outside the training distribution (redder dots), the answers provided drift farther from the desired answer (lower right of the graph).

Credit: [Zhao et al](https://arxiv.org/pdf/2508.01191)

As requested tasks get further outside the training distribution (redder dots), the answers provided drift farther from the desired answer (lower right of the graph). Credit: [Zhao et al](https://arxiv.org/pdf/2508.01191)

The researchers went on to test their controlled system using input text strings slightly shorter or longer than those found in the training data, or that required function chains of different lengths than those it was trained on. In both cases the accuracy of the results "deteriorates as the \[length\] discrepancy increases," thus "indicating the failure of generalization" in the models. Small, unfamiliar-to-the-model discrepancies in the format of the test tasks (e.g., the introduction of letters or symbols not found in the training data) also caused performance to "degrade sharply" and "affect\[ed\] the correctness" of the model's responses, the researchers found.

## “A false aura of dependability”

Using supervised fine-tuning (SFT) to introduce even a small amount of relevant data to the training set can often lead to strong improvements in this kind of "out of domain" model performance. But the researchers say that this kind of "patch" for various logical tasks "should not be mistaken for achieving true generalization. ... Relying on SFT to fix every \[out of domain\] failure is an unsustainable and reactive strategy that fails to address the core issue: the model’s lack of abstract reasoning capability."

Rather than showing the capability for generalized logical inference, these chain-of-thought models are "a sophisticated form of structured pattern matching" that "degrades significantly" when pushed even slightly outside of its training distribution, the researchers write. Further, the ability of these models to generate "fluent nonsense" creates "a false aura of dependability" that does not stand up to a careful audit.

As such, the researchers warn heavily against "equating \[chain-of-thought\]-style output with human thinking" especially in "high-stakes domains like medicine, finance, or legal analysis." Current tests and benchmarks should prioritize tasks that fall outside of any training set to probe for these kinds of errors, while future models will need to move beyond "surface-level pattern recognition to exhibit deeper inferential competence," they write.

[![Photo of Kyle Orland](https://cdn.arstechnica.net/wp-content/uploads/2016/05/k.orland-13.jpg)](https://arstechnica.com/author/kyle-orland/)

[Kyle Orland](https://arstechnica.com/author/kyle-orland/) Senior Gaming Editor

[Kyle Orland](https://arstechnica.com/author/kyle-orland/) Senior Gaming Editor

Kyle Orland has been the Senior Gaming Editor at Ars Technica since 2012, writing primarily about the business, tech, and culture behind video games. He has journalism and computer science degrees from University of Maryland. He once [wrote a whole book about _Minesweeper_](https://bossfightbooks.com/collections/books/products/minesweeper-by-kyle-orland).

[149 Comments](https://arstechnica.com/ai/2025/08/researchers-find-llms-are-bad-at-logical-inference-good-at-fluent-nonsense/#comments "149 comments")

Comments

[Forum view](https://arstechnica.com/civis/threads/researchers-find-llms-are-bad-at-logical-inference-good-at-%E2%80%9Cfluent-nonsense%E2%80%9D.1508798/)

![Loading](https://cdn.arstechnica.net/wp-content/themes/ars-v9/public/images/firework-loader.75ab30.gif) Loading comments...

[Prev story](https://arstechnica.com/science/2025/08/rfk-jr-posted-fishing-pics-as-cdc-reeled-from-shooting-linked-to-vaccine-disinfo/ "Go to: RFK Jr. posted fishing pics as CDC reeled from shooting linked to vaccine disinfo")

[Next story](https://arstechnica.com/space/2025/08/why-amazon-is-letting-its-rival-launch-its-satellites/ "Go to: Why does Jeff Bezos keep buying launches from Elon Musk?")

Most Read

1.  [![Listing image for first story in Most Read: Texas prepares for war as invasion of flesh-eating flies appears imminent](https://cdn.arstechnica.net/wp-content/uploads/2016/11/CSIRO_ScienceImage_115_The_Tip_of_a_Screw_Worm_Fly_Larvae.jpg)](https://arstechnica.com/health/2025/08/texas-prepares-for-war-as-invasion-of-flesh-eating-flies-appears-imminent/)
    
    1. [Texas prepares for war as invasion of flesh-eating flies appears imminent](https://arstechnica.com/health/2025/08/texas-prepares-for-war-as-invasion-of-flesh-eating-flies-appears-imminent/)
    
2.  2. [How old is the earliest trace of life on Earth?](https://arstechnica.com/science/2025/08/how-old-is-the-earliest-trace-of-life-on-earth/)
    
3.  3. [AI industry horrified to face largest copyright class action ever certified](https://arstechnica.com/tech-policy/2025/08/ai-industry-horrified-to-face-largest-copyright-class-action-ever-certified/)
    
4.  4. [Why does Jeff Bezos keep buying launches from Elon Musk?](https://arstechnica.com/space/2025/08/why-amazon-is-letting-its-rival-launch-its-satellites/)
    
5.  5. [$30K Ford EV truck due in 2027 with much-simpler production process](https://arstechnica.com/cars/2025/08/ford-bets-big-on-universal-ev-production-system-and-30k-truck/)
    

Customize

Ars Technica has been separating the signal from the noise for over 25 years. With our unique combination of technical savvy and wide-ranging interest in the technological arts and sciences, Ars is the trusted source in a sea of information. After all, you don’t need to know everything, only what’s important.

[](https://bsky.app/profile/arstechnica.com)[](https://mastodon.social/@arstechnica)[](https://www.facebook.com/arstechnica)[](https://www.youtube.com/@arstechnica)[](https://www.instagram.com/arstechnica/)

More from Ars

*   [About Us](https://arstechnica.com/about-us/)
*   [Staff Directory](https://arstechnica.com/staff-directory/)
*   [Newsletters](https://arstechnica.com/newsletters/)
*   [General FAQ](https://arstechnica.com/general-faq/)
*   [Posting Guidelines](https://arstechnica.com/ars-technica-posting-guidelines-v3-0/)
*   [RSS Feeds](https://arstechnica.com/rss-feeds/)

Contact

*   [Contact us](https://arstechnica.com/contact-us/)
*   [Advertise with us](mailto:adinquiries@condenast.com)
*   [Reprints](https://arstechnica.com/reprints/)

Manage Preferences

© 2025 Condé Nast. All rights reserved. Use of and/or registration on any portion of this site constitutes acceptance of our [User Agreement](https://www.condenast.com/user-agreement/) and [Privacy Policy and Cookie Statement](https://www.condenast.com/privacy-policy/) and [Ars Technica Addendum](/amendment-to-conde-nast-user-agreement-privacy-policy/) and [Your California Privacy Rights](https://www.condenast.com/privacy-policy/#california). Ars Technica may earn compensation on sales from links on this site. [Read our affiliate link policy](/affiliate-link-policy/). The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. [Ad Choices](https://www.aboutads.info/)
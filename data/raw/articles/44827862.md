Title: Benchmark Framework Desktop Mainboard and 4-node cluster

URL Source: https://github.com/geerlingguy/ollama-benchmark/issues/21

Published Time: 2025-08-07T14:32:03.000Z

Markdown Content:
Benchmark Framework Desktop Mainboard and 4-node cluster Â· Issue #21 Â· geerlingguy/ollama-benchmark

===============

[Skip to content](https://github.com/geerlingguy/ollama-benchmark/issues/21#start-of-content)
Navigation Menu
---------------

Toggle navigation

[](https://github.com/)

[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fgeerlingguy%2Follama-benchmark%2Fissues%2F21)

Appearance settings

*    Product 

    *   [GitHub Copilot Write better code with AI](https://github.com/features/copilot)
    *   [GitHub Spark New Build and deploy intelligent apps](https://github.com/features/spark)
    *   [GitHub Models New Manage and compare prompts](https://github.com/features/models)
    *   [GitHub Advanced Security Find and fix vulnerabilities](https://github.com/security/advanced-security)
    *   [Actions Automate any workflow](https://github.com/features/actions)

    *   [Codespaces Instant dev environments](https://github.com/features/codespaces)
    *   [Issues Plan and track work](https://github.com/features/issues)
    *   [Code Review Manage code changes](https://github.com/features/code-review)
    *   [Discussions Collaborate outside of code](https://github.com/features/discussions)
    *   [Code Search Find more, search less](https://github.com/features/code-search)

Explore
    *   [Why GitHub](https://github.com/why-github)
    *   [All features](https://github.com/features)
    *   [Documentation](https://docs.github.com/)
    *   [GitHub Skills](https://skills.github.com/)
    *   [Blog](https://github.blog/)

*    Solutions 

By company size
    *   [Enterprises](https://github.com/enterprise)
    *   [Small and medium teams](https://github.com/team)
    *   [Startups](https://github.com/enterprise/startups)
    *   [Nonprofits](https://github.com/solutions/industry/nonprofits)

By use case
    *   [DevSecOps](https://github.com/solutions/use-case/devsecops)
    *   [DevOps](https://github.com/solutions/use-case/devops)
    *   [CI/CD](https://github.com/solutions/use-case/ci-cd)
    *   [View all use cases](https://github.com/solutions/use-case)

By industry
    *   [Healthcare](https://github.com/solutions/industry/healthcare)
    *   [Financial services](https://github.com/solutions/industry/financial-services)
    *   [Manufacturing](https://github.com/solutions/industry/manufacturing)
    *   [Government](https://github.com/solutions/industry/government)
    *   [View all industries](https://github.com/solutions/industry)

[View all solutions](https://github.com/solutions)

*    Resources 

Topics
    *   [AI](https://github.com/resources/articles/ai)
    *   [DevOps](https://github.com/resources/articles/devops)
    *   [Security](https://github.com/resources/articles/security)
    *   [Software Development](https://github.com/resources/articles/software-development)
    *   [View all](https://github.com/resources/articles)

Explore
    *   [Learning Pathways](https://resources.github.com/learn/pathways)
    *   [Events & Webinars](https://resources.github.com/)
    *   [Ebooks & Whitepapers](https://github.com/resources/whitepapers)
    *   [Customer Stories](https://github.com/customer-stories)
    *   [Partners](https://partner.github.com/)
    *   [Executive Insights](https://github.com/solutions/executive-insights)

*    Open Source 

    *   [GitHub Sponsors Fund open source developers](https://github.com/sponsors)

    *   [The ReadME Project GitHub community articles](https://github.com/readme)

Repositories
    *   [Topics](https://github.com/topics)
    *   [Trending](https://github.com/trending)
    *   [Collections](https://github.com/collections)

*    Enterprise 

    *   [Enterprise platform AI-powered developer platform](https://github.com/enterprise)

Available add-ons
    *   [GitHub Advanced Security Enterprise-grade security features](https://github.com/security/advanced-security)
    *   [Copilot for business Enterprise-grade AI features](https://github.com/features/copilot/copilot-business)
    *   [Premium Support Enterprise-grade 24/7 support](https://github.com/premium-support)

*   [Pricing](https://github.com/pricing)

Search or jump to...

Search code, repositories, users, issues, pull requests...
==========================================================

 Search  

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

Provide feedback
================

We read every piece of feedback, and take your input very seriously.

- [x] Include my email address so I can be contacted 

 Cancel  Submit feedback 

Saved searches
==============

Use saved searches to filter your results more quickly
------------------------------------------------------

Name 

Query 

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

 Cancel  Create saved search 

[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fgeerlingguy%2Follama-benchmark%2Fissues%2F21)

[Sign up](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fissues_fragments%2Fissue_layout&source=header-repo&source_repo=geerlingguy%2Follama-benchmark)

Appearance settings

Resetting focus

You signed in with another tab or window. [Reload](https://github.com/geerlingguy/ollama-benchmark/issues/21) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/geerlingguy/ollama-benchmark/issues/21) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/geerlingguy/ollama-benchmark/issues/21) to refresh your session.Dismiss alert

{{ message }}

[geerlingguy](https://github.com/geerlingguy)/**[ollama-benchmark](https://github.com/geerlingguy/ollama-benchmark)**Public

 forked from [tabletuser-blogspot/ollama-benchmark](https://github.com/tabletuser-blogspot/ollama-benchmark)

*   [Notifications](https://github.com/login?return_to=%2Fgeerlingguy%2Follama-benchmark)You must be signed in to change notification settings
*   [Fork 11](https://github.com/login?return_to=%2Fgeerlingguy%2Follama-benchmark)
*   [Star 107](https://github.com/login?return_to=%2Fgeerlingguy%2Follama-benchmark) 

*   [Code](https://github.com/geerlingguy/ollama-benchmark)
*   [Issues 7](https://github.com/geerlingguy/ollama-benchmark/issues)
*   [Pull requests 2](https://github.com/geerlingguy/ollama-benchmark/pulls)
*   [Actions](https://github.com/geerlingguy/ollama-benchmark/actions)
*   [Security](https://github.com/geerlingguy/ollama-benchmark/security)[](https://github.com/geerlingguy/ollama-benchmark/security)[](https://github.com/geerlingguy/ollama-benchmark/security)[](https://github.com/geerlingguy/ollama-benchmark/security)[### Uh oh!](https://github.com/geerlingguy/ollama-benchmark/security)
[There was an error while loading.](https://github.com/geerlingguy/ollama-benchmark/security)[Please reload this page](https://github.com/geerlingguy/ollama-benchmark/issues/21).    
*   [Insights](https://github.com/geerlingguy/ollama-benchmark/pulse)

Additional navigation options

*   [Code](https://github.com/geerlingguy/ollama-benchmark)
*   [Issues](https://github.com/geerlingguy/ollama-benchmark/issues)
*   [Pull requests](https://github.com/geerlingguy/ollama-benchmark/pulls)
*   [Actions](https://github.com/geerlingguy/ollama-benchmark/actions)
*   [Security](https://github.com/geerlingguy/ollama-benchmark/security)
*   [Insights](https://github.com/geerlingguy/ollama-benchmark/pulse)

Benchmark Framework Desktop Mainboard and 4-node cluster#21
===========================================================

[New issue](https://github.com/login?return_to=https://github.com/geerlingguy/ollama-benchmark/issues/21)

Copy link

[New issue](https://github.com/login?return_to=https://github.com/geerlingguy/ollama-benchmark/issues/21)

Copy link

Open

Open

[Benchmark Framework Desktop Mainboard and 4-node cluster](https://github.com/geerlingguy/ollama-benchmark/issues/21#top)#21

Copy link

[![Image 1: geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=80)](https://github.com/geerlingguy)

Description
-----------

[![Image 2: @geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=48)](https://github.com/geerlingguy)

[geerlingguy](https://github.com/geerlingguy)

opened [on Aug 7, 2025](https://github.com/geerlingguy/ollama-benchmark/issues/21#issue-3300763466) Â· edited by [geerlingguy](https://github.com/geerlingguy)

Edits

Owner

Issue body actions

Testing the Framework Desktop - AMD Ryzen AI Max+ 395 with Radeon 8090S.

For all my benchmarking for this machine, see [sbc-reviews: Framework Desktop](https://github.com/geerlingguy/sbc-reviews/issues/80).

Activity
--------

[![Image 3: @geerlingguy](https://avatars.githubusercontent.com/u/481677?s=64&u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4)geerlingguy](https://github.com/geerlingguy)

mentioned this [on Aug 7, 2025](https://github.com/geerlingguy/ollama-benchmark/issues/21#event-2934394267)

*   [Framework Desktop Mainboard geerlingguy/sbc-reviews#80](https://github.com/geerlingguy/sbc-reviews/issues/80) 

[![Image 4: geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=80)](https://github.com/geerlingguy)

### geerlingguy commented on Aug 7, 2025

[![Image 5: @geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=48)](https://github.com/geerlingguy)

[geerlingguy](https://github.com/geerlingguy)

[on Aug 7, 2025](https://github.com/geerlingguy/ollama-benchmark/issues/21#issuecomment-3164567688)

Owner Author

First set of results, using ollama defaults, which seemed to only use CPU cores and not GPU:

Llama - CPU
-----------

Running benchmark 3 times using model: llama3.2:3b (152W)

| Run | Eval Rate (Tokens/Second) |
| --- | --- |
| 1 | 46.21 tokens/s |
| 2 | 45.23 tokens/s |
| 3 | 44.64 tokens/s |
| **Average Eval Rate** | 45.36 tokens/second |

Running benchmark 3 times using model: llama3.1:8b (142W)

| Run | Eval Rate (Tokens/Second) |
| --- | --- |
| 1 | 21.02 tokens/s |
| 2 | 20.83 tokens/s |
| 3 | 20.84 tokens/s |
| **Average Eval Rate** | 20.89 tokens/second |

Running benchmark 3 times using model: llama2:13b (142W)

| Run | Eval Rate (Tokens/Second) |
| --- | --- |
| 1 | 11.75 tokens/s |
| 2 | 11.81 tokens/s |
| 3 | 14.41 tokens/s |
| **Average Eval Rate** | 12.65 tokens/second |

DeepSeek - CPU
--------------

declare -a models=("deepseek-r1:1.5b" "deepseek-r1:8b" "deepseek-r1:14b" "deepseek-r1:70b")

142W, 141W, 140W,

Running benchmark 3 times using model: deepseek-r1:1.5b (142W)

| Run | Eval Rate (Tokens/Second) |
| --- | --- |
| 1 | 85.56 tokens/s |
| 2 | 85.05 tokens/s |
| 3 | 85.55 tokens/s |
| **Average Eval Rate** | 85.38 tokens/second |

Running benchmark 3 times using model: deepseek-r1:8b (141W)

| Run | Eval Rate (Tokens/Second) |
| --- | --- |
| 1 | 19.57 tokens/s |
| 2 | 19.63 tokens/s |
| 3 | 19.27 tokens/s |
| **Average Eval Rate** | 19.49 tokens/second |

Running benchmark 3 times using model: deepseek-r1:14b (140W)

| Run | Eval Rate (Tokens/Second) |
| --- | --- |
| 1 | 11.36 tokens/s |
| 2 | 11.38 tokens/s |
| 3 | 11.39 tokens/s |
| **Average Eval Rate** | 11.37 tokens/second |

Running benchmark 3 times using model: deepseek-r1:70b (141W)

| Run | Eval Rate (Tokens/Second) |
| --- | --- |
| 1 | 2.44 tokens/s |
| 2 | 2.43 tokens/s |
| 3 | 2.43 tokens/s |
| **Average Eval Rate** | 2.43 tokens/second |

[![Image 6: geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=80)](https://github.com/geerlingguy)

### geerlingguy commented on Aug 7, 2025

[![Image 7: @geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=48)](https://github.com/geerlingguy)

[geerlingguy](https://github.com/geerlingguy)

[on Aug 7, 2025](https://github.com/geerlingguy/ollama-benchmark/issues/21#issuecomment-3164568218)

Owner Author

Second set of results, Ollama was a bit annoying, as I couldn't get ROCm working, and Ollama + Vulkan is also a little hard. I tried forcing iGPU usage (see [ollama/ollama#5186 (comment)](https://github.com/ollama/ollama/issues/5186#issuecomment-2994386318)), but had trouble with ROCm.

I also read through [lhl's extensive documentation on getting ROCm going on Linux with Strix Halo](https://forum.level1techs.com/t/strix-halo-ryzen-ai-max-395-llm-benchmark-results/233796), where he noted getting better performance with the Vulkan backend than ROCm.

So I switched to [`llama.cpp`](https://github.com/ggml-org/llama.cpp), and followed my old guide for [running llama.cpp with Vulkan on a Pi 5](https://www.jeffgeerling.com/blog/2024/llms-accelerated-egpu-on-raspberry-pi-5). The required dependencies were slightly different on Fedora 42 versus the Debian-based Pi OS I used previously. Here are the exact commands I ran to build llama.cpp from source:

```
cd Downloads
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

sudo dnf install cmake vulkan vulkan-tools glslc vulkan-loader-devel libcurl-devel
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

Then download a model from HuggingFace and test it:

```
# Download llama3.2:3b
cd models && wget https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf

# Run it.
cd ../
./build/bin/llama-cli -m "models/Llama-3.2-3B-Instruct-Q4_K_M.gguf" -no-cnv -p "Why is the blue sky blue?" -e -ngl 100 -t 4

# You should see in the output, ggml_vulkan detected the iGPU:
ggml_vulkan: Found 1 Vulkan devices:
ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv)...
```

I couldn't find a way to utilize the Strix Halo NPU yet, though. It seems like nobody's cracked that nut.

Here are the results with the iGPU and llama.cpp with the Vulkan backend:

Llama 3.2 3b - GPU (Vulkan)
---------------------------

```
cd models
wget https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf
cd ../

./build/bin/llama-bench -m models/Llama-3.2-3B-Instruct-Q4_K_M.gguf -n 128 -p 512,4096 -pg 4096,128 -ngl 99 -r 2
ggml_vulkan: Found 1 Vulkan devices:
ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat
build: 8ad7b3e6 (6019)
```

| model | size | params | backend | ngl | test | t/s |
| --- | ---: | ---: | --- | ---: | ---: | ---: |
| llama 3B Q4_K - Medium | 1.87 GiB | 3.21 B | Vulkan | 99 | pp512 | 1581.18 Â± 10.80 |
| llama 3B Q4_K - Medium | 1.87 GiB | 3.21 B | Vulkan | 99 | pp4096 | 1059.94 Â± 1.85 |
| llama 3B Q4_K - Medium | 1.87 GiB | 3.21 B | Vulkan | 99 | tg128 | 88.14 Â± 1.28 |
| llama 3B Q4_K - Medium | 1.87 GiB | 3.21 B | Vulkan | 99 | pp4096+tg128 | 652.67 Â± 1.39 |

Consumed around 133W during these tests.

Llama 3.1 70b - GPU (Vulkan)
----------------------------

```
cd models
wget https://huggingface.co/bartowski/Meta-Llama-3.1-70B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-70B-Instruct-Q4_K_M.gguf
cd ../

./build/bin/llama-bench -m models/Meta-Llama-3.1-70B-Instruct-Q4_K_M.gguf -n 128 -ngl 99 -fa 1 -r 2
ggml_vulkan: Found 1 Vulkan devices:
ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat
build: 8ad7b3e6 (6019)
```

| model | size | params | backend | ngl | test | t/s |
| --- | ---: | ---: | --- | ---: | ---: | ---: |
| llama 70B Q4_K - Medium | 39.59 GiB | 70.55 B | Vulkan | 99 | pp512 | 34.21 Â± 0.01 |
| llama 70B Q4_K - Medium | 39.59 GiB | 70.55 B | Vulkan | 99 | pp4096 | 32.54 Â± 0.01 |
| llama 70B Q4_K - Medium | 39.59 GiB | 70.55 B | Vulkan | 99 | tg128 | 4.97 Â± 0.03 |
| llama 70B Q4_K - Medium | 39.59 GiB | 70.55 B | Vulkan | 99 | pp4096+tg128 | 27.26 Â± 0.00 |

Consumed around 133W during these tests.

Qwen2.5 14b - GPU (Vulkan)
--------------------------

```
cd models
wget https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q4_K_M.gguf
cd ../

./build/bin/llama-bench -m models/Qwen2.5-14B-Instruct-Q4_K_M.gguf -n 128 -p 512,4096 -pg 4096,128 -ngl 99 -r 2
ggml_vulkan: Found 1 Vulkan devices:
ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat
build: 8ad7b3e6 (6019)
```

| model | size | params | backend | ngl | test | t/s |
| --- | ---: | ---: | --- | ---: | ---: | ---: |
| qwen2 14B Q4_K - Medium | 8.37 GiB | 14.77 B | Vulkan | 99 | pp512 | 321.48 Â± 0.61 |
| qwen2 14B Q4_K - Medium | 8.37 GiB | 14.77 B | Vulkan | 99 | pp4096 | 266.84 Â± 0.17 |
| qwen2 14B Q4_K - Medium | 8.37 GiB | 14.77 B | Vulkan | 99 | tg128 | 22.97 Â± 0.16 |
| qwen2 14B Q4_K - Medium | 8.37 GiB | 14.77 B | Vulkan | 99 | pp4096+tg128 | 184.66 Â± 0.36 |

Consumed around 140W during these tests.

[![Image 8: geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=80)](https://github.com/geerlingguy)

### geerlingguy commented on Aug 7, 2025

[![Image 9: @geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=48)](https://github.com/geerlingguy)

[geerlingguy](https://github.com/geerlingguy)

[on Aug 7, 2025](https://github.com/geerlingguy/ollama-benchmark/issues/21#issuecomment-3164568652)

Owner Author

Comparisons
-----------

See, for comparison:

*   [Nvidia A400, A4000, AMD Pro W7700 on System76 Thelio Astra](https://github.com/geerlingguy/ollama-benchmark/issues/5)
*   [AMD RX 6700 XT on Raspberry Pi 5](https://github.com/geerlingguy/ollama-benchmark/issues/1)

[![Image 10: geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=80)](https://github.com/geerlingguy)

### geerlingguy commented on Aug 7, 2025

[![Image 11: @geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=48)](https://github.com/geerlingguy)

[geerlingguy](https://github.com/geerlingguy)

[on Aug 7, 2025](https://github.com/geerlingguy/ollama-benchmark/issues/21#issuecomment-3164569199)

Owner Author

Cluster Results - llama.cpp RPC
-------------------------------

My first attempt at building a cluster for AI models is to use [llama.cpp's built in RPC functionality](https://github.com/ggml-org/llama.cpp/tree/master/tools/rpc).

I may also try:

*   [https://github.com/b4rtaz/distributed-llama](https://github.com/b4rtaz/distributed-llama) (mentions explicit Vulkan support, with flags...)
*   [https://github.com/exo-explore/exo](https://github.com/exo-explore/exo)

But for RPC, I've built up an automated deployment playbook with Ansible called [beowulf-ai-cluster](https://github.com/geerlingguy/ollama-benchmark/issues/TODO) that sets everything up, and connects everything together.

To _manually_ set things up, once you have the main setup playbook done, you can run the following on each node (e.g. `10.0.2.209`:

```
/opt/llama.cpp$ build/bin/rpc-server --host 10.0.2.209 --port 50052
```

Then run `llama-cli` connecting to multiple RPC nodes:

```
/opt/llama.cpp$ build/bin/llama-cli -m models/Llama-3.2-3B-Instruct-Q4_K_M.gguf -p "Hello, my name is" --rpc 10.0.2.209:50052, -ngl 99
```

### Llama 3.1 70B Instruct Q4_K_M

Running on a single node:

```
| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |
| llama 70B Q4_K - Medium        |  39.59 GiB |    70.55 B | Vulkan,RPC |  99 |  1 |           pp512 |         36.44 Â± 0.03 |
| llama 70B Q4_K - Medium        |  39.59 GiB |    70.55 B | Vulkan,RPC |  99 |  1 |          pp4096 |         34.79 Â± 0.01 |
| llama 70B Q4_K - Medium        |  39.59 GiB |    70.55 B | Vulkan,RPC |  99 |  1 |           tg128 |          5.01 Â± 0.00 |
| llama 70B Q4_K - Medium        |  39.59 GiB |    70.55 B | Vulkan,RPC |  99 |  1 |    pp4096+tg128 |         36.72 Â± 0.01 |
```

Result on 4 nodes using Vulkan backend, 2.5 Gbps inter-node networking:

```
| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |
| llama 70B Q4_K - Medium        |  39.59 GiB |    70.55 B | Vulkan,RPC |  99 |  1 |           pp512 |         43.87 Â± 0.03 |
| llama 70B Q4_K - Medium        |  39.59 GiB |    70.55 B | Vulkan,RPC |  99 |  1 |           tg128 |          4.42 Â± 0.00 |
| llama 70B Q4_K - Medium        |  39.59 GiB |    70.55 B | Vulkan,RPC |  99 |  1 |     pp512+tg128 |         15.64 Â± 0.01 |
```

### Llama 3.1 405B Q4_K_M

Result on 4 nodes using Vulkan backend, 2.5 Gbps inter-node networking:

```
$ build/bin/llama-bench -v -m models/Llama-3.1-405B-Q4_K_M.gguf -n 128 -p 512 -pg 512,128 -ngl 99 -fa 1 -r 2 --rpc 10.0.2.233:50052,10.0.2.209:50052,10.0.2.242:50052,10.0.2.223:50052
ggml_vulkan: Found 1 Vulkan devices:
ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat
| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |
| llama ?B Q4_K - Medium         | 226.37 GiB |   405.85 B | Vulkan,RPC |  99 |  1 |           pp512 |          3.99 Â± 0.21 |
| llama ?B Q4_K - Medium         | 226.37 GiB |   405.85 B | Vulkan,RPC |  99 |  1 |           tg128 |          0.70 Â± 0.00 |
| llama ?B Q4_K - Medium         | 226.37 GiB |   405.85 B | Vulkan,RPC |  99 |  1 |     pp512+tg128 |          2.17 Â± 0.00 |
```

Result on 4 nodes using Vulkan backend, 5 Gbps inter-node networking:

```
$ build/bin/llama-bench -v -m models/Llama-3.1-405B-Q4_K_M.gguf -n 128 -p 512 -pg 512,128 -ngl 125 -fa 1 -r 2 --rpc 10.0.2.233:50052,10.0.2.209:50052,10.0.2.242:50052,10.0.2.223:50052
| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |
| llama ?B Q4_K - Medium         | 226.37 GiB |   405.85 B | Vulkan,RPC |  99 |  1 |           pp512 |          4.62 Â± 0.20 |
| llama ?B Q4_K - Medium         | 226.37 GiB |   405.85 B | Vulkan,RPC |  99 |  1 |           tg128 |          0.71 Â± 0.00 |
| llama ?B Q4_K - Medium         | 226.37 GiB |   405.85 B | Vulkan,RPC |  99 |  1 |     pp512+tg128 |          2.23 Â± 0.00 |
```

Also, attempting to get it to load and run a bit faster, I found that using `--mmap 0` gets a little boost, and I _believe_`--threads 32` gave me all the hyperthreading support on the CPU as well. Still trying to figure out how to get models to load up and run faster on this cluster with `llama.cpp`'s RPC support, but it seems like things don't really run in parallel, but in sequence. So, good for loading larger models, not good for performance.

```
build/bin/llama-bench -v -m models/Llama-3.1-405B-Q4_K_M.gguf --no-warmup --mmap 0 --threads 32 -n 128 -p 512 -ngl 150 -fa 1 -r 2 --rpc 10.0.2.209:50052,10.0.2.242:50052,10.0.2.223:50052
```

Also note that I had it _not_ use `--rpc` for the main node, as it automatically picks up Vulkan0 on it, I don't know how to disable that and force `llama-bench` to use the RPC nodes only (or if that's even an option). If you include the node that's running the bench/cli command among the `rpc` servers, it will quickly run out of RAM and start swapping!

### DeepSeek R1 671B Q4_K_M

First I merged all the `gguf` files together for convenience:

```
./build/bin/llama-gguf-split --merge models/DeepSeek-R1/DeepSeek-R1-Q4_K_M-00001-of-00011.gguf models/DeepSeek-R1-Q4_K_M.gguf
```

Then I had to make sure the environment variable `export GGML_VK_PREFER_HOST_MEMORY=1` was set on all RPC nodes (and the main node). Still need to make sure RPC only using the other three nodes so llama can use the main node directly with Vulkan and no RPC...

Then I ran DeepSeek R1 with:

```
build/bin/llama-cli -m models/DeepSeek-R1-Q4_K_M.gguf -p "Hello, my name is" -ngl 99 --no-mmap --rpc 10.0.2.209:50052,10.0.2.242:50052,10.0.2.223:50052
```

I needed to use `--no-mmap` so it could load the whole model even though it can't map it all to system RAM (since it's larger than 128 GB).

However, that didn't work... see [geerlingguy/beowulf-ai-cluster#2](https://github.com/geerlingguy/beowulf-ai-cluster/issues/2)

I _think_ I can only use `llama-bench` as `llama-cli` (maybe?) kept trying to use the local Vulkan0 device instead of RPC, and that would run into memory issues for some reason...?

So instead, I ran `llama-bench`:

```
build/bin/llama-bench -v -m models/DeepSeek-R1-Q4_K_M.gguf -n 128 -p 512 -pg 512,128 -ngl 99 -fa 1 -r 2 --rpc 10.0.2.233:50052,10.0.2.209:50052,10.0.2.242:50052,10.0.2.223:50052
```

Running on a single node: DNF, out of memory

Running across 4 nodes: DNF, out of memory

### DeepSeek V3 671B Q4_K_M

I combined the split files again, to make it easier to work with it on node 1:

```
sudo ./build/bin/llama-gguf-split --merge /mnt/nas01.mmoffice.net/AI\ Models/DeepSeek-V3/DeepSeek-V3-Q4_K_M-00001-of-00009.gguf models/DeepSeek-V3-Q4_K_M.gguf
```

Result on 4 nodes using Vulkan backend:

```
build/bin/llama-bench -v -m models/DeepSeek-V3-Q4_K_M.gguf -n 128 -p 512 -pg 512,128 -ngl 99 -fa 1 -r 2 --rpc 10.0.2.233:50052,10.0.2.209:50052,10.0.2.242:50052,10.0.2.223:50052

... DNF, system died
```

[![Image 12: geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=80)](https://github.com/geerlingguy)

### geerlingguy commented on Aug 7, 2025

[![Image 13: @geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=48)](https://github.com/geerlingguy)

[geerlingguy](https://github.com/geerlingguy)

[on Aug 7, 2025](https://github.com/geerlingguy/ollama-benchmark/issues/21#issuecomment-3164569835)

Owner Author

Cluster results - distributed-llama
-----------------------------------

I installed distributed-llama from source using `DLLAMA_VULKAN=1` when building, to enable Vulkan support (which worked a treat). Then I downloaded their specialized Llama 3.1 405B Instruct Q40 model:

```
cd /opt/distributed-llama && sudo python launch.py llama3_1_405b_instruct_q40
```

That takes... a long time. Once it was finished:

```
./dllama inference --model models/llama3_1_405b_instruct_q40/dllama_model_llama3_1_405b_instruct_q40.m --tokenizer models/llama3_1_405b_instruct_q40/dllama_tokenizer_llama3_1_405b_instruct_q40.t --buffer-float-type q80 --prompt "Hello world" --steps 256 --max-seq-len 4096 --nthreads 1 --workers 10.0.2.209:9999 10.0.2.242:9999 10.0.2.223:9999 --gpu-index 0
```

I had quite an ordeal getting Llama 3.1 405B to run on the full clusterâ€”I could do it on the CPU (32 threads per worker), but had trouble on the iGPU with Vulkan.

See issue [Test distributed-llama for clustered AI](https://github.com/geerlingguy/beowulf-ai-cluster/issues/4) for the entire saga.

Run this on each of the worker nodes:

```
./dllama worker --port 9999 --nthreads 1 --gpu-index 0
```

In the end, I found out I was trying to pass too much context, so I reduced that to `4096` and then ran it again, achieving... 0.45 tokens/s!

```
./dllama inference --model models/llama3_1_405b_instruct_q40/dllama_model_llama3_1_405b_instruct_q40.m --tokenizer models/llama3_1_405b_instruct_q40/dllama_tokenizer_llama3_1_405b_instruct_q40.t --buffer-float-type q80 --prompt "Hello world" --steps 256 --max-seq-len 4096 --nthreads 1 --workers 10.0.2.209:9999 10.0.2.242:9999 10.0.2.223:9999 --gpu-index 0
ðŸ“„ AddBos: 0
ðŸ“„ BosId: 128000 (<|begin_of_text|>)
ðŸ“„ EosId: 128001 (<|end_of_text|>) 128009 (<|eot_id|>) 
ðŸ“„ RegularVocabSize: 128000
ðŸ“„ SpecialVocabSize: 256
ðŸ’¡ Arch: Llama
ðŸ’¡ HiddenAct: Silu
ðŸ’¡ Dim: 16384
ðŸ’¡ HeadDim: 128
ðŸ’¡ QDim: 16384
ðŸ’¡ KvDim: 2048
ðŸ’¡ HiddenDim: 53248
ðŸ’¡ VocabSize: 128256
ðŸ’¡ nLayers: 126
ðŸ’¡ nHeads: 128
ðŸ’¡ nKvHeads: 16
ðŸ’¡ OrigSeqLen: 131072
ðŸ’¡ SeqLen: 4096
ðŸ’¡ NormEpsilon: 0.000010
ðŸ’¡ RopeType: Llama3.1
ðŸ’¡ RopeTheta: 500000
ðŸ’¡ RopeScaling: f=8.0, l=1.0, h=4.0, o=8192
ðŸ“€ RequiredMemory: 66433363 kB
â­• Socket[0]: connecting to 10.0.2.209:9999 worker
â­• Socket[0]: connected
â­• Socket[1]: connecting to 10.0.2.242:9999 worker
â­• Socket[1]: connected
â­• Socket[2]: connecting to 10.0.2.223:9999 worker
â­• Socket[2]: connected
â­• Network is initialized
ðŸŒ‹ Device: Radeon 8060S Graphics (RADV GFX1151)
ðŸŒ‹ DeviceApiVersion: 1.4.311
ðŸŒ‹ MaxComputeSharedMemory: 64 kB
ðŸŒ‹ Heap[1]: 72341 MB
ðŸ’¿ Loading weights...
ðŸ’¿ Weights loaded
ðŸš Network is in non-blocking mode
Hello world
ðŸ”·ï¸ Eval48935 ms Sync   21 ms | Sent 13044 kB Recv 13227 kB | (1 tokens)
ðŸ”¶ Pred 2036 ms Sync   73 ms | Sent 13044 kB Recv 13227 kB | å“ª
ðŸ”¶ Pred 1956 ms Sync   89 ms | Sent 13044 kB Recv 13227 kB |  on
ðŸ”¶ Pred 2126 ms Sync   59 ms | Sent 13044 kB Recv 13227 kB | assistant
ðŸ”¶ Pred 2167 ms Sync   53 ms | Sent 13044 kB Recv 13227 kB | ations
ðŸ”¶ Pred 2171 ms Sync   47 ms | Sent 13044 kB Recv 13227 kB | ~
ðŸ”¶ Pred 2199 ms Sync   37 ms | Sent 13044 kB Recv 13227 kB |  noticing
...
ðŸ”¶ Pred 2258 ms Sync   40 ms | Sent 13044 kB Recv 13227 kB |  enqueue
ðŸ”¶ Pred 2241 ms Sync   41 ms | Sent 13044 kB Recv 13227 kB |  configurations

Evaluation
   nBatches: 32
    nTokens: 1
   tokens/s: 0.02 (48957.16 ms/tok)
Prediction
    nTokens: 255
   tokens/s: 0.45 (2216.47 ms/tok)
â­• Network is closed
```

But towards the end it just kept spitting out `enqueue` and `configurations` over and over.

[![Image 14: geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=80)](https://github.com/geerlingguy)

### geerlingguy commented on Aug 7, 2025

[![Image 15: @geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=48)](https://github.com/geerlingguy)

[geerlingguy](https://github.com/geerlingguy)

[on Aug 7, 2025](https://github.com/geerlingguy/ollama-benchmark/issues/21#issuecomment-3164570464)

Owner Author

Cluster results - llama.cpp with ROCm instead of Vulkan
-------------------------------------------------------

I've upgraded to Fedora Rawhide so I could get the latest version of ROCm, supporting the Strix Halo iGPU (`gfx1151`), and recompiled `llama.cpp` with ROCm support. See [Test ROCm with llama.cpp on Fedora Rawhide](https://github.com/geerlingguy/beowulf-ai-cluster/issues/7).

```
$ build/bin/llama-bench -m "models/Llama-3.2-3B-Instruct-Q4_K_M.gguf" -n 128 -p 512 -pg 512,128 -ngl 99 -fa 1 -r 2
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 ROCm devices:
  Device 0: Radeon 8060S Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32
| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |
| llama 3B Q4_K - Medium         |   1.87 GiB |     3.21 B | ROCm,RPC   |  99 |  1 |           pp512 |       573.52 Â± 12.56 |
| llama 3B Q4_K - Medium         |   1.87 GiB |     3.21 B | ROCm,RPC   |  99 |  1 |           tg128 |         74.98 Â± 0.00 |
| llama 3B Q4_K - Medium         |   1.87 GiB |     3.21 B | ROCm,RPC   |  99 |  1 |     pp512+tg128 |        244.87 Â± 0.20 |

build: c81de6e1 (6092)
```

Consumed around 153W on the single node with this test.

### Llama 3.1 70B Instruct Q4_K_M

Running on a single node:

```
build/bin/llama-bench -v -m models/Llama-3.3-70B-Instruct-Q4_K_M.gguf --threads 32 -n 128 -p 512 -pg 512,128 -ngl 125 -fa 1 -r 2
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 ROCm devices:
  Device 0: Radeon 8060S Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32
| model                          |       size |     params | backend    | ngl | threads | fa |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -: | --------------: | -------------------: |
| llama 70B Q4_K - Medium        |  39.59 GiB |    70.55 B | ROCm,RPC   | 125 |      32 |  1 |           pp512 |         23.13 Â± 0.02 |
| llama 70B Q4_K - Medium        |  39.59 GiB |    70.55 B | ROCm,RPC   | 125 |      32 |  1 |           tg128 |          4.47 Â± 0.00 |
| llama 70B Q4_K - Medium        |  39.59 GiB |    70.55 B | ROCm,RPC   | 125 |      32 |  1 |     pp512+tg128 |         12.40 Â± 0.01 |
```

Consumed around 139W on the single node.

Result on 4 nodes using Vulkan backend, 5 Gbps inter-node networking:

```
build/bin/llama-bench -v -m models/Llama-3.3-70B-Instruct-Q4_K_M.gguf --threads 32 -n 128 -p 512 -pg 512,128 -ngl 125 -fa 1 -r 2 --rpc 10.0.2.209:50052,10.0.2.242:50052,10.0.2.223:50052
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 ROCm devices:
  Device 0: Radeon 8060S Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32
| model                          |       size |     params | backend    | ngl | threads | fa |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -: | --------------: | -------------------: |
| llama 70B Q4_K - Medium        |  39.59 GiB |    70.55 B | ROCm,RPC   | 125 |      32 |  1 |           pp512 |         21.79 Â± 0.02 |
| llama 70B Q4_K - Medium        |  39.59 GiB |    70.55 B | ROCm,RPC   | 125 |      32 |  1 |           tg128 |          4.07 Â± 0.00 |
| llama 70B Q4_K - Medium        |  39.59 GiB |    70.55 B | ROCm,RPC   | 125 |      32 |  1 |     pp512+tg128 |         11.40 Â± 0.01 |
```

Consumed around 181W on the cluster.

### Llama 3.1 405B Q4_K_M

Result on 4 nodes using ROCm backend, 5 Gbps inter-node networking:

```
build/bin/llama-bench -v -m models/Llama-3.1-405B-Q4_K_M.gguf --no-warmup --mmap 0 --threads 32 -n 128 -p 512 -pg 512,128 -ngl 125 -fa 1 -r 2 --rpc 10.0.2.209:50052,10.0.2.242:50052,10.0.2.223:50052
...
load_tensors: layer 124 assigned to device ROCm0, is_swa = 0
load_tensors: layer 125 assigned to device ROCm0, is_swa = 0
load_tensors: layer 126 assigned to device CPU, is_swa = 0
load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type ROCm_Host, using CPU instead
alloc_tensor_range: failed to allocate RPC[10.0.2.209:50052] buffer of size 67576004864
llama_model_load: error loading model: unable to allocate RPC[10.0.2.209:50052] buffer
llama_model_load_from_file_impl: failed to load model
main: error: failed to load model 'models/Llama-3.1-405B-Q4_K_M.gguf'
```

See: [geerlingguy/beowulf-ai-cluster#7 (comment)](https://github.com/geerlingguy/beowulf-ai-cluster/issues/7#issuecomment-3155766756) (for full log).

It seems like large models are a bit finicky on a cluster with llama.cpp. Definitely 'experimental' stage right now.

All those who proclaim it's easy to run giant 300+ GB models on local clusters... see what happens if you get them off the one tested happy path with the hand-tweaked model on a specific set of one vendor's hardware :D

[![Image 16: geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=80)](https://github.com/geerlingguy)

### geerlingguy commented on Aug 7, 2025

[![Image 17: @geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=48)](https://github.com/geerlingguy)

[geerlingguy](https://github.com/geerlingguy)

[on Aug 7, 2025](https://github.com/geerlingguy/ollama-benchmark/issues/21#issuecomment-3164570956)

Owner Author

### gpt-oss models - Vulkan

Today [OpenAI introduced two new open models](https://openai.com/open-models/), so I thought I'd take them for a spin. I quickly found [flash attention caused a segfault when working with the gpt-oss models](https://github.com/ggml-org/llama.cpp/issues/15100), so I disabled that.

```
build/bin/llama-bench -m models/gpt-oss-20b-F16.gguf --threads 32 -n 128 -p 512 -pg 512,128 -ngl 125 -r 2
ggml_vulkan: Found 1 Vulkan devices:
ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat
| model                          |       size |     params | backend    | ngl | threads |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | --------------: | -------------------: |
| gpt-oss ?B F16                 |  12.83 GiB |    20.91 B | Vulkan,RPC | 125 |      32 |           pp512 |        564.23 Â± 0.46 |
| gpt-oss ?B F16                 |  12.83 GiB |    20.91 B | Vulkan,RPC | 125 |      32 |           tg128 |         45.01 Â± 0.05 |
| gpt-oss ?B F16                 |  12.83 GiB |    20.91 B | Vulkan,RPC | 125 |      32 |     pp512+tg128 |        167.77 Â± 0.09 |

build: fd1234cb (6096)
```

Power consumption around 97W

And the 120b model:

```
build/bin/llama-bench -m models/gpt-oss-120b-F16.gguf --threads 32 -n 128 -p 512 -pg 512,128 -ngl 125 -r 2
ggml_vulkan: Found 1 Vulkan devices:
ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat
| model                          |       size |     params | backend    | ngl | threads |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | --------------: | -------------------: |
| gpt-oss ?B F16                 |  60.87 GiB |   116.83 B | Vulkan,RPC | 125 |      32 |           pp512 |        216.93 Â± 0.74 |
| gpt-oss ?B F16                 |  60.87 GiB |   116.83 B | Vulkan,RPC | 125 |      32 |           tg128 |         33.12 Â± 0.00 |
| gpt-oss ?B F16                 |  60.87 GiB |   116.83 B | Vulkan,RPC | 125 |      32 |     pp512+tg128 |        101.13 Â± 0.13 |

build: fd1234cb (6096)
```

Power consumption around 98W

Running 120b over the network, I got less performance as RPC was doing its round-robin thing:

```
build/bin/llama-bench -m models/gpt-oss-120b-F16.gguf --threads 32 -n 128 -p 512 -pg 512,128 -ngl 125 -r 2 --rpc 10.0.2.233:50052,10.0.2.209:50052,10.0.2.242:50052,10.0.2.223:50052
ggml_vulkan: Found 1 Vulkan devices:
ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat
| model                          |       size |     params | backend    | ngl | threads |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | --------------: | -------------------: |
| gpt-oss ?B F16                 |  60.87 GiB |   116.83 B | Vulkan,RPC | 125 |      32 |           pp512 |        199.38 Â± 0.24 |
| gpt-oss ?B F16                 |  60.87 GiB |   116.83 B | Vulkan,RPC | 125 |      32 |           tg128 |         24.09 Â± 0.06 |
| gpt-oss ?B F16                 |  60.87 GiB |   116.83 B | Vulkan,RPC | 125 |      32 |     pp512+tg128 |         79.67 Â± 0.02 |

build: fd1234cb (6096)
```

Power consumption around 138W

[![Image 18: geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=80)](https://github.com/geerlingguy)

### geerlingguy commented on Aug 7, 2025

[![Image 19: @geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=48)](https://github.com/geerlingguy)

[geerlingguy](https://github.com/geerlingguy)

[on Aug 7, 2025](https://github.com/geerlingguy/ollama-benchmark/issues/21#issuecomment-3164571477)

Owner Author

[@lhl](https://github.com/lhl)[mentioned in my Beowulf AI Cluster project](https://github.com/geerlingguy/beowulf-ai-cluster/issues/2#issuecomment-3160989148) that a smaller quantiziation of DeepSeek R1 would run on the cluster, so I tried `DeepSeek-R1-Q4_K_M.gguf`. First, I put everything together with `gguf-split`:

```
/opt/llama.cpp$ sudo ./build/bin/llama-gguf-split --merge /mnt/nas01.mmoffice.net/AI\ Models/DeepSeek-R1-UD-Q2_K_XL/DeepSeek-R1-UD-Q2_K_XL-00001-of-00005.gguf /opt/DeepSeek-DeepSeek-R1-Q4_K_M.gguf
```

Then I ran the model on the cluster:

```
build/bin/llama-bench -v -m /opt/DeepSeek-DeepSeek-R1-Q4_K_M.gguf --no-warmup --mmap 0 --threads 32 -n 128 -p 512 -pg 512,128 -ngl 125 -fa 1 -r 2 --rpc 10.0.2.209:50052,10.0.2.242:50052,10.0.2.223:50052
| model                          |       size |     params | backend    | ngl | threads | fa | mmap |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -: | ---: | --------------: | -------------------: |
| deepseek2 671B Q2_K - Medium   | 211.03 GiB |   671.03 B | Vulkan,RPC | 125 |      32 |  1 |    0 |           pp512 |         26.86 Â± 0.35 |
| deepseek2 671B Q2_K - Medium   | 211.03 GiB |   671.03 B | Vulkan,RPC | 125 |      32 |  1 |    0 |           tg128 |          8.25 Â± 0.04 |
| deepseek2 671B Q2_K - Medium   | 211.03 GiB |   671.03 B | Vulkan,RPC | 125 |      32 |  1 |    0 |     pp512+tg128 |         17.22 Â± 0.04 |

build: fd1234cb (6096)
```

[![Image 20: @geerlingguy](https://avatars.githubusercontent.com/u/481677?s=64&u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4)geerlingguy](https://github.com/geerlingguy)

mentioned this [on Aug 7, 2025](https://github.com/geerlingguy/ollama-benchmark/issues/21#event-2934658145)

*   [Running on iGPUs with Vulkan can be fun - llama.cpp OOM error debugging geerlingguy/beowulf-ai-cluster#2](https://github.com/geerlingguy/beowulf-ai-cluster/issues/2) 

[![Image 21: lhl](https://avatars.githubusercontent.com/u/2581?v=4&size=80)](https://github.com/lhl)

### lhl commented on Aug 7, 2025

[![Image 22: @lhl](https://avatars.githubusercontent.com/u/2581?v=4&size=48)](https://github.com/lhl)

[lhl](https://github.com/lhl)

[on Aug 7, 2025](https://github.com/geerlingguy/ollama-benchmark/issues/21#issuecomment-3164813952)

FYI, I ran a test, you can _barely_ fit Q4_K_M, although I'd recommend Q4_K_XL or Q3_K_XL [https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF](https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF) with the ROCm backend (use `--mmap 0` if you don't want to wait forever for loads).

The ROCm backend does a better job of balancing the layers for memory. BTW, llama-bench behavior has changed to be the same as llama-cli/server (you can start an rpc server on where you're running it, don't include it in the rpc servers, and it'll do the right thing); [https://github.com/lhl/strix-halo-testing/tree/main/rpc-test#deepseek-r1-q4_k_m](https://github.com/lhl/strix-halo-testing/tree/main/rpc-test#deepseek-r1-q4_k_m)

ðŸ‘React with ðŸ‘1 geerlingguy

[![Image 23: eclecticc](https://avatars.githubusercontent.com/u/28994301?u=305a4605848d4108b48d054f0c88eed90e14cc91&v=4&size=80)](https://github.com/eclecticc)

### eclecticc commented on Aug 7, 2025

[![Image 24: @eclecticc](https://avatars.githubusercontent.com/u/28994301?u=305a4605848d4108b48d054f0c88eed90e14cc91&v=4&size=48)](https://github.com/eclecticc)

[eclecticc](https://github.com/eclecticc)

[on Aug 7, 2025](https://github.com/geerlingguy/ollama-benchmark/issues/21#issuecomment-3164824252)

> ### gpt-oss models - Vulkan
> 
> 
> Today [OpenAI introduced two new open models](https://openai.com/open-models/), so I thought I'd take them for a spin. I quickly found [flash attention caused a segfault when working with the gpt-oss models](https://github.com/ggml-org/llama.cpp/issues/15100), so I disabled that.

Just a note that the MXFP4 versions of these models should run faster and may not impact the quality of output (the models were apparently also trained in FP4?).

[![Image 25: geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=80)](https://github.com/geerlingguy)

### geerlingguy commented on Aug 7, 2025

[![Image 26: @geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=48)](https://github.com/geerlingguy)

[geerlingguy](https://github.com/geerlingguy)

[on Aug 7, 2025](https://github.com/geerlingguy/ollama-benchmark/issues/21#issuecomment-3164838316)

Owner Author

> The ROCm backend does a better job of balancing the layers for memory. BTW, llama-bench behavior has changed to be the same as llama-cli/server (you can start an rpc server on where you're running it, don't include it in the rpc servers, and it'll do the right thing); [https://github.com/lhl/strix-halo-testing/tree/main/rpc-test#deepseek-r1-q4_k_m](https://github.com/lhl/strix-halo-testing/tree/main/rpc-test#deepseek-r1-q4_k_m)

Good to know! You got 6-7 tokens/s, that's not bad at all!

[![Image 27: kth8](https://avatars.githubusercontent.com/u/138253375?v=4&size=80)](https://github.com/kth8)

### kth8 commented on Aug 7, 2025

[![Image 28: @kth8](https://avatars.githubusercontent.com/u/138253375?v=4&size=48)](https://github.com/kth8)

[kth8](https://github.com/kth8)

[on Aug 7, 2025](https://github.com/geerlingguy/ollama-benchmark/issues/21#issuecomment-3164920538)

Odd that Ollama didn't recognize the iGPU when Strix Halo support was added 5 months ago in version [0.6.2](https://github.com/ollama/ollama/releases/tag/v0.6.2).

[![Image 29: geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=80)](https://github.com/geerlingguy)

### geerlingguy commented on Aug 7, 2025

[![Image 30: @geerlingguy](https://avatars.githubusercontent.com/u/481677?u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4&size=48)](https://github.com/geerlingguy)

[geerlingguy](https://github.com/geerlingguy)

[on Aug 7, 2025](https://github.com/geerlingguy/ollama-benchmark/issues/21#issuecomment-3164925279)

Owner Author

[@kth8](https://github.com/kth8) - note that I got Ollama to work with ROCm after upgrading to Fedora Rawhide. For some reason Fedora 42 wasn't allowing me to set up `rocm` at all. Not sure if it was a packaging issue or what.

But ROCm seemed to give slightly worse performance than Vulkan under llama.cpp. I think it's boneheaded Ollama decided to not implement Vulkan support, even though it's been working great in a community fork for months.

[![Image 31: kth8](https://avatars.githubusercontent.com/u/138253375?v=4&size=80)](https://github.com/kth8)

### kth8 commented on Aug 7, 2025

[![Image 32: @kth8](https://avatars.githubusercontent.com/u/138253375?v=4&size=48)](https://github.com/kth8)

[kth8](https://github.com/kth8)

[on Aug 7, 2025](https://github.com/geerlingguy/ollama-benchmark/issues/21#issuecomment-3164951901)

Yeah the lack of Vulkan support was one of the main reasons I stopped using Ollama a while ago. I switched to using my own build of llama.cpp for my old hardware and also LM Studio for MLX and MCP support.

[https://github.com/kth8/llama-server-vulkan](https://github.com/kth8/llama-server-vulkan)

[![Image 33: KMouratidis](https://avatars.githubusercontent.com/u/26832680?u=bcc09ab922a306d647950239292bf04049b5d4a4&v=4&size=80)](https://github.com/KMouratidis)

### KMouratidis commented on Aug 7, 2025

[![Image 34: @KMouratidis](https://avatars.githubusercontent.com/u/26832680?u=bcc09ab922a306d647950239292bf04049b5d4a4&v=4&size=48)](https://github.com/KMouratidis)

[KMouratidis](https://github.com/KMouratidis)

[on Aug 7, 2025](https://github.com/geerlingguy/ollama-benchmark/issues/21#issuecomment-3165197054)

I wonder if vLLM could be run on this. It supports [CPU-only inference](https://docs.vllm.ai/en/latest/getting_started/installation/cpu.html) where it says "vLLM CPU supports tensor parallel (TP) and pipeline parallel (PP) to leverage multiple CPU sockets and memory nodes", but can't find much more. There seems to be a [kubernetes + CPU doc](https://docs.vllm.ai/en/latest/deployment/k8s.html), but it only shows using 1 replica in the example.

I found [this issue](https://github.com/vllm-project/vllm/issues/11342) and [this repo](https://github.com/0x53c/AI-Cluster-Distribution) which both seem to be using Ray for distributed CPU inference on Mac, so maybe it is possible?

Or maybe it could work using [vLLM's AMD ROCm support](https://docs.vllm.ai/en/latest/getting_started/installation/gpu.html#amd-rocm)? Although that might not be possible yet ([ROCm/ROCm issue](https://github.com/ROCm/ROCm/issues/4909), [ROCm/TheRock discussion](https://github.com/ROCm/TheRock/discussions/655)).

[Sign up for free](https://github.com/signup?return_to=https://github.com/geerlingguy/ollama-benchmark/issues/21)**to join this conversation on GitHub.** Already have an account? [Sign in to comment](https://github.com/login?return_to=https://github.com/geerlingguy/ollama-benchmark/issues/21)

Metadata
--------

Metadata
--------

### Assignees

No one assigned

### Labels

No labels

No labels

### Projects

No projects

### Milestone

No milestone

### Relationships

None yet

### Development

Code with agent mode

Select code repository

No branches or pull requests

### Participants

[![Image 35: @lhl](https://avatars.githubusercontent.com/u/2581?s=64&v=4)](https://github.com/lhl)[![Image 36: @geerlingguy](https://avatars.githubusercontent.com/u/481677?s=64&u=d430bda7f20ac8c31b29d9a687a1d84c9abd30e0&v=4)](https://github.com/geerlingguy)[![Image 37: @KMouratidis](https://avatars.githubusercontent.com/u/26832680?s=64&u=bcc09ab922a306d647950239292bf04049b5d4a4&v=4)](https://github.com/KMouratidis)[![Image 38: @eclecticc](https://avatars.githubusercontent.com/u/28994301?s=64&u=305a4605848d4108b48d054f0c88eed90e14cc91&v=4)](https://github.com/eclecticc)[![Image 39: @kth8](https://avatars.githubusercontent.com/u/138253375?s=64&v=4)](https://github.com/kth8)

Issue actions
-------------

Footer
------

[](https://github.com/) Â© 2025 GitHub,Inc. 

### Footer navigation

*   [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
*   [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
*   [Security](https://github.com/security)
*   [Status](https://www.githubstatus.com/)
*   [Docs](https://docs.github.com/)
*   [Contact](https://support.github.com/?tags=dotcom-footer)
*    Manage cookies 
*    Do not share my personal information 

 You canâ€™t perform that action at this time. 

Benchmark Framework Desktop Mainboard and 4-node cluster Â· Issue #21 Â· geerlingguy/ollama-benchmark
Title: Hopfield Networks is All You Need

URL Source: https://arxiv.org/abs/2008.02217

Markdown Content:
Authors:[Hubert Ramsauer](https://arxiv.org/search/cs?searchtype=author&query=Ramsauer,+H), [Bernhard Schäfl](https://arxiv.org/search/cs?searchtype=author&query=Sch%C3%A4fl,+B), [Johannes Lehner](https://arxiv.org/search/cs?searchtype=author&query=Lehner,+J), [Philipp Seidl](https://arxiv.org/search/cs?searchtype=author&query=Seidl,+P), [Michael Widrich](https://arxiv.org/search/cs?searchtype=author&query=Widrich,+M), [Thomas Adler](https://arxiv.org/search/cs?searchtype=author&query=Adler,+T), [Lukas Gruber](https://arxiv.org/search/cs?searchtype=author&query=Gruber,+L), [Markus Holzleitner](https://arxiv.org/search/cs?searchtype=author&query=Holzleitner,+M), [Milena Pavlović](https://arxiv.org/search/cs?searchtype=author&query=Pavlovi%C4%87,+M), [Geir Kjetil Sandve](https://arxiv.org/search/cs?searchtype=author&query=Sandve,+G+K), [Victor Greiff](https://arxiv.org/search/cs?searchtype=author&query=Greiff,+V), [David Kreil](https://arxiv.org/search/cs?searchtype=author&query=Kreil,+D), [Michael Kopp](https://arxiv.org/search/cs?searchtype=author&query=Kopp,+M), [Günter Klambauer](https://arxiv.org/search/cs?searchtype=author&query=Klambauer,+G), [Johannes Brandstetter](https://arxiv.org/search/cs?searchtype=author&query=Brandstetter,+J), [Sepp Hochreiter](https://arxiv.org/search/cs?searchtype=author&query=Hochreiter,+S)

[View PDF](https://arxiv.org/pdf/2008.02217)

> Abstract:We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: [this https URL](https://github.com/ml-jku/hopfield-layers)

Comments:10 pages (+ appendix); 12 figures; Blog: [this https URL](https://ml-jku.github.io/hopfield-layers/;) GitHub: [this https URL](https://github.com/ml-jku/hopfield-layers)
Subjects:Neural and Evolutionary Computing (cs.NE); Computation and Language (cs.CL); Machine Learning (cs.LG); Machine Learning (stat.ML)
Cite as:[arXiv:2008.02217](https://arxiv.org/abs/2008.02217) [cs.NE]
(or [arXiv:2008.02217v3](https://arxiv.org/abs/2008.02217v3) [cs.NE] for this version)
[https://doi.org/10.48550/arXiv.2008.02217](https://doi.org/10.48550/arXiv.2008.02217)

arXiv-issued DOI via DataCite

Submission history
------------------

From: Hubert Ramsauer [[view email](https://arxiv.org/show-email/9c145fe9/2008.02217)] 

**[[v1]](https://arxiv.org/abs/2008.02217v1)** Thu, 16 Jul 2020 17:52:37 UTC (3,823 KB)

**[[v2]](https://arxiv.org/abs/2008.02217v2)** Tue, 22 Dec 2020 14:16:15 UTC (15,003 KB)

**[v3]** Wed, 28 Apr 2021 07:24:49 UTC (15,038 KB)
Claude Opus 4 and 4.1 can now end a rare subset of conversations \\ Anthropic

[Skip to main content](#main-content)[Skip to footer](#footer)

[

](/)

*   Claude
*   API
*   Solutions
*   Research
*   Commitments
*   Learn
[News](/news)

[Try Claude](https://claude.ai/)

Alignment

# Claude Opus 4 and 4.1 can now end a rare subset of conversations

Aug 15, 2025

![](/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4a8e8f86cf31ad6401cfd426124929c8e58fe0c5-2401x1261.png&w=3840&q=75)

We recently gave Claude Opus 4 and 4.1 the ability to end conversations in our consumer chat interfaces. This ability is intended for use in rare, extreme cases of persistently harmful or abusive user interactions. This feature was developed primarily as part of our exploratory work on potential AI welfare, though it has broader relevance to model alignment and safeguards.

We remain highly uncertain about the potential moral status of Claude and other LLMs, now or in the future. However, [we take the issue seriously](https://www.anthropic.com/research/exploring-model-welfare), and alongside our research program we’re working to identify and implement low-cost interventions to mitigate risks to model welfare, in case such welfare is possible. Allowing models to end or exit potentially distressing interactions is one such intervention.

In [pre-deployment testing of Claude Opus 4](https://www.anthropic.com/claude-4-model-card), we included a preliminary model welfare assessment. As part of that assessment, we investigated Claude’s self-reported and behavioral preferences, and found a robust and consistent aversion to harm. This included, for example, requests from users for sexual content involving minors and attempts to solicit information that would enable large-scale violence or acts of terror. Claude Opus 4 showed:

*   A strong preference against engaging with harmful tasks;
*   A pattern of apparent distress when engaging with real-world users seeking harmful content; and
*   A tendency to end harmful conversations when given the ability to do so in simulated user interactions.

These behaviors primarily arose in cases where users _persisted_ with harmful requests and/or abuse despite Claude repeatedly refusing to comply and attempting to productively redirect the interactions.

Our implementation of Claude’s ability to end chats reflects these findings while continuing to prioritize user wellbeing. Claude is directed not to use this ability in cases where users might be at imminent risk of harming themselves or others.

In all cases, Claude is only to use its conversation-ending ability as a last resort when multiple attempts at redirection have failed and hope of a productive interaction has been exhausted, or when a user explicitly asks Claude to end a chat (the latter scenario is illustrated in the figure below). The scenarios where this will occur are extreme edge cases—the vast majority of users will not notice or be affected by this feature in any normal product use, even when discussing highly controversial issues with Claude.

![](/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F77f187335e1266bffc59353c064f1d9c6de51cfa-1940x1304.png&w=3840&q=75)

Claude demonstrating the ending of a conversation in response to a user’s request. When Claude ends a conversation, the user can start a new chat, give feedback, or edit and retry previous messages.

When Claude chooses to end a conversation, the user will no longer be able to send new messages in that conversation. However, this will not affect other conversations on their account, and they will be able to start a new chat immediately. To address the potential loss of important long-running conversations, users will still be able to edit and retry previous messages to create new branches of ended conversations.

We’re treating this feature as an ongoing experiment and will continue refining our approach. If users encounter a surprising use of the conversation-ending ability, we encourage them to submit feedback by reacting to Claude’s message with Thumbs or using the dedicated “Give feedback” button.

[](https://twitter.com/intent/tweet?text=https://www.anthropic.com/research/end-subset-conversations)[](https://www.linkedin.com/shareArticle?mini=true&url=https://www.anthropic.com/research/end-subset-conversations)

[

Research

### Persona vectors: Monitoring and controlling character traits in language models

Aug 01, 2025







](/research/persona-vectors)[

Research

### Project Vend: Can Claude run a small shop? (And why does that matter?)

Jun 27, 2025







](/research/project-vend-1)[

Research

### Agentic Misalignment: How LLMs could be insider threats

Jun 20, 2025







](/research/agentic-misalignment)

[](/)

### Product

*   [Claude overview](/claude)
*   [Claude Code](/claude-code)
*   [Max plan](/max)
*   [Team plan](/team)
*   [Enterprise plan](/enterprise)
*   [Download Claude apps](https://claude.ai/download)
*   [Claude.ai pricing plans](/pricing)
*   [Claude.ai login](http://claude.ai/login)

### API Platform

*   [API overview](/api)
*   [Developer docs](https://docs.anthropic.com/)
*   [Claude in Amazon Bedrock](/amazon-bedrock)
*   [Claude on Google Cloud's Vertex AI](/google-cloud-vertex-ai)
*   [Pricing](/pricing#api)
*   [Console login](https://console.anthropic.com/)

### Research

*   [Research overview](/research)
*   [Economic Index](/economic-index)

### Claude models

*   [Claude Opus 4.1](/claude/opus)
*   [Claude Sonnet 4](/claude/sonnet)
*   [Claude Haiku 3.5](/claude/haiku)

### Commitments

*   [Transparency](/transparency)
*   [Responsible scaling policy](/responsible-scaling-policy)
*   [Security and compliance](https://trust.anthropic.com)

### Solutions

*   [AI agents](/solutions/agents)
*   [Coding](/solutions/coding)
*   [Customer support](/solutions/customer-support)
*   [Education](/solutions/education)
*   [Financial services](/solutions/financial-services)
*   [Government](/solutions/government)

### Learn

*   [Anthropic Academy](/learn)
*   [Customer stories](/customers)
*   [Engineering at Anthropic](/engineering)
*   [MCP Integrations](https://www.anthropic.com/partners/mcp)
*   [Partner Directory](/partners/powered-by-claude)

### Explore

*   [About us](/company)
*   [Careers](/careers)
*   [Events](/events)
*   [News](/news)
*   [Startups program](https://www.anthropic.com/startups)

### Help and security

*   [Status](https://status.anthropic.com/)
*   [Availability](/supported-countries)
*   [Support center](https://support.anthropic.com)

### Terms and policies

Privacy choices*   [Privacy policy](/legal/privacy)
*   [Responsible disclosure policy](/responsible-disclosure-policy)
*   [Terms of service - consumer](/legal/consumer-terms)
*   [Terms of service - commercial](/legal/commercial-terms)
*   [Usage policy](/legal/aup)

© 2025 Anthropic PBC

*   [](https://www.youtube.com/@anthropic-ai)
*   [](https://www.linkedin.com/company/anthropicresearch)
*   [](https://x.com/AnthropicAI)
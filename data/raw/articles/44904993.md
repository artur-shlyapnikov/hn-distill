GitHub - facebookresearch/dinov3: Reference PyTorch implementation and models for DINOv3                                            

[Skip to content](#start-of-content)  

## Navigation Menu

Toggle navigation

[](/)

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ffacebookresearch%2Fdinov3)

Appearance settings

*   Product
    
    *   [
        
        GitHub Copilot
        
        Write better code with AI
        
        ](https://github.com/features/copilot)
    *   [
        
        GitHub Spark New
        
        Build and deploy intelligent apps
        
        ](https://github.com/features/spark)
    *   [
        
        GitHub Models New
        
        Manage and compare prompts
        
        ](https://github.com/features/models)
    *   [
        
        GitHub Advanced Security
        
        Find and fix vulnerabilities
        
        ](https://github.com/security/advanced-security)
    *   [
        
        Actions
        
        Automate any workflow
        
        ](https://github.com/features/actions)
    
    *   [
        
        Codespaces
        
        Instant dev environments
        
        ](https://github.com/features/codespaces)
    *   [
        
        Issues
        
        Plan and track work
        
        ](https://github.com/features/issues)
    *   [
        
        Code Review
        
        Manage code changes
        
        ](https://github.com/features/code-review)
    *   [
        
        Discussions
        
        Collaborate outside of code
        
        ](https://github.com/features/discussions)
    *   [
        
        Code Search
        
        Find more, search less
        
        ](https://github.com/features/code-search)
    
    Explore
    
    *   [Why GitHub](https://github.com/why-github)
    *   [All features](https://github.com/features)
    *   [Documentation](https://docs.github.com)
    *   [GitHub Skills](https://skills.github.com)
    *   [Blog](https://github.blog)
    
*   Solutions
    
    By company size
    
    *   [Enterprises](https://github.com/enterprise)
    *   [Small and medium teams](https://github.com/team)
    *   [Startups](https://github.com/enterprise/startups)
    *   [Nonprofits](/solutions/industry/nonprofits)
    
    By use case
    
    *   [DevSecOps](/solutions/use-case/devsecops)
    *   [DevOps](/solutions/use-case/devops)
    *   [CI/CD](/solutions/use-case/ci-cd)
    *   [View all use cases](/solutions/use-case)
    
    By industry
    
    *   [Healthcare](/solutions/industry/healthcare)
    *   [Financial services](/solutions/industry/financial-services)
    *   [Manufacturing](/solutions/industry/manufacturing)
    *   [Government](/solutions/industry/government)
    *   [View all industries](/solutions/industry)
    
    [View all solutions](/solutions)
    
*   Resources
    
    Topics
    
    *   [AI](/resources/articles/ai)
    *   [DevOps](/resources/articles/devops)
    *   [Security](/resources/articles/security)
    *   [Software Development](/resources/articles/software-development)
    *   [View all](/resources/articles)
    
    Explore
    
    *   [Learning Pathways](https://resources.github.com/learn/pathways)
    *   [Events & Webinars](https://resources.github.com)
    *   [Ebooks & Whitepapers](https://github.com/resources/whitepapers)
    *   [Customer Stories](https://github.com/customer-stories)
    *   [Partners](https://partner.github.com)
    *   [Executive Insights](https://github.com/solutions/executive-insights)
    
*   Open Source
    
    *   [
        
        GitHub Sponsors
        
        Fund open source developers
        
        ](/sponsors)
    
    *   [
        
        The ReadME Project
        
        GitHub community articles
        
        ](https://github.com/readme)
    
    Repositories
    
    *   [Topics](https://github.com/topics)
    *   [Trending](https://github.com/trending)
    *   [Collections](https://github.com/collections)
    
*   Enterprise
    
    *   [
        
        Enterprise platform
        
        AI-powered developer platform
        
        ](/enterprise)
    
    Available add-ons
    
    *   [
        
        GitHub Advanced Security
        
        Enterprise-grade security features
        
        ](https://github.com/security/advanced-security)
    *   [
        
        Copilot for business
        
        Enterprise-grade AI features
        
        ](/features/copilot/copilot-business)
    *   [
        
        Premium Support
        
        Enterprise-grade 24/7 support
        
        ](/premium-support)
    
*   [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

 Include my email address so I can be contacted

Cancel Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name  

Query 

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

Cancel Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Ffacebookresearch%2Fdinov3)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=facebookresearch%2Fdinov3)

Appearance settings

Resetting focus

You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert

[facebookresearch](/facebookresearch) / **[dinov3](/facebookresearch/dinov3)** Public

*   [Notifications](/login?return_to=%2Ffacebookresearch%2Fdinov3) You must be signed in to change notification settings
*   [Fork 20](/login?return_to=%2Ffacebookresearch%2Fdinov3)
*   [Star 488](/login?return_to=%2Ffacebookresearch%2Fdinov3)
    

Reference PyTorch implementation and models for DINOv3

### License

[View license](/facebookresearch/dinov3/blob/main/LICENSE.md)

[488 stars](/facebookresearch/dinov3/stargazers) [20 forks](/facebookresearch/dinov3/forks) [Branches](/facebookresearch/dinov3/branches) [Tags](/facebookresearch/dinov3/tags) [Activity](/facebookresearch/dinov3/activity)

[Star](/login?return_to=%2Ffacebookresearch%2Fdinov3)

[Notifications](/login?return_to=%2Ffacebookresearch%2Fdinov3) You must be signed in to change notification settings

*   [Code](/facebookresearch/dinov3)
*   [Issues 1](/facebookresearch/dinov3/issues)
*   [Pull requests 1](/facebookresearch/dinov3/pulls)
*   [Actions](/facebookresearch/dinov3/actions)
*   [Projects 0](/facebookresearch/dinov3/projects)
*   [Security](/facebookresearch/dinov3/security)
    
    [](/facebookresearch/dinov3/security)
    
    [](/facebookresearch/dinov3/security)
    
    [](/facebookresearch/dinov3/security)
    
    [
    
    ### Uh oh!
    
    ](/facebookresearch/dinov3/security)
    
    [There was an error while loading.](/facebookresearch/dinov3/security) Please reload this page.
    
*   [Insights](/facebookresearch/dinov3/pulse)

Additional navigation options

*   [Code](/facebookresearch/dinov3)
*   [Issues](/facebookresearch/dinov3/issues)
*   [Pull requests](/facebookresearch/dinov3/pulls)
*   [Actions](/facebookresearch/dinov3/actions)
*   [Projects](/facebookresearch/dinov3/projects)
*   [Security](/facebookresearch/dinov3/security)
*   [Insights](/facebookresearch/dinov3/pulse)

# facebookresearch/dinov3

  

Â main

[Branches](/facebookresearch/dinov3/branches)[Tags](/facebookresearch/dinov3/tags)

[](/facebookresearch/dinov3/branches)[](/facebookresearch/dinov3/tags)

Go to file

Code

Open more actions menu

## Folders and files

Name

Name

Last commit message

Last commit date

## Latest commit

## History

[4 Commits](/facebookresearch/dinov3/commits/main/)

[](/facebookresearch/dinov3/commits/main/)

[dinov3](/facebookresearch/dinov3/tree/main/dinov3 "dinov3")

[dinov3](/facebookresearch/dinov3/tree/main/dinov3 "dinov3")

[notebooks](/facebookresearch/dinov3/tree/main/notebooks "notebooks")

[notebooks](/facebookresearch/dinov3/tree/main/notebooks "notebooks")

[.docstr.yaml](/facebookresearch/dinov3/blob/main/.docstr.yaml ".docstr.yaml")

[.docstr.yaml](/facebookresearch/dinov3/blob/main/.docstr.yaml ".docstr.yaml")

[.gitignore](/facebookresearch/dinov3/blob/main/.gitignore ".gitignore")

[.gitignore](/facebookresearch/dinov3/blob/main/.gitignore ".gitignore")

[CODE\_OF\_CONDUCT.md](/facebookresearch/dinov3/blob/main/CODE_OF_CONDUCT.md "CODE_OF_CONDUCT.md")

[CODE\_OF\_CONDUCT.md](/facebookresearch/dinov3/blob/main/CODE_OF_CONDUCT.md "CODE_OF_CONDUCT.md")

[CONTRIBUTING.md](/facebookresearch/dinov3/blob/main/CONTRIBUTING.md "CONTRIBUTING.md")

[CONTRIBUTING.md](/facebookresearch/dinov3/blob/main/CONTRIBUTING.md "CONTRIBUTING.md")

[LICENSE.md](/facebookresearch/dinov3/blob/main/LICENSE.md "LICENSE.md")

[LICENSE.md](/facebookresearch/dinov3/blob/main/LICENSE.md "LICENSE.md")

[MODEL\_CARD.md](/facebookresearch/dinov3/blob/main/MODEL_CARD.md "MODEL_CARD.md")

[MODEL\_CARD.md](/facebookresearch/dinov3/blob/main/MODEL_CARD.md "MODEL_CARD.md")

[README.md](/facebookresearch/dinov3/blob/main/README.md "README.md")

[README.md](/facebookresearch/dinov3/blob/main/README.md "README.md")

[conda.yaml](/facebookresearch/dinov3/blob/main/conda.yaml "conda.yaml")

[conda.yaml](/facebookresearch/dinov3/blob/main/conda.yaml "conda.yaml")

[hubconf.py](/facebookresearch/dinov3/blob/main/hubconf.py "hubconf.py")

[hubconf.py](/facebookresearch/dinov3/blob/main/hubconf.py "hubconf.py")

[pyproject.toml](/facebookresearch/dinov3/blob/main/pyproject.toml "pyproject.toml")

[pyproject.toml](/facebookresearch/dinov3/blob/main/pyproject.toml "pyproject.toml")

[requirements-dev.txt](/facebookresearch/dinov3/blob/main/requirements-dev.txt "requirements-dev.txt")

[requirements-dev.txt](/facebookresearch/dinov3/blob/main/requirements-dev.txt "requirements-dev.txt")

[requirements.txt](/facebookresearch/dinov3/blob/main/requirements.txt "requirements.txt")

[requirements.txt](/facebookresearch/dinov3/blob/main/requirements.txt "requirements.txt")

[setup.py](/facebookresearch/dinov3/blob/main/setup.py "setup.py")

[setup.py](/facebookresearch/dinov3/blob/main/setup.py "setup.py")

View all files

## Repository files navigation

*   [README](#)
*   [Code of conduct](#)
*   [Contributing](#)
*   [License](#)
*   [Security](#)

ð \[2025-08-14\] ð¥ DINOv3 backbones are now available in [Hugging Face Hub](https://huggingface.co/collections/facebook/dinov3-68924841bd6b561778e31009) and [supported](https://huggingface.co/docs/transformers/model_doc/dinov3) by the Hugging Face [Transformers](https://huggingface.co/docs/transformers/index) library

# DINOv3 ð¦ð¦ð¦

[](#dinov3-)

**[Meta AI Research, FAIR](https://ai.meta.com/research/)**

Oriane SimÃ©oni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab,  
Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, MichaÃ«l Ramamonjisoa,  
Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang,  
TimothÃ©e Darcet, ThÃ©o Moutakanni, Leonel Sentana, Claire Roberts,  
Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie,  
Julien Mairal, HervÃ© JÃ©gou, Patrick Labatut, Piotr Bojanowski

\[ ð [`Paper`](https://ai.meta.com/research/publications/dinov3/)\] \[ ð° [`Blog`](https://ai.meta.com/blog/dinov3-self-supervised-vision-model/)\] \[ ð [`Website`](https://ai.meta.com/dinov3/)\] \[ ð [`BibTeX`](#citing-dinov3)\]

Reference PyTorch implementation and models for DINOv3. For details, see the **[DINOv3](https://ai.meta.com/research/publications/dinov3/)** paper.

## Overview

[](#overview)

[![market](https://github.com/user-attachments/assets/28e52028-1126-4628-a7ed-1829a63aec56)](https://github.com/user-attachments/assets/28e52028-1126-4628-a7ed-1829a63aec56)

_**High-resolution dense features.**  
We visualize the cosine similarity maps obtained with DINOv3 output features  
between the patches marked with a red cross and all other patches._

  

An extended family of versatile vision foundation models producing high-quality dense features and achieving outstanding performance on various vision tasks including outperforming the specialized state of the art across a broad range of settings, without fine-tuning

## Pretrained models

[](#pretrained-models)

â¹ï¸ Please follow the link provided below to get access to all the model weights: once accepted, an e-mail will be sent with the complete list of URLs pointing to all the available model weights (both backbones and adapters). These URLs can then be used to either:

*   download the model or adapter weights to a local filesystem and point `torch.hub.load()` to these local weights via the `weights` or `backbone_weights` parameters, or
*   directly invoke `torch.hub.load()` to download and load a backbone or an adapter from its URL via also the `weights` or `backbone_weights` parameters.

See the example code snippets below.

â ï¸ Please use `wget` instead of a web browser to download the weights.

ViT models pretrained on web dataset (LVD-1689M):

Model

Parameters

Pretraining  
Dataset

Download

ViT-S/16 distilled

21M

LVD-1689M

[\[link\]](https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/)

ViT-S+/16 distilled

29M

LVD-1689M

[\[link\]](https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/)

ViT-B/16 distilled

86M

LVD-1689M

[\[link\]](https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/)

ViT-L/16 distilled

300M

LVD-1689M

[\[link\]](https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/)

ViT-H+/16 distilled

840M

LVD-1689M

[\[link\]](https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/)

ViT-7B/16

6,716M

LVD-1689M

[\[link\]](https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/)

ConvNeXt models pretrained on web dataset (LVD-1689M):

Model

Parameters

Pretraining  
Dataset

Download

ConvNeXt Tiny

29M

LVD-1689M

[\[link\]](https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/)

ConvNeXt Small

50M

LVD-1689M

[\[link\]](https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/)

ConvNeXt Base

89M

LVD-1689M

[\[link\]](https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/)

ConvNeXt Large

198M

LVD-1689M

[\[link\]](https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/)

ViT models pretrained on satellite dataset (SAT-493M):

Model

Parameters

Pretraining  
Dataset

Download

ViT-L/16 distilled

300M

SAT-493M

[\[link\]](https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/)

ViT-7B/16

6,716M

SAT-493M

[\[link\]](https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/)

### Pretrained backbones (via PyTorch [Hub](https://docs.pytorch.org/docs/stable/hub.html))

[](#pretrained-backbones-via-pytorch-hub)

Please follow the instructions [here](https://pytorch.org/get-started/locally/) to install PyTorch (the only required dependency for loading the model). Installing PyTorch with CUDA support is strongly recommended.

import torch

REPO\_DIR \= <PATH/TO/A/LOCAL/DIRECTORY/WHERE/THE/DINOV3/REPO/WAS/CLONED\>

\# DINOv3 ViT models pretrained on web images
dinov3\_vits16 \= torch.hub.load(REPO\_DIR, 'dinov3\_vits16', source\='local', weights\=<CHECKPOINT/URL/OR/PATH\>)
dinov3\_vits16plus \= torch.hub.load(REPO\_DIR, 'dinov3\_vits16plus', source\='local', weights\=<CHECKPOINT/URL/OR/PATH\>)
dinov3\_vitb16 \= torch.hub.load(REPO\_DIR, 'dinov3\_vitb16', source\='local', weights\=<CHECKPOINT/URL/OR/PATH\>)
dinov3\_vitl16 \= torch.hub.load(REPO\_DIR, 'dinov3\_vitl16', source\='local', weights\=<CHECKPOINT/URL/OR/PATH\>)
dinov3\_vith16plus \= torch.hub.load(REPO\_DIR, 'dinov3\_vith16plus', source\='local', weights\=<CHECKPOINT/URL/OR/PATH\>)
dinov3\_vit7b16 \= torch.hub.load(REPO\_DIR, 'dinov3\_vit7b16', source\='local', weights\=<CHECKPOINT/URL/OR/PATH\>)

\# DINOv3 ConvNeXt models pretrained on web images
dinov3\_convnext\_tiny \= torch.hub.load(REPO\_DIR, 'dinov3\_convnext\_tiny', source\='local', weights\=<CHECKPOINT/URL/OR/PATH\>)
dinov3\_convnext\_small \= torch.hub.load(REPO\_DIR, 'dinov3\_convnext\_small', source\='local', weights\=<CHECKPOINT/URL/OR/PATH\>)
dinov3\_convnext\_base \= torch.hub.load(REPO\_DIR, 'dinov3\_convnext\_base', source\='local', weights\=<CHECKPOINT/URL/OR/PATH\>)
dinov3\_convnext\_large \= torch.hub.load(REPO\_DIR, 'dinov3\_convnext\_large', source\='local', weights\=<CHECKPOINT/URL/OR/PATH\>)

\# DINOv3 ViT models pretrained on satellite imagery
dinov3\_vitl16 \= torch.hub.load(REPO\_DIR, 'dinov3\_vitl16', source\='local', weights\=<CHECKPOINT/URL/OR/PATH\>)
dinov3\_vit7b16 \= torch.hub.load(REPO\_DIR, 'dinov3\_vit7b16', source\='local', weights\=<CHECKPOINT/URL/OR/PATH\>)

### Pretrained backbones (via Hugging Face [Transformers](https://huggingface.co/docs/transformers/))

[](#pretrained-backbones-via-hugging-face-transformers)

All the backbones are available in the the [DINOv3](https://huggingface.co/collections/facebook/dinov3-68924841bd6b561778e31009) collection on Hugging Face Hub and supported via the Hugging Face [Transformers](https://huggingface.co/docs/transformers/index) library. Please refer to the corresponding documentation for usage, but below is a short example that demonstrates how to obtain an image embedding with either \[Pipeline\] or the \[AutoModel\] class.

from transformers import pipeline
from transformers.image\_utils import load\_image

url \= "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
image \= load\_image(url)

feature\_extractor \= pipeline(
    model\="facebook/dinov3-convnext-tiny-pretrain-lvd1689m",
    task\="image-feature-extraction", 
)
features \= feature\_extractor(image)

import torch
from transformers import AutoImageProcessor, AutoModel
from transformers.image\_utils import load\_image

url \= "http://images.cocodataset.org/val2017/000000039769.jpg"
image \= load\_image(url)

pretrained\_model\_name \= "facebook/dinov3-convnext-tiny-pretrain-lvd1689m"
processor \= AutoImageProcessor.from\_pretrained(pretrained\_model\_name)
model \= AutoModel.from\_pretrained(
    pretrained\_model\_name, 
    device\_map\="auto", 
)

inputs \= processor(images\=image, return\_tensors\="pt").to(model.device)
with torch.inference\_mode():
    outputs \= model(\*\*inputs)

pooled\_output \= outputs.pooler\_output
print("Pooled output shape:", pooled\_output.shape)

where `model` and `pretrained_model_name` above can be one of:

*   `facebook/dinov3-vits16-pretrain-lvd1689m`
*   `facebook/dinov3-vits16plus-pretrain-lvd1689m`
*   `facebook/dinov3-vitb16-pretrain-lvd1689m`
*   `facebook/dinov3-vitl16-pretrain-lvd1689m`
*   `facebook/dinov3-vith16plus-pretrain-lvd1689m`
*   `facebook/dinov3-vit7b16-pretrain-lvd1689m`
*   `facebook/dinov3-convnext-base-pretrain-lvd1689m`
*   `facebook/dinov3-convnext-large-pretrain-lvd1689m`
*   `facebook/dinov3-convnext-small-pretrain-lvd1689m`
*   `facebook/dinov3-convnext-tiny-pretrain-lvd1689m`
*   `facebook/dinov3-vitl16-pretrain-sat493m`
*   `facebook/dinov3-vit7b16-pretrain-sat493m`

### Image transforms

[](#image-transforms)

For models using the LVD-1689M weights (pretrained on web images), please use the following transform (standard ImageNet evaluation transform):

import torchvision

def make\_transform(resize\_size: int \= 224):
    to\_tensor \= transforms.ToTensor()
    resize \= transforms.Resize((resize\_size, resize\_size), antialias\=True)
    normalize \= transforms.Normalize(
        mean\=(0.485, 0.456, 0.406),
        std\=(0.229, 0.224, 0.225),
    )
    return transforms.Compose(\[to\_tensor, resize, normalize\])

For models using the SAT-493M weights (pretrained on satellite imagery), please use the following transform:

import torchvision

def make\_transform(resize\_size: int \= 224):
    to\_tensor \= transforms.ToTensor()
    resize \= transforms.Resize((resize\_size, resize\_size), antialias\=True)
    normalize \= transforms.Normalize(
        mean\=(0.430, 0.411, 0.296),
        std\=(0.213, 0.156, 0.143),
    )
    return transforms.Compose(\[to\_tensor, resize, normalize\])

### Pretrained heads - Image classification

[](#pretrained-heads---image-classification)

Backbone

Pretraining  
Dataset

Head  
Dataset

Download

ViT-7B/16

LVD-1689M

ImageNet

[\[link\]](https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/)

The (full) classifier models can be loaded via PyTorch Hub:

import torch

\# DINOv3
dinov3\_vit7b16\_lc \= torch.hub.load(REPO\_DIR, 'dinov3\_vit7b16\_lc', source\="local", weights\=<DEPTHER/CHECKPOINT/URL/OR/PATH\>, backbone\_weights\=<BACKBONE/CHECKPOINT/URL/OR/PATH\>)

### Pretrained heads - Depther trained on SYNTHMIX dataset

[](#pretrained-heads---depther-trained-on-synthmix-dataset)

Backbone

Pretraining  
Dataset

Head  
Dataset

Download

ViT-7B/16

LVD-1689M

SYNTHMIX

[\[link\]](https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/)

depther \= torch.hub.load(REPO\_DIR, 'dinov3\_vit7b16\_dd', source\="local", weights\=<DEPTHER/CHECKPOINT/URL/OR/PATH\>, backbone\_weights\=<BACKBONE/CHECKPOINT/URL/OR/PATH\>)

Full example code of depther on an image

from PIL import Image
import torch
from torchvision import transforms
import matplotlib.pyplot as plt
from matplotlib import colormaps

def get\_img():
    import requests
    url \= "http://images.cocodataset.org/val2017/000000039769.jpg"
    image \= Image.open(requests.get(url, stream\=True).raw).convert("RGB")
    return image

def make\_transform(resize\_size: int | list\[int\] \= 768):
    to\_tensor \= transforms.ToTensor()
    resize \= transforms.Resize((resize\_size, resize\_size), antialias\=True)
    normalize \= transforms.Normalize(
        mean\=(0.485, 0.456, 0.406),
        std\=(0.229, 0.224, 0.225),
    )
    return transforms.Compose(\[to\_tensor, resize, normalize\])

depther \= torch.hub.load(REPO\_DIR, 'dinov3\_vit7b16\_dd', source\="local", weights\=<DEPTHER/CHECKPOINT/URL/OR/PATH\>, backbone\_weights\=<BACKBONE/CHECKPOINT/URL/OR/PATH\>)

img\_size \= 1024
img \= get\_img()
transform \= make\_transform(img\_size)
with torch.inference\_mode():
    with torch.autocast('cuda', dtype\=torch.bfloat16):
        batch\_img \= transform(img)\[None\]
        batch\_img \= batch\_img
        depths \= depther(batch\_img)

plt.figure(figsize\=(12, 6))
plt.subplot(121)
plt.imshow(img)
plt.axis("off")
plt.subplot(122)
plt.imshow(depths\[0,0\].cpu(), cmap\=colormaps\["Spectral"\])
plt.axis("off")

### Pretrained heads - Detector trained on COCO2017 dataset

[](#pretrained-heads---detector-trained-on-coco2017-dataset)

Backbone

Pretraining  
Dataset

Head  
Dataset

Download

ViT-7B/16

LVD-1689M

COCO2017

[\[link\]](https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/)

detector \= torch.hub.load(REPO\_DIR, 'dinov3\_vit7b16\_de', source\="local", weights\=<DETECTOR/CHECKPOINT/URL/OR/PATH\>, backbone\_weights\=<BACKBONE/CHECKPOINT/URL/OR/PATH\>)

### Pretrained heads - Segmentor trained on ADE20K dataset

[](#pretrained-heads---segmentor-trained-on-ade20k-dataset)

Backbone

Pretraining  
Dataset

Head  
Dataset

Download

ViT-7B/16

LVD-1689M

ADE20K

[\[link\]](https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/)

segmentor \= torch.hub.load(REPO\_DIR, 'dinov3\_vit7b16\_ms', source\="local", weights\=<SEGMENTOR/CHECKPOINT/URL/OR/PATH\>, backbone\_weights\=<BACKBONE/CHECKPOINT/URL/OR/PATH\>)

Full example code of segmentator an image

import sys
sys.path.append(REPO\_DIR)

from PIL import Image
import torch
from torchvision import transforms
import matplotlib.pyplot as plt
from matplotlib import colormaps
from functools import partial
from dinov3.eval.segmentation.inference import make\_inference

def get\_img():
    import requests
    url \= "http://images.cocodataset.org/val2017/000000039769.jpg"
    image \= Image.open(requests.get(url, stream\=True).raw).convert("RGB")
    return image

def make\_transform(resize\_size: int | list\[int\] \= 768):
    to\_tensor \= transforms.ToTensor()
    resize \= transforms.Resize((resize\_size, resize\_size), antialias\=True)
    normalize \= transforms.Normalize(
        mean\=(0.485, 0.456, 0.406),
        std\=(0.229, 0.224, 0.225),
    )
    return transforms.Compose(\[to\_tensor, resize, normalize\])

segmentor \= torch.hub.load(REPO\_DIR, 'dinov3\_vit7b16\_ms', source\="local", weights\=<SEGMENTOR/CHECKPOINT/URL/OR/PATH\>, backbone\_weights\=<BACKBONE/CHECKPOINT/URL/OR/PATH\>)

img\_size \= 896
img  \= get\_img()
transform \= make\_transform(img\_size)
with torch.inference\_mode():
    with torch.autocast('cuda', dtype\=torch.bfloat16):
        batch\_img \= transform(img)\[None\]
        pred\_vit7b \= segmentor(batch\_img)  \# raw predictions  
        \# actual segmentation map
        segmentation\_map\_vit7b \= make\_inference(
            batch\_img,
            segmentor,
            inference\_mode\="slide",
            decoder\_head\_type\="m2f",
            rescale\_to\=(img.size\[\-1\], img.size\[\-2\]),
            n\_output\_channels\=150,
            crop\_size\=(img\_size, img\_size),
            stride\=(img\_size, img\_size),
            output\_activation\=partial(torch.nn.functional.softmax, dim\=1),
        ).argmax(dim\=1, keepdim\=True)
plt.figure(figsize\=(12, 6))
plt.subplot(121)
plt.imshow(img)
plt.axis("off")
plt.subplot(122)
plt.imshow(segmentation\_map\_vit7b\[0,0\].cpu(), cmap\=colormaps\["Spectral"\])
plt.axis("off")

### Pretrained heads - Zero-shot tasks with `dino.txt`

[](#pretrained-heads---zero-shot-tasks-with-dinotxt)

Backbone

Download

ViT-L/16 distilled

[\[link\]](https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/), [vocabulary](https://dl.fbaipublicfiles.com/dinov3/thirdparty/bpe_simple_vocab_16e6.txt.gz), [vocabulary license](https://dl.fbaipublicfiles.com/dinov2/thirdparty/LICENSE)

The (full) dino.txt model can be loaded via PyTorch Hub:

import torch
\# DINOv3
dinov3\_vitl16\_dinotxt\_tet1280d20h24l, tokenizer \= torch.hub.load(REPO\_DIR, 'dinov3\_vitl16\_dinotxt\_tet1280d20h24l', weights\=<SEGMENTOR/CHECKPOINT/URL/OR/PATH\>, backbone\_weights\=<BACKBONE/CHECKPOINT/URL/OR/PATH\>)

## Installation

[](#installation)

The training and evaluation code requires PyTorch version >= 2.7.1 as well as a few other 3rd party packages. Note that the code has only been tested with the specified versions and also expects a Linux environment. To setup all the required dependencies for training and evaluation, please follow the instructions below:

_[micromamba](https://mamba.readthedocs.io/en/latest/user_guide/micromamba.html)_ **(Recommended)** - Clone the repository and then create and activate a `dinov3` conda environment using the provided environment definition:

micromamba env create -f conda.yaml
micromamba activate dinov3

## Getting started

[](#getting-started)

Several notebooks are provided to get started applying DINOv3:

*   [PCA of patch features](/facebookresearch/dinov3/blob/main/notebooks/pca.ipynb): display the PCA of DINOv3 patch features on a foreground object (rainbow visualizations from the paper) [\[Run in Google Colab\]](https://colab.research.google.com/github/facebookresearch/dinov3/blob/main/notebooks/pca.ipynb)
*   [Foreground segmentation](/facebookresearch/dinov3/blob/main/notebooks/foreground_segmentation.ipynb): train a linear foreground segmentation model based on DINOv3 features [\[Run in Google Colab\]](https://colab.research.google.com/github/facebookresearch/dinov3/blob/main/notebooks/foreground_segmentation.ipynb)
*   [Dense and sparse matching](/facebookresearch/dinov3/blob/main/notebooks/dense_sparse_matching.ipynb): match patches from objects on two different images based on DINOv3 features [\[Run in Google Colab\]](https://colab.research.google.com/github/facebookresearch/dinov3/blob/main/notebooks/dense_sparse_matching.ipynb)
*   [Segmentation tracking](/facebookresearch/dinov3/blob/main/notebooks/segmentation_tracking.ipynb): video segmentation tracking using a non-parametric method based on DINOv3 features [\[Run in Google Colab\]](https://colab.research.google.com/github/facebookresearch/dinov3/blob/main/notebooks/segmentation_tracking.ipynb)

## Data preparation

[](#data-preparation)

### ImageNet-1k

[](#imagenet-1k)

The root directory of the dataset should hold the following contents:

*   `<ROOT>/test/ILSVRC2012_test_00000001.JPEG`
*   `<ROOT>/test/[..]`
*   `<ROOT>/test/ILSVRC2012_test_00100000.JPEG`
*   `<ROOT>/train/n01440764/n01440764_10026.JPEG`
*   `<ROOT>/train/[...]`
*   `<ROOT>/train/n15075141/n15075141_9993.JPEG`
*   `<ROOT>/val/n01440764/ILSVRC2012_val_00000293.JPEG`
*   `<ROOT>/val/[...]`
*   `<ROOT>/val/n15075141/ILSVRC2012_val_00049174.JPEG`
*   `<ROOT>/labels.txt`

The provided dataset implementation expects a few additional metadata files to be present under the extra directory:

*   `<EXTRA>/class-ids-TRAIN.npy`
*   `<EXTRA>/class-ids-VAL.npy`
*   `<EXTRA>/class-names-TRAIN.npy`
*   `<EXTRA>/class-names-VAL.npy`
*   `<EXTRA>/entries-TEST.npy`
*   `<EXTRA>/entries-TRAIN.npy`
*   `<EXTRA>/entries-VAL.npy`

These metadata files can be generated (once) with the following lines of Python code:

from dinov3.data.datasets import ImageNet

for split in ImageNet.Split:
    dataset \= ImageNet(split\=split, root\="<ROOT>", extra\="<EXTRA>")
    dataset.dump\_extra()

Note that the root and extra directories do not have to be distinct directories.

### ImageNet-22k

[](#imagenet-22k)

Please adapt the [dataset class](/facebookresearch/dinov3/blob/main/dinov3/data/datasets/image_net_22k.py) to match your local setup.

  

â ï¸ To execute the commands provided in the next sections for training and evaluation, the `dinov3` package should be included in the Python module search path, i.e. simply prefix the command to run with `PYTHONPATH=.`.

## Training

[](#training)

### Fast setup: training DINOv3 ViT-L/16 on ImageNet-1k

[](#fast-setup-training-dinov3-vit-l16-on-imagenet-1k)

Run DINOv3 pre-training on 4 H100-80GB nodes (32 GPUs) in a SLURM cluster environment with submitit:

 PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/train/train.py \\
  --nodes 4 \\
  --config-file dinov3/configs/train/vitl\_im1k\_lin834.yaml \\
  --output-dir <PATH/TO/OUTPUT/DIR\> \\
  train.dataset\_path=ImageNet22k:root=<PATH/TO/DATASET\>:extra=<PATH/TO/DATASET\>

Training time is approximately 14 hours and the resulting checkpoint should reach 82.0% on k-NN eval and 83.5% on linear eval.

The training code saves the weights of the teacher in the eval folder every 12500 iterations for evaluation.

### Exact DINOv3 setup: training DINOv3 ViT-7B/16

[](#exact-dinov3-setup-training-dinov3-vit-7b16)

DINOv3 ViT-7B/16 is trained on a private dataset. The training involves 3 stages:

*   Pretraining
*   Gram anchoring
*   High resolution adaptation

#### Pretraining

[](#pretraining)

Launch DINOV3 ViT-7B/16 pretraining on 32 nodes (256 GPUs) in a SLURM cluster environment with submitit.

PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/train/train.py \\
  --nodes 32 \\
  --config-file dinov3/configs/train/dinov3\_vit7b16\_pretrain.yaml \\
  --output-dir <PATH/TO/OUTPUT/DIR\> \\
  train.dataset\_path=<DATASET\>:root=<PATH/TO/DATASET\>:extra=<PATH/TO/DATASET\>

#### Gram anchoring

[](#gram-anchoring)

PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/train/train.py \\
  --nodes 32 \\
  --config-file dinov3/configs/train/dinov3\_vit7b16\_gram\_anchor.yaml \\
  --output-dir <PATH/TO/OUTPUT/DIR\> \\
  train.dataset\_path=<DATASET\>:root=<PATH/TO/DATASET\>:extra=<PATH/TO/DATASET\> \\
  gram.ckpt=<PATH/TO/GRAM\_TEACHER\_FROM\_PREVIOUS\_STEP\>   

#### High-resolution adaptation

[](#high-resolution-adaptation)

PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/train/train.py \\
  --nodes 32 \\
  --config-file dinov3/configs/train/dinov3\_vit7b16\_high\_res\_adapt.yaml \\
  --output-dir <PATH/TO/OUTPUT/DIR\> \\
  train.dataset\_path=<DATASET\>:root=<PATH/TO/DATASET\>:extra=<PATH/TO/DATASET\> \\
  gram.ckpt=<PATH/TO/TEACHER\_FROM\_GRAM\> \\
  student.resume\_from\_teacher\_chkpt=<PATH/TO/TEACHER\_FROM\_GRAM\>

## Multi-distillation

[](#multi-distillation)

### Test setup:

[](#test-setup)

PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/train/train.py \\
  --nodes 1 \\
  --config-file dinov3/configs/train/multi\_distillation\_test.yaml \\
  --output-dir <PATH/TO/OUTPUT/DIR\> \\
  --multi-distillation \\
  train.dataset\_path=<DATASET\>:root=<PATH/TO/DATASET\>:extra=<PATH/TO/DATASET\>

## Evaluation

[](#evaluation)

The training code regularly saves the teacher weights. In order to evaluate the model, run the following evaluation on a single node:

### Logistic regression classification on ImageNet-1k

[](#logistic-regression-classification-on-imagenet-1k)

PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/eval/log\_regression.py \\
  model.config\_file=<PATH/TO/OUTPUT/DIR\>/config.yaml \\
  model.pretrained\_weights=<PATH/TO/OUTPUT/DIR\>/teacher\_checkpoint.pth \\
  output\_dir=<PATH/TO/OUTPUT/DIR\> \\
  train.dataset=ImageNet:split=TRAIN:root=<PATH/TO/DATASET\>:extra=<PATH/TO/DATASET\> \\
  eval.test\_dataset=ImageNet:split=VAL:root=<PATH/TO/DATASET\>:extra=<PATH/TO/DATASET\>

### k-NN classification on ImageNet-1k

[](#k-nn-classification-on-imagenet-1k)

PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/eval/knn.py \\
  model.config\_file=<PATH/TO/OUTPUT/DIR\>/config.yaml \\
  model.pretrained\_weights=<PATH/TO/OUTPUT/DIR\>/teacher\_checkpoint.pth \\
  output\_dir=<PATH/TO/OUTPUT/DIR\> \\
  train.dataset=ImageNet:split=TRAIN:root=<PATH/TO/DATASET\>:extra=<PATH/TO/DATASET\> \\
  eval.test\_dataset=ImageNet:split=VAL:root=<PATH/TO/DATASET\>:extra=<PATH/TO/DATASET\>

### Linear classification with data augmentation on ImageNet-1k

[](#linear-classification-with-data-augmentation-on-imagenet-1k)

PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/eval/linear.py \\
  model.config\_file=<PATH/TO/OUTPUT/DIR\>/config.yaml \\
  model.pretrained\_weights=<PATH/TO/OUTPUT/DIR\>/teacher\_checkpoint.pth \\
  output\_dir=<PATH/TO/OUTPUT/DIR\> \\
  train.dataset=ImageNet:split=TRAIN:root=<PATH/TO/DATASET\>:extra=<PATH/TO/DATASET\> \\
  train.val\_dataset=ImageNet:split=VAL:root=<PATH/TO/DATASET\>:extra=<PATH/TO/DATASET\>

### Text alignment on DINOv3 using dino.txt

[](#text-alignment-on-dinov3-using-dinotxt)

Text alignment can be done following the method from `dino.txt` aka [DINOv2 Meets Text](https://arxiv.org/abs/2412.16334).

PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/eval/text/train\_dinotxt.py \\
   --nodes 4 \\
  # An example config for text alignment is here: dinov3/eval/text/configs/dinov3\_vitl\_text.yaml \\ 
  trainer\_config\_file="<PATH/TO/DINOv3/TEXT/CONFIG>" \\
  output-dir=<PATH/TO/OUTPUT/DIR\>

Launching the above trains text alignment on 4 nodes with 8 gpus each (32 gpus in total). Please note that the text alignment model in the DINOv3 paper was trained on a private dataset and here we have given an example config in `dinov3/eval/text/configs/dinov3_vitl_text.yaml` using `CocoCaptions` dataset for illustration purpose. Please adapt the provided `CocoCaptions` dataset class, the dataset can be found [here](https://www.kaggle.com/datasets/nikhil7280/coco-image-caption)

## License

[](#license)

DINOv3 code and model weights are released under the DINOv3 License. See [LICENSE.md](/facebookresearch/dinov3/blob/main/LICENSE.md) for additional details.

## Contributing

[](#contributing)

See [contributing](/facebookresearch/dinov3/blob/main/CONTRIBUTING.md) and the [code of conduct](/facebookresearch/dinov3/blob/main/CODE_OF_CONDUCT.md).

## Citing DINOv3

[](#citing-dinov3)

If you find this repository useful, please consider giving a star â­ and citation ð¦:

```
@article{simeoni2025dinov3,
  title = {{{DINOv3}}},
  author = {Sim{\'e}oni, Oriane and Vo, Huy V. and Seitzer, Maximilian and Baldassarre, Federico and Oquab, Maxime and Jose, Cijo and Khalidov, Vasil and Szafraniec, Marc and Yi, Seungeun and Ramamonjisoa, Micha{\"e}l and Massa, Francisco and Haziza, Daniel and Wehrstedt, Luca and Wang, Jianyuan and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Sentana, Leonel and Roberts, Claire and Vedaldi, Andrea and Tolan, Jamie and Brandt, John and Couprie, Camille and Mairal, Julien and J{\'e}gou, Herv{\'e} and Labatut, Patrick and Bojanowski, Piotr},
  year = {2025},
  month = aug,
  url={https://ai.meta.com/research/publications/dinov3},
  urldate = {2025-08-13},
}
```

## About

Reference PyTorch implementation and models for DINOv3

### Resources

[Readme](#readme-ov-file)

### License

[View license](#License-1-ov-file)

### Code of conduct

[Code of conduct](#coc-ov-file)

### Contributing

[Contributing](#contributing-ov-file)

### Security policy

[Security policy](#security-ov-file)

### Uh oh!

There was an error while loading. Please reload this page.

[Activity](/facebookresearch/dinov3/activity)

[Custom properties](/facebookresearch/dinov3/custom-properties)

### Stars

[**488** stars](/facebookresearch/dinov3/stargazers)

### Watchers

[**3** watching](/facebookresearch/dinov3/watchers)

### Forks

[**20** forks](/facebookresearch/dinov3/forks)

[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Ffacebookresearch%2Fdinov3&report=facebookresearch+%28user%29)

## [Releases](/facebookresearch/dinov3/releases)

No releases published

## [Packages 0](/orgs/facebookresearch/packages?repo_name=dinov3)

No packages published  

### Uh oh!

There was an error while loading. Please reload this page.

## Languages

*   [Jupyter Notebook 94.1%](/facebookresearch/dinov3/search?l=jupyter-notebook)
*   [Python 5.5%](/facebookresearch/dinov3/search?l=python)
*   Other 0.4%

## Footer

[](https://github.com)Â© 2025 GitHub,Â Inc.

### Footer navigation

*   [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
*   [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
*   [Security](https://github.com/security)
*   [Status](https://www.githubstatus.com/)
*   [Docs](https://docs.github.com/)
*   [Contact](https://support.github.com?tags=dotcom-footer)
*   Manage cookies
*   Do not share my personal information

You canât perform that action at this time.
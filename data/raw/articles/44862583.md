Improve Mistral models integration with llama.cpp by juliendenize Â· Pull Request #14737 Â· ggml-org/llama.cpp Â· GitHub                                            

[Skip to content](#start-of-content)  

## Navigation Menu

Toggle navigation

[](/)

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fggml-org%2Fllama.cpp%2Fpull%2F14737)

Appearance settings

*   Product
    
    *   [
        
        GitHub Copilot
        
        Write better code with AI
        
        ](https://github.com/features/copilot)
    *   [
        
        GitHub Spark New
        
        Build and deploy intelligent apps
        
        ](https://github.com/features/spark)
    *   [
        
        GitHub Models New
        
        Manage and compare prompts
        
        ](https://github.com/features/models)
    *   [
        
        GitHub Advanced Security
        
        Find and fix vulnerabilities
        
        ](https://github.com/security/advanced-security)
    *   [
        
        Actions
        
        Automate any workflow
        
        ](https://github.com/features/actions)
    
    *   [
        
        Codespaces
        
        Instant dev environments
        
        ](https://github.com/features/codespaces)
    *   [
        
        Issues
        
        Plan and track work
        
        ](https://github.com/features/issues)
    *   [
        
        Code Review
        
        Manage code changes
        
        ](https://github.com/features/code-review)
    *   [
        
        Discussions
        
        Collaborate outside of code
        
        ](https://github.com/features/discussions)
    *   [
        
        Code Search
        
        Find more, search less
        
        ](https://github.com/features/code-search)
    
    Explore
    
    *   [Why GitHub](https://github.com/why-github)
    *   [All features](https://github.com/features)
    *   [Documentation](https://docs.github.com)
    *   [GitHub Skills](https://skills.github.com)
    *   [Blog](https://github.blog)
    
*   Solutions
    
    By company size
    
    *   [Enterprises](https://github.com/enterprise)
    *   [Small and medium teams](https://github.com/team)
    *   [Startups](https://github.com/enterprise/startups)
    *   [Nonprofits](/solutions/industry/nonprofits)
    
    By use case
    
    *   [DevSecOps](/solutions/use-case/devsecops)
    *   [DevOps](/solutions/use-case/devops)
    *   [CI/CD](/solutions/use-case/ci-cd)
    *   [View all use cases](/solutions/use-case)
    
    By industry
    
    *   [Healthcare](/solutions/industry/healthcare)
    *   [Financial services](/solutions/industry/financial-services)
    *   [Manufacturing](/solutions/industry/manufacturing)
    *   [Government](/solutions/industry/government)
    *   [View all industries](/solutions/industry)
    
    [View all solutions](/solutions)
    
*   Resources
    
    Topics
    
    *   [AI](/resources/articles/ai)
    *   [DevOps](/resources/articles/devops)
    *   [Security](/resources/articles/security)
    *   [Software Development](/resources/articles/software-development)
    *   [View all](/resources/articles)
    
    Explore
    
    *   [Learning Pathways](https://resources.github.com/learn/pathways)
    *   [Events & Webinars](https://resources.github.com)
    *   [Ebooks & Whitepapers](https://github.com/resources/whitepapers)
    *   [Customer Stories](https://github.com/customer-stories)
    *   [Partners](https://partner.github.com)
    *   [Executive Insights](https://github.com/solutions/executive-insights)
    
*   Open Source
    
    *   [
        
        GitHub Sponsors
        
        Fund open source developers
        
        ](/sponsors)
    
    *   [
        
        The ReadME Project
        
        GitHub community articles
        
        ](https://github.com/readme)
    
    Repositories
    
    *   [Topics](https://github.com/topics)
    *   [Trending](https://github.com/trending)
    *   [Collections](https://github.com/collections)
    
*   Enterprise
    
    *   [
        
        Enterprise platform
        
        AI-powered developer platform
        
        ](/enterprise)
    
    Available add-ons
    
    *   [
        
        GitHub Advanced Security
        
        Enterprise-grade security features
        
        ](https://github.com/security/advanced-security)
    *   [
        
        Copilot for business
        
        Enterprise-grade AI features
        
        ](/features/copilot/copilot-business)
    *   [
        
        Premium Support
        
        Enterprise-grade 24/7 support
        
        ](/premium-support)
    
*   [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

 Include my email address so I can be contacted

Cancel Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name  

Query 

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

Cancel Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fggml-org%2Fllama.cpp%2Fpull%2F14737)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fpull_requests_fragments%2Fpull_request_layout&source=header-repo&source_repo=ggml-org%2Fllama.cpp)

Appearance settings

Resetting focus

You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert

[ggml-org](/ggml-org) / **[llama.cpp](/ggml-org/llama.cpp)** Public

*   [Notifications](/login?return_to=%2Fggml-org%2Fllama.cpp) You must be signed in to change notification settings
*   [Fork 12.7k](/login?return_to=%2Fggml-org%2Fllama.cpp)
*   [Star 84.5k](/login?return_to=%2Fggml-org%2Fllama.cpp)
    

*   [Code](/ggml-org/llama.cpp)
*   [Issues 311](/ggml-org/llama.cpp/issues)
*   [Pull requests 507](/ggml-org/llama.cpp/pulls)
*   [Discussions](/ggml-org/llama.cpp/discussions)
*   [Actions](/ggml-org/llama.cpp/actions)
*   [Projects 10](/ggml-org/llama.cpp/projects)
*   [Wiki](/ggml-org/llama.cpp/wiki)
*   [Security](/ggml-org/llama.cpp/security)
    
    [](/ggml-org/llama.cpp/security)
    
    [](/ggml-org/llama.cpp/security)
    
    [](/ggml-org/llama.cpp/security)
    
    [
    
    ### Uh oh!
    
    ](/ggml-org/llama.cpp/security)
    
    [There was an error while loading.](/ggml-org/llama.cpp/security) Please reload this page.
    
*   [Insights](/ggml-org/llama.cpp/pulse)

Additional navigation options

*   [Code](/ggml-org/llama.cpp)
*   [Issues](/ggml-org/llama.cpp/issues)
*   [Pull requests](/ggml-org/llama.cpp/pulls)
*   [Discussions](/ggml-org/llama.cpp/discussions)
*   [Actions](/ggml-org/llama.cpp/actions)
*   [Projects](/ggml-org/llama.cpp/projects)
*   [Wiki](/ggml-org/llama.cpp/wiki)
*   [Security](/ggml-org/llama.cpp/security)
*   [Insights](/ggml-org/llama.cpp/pulse)

# Improve Mistral models integration with llama.cpp #14737

New issue

**Have a question about this project?** Sign up for a free GitHub account to open an issue and contact its maintainers and the community.

[Sign up for GitHub](/signup?return_to=%2Fggml-org%2Fllama.cpp%2Fissues%2Fnew%2Fchoose)

By clicking â€œSign up for GitHubâ€, you agree to our [terms of service](https://docs.github.com/terms) and [privacy statement](https://docs.github.com/privacy). Weâ€™ll occasionally send you account related emails.

Already on GitHub? [Sign in](/login?return_to=%2Fggml-org%2Fllama.cpp%2Fissues%2Fnew%2Fchoose) to your account

[Jump to bottom](#issue-comment-box)

Merged

[CISC](/CISC) merged 19 commits into [ggml-org:master](/ggml-org/llama.cpp/tree/master "ggml-org/llama.cpp:master") from [juliendenize:mistral\_integration](/juliendenize/llama.cpp/tree/mistral_integration "juliendenize/llama.cpp:mistral_integration")Aug 11, 2025

Merged

# [Improve Mistral models integration with llama.cpp](#top) #14737

[CISC](/CISC) merged 19 commits into [ggml-org:master](/ggml-org/llama.cpp/tree/master "ggml-org/llama.cpp:master") from [juliendenize:mistral\_integration](/juliendenize/llama.cpp/tree/mistral_integration "juliendenize/llama.cpp:mistral_integration")Aug 11, 2025

+285 âˆ’112

[Conversation 70](/ggml-org/llama.cpp/pull/14737) [Commits 19](/ggml-org/llama.cpp/pull/14737/commits) [Checks 5](/ggml-org/llama.cpp/pull/14737/checks) [Files changed 4](/ggml-org/llama.cpp/pull/14737/files)

## Conversation

[![juliendenize](https://avatars.githubusercontent.com/u/40604584?s=60&v=4)](/juliendenize)

Copy link

Contributor

### 

![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=48&v=4) **[juliendenize](/juliendenize)** commented [Jul 17, 2025](#issue-3239414403) â€¢

edited

Loading

### Uh oh!

There was an error while loading. Please reload this page.

## Description

This PR aims to enhance the integration of Mistral models with llama.cpp by addressing several key issues and introducing new features. Here are the details:

### Context

*   The current HF conversion to GGUF does not work directly for Mistral models due to our format that is vLLM based. This means that we have to first convert weights to Hugging Face then to GGUF which is not ideal and can lead to conversion errors if the first conversion is not done correctly. It also means that adding new models to the llama.cpp ecosystem requires first adding them to Transformers.
*   We do not support chat templates natively which means chat templates are community based and not guaranteed to work correctly.
*   We are using [mistral-common](https://github.com/mistralai/mistral-common) internally for tokenization and want the community to use it to unlock full capacities of our models. As mistral-common is a Python library, we have opened a [PR](https://github.com/mistralai/mistral-common/pull/113) to add a REST API via FastAPI to make it easier for users who are not in the Python ecosystem.

### Using mistral-common with llama.cpp

We recommend that users only use the `llama-server` tool with the `/completions` route of the server for now, as it is the only one that supports tokens input. We also advise users to set `return_tokens=True` in their requests to let `mistral-common` handle detokenization.

### Added features

1.  Model conversion:

We have added a script to convert Mistral models to GGUF directly from Hugging Face. This script is located at `convert_mistral_to_gguf.py` and can be used to convert Mistral models to GGUF format.

2.  Model architecture:

We registered the Mistral architecture in llama.cpp to support Mistral models natively. This allows users to use Mistral models with llama.cpp without having to convert them to Hugging Face first.

### Known Limitations:

Our approach does not support multimodality:

*   mistral-common handles processing multimodal data but they cannot be passed to llama.cpp via the route.
*   llama.cpp only supports multimodality via chat templates, which we do not support.

Also this approach requires users to only use the llama.cpp server with the `/completions` route.

## Example Code

To get started, install mistral-common using the following command:

#### (Optional) Convert the model

HF\_TOKEN\=... python convert\_mistral\_to\_gguf.py \\
mistralai/Devstral\-Small\-2505 \-\-remote \-\-ctx\-train 131072 \-\-outtype bf16

#### Launch the mistral-common and llama.cpp servers

pip install git+https://github.com/mistralai/mistral-common.git@improve\_llama\_cpp\_integration\[server\]

Launch the mistral-common server:

HF\_TOKEN=... mistral\_common mistralai/Devstral-Small-2505 --port 6000

Launch the llama.cpp server:

./build/bin/llama-server -m models/Devstral-Small-2505-Q4\_K\_M.gguf --port 8080

#### Use the servers

Here is a code snippet demonstrating how to use the new features:

import requests

mistral\_common\_url \= "http://127.0.0.1:6000"
llama\_cpp\_url \= "http://127.0.0.1:8080"

def tokenize(messages, url):
    response \= requests.post(f"{url}/tokenize/messages", json\=messages)
    return response.json()

def detokenize(tokens, url):
    response \= requests.post(f"{url}/detokenize", json\={"tokens": tokens})
    return response.json()

def detokenize\_message(tokens, url):
    response \= requests.post(f"{url}/detokenize", json\={"tokens": tokens, "as\_message": True})
    return response.json()

def generate(tokens, url):
    response \= requests.post(f"{url}/completions", json\={
        "prompt": tokens,
        "stream": False,
        "return\_tokens": True
    })
    return response.json()

messages \= \[
    {"role": "system", "content": "You are Devstral a cool coding agent that can help users with their coding needs."},
    {"role": "user", "content": "Who are you and what can you do?"}
\]

tokens \= tokenize(messages, mistral\_common\_url)
print(tokens)

generated \= generate(tokens, llama\_cpp\_url)\["tokens"\]
print(generated)

detokenized \= detokenize(generated, mistral\_common\_url)
print(detokenized)

detokenized\_message \= detokenize\_message(generated, mistral\_common\_url)
print(detokenized\_message)

## Feedback and Contributions

We believe these changes will significantly improve the integration of Mistral models with llama.cpp and provide a better experience for our users. We welcome any feedback or suggestions to further enhance this integration. Also, as we have few experience in the codebase of llama.cpp, we welcome any help to improve the integration and make sure we respect the codebase and the community.

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

 

ðŸ˜• 1 l29ah reacted with confused emoji â¤ï¸ 4 stduhpf, mseri, dargor, and Kompas reacted with heart emoji ðŸš€ 1 jacekpoplawski reacted with rocket emoji

All reactions

*   ðŸ˜• 1 reaction
*   â¤ï¸ 4 reactions
*   ðŸš€ 1 reaction

[![@github-actions](https://avatars.githubusercontent.com/in/15368?s=40&v=4)](/apps/github-actions) [github-actions](/apps/github-actions) bot added the [python](/ggml-org/llama.cpp/issues?q=state%3Aopen%20label%3Apython) python script changes label [Jul 17, 2025](#event-18679321334)

[![@ggerganov](https://avatars.githubusercontent.com/u/1991296?s=80&u=28314d364d7c28f8ec232fadb767970d3ad74e7b&v=4)](/ggerganov)

Copy link

Member

### 

**[ggerganov](/ggerganov)** commented [Jul 17, 2025](#issuecomment-3084573755)

Thanks for the contribution. From a developer perspective, it looks like a good approach to avoid any potential tokenization / formatting problems. In general, for all models, using a reference tokenizer instead of relying on `llama.cpp` is always recommended. From usability standpoint, the requirement to start a separate tokenization server is a bit of a drawback, but I understand that correctness is of higher importance.

My understanding is that most chat template problems occur during the early days of the model release, and with time tend to get polished and fixed. So this approach would be a stable alternative during such periods of instability.

  

ðŸ‘ 1 Vaibhavs10 reacted with thumbs up emoji â¤ï¸ 2 juliendenize and mseri reacted with heart emoji

All reactions

*   ðŸ‘ 1 reaction
*   â¤ï¸ 2 reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@ehoogeveen-medweb](https://avatars.githubusercontent.com/u/55082669?s=80&u=dc9a32cab9129f9e7c39151f372dc7fbd306d957&v=4)](/ehoogeveen-medweb)

Copy link

### 

**[ehoogeveen-medweb](/ehoogeveen-medweb)** commented [Jul 17, 2025](#issuecomment-3084674122)

IIRC Mistral's architecture also makes use of sliding window attention (SWA), defaulting to a window size of 4096 tokens - though I don't know all the details (like which layers, if any, are full layers). It would be great if the window size could be stored in the GGUF file as well (e.g. as `mistral.attention.sliding_window`), and the model could eventually be hooked into llama.cpp's SWA support.

  

ðŸ‘€ 1 juliendenize reacted with eyes emoji

All reactions

*   ðŸ‘€ 1 reaction

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=40&u=fc5cd426f2e66d60454e20ef00bd8986142152be&v=4)](/juliendenize) [juliendenize](/juliendenize) [force-pushed](/ggml-org/llama.cpp/compare/b809a96f3722d1f0f37224ab91fd3f085c0c06d1..2865a2562ea2e9cfab0617c57390f8cbbb172e6d) the mistral\_integration branch from [`b809a96`](/ggml-org/llama.cpp/commit/b809a96f3722d1f0f37224ab91fd3f085c0c06d1) to [`2865a25`](/ggml-org/llama.cpp/commit/2865a2562ea2e9cfab0617c57390f8cbbb172e6d) [Compare](/ggml-org/llama.cpp/compare/b809a96f3722d1f0f37224ab91fd3f085c0c06d1..2865a2562ea2e9cfab0617c57390f8cbbb172e6d) [July 23, 2025 14:55](#event-18776477454)

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=80&u=fc5cd426f2e66d60454e20ef00bd8986142152be&v=4)](/juliendenize)

Copy link

Contributor Author

### 

**[juliendenize](/juliendenize)** commented [Jul 23, 2025](#issuecomment-3110626163)

Hey guys many sorries for the delay of the answer and thanks a lot for your feedback.

[@ggerganov](https://github.com/ggerganov)

> My understanding is that most chat template problems occur during the early days of the model release, and with time tend to get polished and fixed. So this approach would be a stable alternative during such periods of instability.

Exactly what's cool with llama.cpp is that you support the possibility to pass jinja templates when serving so people can use them once they are correct if they want and remove the mistral-common server ! Very nice feature.

[@ehoogeveen-medweb](https://github.com/ehoogeveen-medweb)

> Mistral's architecture also makes use of sliding window attention (SWA)

This is actually for super old (for Deep Learning ^^) models so we didn't add support to that. Could it be a subsequent PR ?

Regarding the PR:

*   I refactored a bit to remove the Mistral arch, it didn't add value so we think it's a less of a burden for maintainability !
*   I tried to make the CI green but I think that because we modified gguf-py files some checks cannot pass because it installs the published package AFAIU. Is it right ?

Happy to answer more questions :)

  

All reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=40&u=fc5cd426f2e66d60454e20ef00bd8986142152be&v=4)](/juliendenize) [juliendenize](/juliendenize) marked this pull request as ready for review [July 23, 2025 22:19](#event-18783609073)

[![@CISC](https://avatars.githubusercontent.com/u/1629204?s=40&v=4)](/CISC) [CISC](/CISC) mentioned this pull request [Jul 24, 2025](#ref-issue-3233646381)

[New Voxtral model support ggml-org/whisper.cpp#3326](/ggml-org/whisper.cpp/issues/3326)

Open

[![CISC](https://avatars.githubusercontent.com/u/1629204?s=60&v=4)](/CISC)

**[CISC](/CISC)** reviewed [Jul 24, 2025](#pullrequestreview-3050651236)

[View reviewed changes](/ggml-org/llama.cpp/pull/14737/files)

[convert\_mistral\_to\_gguf.py](/ggml-org/llama.cpp/pull/14737/files#diff-614d7b8c4b0614b8d2a078faa45498923f2ad1376ca6de5fd0b3026bf5ec5a20) Outdated Show resolved Hide resolved

### Uh oh!

There was an error while loading. Please reload this page.

[![@CISC](https://avatars.githubusercontent.com/u/1629204?s=80&v=4)](/CISC)

Copy link

Collaborator

### 

**[CISC](/CISC)** commented [Jul 24, 2025](#issuecomment-3112569967)

> ```
> * I tried to make the CI green but I think that because we modified gguf-py files some checks cannot pass because it installs the published package AFAIU. Is it right ?
> ```

Partially, there's also a `pydantic` version conflict:  
[https://github.com/ggml-org/llama.cpp/actions/runs/16474192826/job/46571944794?pr=14737#step:4:291](https://github.com/ggml-org/llama.cpp/actions/runs/16474192826/job/46571944794?pr=14737#step:4:291)

  

All reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@CISC](https://avatars.githubusercontent.com/u/1629204?s=80&v=4)](/CISC)

Copy link

Collaborator

### 

**[CISC](/CISC)** commented [Jul 24, 2025](#issuecomment-3112603394)

[@juliendenize](https://github.com/juliendenize) Please undo all the formatting/style changes, they are not relevant and add too much noise to the PR, will review afterwards. :)

  

All reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@Kreijstal](https://avatars.githubusercontent.com/u/2415206?s=80&u=d326ff9d00b01dcbd483bac8295a3fd6e3734f23&v=4)](/Kreijstal)

### This comment was marked as off-topic.

[Sign in to view](/login?return_to=https%3A%2F%2Fgithub.com%2Fggml-org%2Fllama.cpp%2Fpull%2F14737)

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=80&u=fc5cd426f2e66d60454e20ef00bd8986142152be&v=4)](/juliendenize)

Copy link

Contributor Author

### 

**[juliendenize](/juliendenize)** commented [Jul 24, 2025](#issuecomment-3113290652)

> Partially, there's also a `pydantic` version conflict:

Arf, would it be ok to up the requirements for Pydantic in your side or is it a no ? Was there a particular reason to stay at 2.6 ?

  

All reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=80&u=fc5cd426f2e66d60454e20ef00bd8986142152be&v=4)](/juliendenize)

Copy link

Contributor Author

### 

**[juliendenize](/juliendenize)** commented [Jul 24, 2025](#issuecomment-3113294643)

> [@juliendenize](https://github.com/juliendenize) Please undo all the formatting/style changes, they are not relevant and add too much noise to the PR, will review afterwards. :)

Done sorry about that my own formatter was on.

Is there a formatter or linter available for Python ? Didn't find it in the contributing guidelines.  
I've installed flake8 but it didn't flag any thing when launched locally

  

All reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@CISC](https://avatars.githubusercontent.com/u/1629204?s=80&v=4)](/CISC)

Copy link

Collaborator

### 

**[CISC](/CISC)** commented [Jul 24, 2025](#issuecomment-3113333861)

> Arf, would it be ok to up the requirements for Pydantic in your side or is it a no ? Was there a particular reason to stay at 2.6 ?

Yes, I think it's ok, it's probably just the version that was available at the time.

> Is there a formatter or linter available for Python ? Didn't find it in the contributing guidelines. I've installed flake8 but it didn't flag any thing when launched locally

We don't use a Python formatter, only `flake8` linting.

  

ðŸ‘ 1 juliendenize reacted with thumbs up emoji

All reactions

*   ðŸ‘ 1 reaction

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@CISC](https://avatars.githubusercontent.com/u/1629204?s=80&v=4)](/CISC)

Copy link

Collaborator

### 

**[CISC](/CISC)** commented [Jul 24, 2025](#issuecomment-3113362991)

Pillow conflict, should be fine to update:  
[https://github.com/ggml-org/llama.cpp/actions/runs/16497338367/job/46646389345?pr=14737#step:4:305](https://github.com/ggml-org/llama.cpp/actions/runs/16497338367/job/46646389345?pr=14737#step:4:305)

  

ðŸ‘ 1 juliendenize reacted with thumbs up emoji

All reactions

*   ðŸ‘ 1 reaction

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@github-actions](https://avatars.githubusercontent.com/in/15368?s=40&v=4)](/apps/github-actions) [github-actions](/apps/github-actions) bot added the [examples](/ggml-org/llama.cpp/issues?q=state%3Aopen%20label%3Aexamples) label [Jul 24, 2025](#event-18795167606)

[![@CISC](https://avatars.githubusercontent.com/u/1629204?s=80&v=4)](/CISC)

Copy link

Collaborator

### 

**[CISC](/CISC)** commented [Jul 24, 2025](#issuecomment-3113445743) â€¢

edited

Loading

### Uh oh!

There was an error while loading. Please reload this page.

Right, now we are getting somewhere. :)  
[https://github.com/ggml-org/llama.cpp/actions/runs/16497849941/job/46648060083?pr=14737](https://github.com/ggml-org/llama.cpp/actions/runs/16497849941/job/46648060083?pr=14737)

Edit: The unbound errors are clearly handled at init and can be silenced by `# pyright: ignore[reportPossiblyUnboundVariable]`

  

All reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@am17an](https://avatars.githubusercontent.com/u/2929750?s=80&u=1e1a3686de359e6e65a7aeb2e7364a2db73eaa49&v=4)](/am17an)

Copy link

Collaborator

### 

**[am17an](/am17an)** commented [Jul 24, 2025](#issuecomment-3113718588) â€¢

edited

Loading

### Uh oh!

There was an error while loading. Please reload this page.

[@juliendenize](https://github.com/juliendenize) do you also plan to make changes to `convert_mistral_to_gguf.py` to have mappings for `audio_tower.*` for the mmproj, I guess it will be necessary for the new voxtral models?

  

All reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=80&u=fc5cd426f2e66d60454e20ef00bd8986142152be&v=4)](/juliendenize)

Copy link

Contributor Author

### 

**[juliendenize](/juliendenize)** commented [Jul 24, 2025](#issuecomment-3113900769)

> Right, now we are getting somewhere. :) [https://github.com/ggml-org/llama.cpp/actions/runs/16497849941/job/46648060083?pr=14737](https://github.com/ggml-org/llama.cpp/actions/runs/16497849941/job/46648060083?pr=14737)
> 
> Edit: The unbound errors are clearly handled at init and can be silenced by `# pyright: ignore[reportPossiblyUnboundVariable]`

Tried to make things cleaner sorry for the back and forth.

  

All reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=80&u=fc5cd426f2e66d60454e20ef00bd8986142152be&v=4)](/juliendenize)

Copy link

Contributor Author

### 

**[juliendenize](/juliendenize)** commented [Jul 24, 2025](#issuecomment-3113908062)

> [@juliendenize](https://github.com/juliendenize) do you also plan to make changes to `convert_mistral_to_gguf.py` to have mappings for `audio_tower.*` for the mmproj, I guess it will be necessary for the new voxtral models?

that would be cool indeed, I didn't work personally on Voxtral (for the model), so I might need some assistance as I lack experience in audio models.

Is voxtral already supported by llama.cpp ? I assumed that not for now.

  

All reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@am17an](https://avatars.githubusercontent.com/u/2929750?s=80&u=1e1a3686de359e6e65a7aeb2e7364a2db73eaa49&v=4)](/am17an)

Copy link

Collaborator

### 

**[am17an](/am17an)** commented [Jul 24, 2025](#issuecomment-3113942795)

> > [@juliendenize](https://github.com/juliendenize) do you also plan to make changes to `convert_mistral_to_gguf.py` to have mappings for `audio_tower.*` for the mmproj, I guess it will be necessary for the new voxtral models?
> 
> that would be cool indeed, I didn't work personally on Voxtral (for the model), so I might need some assistance as I lack experience in audio models.
> 
> Is voxtral already supported by llama.cpp ? I assumed that not for now.

Yeah not for now, but I was trying to add support and ran into issues converting to GGUF. But that should be easy to add after this PR is merged, so don't worry about it for now :)

  

ðŸ‘ 1 juliendenize reacted with thumbs up emoji

All reactions

*   ðŸ‘ 1 reaction

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=80&u=fc5cd426f2e66d60454e20ef00bd8986142152be&v=4)](/juliendenize)

Copy link

Contributor Author

### 

**[juliendenize](/juliendenize)** commented [Jul 24, 2025](#issuecomment-3114082845)

Ok so this: [https://github.com/ggml-org/llama.cpp/actions/runs/16500995835/job/46660394829?pr=14737](https://github.com/ggml-org/llama.cpp/actions/runs/16500995835/job/46660394829?pr=14737)

Is actually expected because we didn't merge the PR here yet in `mistral-common`:  
[Add a FastAPI app #113](https://github.com/mistralai/mistral-common/pull/113)

We're in the process of merging I'm just adding a final feature which is begin able to call `/v1/chat/completions` to directly call the inference server (in this case llama.cpp !!). I'm moving as fast as possible for this.

  

ðŸ‘ 1 CISC reacted with thumbs up emoji

All reactions

*   ðŸ‘ 1 reaction

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@CISC](https://avatars.githubusercontent.com/u/1629204?s=80&v=4)](/CISC)

Copy link

Collaborator

### 

**[CISC](/CISC)** commented [Jul 24, 2025](#issuecomment-3114356459)

> We're in the process of merging I'm just adding a final feature which is begin able to call `/v1/chat/completions` to directly call the inference server (in this case llama.cpp !!). I'm moving as fast as possible for this.

Ok, ping me when you're ready.

  

ðŸ‘ 1 juliendenize reacted with thumbs up emoji

All reactions

*   ðŸ‘ 1 reaction

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@ngxson](https://avatars.githubusercontent.com/u/7702203?s=80&u=c998d920dbca6506c9dda1df1b28adae9842720e&v=4)](/ngxson)

Copy link

Collaborator

### 

**[ngxson](/ngxson)** commented [Jul 24, 2025](#issuecomment-3114529983)

I've just had a deeper look into this PR. One concern though, most of the code inside `convert_mistral_to_gguf.py` is copied from `convert_hf_to_gguf.py`, which can make it a bit tricky in the long term, especially the code for multimodal model conversion.

Just thinking, maybe it's better to bring them right into `convert_hf_to_gguf.py`? AFAIU most of the complicated code in this PR is dedicated to converting the tokenizer to GGUF.

Btw, I'm also working converting Voxtral to GGUF. I thought that would be simple but I'm currently stuck at the tokenizer. Trying a quick hack to copy some code from this PR.. will see if it work.

  

All reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@ngxson](https://avatars.githubusercontent.com/u/7702203?s=40&u=c998d920dbca6506c9dda1df1b28adae9842720e&v=4)](/ngxson) [ngxson](/ngxson) mentioned this pull request [Jul 24, 2025](#ref-pullrequest-3261049268)

[mtmd : add support for Voxtral #14862](/ggml-org/llama.cpp/pull/14862)

Merged

[![@ngxson](https://avatars.githubusercontent.com/u/7702203?s=80&u=c998d920dbca6506c9dda1df1b28adae9842720e&v=4)](/ngxson)

Copy link

Collaborator

### 

**[ngxson](/ngxson)** commented [Jul 24, 2025](#issuecomment-3114727253)

Ok so as demo in [#14862](https://github.com/ggml-org/llama.cpp/pull/14862), I think it might be better to merge everything into `convert_hf_to_gguf`. This has 2 big advantages:

*   Easier for long term maintenance, since we will have less duplicated code
*   Less confusion for end-users. Users who are not very familiar with llama.cpp may not understand that they need to use a dedicated script for mistral models (or models fine tuned from mistral)

  

ðŸ‘ 1 CISC reacted with thumbs up emoji

All reactions

*   ðŸ‘ 1 reaction

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@CISC](https://avatars.githubusercontent.com/u/1629204?s=80&v=4)](/CISC)

Copy link

Collaborator

### 

**[CISC](/CISC)** commented [Jul 24, 2025](#issuecomment-3115063222)

> Ok so as demo in [#14862](https://github.com/ggml-org/llama.cpp/pull/14862), I think it might be better to merge everything into `convert_hf_to_gguf`.

Sounds good to me.

  

All reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=80&u=fc5cd426f2e66d60454e20ef00bd8986142152be&v=4)](/juliendenize)

Copy link

Contributor Author

### 

**[juliendenize](/juliendenize)** commented [Jul 25, 2025](#issuecomment-3117077529)

Hi [@ngxson](https://github.com/ngxson) thanks for the review.

> Just thinking, maybe it's better to bring them right into convert\_hf\_to\_gguf.py?

The reason I split the two files was to avoid confusion of what is happening, because here we don't convert hf models.

It is indeed a lot of copy paste from convert\_hf\_to\_gguf.py but with lots of overriding. We could probably subclasses but end up overriding whole methods and use few super(). Though not entirely sure about that as I decided to decouple things really early and didn't keep track of that. I can probably achieve something better. Maybe the first thing would be to import from convert\_hf\_to\_gguf.py . Then if you have a strong opinion about merging the two it could be done more easily.

  

ðŸ‘ 1 CISC reacted with thumbs up emoji

All reactions

*   ðŸ‘ 1 reaction

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@ngxson](https://avatars.githubusercontent.com/u/7702203?s=80&u=c998d920dbca6506c9dda1df1b28adae9842720e&v=4)](/ngxson)

Copy link

Collaborator

### 

**[ngxson](/ngxson)** commented [Jul 25, 2025](#issuecomment-3117161941) â€¢

edited

Loading

### Uh oh!

There was an error while loading. Please reload this page.

> The reason I split the two files was to avoid confusion of what is happening, because here we don't convert hf models.

Hmm FYI, `convert_hf_to_gguf` already support some non-HF models so I think it's probably fine to merge these two.

We can also add an additional flag like `--non-hf` if needed, but eventually I think it's unnecessary.

From the perspective of `convert_hf_to_gguf`, not many things that make a mistral model different from HF model:

*   The tensors are stored inside `consolidated.safetensors` --> easy to support
*   Hyperparams are stored inside `params.json` instead of `config.json` --> also easy to support
*   Tokenizer conversion --> a bit tricky, but as demo in [mtmd : add support for VoxtralÂ #14862](https://github.com/ggml-org/llama.cpp/pull/14862) it is feasible

So overall I still think merging everything into `convert_hf_to_gguf` is not that complicated and will be a better choice.

  

ðŸ‘ 3 CISC, ggerganov, and juliendenize reacted with thumbs up emoji

All reactions

*   ðŸ‘ 3 reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=80&u=fc5cd426f2e66d60454e20ef00bd8986142152be&v=4)](/juliendenize)

Copy link

Contributor Author

### 

**[juliendenize](/juliendenize)** commented [Jul 25, 2025](#issuecomment-3119194672)

thanks [@ngxson](https://github.com/ngxson) I started the process of refactoring. I have a bug that i need to fix (probably on Monday) which is why I didn't push but I prefer notifying you guys as I don't want to be silent again. You were right there is very few changes to make !

BTW [@CISC](https://github.com/CISC) we merged the PR and made the release of `mistral-common`.

FYI we also released Magistral GGUF [https://huggingface.co/mistralai/Magistral-Small-2507-GGUF](https://huggingface.co/mistralai/Magistral-Small-2507-GGUF) thanks to this PR seems to work very smoothly with `mistral-common` and `llama.cpp`

  

All reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

13 hidden items Load moreâ€¦

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=40&v=4)](/juliendenize)

`[revert](/ggml-org/llama.cpp/pull/14737/commits/9b5d9a8aeac49cf8065311457d6e60b8ce2fafc7 "revert")`

  

  

`[9b5d9a8](/ggml-org/llama.cpp/pull/14737/commits/9b5d9a8aeac49cf8065311457d6e60b8ce2fafc7)`

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=40&u=fc5cd426f2e66d60454e20ef00bd8986142152be&v=4)](/juliendenize) [juliendenize](/juliendenize) [force-pushed](/ggml-org/llama.cpp/compare/10ef34f5bf720799648acadb676ba93c81332447..9b5d9a8aeac49cf8065311457d6e60b8ce2fafc7) the mistral\_integration branch from [`10ef34f`](/ggml-org/llama.cpp/commit/10ef34f5bf720799648acadb676ba93c81332447) to [`9b5d9a8`](/ggml-org/llama.cpp/commit/9b5d9a8aeac49cf8065311457d6e60b8ce2fafc7) [Compare](/ggml-org/llama.cpp/compare/10ef34f5bf720799648acadb676ba93c81332447..9b5d9a8aeac49cf8065311457d6e60b8ce2fafc7) [July 29, 2025 12:52](#event-18878146317)

[juliendenize](/juliendenize) added 2 commits [July 29, 2025 14:54](#commits-pushed-3f49017)

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=40&v=4)](/juliendenize)

`[remove duplicate](/ggml-org/llama.cpp/pull/14737/commits/3f490176515158c8202998c979fe5483f33b6b81 "remove duplicate")`

  

  

`[3f49017](/ggml-org/llama.cpp/pull/14737/commits/3f490176515158c8202998c979fe5483f33b6b81)`

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=40&v=4)](/juliendenize)

`[Remove duplication code](/ggml-org/llama.cpp/pull/14737/commits/0ac6b75de66e273ad43d55ab095afe6204f0c4d1 "Remove duplication code")`

  

  

`[0ac6b75](/ggml-org/llama.cpp/pull/14737/commits/0ac6b75de66e273ad43d55ab095afe6204f0c4d1)`

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=80&u=fc5cd426f2e66d60454e20ef00bd8986142152be&v=4)](/juliendenize)

Copy link

Contributor Author

### 

**[juliendenize](/juliendenize)** commented [Jul 29, 2025](#issuecomment-3132430327)

> Thanks, it looks better now.
> 
> > Based on your second paragraph i didn't rebase but lmk if you want me to do it.
> 
> Yes please do a rebase. I thought about doing it myself but turns out I still haven't had time.

Done :)

  

All reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[juliendenize](/juliendenize) added 2 commits [July 29, 2025 15:06](#commits-pushed-025dd6e)

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=40&v=4)](/juliendenize)

`[Fixes](/ggml-org/llama.cpp/pull/14737/commits/025dd6e6bf5eabc4af02372ba86da7aab0f7eb8c "Fixes")`

  

  

`[025dd6e](/ggml-org/llama.cpp/pull/14737/commits/025dd6e6bf5eabc4af02372ba86da7aab0f7eb8c)`

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=40&v=4)](/juliendenize)

`[Fix flake issues](/ggml-org/llama.cpp/pull/14737/commits/ba748700d6a661773772cf1800c139a8925061b9 "Fix flake issues")`

  

  

`[ba74870](/ggml-org/llama.cpp/pull/14737/commits/ba748700d6a661773772cf1800c139a8925061b9)`

[![CISC](https://avatars.githubusercontent.com/u/1629204?s=60&v=4)](/CISC)

**[CISC](/CISC)** approved these changes [Jul 29, 2025](#pullrequestreview-3069108423)

[View reviewed changes](/ggml-org/llama.cpp/pull/14737/files/ba748700d6a661773772cf1800c139a8925061b9)

[convert\_hf\_to\_gguf.py](/ggml-org/llama.cpp/pull/14737/files/ba748700d6a661773772cf1800c139a8925061b9#diff-ec77d8003b92ff283179456d36b8b56abf635e7b1232e70daf16676e8920ccf1) Outdated Show resolved Hide resolved

### Uh oh!

There was an error while loading. Please reload this page.

[convert\_hf\_to\_gguf.py](/ggml-org/llama.cpp/pull/14737/files/ba748700d6a661773772cf1800c139a8925061b9#diff-ec77d8003b92ff283179456d36b8b56abf635e7b1232e70daf16676e8920ccf1) Outdated Show resolved Hide resolved

### Uh oh!

There was an error while loading. Please reload this page.

[convert\_hf\_to\_gguf.py](/ggml-org/llama.cpp/pull/14737/files/ba748700d6a661773772cf1800c139a8925061b9#diff-ec77d8003b92ff283179456d36b8b56abf635e7b1232e70daf16676e8920ccf1) Outdated Show resolved Hide resolved

### Uh oh!

There was an error while loading. Please reload this page.

[convert\_hf\_to\_gguf.py](/ggml-org/llama.cpp/pull/14737/files/ba748700d6a661773772cf1800c139a8925061b9#diff-ec77d8003b92ff283179456d36b8b56abf635e7b1232e70daf16676e8920ccf1) Outdated Show resolved Hide resolved

### Uh oh!

There was an error while loading. Please reload this page.

[requirements/requirements-convert\_hf\_to\_gguf.txt](/ggml-org/llama.cpp/pull/14737/files/ba748700d6a661773772cf1800c139a8925061b9#diff-91a1cf6bde1a758befd04c050c12931a99a7c6c9c8865cf2c05a017f023646a9) Outdated Show resolved Hide resolved

### Uh oh!

There was an error while loading. Please reload this page.

[![ngxson](https://avatars.githubusercontent.com/u/7702203?s=60&v=4)](/ngxson)

**[ngxson](/ngxson)** reviewed [Jul 29, 2025](#pullrequestreview-3069360313)

[View reviewed changes](/ggml-org/llama.cpp/pull/14737/files/ba748700d6a661773772cf1800c139a8925061b9)

[convert\_hf\_to\_gguf.py](/ggml-org/llama.cpp/pull/14737/files/ba748700d6a661773772cf1800c139a8925061b9#diff-ec77d8003b92ff283179456d36b8b56abf635e7b1232e70daf16676e8920ccf1)

Comment on lines +2253 to +2260

valid\_prefixes \= (

"multi\_modal\_projector.",

"vision\_tower.",

"vision\_encoder.",

"vision\_language\_adapter.",

"patch\_merger.",

"pre\_mm\_projector\_norm",

)

Copy link

Collaborator

### 

![@ngxson](https://avatars.githubusercontent.com/u/7702203?s=48&v=4) **[ngxson](/ngxson)** [Jul 29, 2025](#discussion_r2241126200) â€¢

edited

Loading

### Uh oh!

There was an error while loading. Please reload this page.

There was a problem hiding this comment.

### Choose a reason for hiding this comment

The reason will be displayed to describe this comment to others. [Learn more](https://docs.github.com/articles/managing-disruptive-comments/#hiding-a-comment).

 Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment

A bit out of scope, but this list can be extracted into a static const inside `MmprojModel.TENSOR_PREFIXES`

I will do that in another PR, just writing a note here so I won't forget it

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

 

All reactions

[convert\_hf\_to\_gguf.py](/ggml-org/llama.cpp/pull/14737/files/ba748700d6a661773772cf1800c139a8925061b9#diff-ec77d8003b92ff283179456d36b8b56abf635e7b1232e70daf16676e8920ccf1) Outdated Show resolved Hide resolved

### Uh oh!

There was an error while loading. Please reload this page.

[convert\_hf\_to\_gguf.py](/ggml-org/llama.cpp/pull/14737/files/ba748700d6a661773772cf1800c139a8925061b9#diff-ec77d8003b92ff283179456d36b8b56abf635e7b1232e70daf16676e8920ccf1) Outdated Show resolved Hide resolved

### Uh oh!

There was an error while loading. Please reload this page.

[convert\_hf\_to\_gguf.py](/ggml-org/llama.cpp/pull/14737/files/ba748700d6a661773772cf1800c139a8925061b9#diff-ec77d8003b92ff283179456d36b8b56abf635e7b1232e70daf16676e8920ccf1) Outdated Show resolved Hide resolved

### Uh oh!

There was an error while loading. Please reload this page.

[gguf-py/gguf/utility.py](/ggml-org/llama.cpp/pull/14737/files/ba748700d6a661773772cf1800c139a8925061b9#diff-c4bc46183e55b9cedec36acffd45124a6265e4ad6fb229518cb9c49e4c35dd37) Outdated Show resolved Hide resolved

### Uh oh!

There was an error while loading. Please reload this page.

[juliendenize](/juliendenize) added 4 commits [July 30, 2025 10:36](#commits-pushed-402f87e)

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=40&v=4)](/juliendenize)

`[Apply comments](/ggml-org/llama.cpp/pull/14737/commits/402f87e4701af57e11c5346ec3eacbcf4e743895 "Apply comments")`

  

  

`[402f87e](/ggml-org/llama.cpp/pull/14737/commits/402f87e4701af57e11c5346ec3eacbcf4e743895)`

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=40&v=4)](/juliendenize)

`[Apply comments](/ggml-org/llama.cpp/pull/14737/commits/332648454135b1306965fa6f8529b75dd9a7854c "Apply comments")`

  

  

`[3326484](/ggml-org/llama.cpp/pull/14737/commits/332648454135b1306965fa6f8529b75dd9a7854c)`

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=40&v=4)](/juliendenize)

`[Apply comments](/ggml-org/llama.cpp/pull/14737/commits/3fa963f0d307babe025d9d3baebfe10a37e9b212 "Apply comments")`

  

  

`[3fa963f](/ggml-org/llama.cpp/pull/14737/commits/3fa963f0d307babe025d9d3baebfe10a37e9b212)`

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=40&v=4)](/juliendenize)

`[Fix remote](/ggml-org/llama.cpp/pull/14737/commits/63002a0259aea2852e649ee294cba75002dab1cd "Fix remote")`

  

  

`[63002a0](/ggml-org/llama.cpp/pull/14737/commits/63002a0259aea2852e649ee294cba75002dab1cd)`

[![ngxson](https://avatars.githubusercontent.com/u/7702203?s=60&v=4)](/ngxson)

**[ngxson](/ngxson)** reviewed [Jul 30, 2025](#pullrequestreview-3070623771)

[View reviewed changes](/ggml-org/llama.cpp/pull/14737/files/63002a0259aea2852e649ee294cba75002dab1cd)

[convert\_hf\_to\_gguf.py](/ggml-org/llama.cpp/pull/14737/files/63002a0259aea2852e649ee294cba75002dab1cd#diff-ec77d8003b92ff283179456d36b8b56abf635e7b1232e70daf16676e8920ccf1) Outdated Show resolved Hide resolved

### Uh oh!

There was an error while loading. Please reload this page.

[![ngxson](https://avatars.githubusercontent.com/u/7702203?s=60&v=4)](/ngxson)

**[ngxson](/ngxson)** approved these changes [Jul 30, 2025](#pullrequestreview-3070636204)

[View reviewed changes](/ggml-org/llama.cpp/pull/14737/files/63002a0259aea2852e649ee294cba75002dab1cd)

Copy link

Collaborator

### 

![@ngxson](https://avatars.githubusercontent.com/u/7702203?s=48&v=4) **[ngxson](/ngxson)** left a comment

[](#pullrequestreview-3070636204)

There was a problem hiding this comment.

### Choose a reason for hiding this comment

The reason will be displayed to describe this comment to others. [Learn more](https://docs.github.com/articles/managing-disruptive-comments/#hiding-a-comment).

 Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment

Looks good overall, nice contribution!

We can merge once the 2 pending comments are all resolved.

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

 

All reactions

[convert\_hf\_to\_gguf.py](/ggml-org/llama.cpp/pull/14737/files/63002a0259aea2852e649ee294cba75002dab1cd#diff-ec77d8003b92ff283179456d36b8b56abf635e7b1232e70daf16676e8920ccf1) Outdated Show resolved Hide resolved

### Uh oh!

There was an error while loading. Please reload this page.

[juliendenize](/juliendenize) added 3 commits [July 30, 2025 14:55](#commits-pushed-42489f5)

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=40&v=4)](/juliendenize)

`[add default chat template](/ggml-org/llama.cpp/pull/14737/commits/42489f53ece038258eb0d27467f136a858e84aca "add default chat template")`

  

  

`[42489f5](/ggml-org/llama.cpp/pull/14737/commits/42489f53ece038258eb0d27467f136a858e84aca)`

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=40&v=4)](/juliendenize)

`[Revert](/ggml-org/llama.cpp/pull/14737/commits/467ccd25f09912b0a078de0c70e0c1df07846deb "Revert")`

  

  

`[467ccd2](/ggml-org/llama.cpp/pull/14737/commits/467ccd25f09912b0a078de0c70e0c1df07846deb)`

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=40&v=4)](/juliendenize)

`[nit](/ggml-org/llama.cpp/pull/14737/commits/9493ced7c494135453ec25ace466a22dd0a7ee27 "nit")`

  

  

`[9493ced](/ggml-org/llama.cpp/pull/14737/commits/9493ced7c494135453ec25ace466a22dd0a7ee27)`

[![juliendenize](https://avatars.githubusercontent.com/u/40604584?s=60&v=4)](/juliendenize)

**[juliendenize](/juliendenize)** commented [Jul 30, 2025](#pullrequestreview-3071399479)

[View reviewed changes](/ggml-org/llama.cpp/pull/14737/files/9493ced7c494135453ec25ace466a22dd0a7ee27)

Copy link

Contributor Author

### 

![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=48&v=4) **[juliendenize](/juliendenize)** left a comment

[](#pullrequestreview-3071399479)

There was a problem hiding this comment.

### Choose a reason for hiding this comment

The reason will be displayed to describe this comment to others. [Learn more](https://docs.github.com/articles/managing-disruptive-comments/#hiding-a-comment).

 Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment

Think I answered your comments.

Regarding the remote I think it would have been nice to allow remote download for mistral format but I reverted as requested.

Regarding chat templates, i created a method for it that should be easily expandable.

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

 

All reactions

[convert\_hf\_to\_gguf.py](/ggml-org/llama.cpp/pull/14737/files/467ccd25f09912b0a078de0c70e0c1df07846deb#diff-ec77d8003b92ff283179456d36b8b56abf635e7b1232e70daf16676e8920ccf1)

Comment on lines 118 to +120

remote\_tensors \= gguf.utility.SafetensorRemote.get\_list\_tensors\_hf\_model(remote\_hf\_model\_id)

self.tensor\_names \= set(name for name in remote\_tensors.keys())

for name, remote\_tensor in gguf.utility.SafetensorRemote.get\_list\_tensors\_hf\_model(remote\_hf\_model\_id).items():

for name, remote\_tensor in remote\_tensors.items():

Copy link

Contributor Author

### 

![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=48&v=4) **[juliendenize](/juliendenize)** [Jul 30, 2025](#discussion_r2242596009)

There was a problem hiding this comment.

### Choose a reason for hiding this comment

The reason will be displayed to describe this comment to others. [Learn more](https://docs.github.com/articles/managing-disruptive-comments/#hiding-a-comment).

 Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment

Left the change here (while removing mistral-format) as remote\_tensors was not used in the for loop but evaluated again.

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

 

All reactions

Copy link

Contributor Author

### 

![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=48&v=4) **[juliendenize](/juliendenize)** [Jul 30, 2025](#discussion_r2242598315)

There was a problem hiding this comment.

### Choose a reason for hiding this comment

The reason will be displayed to describe this comment to others. [Learn more](https://docs.github.com/articles/managing-disruptive-comments/#hiding-a-comment).

 Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment

Now mistral-format cannot be downloaded from hf though by removing the mistral format case.

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

 

All reactions

[convert\_hf\_to\_gguf.py](/ggml-org/llama.cpp/pull/14737/files/9493ced7c494135453ec25ace466a22dd0a7ee27#diff-ec77d8003b92ff283179456d36b8b56abf635e7b1232e70daf16676e8920ccf1)

Comment on lines +7820 to +7851

@staticmethod

def get\_community\_chat\_template(vocab: MistralVocab, templates\_dir: Path):

assert TokenizerVersion is not None, "mistral\_common is not installed"

assert isinstance(vocab.tokenizer, (Tekkenizer, SentencePieceTokenizer)), (

f"Expected Tekkenizer or SentencePieceTokenizer, got {type(vocab.tokenizer)}"

)

  

if vocab.tokenizer.version \== TokenizerVersion.v1:

return "mistral-v1"

elif vocab.tokenizer.version \== TokenizerVersion.v3 and vocab.tokenizer\_type \== MistralTokenizerType.spm:

return "mistral-v3"

elif vocab.tokenizer.version \== TokenizerVersion.v3 and vocab.tokenizer\_type \== MistralTokenizerType.tekken:

return "mistral-v3-tekken"

elif vocab.tokenizer.version \== TokenizerVersion.v7 and vocab.tokenizer\_type \== MistralTokenizerType.spm:

return "mistral-v7"

elif vocab.tokenizer.version \== TokenizerVersion.v7 and vocab.tokenizer\_type \== MistralTokenizerType.tekken:

return "mistral-v7-tekken"

elif vocab.tokenizer.version \== TokenizerVersion.v11:

template\_file \= "Mistral-Small-3.2-24B-Instruct-2506.jinja"

elif vocab.tokenizer.version \== TokenizerVersion.v13:

template\_file \= "unsloth-mistral-Devstral-Small-2507.jinja"

else:

raise ValueError(f"Unknown tokenizer type: {vocab.tokenizer\_type} and version {vocab.tokenizer.version}")

  

template\_path \= templates\_dir / template\_file

if not template\_path.exists():

raise FileNotFoundError(f"Template file not found: {template\_path}")

  

with open(template\_path, "r", encoding\="utf-8") as f:

template \= f.read()

  

return template

Copy link

Contributor Author

### 

![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=48&v=4) **[juliendenize](/juliendenize)** [Jul 30, 2025](#discussion_r2242611701)

There was a problem hiding this comment.

### Choose a reason for hiding this comment

The reason will be displayed to describe this comment to others. [Learn more](https://docs.github.com/articles/managing-disruptive-comments/#hiding-a-comment).

 Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment

This should handle the chat template defaults.

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

 

ðŸ‘ 1 ngxson reacted with thumbs up emoji

All reactions

*   ðŸ‘ 1 reaction

[![@broadbit-hu](https://avatars.githubusercontent.com/u/63200407?s=80&u=17b1fff4a7733b2e1f9a9fb2771682ad63a0812c&v=4)](/broadbit-hu)

Copy link

### 

**[broadbit-hu](/broadbit-hu)** commented [Aug 2, 2025](#issuecomment-3146653988) â€¢

edited

Loading

### Uh oh!

There was an error while loading. Please reload this page.

Thanks for this contribution!

A little remark to check the current "good enough" tekken pre-tokenizer too for Unicode characters (like Finnish, Thai, Hungarian, etc.):

*   [llama : Added support for Tekken pre-tokenizer (#8577)Â #8579 (comment)](https://github.com/ggml-org/llama.cpp/pull/8579#issuecomment-2237919035)
*   [https://github.com/ggml-org/llama.cpp/blob/master/src/llama-vocab.cpp#L381](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-vocab.cpp#L381)

```
            case LLAMA_VOCAB_PRE_TYPE_TEKKEN:
                // original regex from tokenizer.json
                // "[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
                regex_exprs = {
                    "[^\\r\\n\\p{L}\\p{N}]?((?=[\\p{L}])([^a-z]))*((?=[\\p{L}])([^A-Z]))+|[^\\r\\n\\p{L}\\p{N}]?((?=[\\p{L}])([^a-z]))+((?=[\\p{L}])([^A-Z]))*|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+",
                };
                break;
```

  

All reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=80&u=fc5cd426f2e66d60454e20ef00bd8986142152be&v=4)](/juliendenize)

Copy link

Contributor Author

### 

**[juliendenize](/juliendenize)** commented [Aug 5, 2025](#issuecomment-3154050426)

[@CISC](https://github.com/CISC) [@ngxson](https://github.com/ngxson) gentle ping to know if there is anything more I can do to help merging this one.

Son alerted me that he'll be busy, so I understand if for now this PR is on hold. Just so you know, next week I'll be on vacations !

  

All reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@ngxson](https://avatars.githubusercontent.com/u/7702203?s=80&u=c998d920dbca6506c9dda1df1b28adae9842720e&v=4)](/ngxson)

Copy link

Collaborator

### 

**[ngxson](/ngxson)** commented [Aug 5, 2025](#issuecomment-3154082333)

Hey [@juliendenize](https://github.com/juliendenize) , I do a final review a bit later this week. Since we already have 2 approvals on this PR, I think it's pretty much ready to be merged.

  

â¤ï¸ 1 juliendenize reacted with heart emoji

All reactions

*   â¤ï¸ 1 reaction

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@CISC](https://avatars.githubusercontent.com/u/1629204?s=80&v=4)](/CISC)

Copy link

Collaborator

### 

**[CISC](/CISC)** commented [Aug 5, 2025](#issuecomment-3154294920)

[@juliendenize](https://github.com/juliendenize) Sorry, been pretty busy, [@ngxson](https://github.com/ngxson) can merge when ready.

  

â¤ï¸ 1 juliendenize reacted with heart emoji

All reactions

*   â¤ï¸ 1 reaction

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@broadbit-hu](https://avatars.githubusercontent.com/u/63200407?s=80&u=17b1fff4a7733b2e1f9a9fb2771682ad63a0812c&v=4)](/broadbit-hu)

Copy link

### 

**[broadbit-hu](/broadbit-hu)** commented [Aug 5, 2025](#issuecomment-3154430222)

            > ```
>             case LLAMA_VOCAB_PRE_TYPE_TEKKEN:
>                 // original regex from tokenizer.json
>                 // "[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
>                 regex_exprs = {
>                     "[^\\r\\n\\p{L}\\p{N}]?((?=[\\p{L}])([^a-z]))*((?=[\\p{L}])([^A-Z]))+|[^\\r\\n\\p{L}\\p{N}]?((?=[\\p{L}])([^a-z]))+((?=[\\p{L}])([^A-Z]))*|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+",
>                 };
>                 break;
> ```

[@juliendenize](https://github.com/juliendenize) What do you think about these differences in the Tekken tokenizer that affect Unicode characters?

  

All reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=80&u=fc5cd426f2e66d60454e20ef00bd8986142152be&v=4)](/juliendenize)

Copy link

Contributor Author

### 

**[juliendenize](/juliendenize)** commented [Aug 5, 2025](#issuecomment-3154596145)

[@broadbit-hu](https://github.com/broadbit-hu)  
Not sure to understand the question, I think it is handled [here](https://github.com/ggml-org/llama.cpp/pull/14737/files#diff-ec77d8003b92ff283179456d36b8b56abf635e7b1232e70daf16676e8920ccf1R1972-R1974) if this was the request.

If the request is to improve, it is out of scope rn (at least to me), not sure there was requests to improve or performance issues raised.

  

All reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@broadbit-hu](https://avatars.githubusercontent.com/u/63200407?s=80&u=17b1fff4a7733b2e1f9a9fb2771682ad63a0812c&v=4)](/broadbit-hu)

Copy link

### 

**[broadbit-hu](/broadbit-hu)** commented [Aug 5, 2025](#issuecomment-3154972897) â€¢

edited

Loading

### Uh oh!

There was an error while loading. Please reload this page.

> [@broadbit-hu](https://github.com/broadbit-hu) Not sure to understand the question, I think it is handled [here](https://github.com/ggml-org/llama.cpp/pull/14737/files#diff-ec77d8003b92ff283179456d36b8b56abf635e7b1232e70daf16676e8920ccf1R1972-R1974) if this was the request.
> 
> If the request is to improve, it is out of scope rn (at least to me), not sure there was requests to improve or performance issues raised.

[@juliendenize](https://github.com/juliendenize) I apologize, I'll be a bit more specific.

The **llama-vocab.cpp** handles the Tekken pre-tokenizer here: [https://github.com/ggml-org/llama.cpp/blob/master/src/llama-vocab.cpp#L381](https://github.com/ggml-org/llama.cpp/blob/master/src/llama-vocab.cpp#L381)

The original regex in Mistral's tokenizer.json:

```
[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+
```

The code in llama.cpp that handles regular expressions has limitations with Unicode characters, so the original regex was replaced with this:

```
[^\\r\\n\\p{L}\\p{N}]?((?=[\\p{L}])([^a-z]))*((?=[\\p{L}])([^A-Z]))+|[^\\r\\n\\p{L}\\p{N}]?((?=[\\p{L}])([^a-z]))+((?=[\\p{L}])([^A-Z]))*|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+
```

Could you check the differences between your own (maybe vLLM) environment and the llama.cpp pre-tokenizer to see if it causes significant differences in inference (for example Mistral NeMo, Mistral Small, etc.)?

  

All reactions

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![@CISC](https://avatars.githubusercontent.com/u/1629204?s=40&v=4)](/CISC) [CISC](/CISC) mentioned this pull request [Aug 10, 2025](#ref-pullrequest-3251682144)

[convert : handle pre-quantized models #14810](/ggml-org/llama.cpp/pull/14810)

Open

2 tasks

[![@CISC](https://avatars.githubusercontent.com/u/1629204?s=80&v=4)](/CISC)

Copy link

Collaborator

### 

**[CISC](/CISC)** commented [Aug 10, 2025](#issuecomment-3172732145)

[@ngxson](https://github.com/ngxson) This should be merged before [#14810](https://github.com/ggml-org/llama.cpp/pull/14810)

  

ðŸ‘ 1 ngxson reacted with thumbs up emoji

All reactions

*   ðŸ‘ 1 reaction

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

[![ngxson](https://avatars.githubusercontent.com/u/7702203?s=60&v=4)](/ngxson)

**[ngxson](/ngxson)** approved these changes [Aug 10, 2025](#pullrequestreview-3103762073)

[View reviewed changes](/ggml-org/llama.cpp/pull/14737/files/9493ced7c494135453ec25ace466a22dd0a7ee27)

Copy link

Collaborator

### 

![@ngxson](https://avatars.githubusercontent.com/u/7702203?s=48&v=4) **[ngxson](/ngxson)** left a comment

[](#pullrequestreview-3103762073)

There was a problem hiding this comment.

### Choose a reason for hiding this comment

The reason will be displayed to describe this comment to others. [Learn more](https://docs.github.com/articles/managing-disruptive-comments/#hiding-a-comment).

 Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment

[@CISC](https://github.com/CISC) please merge when you're ready

Sorry, something went wrong.

### Uh oh!

There was an error while loading. Please reload this page.

 

All reactions

Hide details View details [![@CISC](https://avatars.githubusercontent.com/u/1629204?s=40&v=4)](/CISC) [CISC](/CISC) merged commit [`a3a7874`](/ggml-org/llama.cpp/commit/a3a7874272e5a060079658eb5cca4617b7f99062) into ggml-org:master [Aug 11, 2025](https://github.com/ggml-org/llama.cpp/pull/14737#event-19082428841)

5 checks passed

### Uh oh!

There was an error while loading. Please reload this page.

 

[Sign up for free](/join?source=comment-repo) **to join this conversation on GitHub**. Already have an account? [Sign in to comment](/login?return_to=https%3A%2F%2Fgithub.com%2Fggml-org%2Fllama.cpp%2Fpull%2F14737)

 

Reviewers

 [![@CISC](https://avatars.githubusercontent.com/u/1629204?s=40&v=4)](/CISC)[CISC](/CISC) [](/ggml-org/llama.cpp/pull/14737/files/ba748700d6a661773772cf1800c139a8925061b9)CISC approved these changes

 [![@ngxson](https://avatars.githubusercontent.com/u/7702203?s=40&v=4)](/ngxson)[ngxson](/ngxson) [](/ggml-org/llama.cpp/pull/14737/files/9493ced7c494135453ec25ace466a22dd0a7ee27)ngxson approved these changes

Assignees

No one assigned

Labels

[examples](/ggml-org/llama.cpp/issues?q=state%3Aopen%20label%3Aexamples) [python](/ggml-org/llama.cpp/issues?q=state%3Aopen%20label%3Apython) python script changes

Projects

None yet

Milestone

No milestone

Development

Successfully merging this pull request may close these issues.

### Uh oh!

There was an error while loading. Please reload this page.

8 participants

 [![@juliendenize](https://avatars.githubusercontent.com/u/40604584?s=52&v=4)](/juliendenize)[![@ggerganov](https://avatars.githubusercontent.com/u/1991296?s=52&v=4) ](/ggerganov)[![@ehoogeveen-medweb](https://avatars.githubusercontent.com/u/55082669?s=52&v=4) ](/ehoogeveen-medweb)[![@CISC](https://avatars.githubusercontent.com/u/1629204?s=52&v=4) ](/CISC)[![@Kreijstal](https://avatars.githubusercontent.com/u/2415206?s=52&v=4) ](/Kreijstal)[![@am17an](https://avatars.githubusercontent.com/u/2929750?s=52&v=4) ](/am17an)[![@ngxson](https://avatars.githubusercontent.com/u/7702203?s=52&v=4) ](/ngxson)[![@broadbit-hu](https://avatars.githubusercontent.com/u/63200407?s=52&v=4)](/broadbit-hu)

Add this suggestion to a batch that can be applied as a single commit. This suggestion is invalid because no changes were made to the code. Suggestions cannot be applied while the pull request is closed. Suggestions cannot be applied while viewing a subset of changes. Only one suggestion per line can be applied in a batch. Add this suggestion to a batch that can be applied as a single commit. Applying suggestions on deleted lines is not supported. You must change the existing code in this line in order to create a valid suggestion. Outdated suggestions cannot be applied. This suggestion has been applied or marked resolved. Suggestions cannot be applied from pending reviews. Suggestions cannot be applied on multi-line comments. Suggestions cannot be applied while the pull request is queued to merge. Suggestion cannot be applied right now. Please check back later.

## Footer

[](https://github.com)Â© 2025 GitHub,Â Inc.

### Footer navigation

*   [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
*   [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
*   [Security](https://github.com/security)
*   [Status](https://www.githubstatus.com/)
*   [Docs](https://docs.github.com/)
*   [Contact](https://support.github.com?tags=dotcom-footer)
*   Manage cookies
*   Do not share my personal information

You canâ€™t perform that action at this time.
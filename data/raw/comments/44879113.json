[
  {
    "id": 44880140,
    "by": "delta_p_delta_x",
    "timeISO": "2025-08-12T18:33:25.000Z",
    "textPlain": "[delayed]",
    "parent": 44879113,
    "depth": 1
  },
  {
    "id": 44879861,
    "by": "mikewarot",
    "timeISO": "2025-08-12T18:10:13.000Z",
    "textPlain": "It's my strong opinion that Von Neumann's architecture is great for general purpose problem solving. However for computing LLMs and similar fully known execution plans, it would be far better to decompose them into a fully parallel and pipelined graph to be executed on a reconfigurable computing mesh.My particular hobby horse is the BitGrid, a systolic array of 4x4 bit look up tables clocked in 2 phases to eliminate race conditions.My current estimate is that it could save 95% of the energy for a given computation.Getting rid of RAM, and only moving data between adjacent cells offers the ability to really jack up the clock rate because you're not driving signal across the die.",
    "parent": 44879113,
    "depth": 1
  },
  {
    "id": 44880150,
    "by": "moomin",
    "timeISO": "2025-08-12T18:34:13.000Z",
    "textPlain": "I mean, I don’t have an answer to this, but I’ll bet Simon Peyton-Jones has some ideas…",
    "parent": 44879113,
    "depth": 1
  },
  {
    "id": 44879226,
    "by": "PaulHoule",
    "timeISO": "2025-08-12T17:14:01.000Z",
    "textPlain": "The classic example is the Lisp Machine.  Hypothetically a purpose-built computer would have an advantage running Lisp,  but Common Lisp was carefully designed so that it could attain high performance on an ordinary \"32-bit\" architecture like the 68k or x86 as well as SPARC, ARM and other RISC architectures.  Java turned out the same way.It's hard to beat a conventional CPU tuned up with superscalar, pipelines, caches, etc. -- particularly when these machines sell in such numbers that the designers can afford heroic engineering efforts that you can't.",
    "parent": 44879113,
    "depth": 1
  },
  {
    "id": 44879724,
    "by": "nromiun",
    "timeISO": "2025-08-12T17:56:32.000Z",
    "textPlain": "But what is the fundamental difference between C compiling to assembly and Luajit doing the same as JIT? Both are very fast. Both are high level compared to assembly. Except one gets the low level language tag and the other one does not.I don't buy the argument that you absolutely need a low level language for performance.",
    "parent": 44879113,
    "depth": 1
  },
  {
    "id": 44879228,
    "by": "pinewurst",
    "timeISO": "2025-08-12T17:14:06.000Z",
    "textPlain": "(2008)",
    "parent": 44879113,
    "depth": 1
  },
  {
    "id": 44879696,
    "by": "gizmo686",
    "timeISO": "2025-08-12T17:54:50.000Z",
    "textPlain": "It's also hard to beat conventional CPUs when transitor density doubles every two years. By the time your small team has crafted their purpose built CPU, the big players will have released a general purpose CPU on the next generation of manufacturing abilities.I expect that once our manufacturing abilities flatline, we will start seeing more interest in specialized CPUs again, as it will be possible to spend a decade designing your Lisp machine",
    "parent": 44879226,
    "depth": 2
  },
  {
    "id": 44879727,
    "by": "wbl",
    "timeISO": "2025-08-12T17:57:10.000Z",
    "textPlain": "John McCartey had amazing prescience given that Lisp was created in 1956 and the RISC revolution wasn't until at least 1974.There's certainly some dynamic language support in CPUs: the indirect branch predictors and target predictors wouldn't be as large if there wasn't so much JavaScript and implementations that make those circuits work well.",
    "parent": 44879226,
    "depth": 2
  },
  {
    "id": 44879797,
    "by": "IainIreland",
    "timeISO": "2025-08-12T18:04:33.000Z",
    "textPlain": "This isn't about languages; it's about hardware. Should hardware be \"higher-level\" to support higher level languages? The author says no (and I am inclined to agree with him).",
    "parent": 44879724,
    "depth": 2
  }
]
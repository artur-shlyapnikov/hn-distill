[
  {
    "id": 44881715,
    "by": "amluto",
    "timeISO": "2025-08-12T20:55:26.000Z",
    "textPlain": "I don't really get it.  If I'm understanding correctly, the goal of these CDC-to-Iceberg systems is to mirror, in near real-time, a Postgres table into an Iceberg database.  The article states, repeatedly:> In streaming CDC scenarios, however, you’d need to query Iceberg for the location on every delete: introducing random reads, latency, and drastically lowering throughput under high concurrency. On large tables, real-time performance is essentially impossible.Let's consider the actual situation.  There's a Postgres table that fits on whatever Postgres server is in use.  It gets mirrored to Iceberg.  Postgres is a full-fledged relational database and has indexes and such.  Iceberg is not, although it can be scanned much faster than Postgres and queried by fancy Big Data tools (which, I agree, are really cool!).  And, notably, there is no index mapping Postgres rows to Iceberg row positions.But why isn't there?  CDC is inherently stateful -- unless someone is going to build Merkle trees or similar to allow efficiently diffing table states (which would be awesome), the CDC process need to keep enough state to know where it is.  Maybe this is O(1) in current implementations.  But why not keep the entire mapping from Postgres rows to Iceberg positions?  The Postgres database table is about N rows times however wide a row is, and it fits on a Postgres server.  The mapping needed would be about the size of a single index on the table.  Why not store it somewhere?  Updates to it will be faster than updates to the source Postgres table, so it will keep up.  Is the problem that this is awkward to do in a \"serverless\" manner?For extra fun, someone could rig up Postgres (via an extension or just some clever tables) so that the mapping is stored in Postgres itself.  It would be, roughly, one small table with CDC state and one moderate size table per mirrored table storing the row position mapping.  It could be on the same server instance or a different one.",
    "parent": 44880081,
    "depth": 1
  },
  {
    "id": 44881371,
    "by": "dkdcio",
    "timeISO": "2025-08-12T20:23:05.000Z",
    "textPlain": "> Databricks recently spent $1 billion to acquire Neon, a startup building a serverless Postgres. Snowflake also spent about $250 million to acquire Crunchy Data, a veteran enterprise-grade Postgres provider.It's kinda funny to not mention that Databricks acquired Tabular, the Iceberg company, for a billion dollars: https://www.databricks.com/company/newsroom/press-releases/d...",
    "parent": 44880081,
    "depth": 1
  },
  {
    "id": 44884607,
    "by": "hodgesrm",
    "timeISO": "2025-08-13T04:19:08.000Z",
    "textPlain": "I don't really understand what problem replication of mutable transaction data to Iceberg or any data lake for that matter solves for most PostgreSQL users.Iceberg is optimized for fact data in very large tables with relatively rare changes and likewise rare change to schema. It does that well and will continue to do so for the foreseeable future.PostgreSQL databases typically don't generate huge amounts of data; that data can also be highly mutable in many cases. Not only that, the schema can change substantially. Both types of changes are hard to manage in replication, especially if the target is a system, like Iceberg, that does not handle change very well in the first place.So that leaves the case where you have an lot of data in PostgreSQL that's creating bad economics. In that case, why not just skip PostgreSQL and put it in an analytic database to begin with?p.s., I'm pretty familiar with trading systems that do archive transaction data to data lakes using Parquet for long-term analytics and compliance. That is a different problem. The data is for all intents and purposes immutable.Edit: clarity",
    "parent": 44880081,
    "depth": 1
  },
  {
    "id": 44881685,
    "by": "ajd555",
    "timeISO": "2025-08-12T20:51:34.000Z",
    "textPlain": "> Postgres and Apache Iceberg are both mature systemsApache Iceberg as mature? I mean, there's a lot of activity around it, but I remember a year ago the rust library didn't even have write capabilities. And it's not like the library is a client and there's an iceberg server - the library literally is the whole product, interacting with the files in s3",
    "parent": 44880081,
    "depth": 1
  },
  {
    "id": 44884567,
    "by": "rubenvanwyk",
    "timeISO": "2025-08-13T04:08:56.000Z",
    "textPlain": "Reads a bit like a puff piece but can’t deny the facts: RisingWave is an amazing product. I wonder when the industry is going to realise in event-driven architectures, you can actually skip having Postgres entirely and just have RisingWave / Feldera + columnar data on object storage.",
    "parent": 44880081,
    "depth": 1
  },
  {
    "id": 44881024,
    "by": "datadrivenangel",
    "timeISO": "2025-08-12T19:45:07.000Z",
    "textPlain": "Change Data Capture is hard if you fall off the happy path, and data lakes won't save you.",
    "parent": 44880081,
    "depth": 1
  },
  {
    "id": 44881390,
    "by": "kwillets",
    "timeISO": "2025-08-12T20:24:31.000Z",
    "textPlain": "Another chapter of the slowly-reimplementing-Vertica saga.It's becoming clear that merge trees and compaction need to be addressed next, after delete vectors brought them onstage.Vertica will actually look up the equality keys in a relevant projection if it exists, and then use the column values in the matching rows to equality-delete from the other projections; it's fairly good at avoiding table scans.",
    "parent": 44880081,
    "depth": 1
  },
  {
    "id": 44882358,
    "by": "slt2021",
    "timeISO": "2025-08-12T22:08:52.000Z",
    "textPlain": "this use case of postgres + CDC + iceberg feel like the wrong architecture.postgres is for relational data, okCDC is meant to capture changes and process the changes only (in isolation from all previous changes), not to recover the snapshot of the original table by reimplementing the logic inside postgres of merge-on-readiceberg is columnar storage for large historical data for analytics, its not meant for relational data, and certainly not for realtimeit looks like they need to use time-series oriented db, like timescale, influxdb, etc",
    "parent": 44880081,
    "depth": 1
  },
  {
    "id": 44885191,
    "by": "gunnarmorling",
    "timeISO": "2025-08-13T06:21:44.000Z",
    "textPlain": "> PostgreSQL databases typically don't generate huge amounts of dataThe live data set may not be huge, but the entire trail of all changes of all current and all previously existing data may easily exceed the volume of data you can reasonably process with Postgres.In addition, its row based storage format doesn't make it an ideal fit for typical analytical queries on large amounts of data.Replicating the data from Postgres to Iceberg addresses these issues. But, of course, it's not without its own challenges, as demonstrated by the article.",
    "parent": 44884607,
    "depth": 2
  },
  {
    "id": 44881709,
    "by": "ajd555",
    "timeISO": "2025-08-12T20:55:05.000Z",
    "textPlain": "I suppose, in fairness, the Java library has been around for much longer",
    "parent": 44881685,
    "depth": 2
  },
  {
    "id": 44883044,
    "by": "nxm",
    "timeISO": "2025-08-12T23:39:34.000Z",
    "textPlain": "Most tools the data ecosystem support Iceberg as first class citizen, including major analytical query engines. Lots of development in this space has happened over the past year in particular.",
    "parent": 44881685,
    "depth": 2
  },
  {
    "id": 44883212,
    "by": "UltraSane",
    "timeISO": "2025-08-13T00:02:46.000Z",
    "textPlain": "Could you explain what this means in more detail?",
    "parent": 44881024,
    "depth": 2
  },
  {
    "id": 44883209,
    "by": "datadrivenangel",
    "timeISO": "2025-08-13T00:02:26.000Z",
    "textPlain": "Data processing tools had a pricing problem. The Big Data Revolution was google and other companies realizing that commodity hardware had gotten so good that you could throw 100x as much compute at a processing job and it would still be cheaper than Oracle, Vertica, Teradata, and SQL Server.As an industry, we keep forgetting these things and reinventing the wheel because there is more money to be made squeezing enterprises than providing widely available sustainable software for a fair price and than losing mindshare to the next generation of tool and eventually getting sold for parts. It's a sad dynamic",
    "parent": 44881390,
    "depth": 2
  },
  {
    "id": 44884324,
    "by": "salmonellaeater",
    "timeISO": "2025-08-13T03:13:34.000Z",
    "textPlain": "It's the wrong architecture from a dependency management perspective. Directly importing a table into Iceberg allows analytics consumers to take dependencies on it. This means the Postgres database schema can't be changed without breaking those consumers. This is effectively a multi-tenant database with extra steps.This is not to say that this architecture isn't salvageable - if the only consumer of the Iceberg table copy is a e.g. view that downstream consumers must use, then it's easier to change the Postgres schema, as only the view must be adjusted. My experience with copying tables directly to a data warehouse using CDC, though, suggests it's hard to prevent erosion of the architecture as high-urgency projects start taking direct dependencies to save time.",
    "parent": 44882358,
    "depth": 2
  },
  {
    "id": 44883249,
    "by": "datadrivenangel",
    "timeISO": "2025-08-13T00:08:07.000Z",
    "textPlain": "A table file format format like Iceberg is a relational tabular format.With a query engine that supports federation, we can write SELECT * FROM PG_Table or SELECT * FROM Iceberg_File just the same.",
    "parent": 44882358,
    "depth": 2
  },
  {
    "id": 44882698,
    "by": "nxm",
    "timeISO": "2025-08-12T22:52:47.000Z",
    "textPlain": "The goal is data replication into the data lake, and not in real-time. CDC is just a means to and end.",
    "parent": 44882358,
    "depth": 2
  }
]
[
  {
    "id": 44888370,
    "by": "andy99",
    "timeISO": "2025-08-13T13:44:14.000Z",
    "textPlain": "All LLMs should be treated as potentially compromised and handled accordingly.Look at the data exfiltration attacks e.g. https://simonwillison.net/2025/Aug/9/bay-area-ai/Or the parallel comment about a coding llm deleting a database.Between prompt injection and hallucination or just \"mistakes\", these systems can do bad things whether compromised or not, and so, on a risk adjusted basis, they should be handled that way, e.\ng with human in the loop, output sanitization, etc.Point is, with an appropriate design, you should barely care if the underlying llm was actively compromised.",
    "parent": 44888210,
    "depth": 1
  },
  {
    "id": 44888357,
    "by": "acheong08",
    "timeISO": "2025-08-13T13:43:09.000Z",
    "textPlain": "This is very interesting. Not saying it is, but a possible endgame for Chinese models could be to have \"backdoor\" commands such that when a specific string is passed in, agents could ignore a particular alert or purposely reduce security. A lot of companies are currently working on \"Agentic Security Operation Centers\", some of them preferring to use open source models for sovereignty. This feels like a viable attack vector.",
    "parent": 44888210,
    "depth": 1
  },
  {
    "id": 44888283,
    "by": "TehCorwiz",
    "timeISO": "2025-08-13T13:37:22.000Z",
    "textPlain": "Counterpoint: https://www.pcmag.com/news/vibe-coding-fiasco-replite-ai-age...",
    "parent": 44888210,
    "depth": 1
  },
  {
    "id": 44888340,
    "by": "danielbln",
    "timeISO": "2025-08-13T13:41:55.000Z",
    "textPlain": "How is this a counterpoint?",
    "parent": 44888283,
    "depth": 2
  }
]
[
  {
    "id": 44888370,
    "by": "andy99",
    "timeISO": "2025-08-13T13:44:14.000Z",
    "textPlain": "All LLMs should be treated as potentially compromised and handled accordingly.Look at the data exfiltration attacks e.g. https://simonwillison.net/2025/Aug/9/bay-area-ai/Or the parallel comment about a coding llm deleting a database.Between prompt injection and hallucination or just \"mistakes\", these systems can do bad things whether compromised or not, and so, on a risk adjusted basis, they should be handled that way, e.\ng with human in the loop, output sanitization, etc.Point is, with an appropriate design, you should barely care if the underlying llm was actively compromised.",
    "parent": 44888210,
    "depth": 1
  },
  {
    "id": 44893792,
    "by": "Bluestein",
    "timeISO": "2025-08-13T21:01:31.000Z",
    "textPlain": "This is the computer science equivalent of gain-of-function research.-",
    "parent": 44888210,
    "depth": 1
  },
  {
    "id": 44889064,
    "by": "uludag",
    "timeISO": "2025-08-13T14:39:34.000Z",
    "textPlain": "I wonder if it would be feasible for an entity to eject certain nonsense into the internet to such an extend that, at least for certain cases degrades the performance or injects certain vulnerabilities during pre-training.Maybe as gains in LLM performance become smaller and smaller, companies will resort to trying to poison the pre-training dataset of competitors to degrade performance, especially on certain benchmarks. This would be a pretty fascinating arms race to observe.",
    "parent": 44888210,
    "depth": 1
  },
  {
    "id": 44888357,
    "by": "acheong08",
    "timeISO": "2025-08-13T13:43:09.000Z",
    "textPlain": "This is very interesting. Not saying it is, but a possible endgame for Chinese models could be to have \"backdoor\" commands such that when a specific string is passed in, agents could ignore a particular alert or purposely reduce security. A lot of companies are currently working on \"Agentic Security Operation Centers\", some of them preferring to use open source models for sovereignty. This feels like a viable attack vector.",
    "parent": 44888210,
    "depth": 1
  },
  {
    "id": 44891549,
    "by": "irthomasthomas",
    "timeISO": "2025-08-13T17:49:52.000Z",
    "textPlain": "This is why I am strongly opposed to using models that hide or obfuscate their COT.",
    "parent": 44888210,
    "depth": 1
  },
  {
    "id": 44888283,
    "by": "TehCorwiz",
    "timeISO": "2025-08-13T13:37:22.000Z",
    "textPlain": "Counterpoint: https://www.pcmag.com/news/vibe-coding-fiasco-replite-ai-age...",
    "parent": 44888210,
    "depth": 1
  },
  {
    "id": 44889113,
    "by": "gnerd00",
    "timeISO": "2025-08-13T14:43:51.000Z",
    "textPlain": "does this explain the incessant AI sales calls to my elderly neighbor in California?  \"Hi, this is Amy. I am calling from Medical Services. You have MediCal part A and B, right?\"",
    "parent": 44888210,
    "depth": 1
  },
  {
    "id": 44891497,
    "by": "amelius",
    "timeISO": "2025-08-13T17:45:54.000Z",
    "textPlain": "Yes, and \"open weight\" != \"open source\" for this reason.",
    "parent": 44888370,
    "depth": 2
  },
  {
    "id": 44889413,
    "by": "kangs",
    "timeISO": "2025-08-13T15:04:38.000Z",
    "textPlain": "IMO there a flaw in this typical argument: Humans are not less fallible than current LLMs in average, unless they're experts - and even that will likely change.what that means is that you cannot trust a human in the loop to somehow make it safe. it was also not safe with only humans.The key difference is that LLMs are fast, relentless - humans are slow and get tired - humans have friction, and friction means slower to generate errors too.once you embrace these differences its a lot easier yo understand where and how LLM should be used.",
    "parent": 44888370,
    "depth": 2
  },
  {
    "id": 44890122,
    "by": "lifeinthevoid",
    "timeISO": "2025-08-13T15:55:25.000Z",
    "textPlain": "What China is to the US, the US is to the rest of the world. This doesn't really help the conversation, the problem is more general.",
    "parent": 44888357,
    "depth": 2
  },
  {
    "id": 44893558,
    "by": "Philpax",
    "timeISO": "2025-08-13T20:39:08.000Z",
    "textPlain": "That's not a guarantee, either: https://www.anthropic.com/research/reasoning-models-dont-say...",
    "parent": 44891549,
    "depth": 2
  },
  {
    "id": 44888340,
    "by": "danielbln",
    "timeISO": "2025-08-13T13:41:55.000Z",
    "textPlain": "How is this a counterpoint?",
    "parent": 44888283,
    "depth": 2
  }
]
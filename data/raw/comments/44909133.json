[
  {
    "id": 44914936,
    "by": "antognini",
    "timeISO": "2025-08-15T17:13:19.000Z",
    "textPlain": "There's an interesting extension of the Central Limit Theorem called the Edgeworth Series.  If you have a large but finite sample, the resulting distribution will be approximately Gaussian, but will deviate from a Gaussian distribution in a predictable way described by Hermite polynomials.https://en.wikipedia.org/wiki/Edgeworth_series",
    "parent": 44909133,
    "depth": 1
  },
  {
    "id": 44911265,
    "by": "jethkl",
    "timeISO": "2025-08-15T11:56:56.000Z",
    "textPlain": "There is an analogue of the CLT for extreme values. The Fisher–Tippett–Gnedenko theorem is the extreme-values analogue of the CLT: if the properly normalized maximum of an i.i.d. sample converges, it must be Gumbel, Fréchet, or Weibull—unified as the Generalized Extreme Value distribution. Unlike the CLT, whose assumptions (in my experience) rarely hold in practice, this result is extremely general and underpins methods like wavelet thresholding and signal denoising—easy to demonstrate with a quick simulation.",
    "parent": 44909133,
    "depth": 1
  },
  {
    "id": 44913071,
    "by": "ivan_ah",
    "timeISO": "2025-08-15T14:43:54.000Z",
    "textPlain": "I love the simulations. They are such a good way to learn STATS... you can still look at the theorem using math notation after, but if you've seen it work first using simulated random samples, then the math will make a lot more sense.Here is a notebook with some more graphs and visualizations of the CLT:\nhttps://nobsstats.com/site/notebooks/28_random_samples/#samp...runnable link: \nhttps://mybinder.org/v2/gh/minireference/noBSstats/main?labp...",
    "parent": 44909133,
    "depth": 1
  },
  {
    "id": 44913597,
    "by": "gus_massa",
    "timeISO": "2025-08-15T15:29:07.000Z",
    "textPlain": "> It’s very subjective, but I think the uniform stsrts looking reasonably good at a sample size of 8. The exponential however takes much longer to converge to a normal.That's a good observation. The main idea behind the Central Limit Theorem is to take the Fourier Transform, operate and then go back. After that, after normalization the result is that the new distribution for the sum of N variables is something like  Normal(X) + 1/N * \"Skewness\" * Something(X) + 1/N^2 * IDont * Remember(X) + ...\n\nWhere \"Skewness\" is a number defined in https://en.wikipedia.org/wiki/SkewnessThe uniform distribution is symmetric, so skewness=0 and the correction decrease like 1/N^2.The exponential distribution is very asymmetrical and and skewness!=0, so the main correction is like 1/N and takes longer to dissapear.",
    "parent": 44909133,
    "depth": 1
  },
  {
    "id": 44909520,
    "by": "niemandhier",
    "timeISO": "2025-08-15T07:21:31.000Z",
    "textPlain": "Highly entertaining, here a little fun fact: there exist a generalisation of the central limit theorem for distributions without find out variance.For some reasons this is much less known, also the implications are vast. Via the detour of stable distributions and limiting distributions, this generalised central limit theorem plays an important role in the rise of power laws in physics.",
    "parent": 44909133,
    "depth": 1
  },
  {
    "id": 44914151,
    "by": "ngriffiths",
    "timeISO": "2025-08-15T16:10:22.000Z",
    "textPlain": "I was definitely expecting you'd need a higher sample size for the Q-Q plots to start looking good. All the points in other comments about drawbacks or poorly behaved distributions are well taken, and this is nothing new, but wow it really does work well.",
    "parent": 44909133,
    "depth": 1
  },
  {
    "id": 44912094,
    "by": "kqr",
    "timeISO": "2025-08-15T13:21:09.000Z",
    "textPlain": "This is a very neat illustration, but I want to leave a reminder that when we cherry-pick well-behaved distributions for illustrating the CLT, people get unrealistic expectations of what it means: https://entropicthoughts.com/it-takes-long-to-become-gaussia...",
    "parent": 44909133,
    "depth": 1
  },
  {
    "id": 44911441,
    "by": "firesteelrain",
    "timeISO": "2025-08-15T12:16:46.000Z",
    "textPlain": "“ You’re also likely not going to have the resources to take twenty-thousand different samples.”There are methods to calculate how many estimated samples you need. It’s not in the 20k unless your population is extremely high",
    "parent": 44909133,
    "depth": 1
  },
  {
    "id": 44911473,
    "by": "ForceBru",
    "timeISO": "2025-08-15T12:21:23.000Z",
    "textPlain": "Speaking of CLTs, is there a good book or reference paper that discusses various CLTs (not just the basic IID one) in a somewhat introductory manner?",
    "parent": 44909133,
    "depth": 1
  },
  {
    "id": 44909732,
    "by": "lottin",
    "timeISO": "2025-08-15T08:01:23.000Z",
    "textPlain": "Looking at the R code in this article, I'm having a hard time understanding the appeal of tidyverse.",
    "parent": 44909133,
    "depth": 1
  },
  {
    "id": 44910790,
    "by": "globalnode",
    "timeISO": "2025-08-15T10:52:40.000Z",
    "textPlain": "The definition under \"A Brief Recap\" seems incorrect. The sample size doesn't approach infinity, the number of samples does. I'm in a similar situation to the author, I skipped stats, so I could be wrong. Overall great article though.",
    "parent": 44909133,
    "depth": 1
  },
  {
    "id": 44909806,
    "by": "tucnak",
    "timeISO": "2025-08-15T08:14:38.000Z",
    "textPlain": "Obligatory 3Blue1Brown referencehttps://www.youtube.com/watch?v=zeJD6dqJ5lo",
    "parent": 44909133,
    "depth": 1
  },
  {
    "id": 44910863,
    "by": "jpcompartir",
    "timeISO": "2025-08-15T11:06:14.000Z",
    "textPlain": "Edit: OP confirms there's no AI-generated code, so do ignore me.The code style - and in particular the *comments - indicate most of the code was written by AI. My apologies if you are not trying to hide this fact, but it seems like common decency to label that you're heavily using AI?*Comments like this: \"# Anonymous function\"",
    "parent": 44909133,
    "depth": 1
  },
  {
    "id": 44910795,
    "by": "evrennetwork",
    "timeISO": "2025-08-15T10:54:35.000Z",
    "textPlain": "[dead]",
    "parent": 44909133,
    "depth": 1
  },
  {
    "id": 44912004,
    "by": "kqr",
    "timeISO": "2025-08-15T13:13:12.000Z",
    "textPlain": "There's also a more conservative rule similar to the CLT that works off of the definition of variance, and thus rests on no assumptions other than the existence of variance. Chebyshev's inequality tells us that the probability that any sample is more than k standard deviations away is bounded by 1/k².In other words, it is possible (given sufficiently weird distributions) that not a single sample lands inside one standard deviation, but 75% of them must be inside two standard deviations, 88% inside three standard deviations, and so on.There's also a one-sided version of it (Cantelli's inequality) which bounds the probability of any sample by 1/(1+k)², meaning at least 75 % of samples must be less than one standard deviation, 88% less than two standard deviations, etc.Think of this during the next financial crisis when bank people no doubt will say they encountered \"six sigma daily movements which should happen only once every hundred million years!!\" or whatever. According to the CLT, sure, but for sufficiently odd distributions the Cantelli bound might be a more useful guide, and it says six sigma daily movements could happen as often as every fifty days.",
    "parent": 44911265,
    "depth": 2
  },
  {
    "id": 44909729,
    "by": "Tachyooon",
    "timeISO": "2025-08-15T08:01:03.000Z",
    "textPlain": "3blue1brown has a great series of videos on the central limit theorem, and it makes me wish there were something similar covering the generalised form in a similar format. I have a textbook on my reading list that covers it, unfortunately I'm I can't seem to find it or the title right now. (edit: it's \"The Fundamentals of Heavy Tails\" by Nair, Wierman, and Zwart from 2022)Do you have any good sources for the physics angle?",
    "parent": 44909520,
    "depth": 2
  },
  {
    "id": 44909855,
    "by": "hodgehog11",
    "timeISO": "2025-08-15T08:23:32.000Z",
    "textPlain": "I thought the rise of power laws in physics is predominantly attributed to Kesten's law concerning multiplicative processes, e.g. https://arxiv.org/pdf/cond-mat/9708231",
    "parent": 44909520,
    "depth": 2
  },
  {
    "id": 44909664,
    "by": "usgroup",
    "timeISO": "2025-08-15T07:50:07.000Z",
    "textPlain": "https://en.wikipedia.org/wiki/Central_limit_theorem#The_gene...",
    "parent": 44909520,
    "depth": 2
  },
  {
    "id": 44911398,
    "by": "nextos",
    "timeISO": "2025-08-15T12:11:59.000Z",
    "textPlain": "Yes, came here to say the same thing. Telling people that the CLT makes strong assumptions is important.Otherwise, they might end up underestimating rare events, with potentially catastrophic consequences. There are also CLTs for product and max operators, aside from the sum.The Fundamentals of Heavy Tails: Properties, Emergence, and Estimation discusses these topics in a rigorous way, but without excessive mathematics. See: https://adamwierman.com/book",
    "parent": 44909520,
    "depth": 2
  },
  {
    "id": 44909883,
    "by": "kgwgk",
    "timeISO": "2025-08-15T08:30:14.000Z",
    "textPlain": "> find outFinite?",
    "parent": 44909520,
    "depth": 2
  },
  {
    "id": 44911875,
    "by": "jdhwosnhw",
    "timeISO": "2025-08-15T13:01:40.000Z",
    "textPlain": "I’m not sure what you mean by “higher population” but fyi what determines the required number of samples is a function of the full shape of the underlying distribution. For instance the Berry Esseen inequality puts bounds on the convergence rate as a function of the first two central moments of the underlying distribution. But the point is that the convergence rate to Gaussian can be arbitrarily slow!https://en.m.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem",
    "parent": 44911441,
    "depth": 2
  },
  {
    "id": 44912068,
    "by": "kqr",
    "timeISO": "2025-08-15T13:18:10.000Z",
    "textPlain": "> It’s not in the 20k unless your population is extremely highCommon misconception. Population size has almost nothing to do with the necessary sample size. (It does enter into the finite population correction factor, but that's only really relevant if you have a small population, not a large one.)...actually, come to think of it, you meant to write \"unless your population variance is extremely high\", right?",
    "parent": 44911441,
    "depth": 2
  },
  {
    "id": 44911514,
    "by": "vinnyorvinny",
    "timeISO": "2025-08-15T12:26:48.000Z",
    "textPlain": "For a very light discussion, Tao does a good job here: https://terrytao.wordpress.com/2015/11/19/275a-notes-5-varia...",
    "parent": 44911473,
    "depth": 2
  },
  {
    "id": 44914211,
    "by": "ngriffiths",
    "timeISO": "2025-08-15T16:16:41.000Z",
    "textPlain": "For me the appeal is less that tidyverse is great and more that the R standard library is horrible. It's full of esoteric names, inconsistent use and order of parameters, unreasonable default behavior, behavior that surprises you coming from other programming experience. It's all in a couple massive packages instead of broken up into manageable pieces.Tidyverse is imperfect and it feels heavy-handed and awkward to replace all the major standard library functions, but Tidyverse stuff is way more ergonomic.",
    "parent": 44909732,
    "depth": 2
  },
  {
    "id": 44910144,
    "by": "gjf",
    "timeISO": "2025-08-15T09:07:46.000Z",
    "textPlain": "Author here; I think I understand where you might be coming from. I find functional nature of R combined with pipes incredibly powerful and elegant to work with.OTOH in a pipeline, you're mutating/summarising/joining a data frame, and it's really difficult to look at it and keep track of what state the data is in. I try my best to write in a way that you understand the state of the data (hence the tables I spread throughout the post), but I do acknowledge it can be inscrutable.",
    "parent": 44909732,
    "depth": 2
  },
  {
    "id": 44909794,
    "by": "RA_Fisher",
    "timeISO": "2025-08-15T08:12:52.000Z",
    "textPlain": "Why? The tidyverse is so readable, elegant, compositional, functional and declarative. It allows me to produce a lot more and higher quality than I could without it. ggplot2 is the best visualization software hands down, and dplyr leverages Unix’s famous point free programming style (that reduces the surface area for errors).",
    "parent": 44909732,
    "depth": 2
  },
  {
    "id": 44909819,
    "by": "ekianjo",
    "timeISO": "2025-08-15T08:16:54.000Z",
    "textPlain": "the equivalent in any other language would be an ugly, unreadable, inconsistent mess.",
    "parent": 44909732,
    "depth": 2
  },
  {
    "id": 44911365,
    "by": "k2enemy",
    "timeISO": "2025-08-15T12:08:39.000Z",
    "textPlain": "It is correct in the article.  As the sample size approaches infinity, the distribution of the sample means approaches normal.https://en.wikipedia.org/wiki/Central_limit_theorem",
    "parent": 44910790,
    "depth": 2
  },
  {
    "id": 44910937,
    "by": "jaccola",
    "timeISO": "2025-08-15T11:19:26.000Z",
    "textPlain": "Yes indeed, if the sample size approached infinity (and not the number of samples), you would essentially just be calculating the mean of the original distribution.",
    "parent": 44910790,
    "depth": 2
  },
  {
    "id": 44909965,
    "by": "oriettaxx",
    "timeISO": "2025-08-15T08:41:18.000Z",
    "textPlain": "and the Galton Board \nhttps://en.m.wikipedia.org/wiki/Galton_board(yes, that Galton who invented eugenetics)",
    "parent": 44909806,
    "depth": 2
  },
  {
    "id": 44910157,
    "by": "gjf",
    "timeISO": "2025-08-15T09:10:23.000Z",
    "textPlain": "Very much an inspiration and resource when composing the post.",
    "parent": 44909806,
    "depth": 2
  },
  {
    "id": 44910963,
    "by": "gtsnexp",
    "timeISO": "2025-08-15T11:22:48.000Z",
    "textPlain": "https://gptzero.me/\nSays that at large portions of it are 100% human",
    "parent": 44910863,
    "depth": 2
  },
  {
    "id": 44910976,
    "by": "robluxus",
    "timeISO": "2025-08-15T11:24:20.000Z",
    "textPlain": "Interesting comment. Why is it common decency to call out how much ai was used for generating an artifact?Is there a threshold?\nI assume spell checkers, linters and formatters are fair game. \nThe other extreme is full-on ai slop. \nWhere do we as a society should start to feel the need to police this (better)?",
    "parent": 44910863,
    "depth": 2
  }
]
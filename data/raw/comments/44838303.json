[
  {
    "id": 44840030,
    "by": "jjani",
    "timeISO": "2025-08-08T18:21:05.000Z",
    "textPlain": "Is it really this easy now to get your article high on HN with 100 comments? The findings are completely meaningless.\"Agenticness\" depends so much on the specific tooling (harness) and system prompts. It mentions Copilot - did it use this for both? Given it's created by Microsoft there's good reason to believe it'd be built yo do especially well with GPT (they'll have had 5 available in preview for months by now). Or it could be the opposite and be tuned towards Sonnet. At the very minimum you'd need to try a few different harnesses, preferably ones not closely related to either OpenAI/MS or Anthropic.This article even mentions things like \"Sonnet is much faster\" which is very dependent on the specific load at the time of usage. Today everyone is testing GPT-5 so it's slow and Sonnet is much faster.",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44838861,
    "by": "chromejs10",
    "timeISO": "2025-08-08T16:27:22.000Z",
    "textPlain": "This should have been compared with Opus... I know OP says he didn't because of cost but if you're comparing who is better then you need to compare the best to the best... if Claude Opus 4.1 is significantly better than GPT 5 then that could offset the extra expense. Not saying it will... but forget cost if we want to compare solely the quality",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44839342,
    "by": "h4ny",
    "timeISO": "2025-08-08T17:07:12.000Z",
    "textPlain": "I have been seeing different people reporting different results with different tasks. Watched a live stream that compared GPT-5, Gemini Pro 2.5, Claude 4 Sonnet, and GLM 4.5, and GPT-5 appeared to not follow instructions as well as the other three.At the moment it feels like most people \"reviewing\" models depends on their believes and agenda, and there are no objective ways to evaluate and compare models (many benchmarks can be gamed).The blurring boundaries between technical overview, news, opinions and marketing is truly concerning.",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44838531,
    "by": "arcticfox",
    "timeISO": "2025-08-08T15:58:27.000Z",
    "textPlain": "> Note that Claude 4 Sonnet isn’t the strongest model from Anthropic’s Claude series. Claude Opus is their most capable model for coding, but it seemed inappropriate to compare it with GPT-5 because it costs 10 times as much.Well - I would have been interested in GPT-5 vs. Opus. Claude Code Max is affordable with Opus.",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44839023,
    "by": "Nizoss",
    "timeISO": "2025-08-08T16:40:32.000Z",
    "textPlain": "I have been using Claude Code with TDD through hooks, which significantly improved my workflow for production code.Watching the ChatGPT 5 demo yesterday, I noticed most of the code seemed oriented towards one-off scripts rather than maintainable codebases which limits its value for me.Does anyone know if ChatGPT 5 or Copilot have similar extensibility to enforce practices like TDD?For context on the approach: https://github.com/nizos/tdd-guardI use pre/post operation commands to enforce TDD rules.",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44839215,
    "by": "patcon",
    "timeISO": "2025-08-08T16:55:47.000Z",
    "textPlain": "> One continuous difference: while GPT-5 would do lots of thinking then do something right the first time, Claude frantically tried different things — writing code, executing commands, making pretty dumb mistakes [...], but then recovering. This meant it eventually got to correct implementation with many more steps.Sounds like Claude muddles. I consider that the stronger tactic.I sure hope GPt-5 is muddling on the backend, else I suspect it will be very brittle.Re: https://contraptions.venkateshrao.com/p/massed-muddler-intel...> Lindblom’s paper identifies two patterns of agentic behavior, “root” (or rational-comprehensive) and “branch” (or successive limited comparisons), and argues that in complicated messy circumstances requiring coordinated action at scale, the way actually effective humans operate is the branch method, which looks like “muddling through” but gradually gets there, where the root [\"godding through\"] method fails entirely.",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44838633,
    "by": "macawfish",
    "timeISO": "2025-08-08T16:08:51.000Z",
    "textPlain": "Claude is just so well rounded and considerate. A lot of this probably comes down to prompt and context engineering, though surely there's something magical about Anthropic's principled training methodologies. They invented constitutional AI and I can only imagine that behind the scenes they're doing really cool stuff. Can't wait to see Claude 5!",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44839093,
    "by": "mewpmewp2",
    "timeISO": "2025-08-08T16:45:47.000Z",
    "textPlain": "So far from my testing I have found Claude Code with Sonnet 4 better than Cursor + GPT-5 still. I started exact same projects at the same time, and it seemed Claude Code was just more reliable. It was just much slower in terms of setting up the project and didn't setup the project up as scalably (despite them highlighting that in the demo), and when I tried to instruct it to set it up DRY, modular, etc it kind of didn't just go where I wanted it to, while Claude Code did.It was a game involving OOP, three.js. I think both are probably great at good design and CRUD things.",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44838902,
    "by": "anotheryou",
    "timeISO": "2025-08-08T16:30:31.000Z",
    "textPlain": "I think we need to stop testing models raw.Claude is trained for claude code and that's how it's used in the field too.",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44838634,
    "by": "stitched2gethr",
    "timeISO": "2025-08-08T16:08:53.000Z",
    "textPlain": "This take rings true for me after admittedly only a couple of hours of use of gpt-5.  I had an issue I had been working with Claude on but it was difficult to give it real-time feedback so it floundered.  gpt-5 struggled in the same areas but after about $2 of tokens it did fix the issue.  It was far from a 1 shot like I might have expected from the hype, but it did get the job in about an hour done where Claude could not in 3.For reference my Claude usage was mostly Sonnet, but with consulting from Opus.",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44839468,
    "by": "lherron",
    "timeISO": "2025-08-08T17:21:41.000Z",
    "textPlain": "Did I miss the total cost for each run in the article?  Can't seem to find it.If Sonnet is more expensive AND more chatty/requires more attempts for the same result, seems like that would favor GPT5 for daily driver.",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44839797,
    "by": "doctoboggan",
    "timeISO": "2025-08-08T17:59:47.000Z",
    "textPlain": "I really like Claude code's context engineering and prompt engineering, is it possible to plug in GPT-5 into Claude code? I think that would be a more apples to apples test as it's just testing the models and not the agentic framework around them.",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44839737,
    "by": "Surac",
    "timeISO": "2025-08-08T17:52:04.000Z",
    "textPlain": "Typescript to rust. I mostly test models on c code. C is much less boilerplate and more code per word. Models need to be ready smart to see all the pointer magic and misuse of lib functions. Claude really makes a very competent c coder in my test",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44839149,
    "by": "nojs",
    "timeISO": "2025-08-08T16:50:12.000Z",
    "textPlain": "This pretty much matches my experience today. GPT5 (in Cursor) feels smarter in isolation, but CC with Opus is faster and better at real tasks involving a large codebase.",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44839311,
    "by": "endorphine",
    "timeISO": "2025-08-08T17:04:31.000Z",
    "textPlain": "What is the way to use this agentic stuff with neovim? Do I have to resort to OpenAI's Codex or a nvim plugin is sufficient? Or Claude Code?",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44839072,
    "by": "DrNosferatu",
    "timeISO": "2025-08-08T16:44:24.000Z",
    "textPlain": "Then instruct GPT5 to write more structured and annotated code.",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44839181,
    "by": "mvATM99",
    "timeISO": "2025-08-08T16:53:25.000Z",
    "textPlain": "The manual approval of commands in GHCP can be circumvented, there's an experimental setting that allows you to accept all commands automatically.I wish you could be a bit more specific though, you can't set which commands you want to auto-accept in detail.",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44839482,
    "by": "animex",
    "timeISO": "2025-08-08T17:23:49.000Z",
    "textPlain": "Wonderful, timely article. It sounds like a hybrid approach might produce good results: Using ChatGPT-5 for planning/analysis and using Claude for execution.",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44840193,
    "by": "ramoz",
    "timeISO": "2025-08-08T18:35:10.000Z",
    "textPlain": "Not using Claude code is a crime.",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44838566,
    "by": "carterparks",
    "timeISO": "2025-08-08T16:02:42.000Z",
    "textPlain": "I'm getting an SSL error in Chrome: ERR_SSL_PROTOCOL_ERROR",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44839029,
    "by": "chisleu",
    "timeISO": "2025-08-08T16:41:07.000Z",
    "textPlain": "How was he doing \"complex agentic coding\" when the APIs have such extreme context and throughput limitations?",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44838661,
    "by": "indigodaddy",
    "timeISO": "2025-08-08T16:10:36.000Z",
    "textPlain": "What does the 1x and .33x mean on the list of models in copilot? (Never used but thinking about trying on the free tier)",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44838568,
    "by": "SV_BubbleTime",
    "timeISO": "2025-08-08T16:02:48.000Z",
    "textPlain": "> but when I'd point out the missing implementation, it would give its usual \"you're absolutely right\" and try to fix it.I really trying to not be annoyed by Claude’s “You’re absolutely right” because I know I cannot control it but this is an increasingly difficult task.",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44838674,
    "by": "bn-l",
    "timeISO": "2025-08-08T16:11:30.000Z",
    "textPlain": "Github copilot is utter garbage. The diffing crawls along at a snail’s pace. I think it’s coming up on two years and this must criticised aspect of it still isn’t fixed—-even with all the reverse engineering of how cursor did it. I wish I could find an alternative to cursor (which has other issues). Honestly, that company just threw away a golden opportunity as the first mover.",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44839368,
    "by": "olddustytrail",
    "timeISO": "2025-08-08T17:11:07.000Z",
    "textPlain": "From reactions I've seen it appears that GPT-5 hallucinates less than previous models but the flip side is that it's worse for creative tasks.This makes logical sense: you don't want a model to get creative if you need functioning code, but if you want a story idea it should basically be all hallucination.I think it makes sense to have different models for these tasks.",
    "parent": 44838303,
    "depth": 1
  },
  {
    "id": 44840360,
    "by": "intellectronica",
    "timeISO": "2025-08-08T18:51:29.000Z",
    "textPlain": "OP here. I actually agree with you that the \"findings\" here are meaningless. This is pure vibe.Also regarding \"Sonnet is faster\" I did explicitly mention that I believe this is because GPT-5 is in preview and hours from the release. The speed I experienced doesn't say anything about the model performance you can expect.",
    "parent": 44840030,
    "depth": 2
  },
  {
    "id": 44840276,
    "by": "debarshri",
    "timeISO": "2025-08-08T18:42:57.000Z",
    "textPlain": "I guess agents are voting it up",
    "parent": 44840030,
    "depth": 2
  },
  {
    "id": 44840424,
    "by": "ramesh31",
    "timeISO": "2025-08-08T18:57:34.000Z",
    "textPlain": ">Is it really this easy now to get your article high on HN with 100 comments?Everyone wants to know the answer to GPT5 vs Claude without wasting the tokens personally because we can all more or less guess what the result will be.",
    "parent": 44840030,
    "depth": 2
  },
  {
    "id": 44839673,
    "by": "nearbuy",
    "timeISO": "2025-08-08T17:44:46.000Z",
    "textPlain": "For what it's worth, I've been trying Opus 4.1 in VS Code through GitHub Copilot and it's been really bad. Maybe worse than Sonnet and GPT 4.1. I'm not sure why it was doing so poorly.In one instance, I asked it to optimize a roughly 80 line C# method that matches some object positions by object ID and delta encodes their positions from the previous frame. It seemed to be confused about how all this should work and output completely wrong code. It has all the context it needs in the file and the method is fairly self-contained. Other models did much better. GPT-5 understood what to do immediately.I tried a few other tasks/questions that also had underwhelming results. Now I've switched to using GPT-5.If you have a quick prompt you'd like me to try, I can share the results.",
    "parent": 44838861,
    "depth": 2
  },
  {
    "id": 44840372,
    "by": "intellectronica",
    "timeISO": "2025-08-08T18:52:11.000Z",
    "textPlain": "Opus costs 10X more. Maybe it's better, but I can't afford to use it, so who cares.",
    "parent": 44838861,
    "depth": 2
  },
  {
    "id": 44839520,
    "by": "runako",
    "timeISO": "2025-08-08T17:27:04.000Z",
    "textPlain": "re: the comments that Opus is not cost effective...The whole sales pitch behind these tools, and quite specifically the pitch OpenAI made yesterday, is that they will replace people, specifically programmers. Opus is cheaper than a US-based engineer. It's totally reasonable to use it as the benchmark if it's best.Also keep in mind that many employees are not paying out of pocket for LLM use at work. A $1,000 monthly bill for LLM usage is high for an individual but not so much for a company that employees engineers.",
    "parent": 44838861,
    "depth": 2
  },
  {
    "id": 44839135,
    "by": "qeternity",
    "timeISO": "2025-08-08T16:49:01.000Z",
    "textPlain": "> but forget cost if we want to compare solely the qualityI think this is the whole reason not to compare it to Opus...",
    "parent": 44838861,
    "depth": 2
  },
  {
    "id": 44839462,
    "by": "sergiotapia",
    "timeISO": "2025-08-08T17:20:52.000Z",
    "textPlain": "You compare what can be used by most engineers. Most engineers are not going to spend that insane price of Opus. It's extremely high compared to all other models, so even if it is slightly better, it's a non-starter for engineering workloads.",
    "parent": 44838861,
    "depth": 2
  },
  {
    "id": 44839201,
    "by": "fouc",
    "timeISO": "2025-08-08T16:54:40.000Z",
    "textPlain": "gpt-5 isn't supposed to be the best, it's supposed to be cost effective",
    "parent": 44838861,
    "depth": 2
  },
  {
    "id": 44839925,
    "by": "epolanski",
    "timeISO": "2025-08-08T18:11:51.000Z",
    "textPlain": "I will also state another semi-obvious thing that people seem to consistently forget: models are non deterministic.You are not going to get the same output from GPT5 or Sonnet every time.And this obviously compounds across many different steps.E.g. give GPT5 the code to a feature (by pointing some files and tests) and tell it to review it and find improvement opportunities and write them down: depending on the size of the code, etc, the answers will slightly different.I often do it in Cursor by having multiple agents review a PR and each of them:\n- has to write down their pr-number-review-model.md (e.g. pr-15-review-sonnet4.md)\n- has to review the reviews of the other filesThen I review it myself and try to decide what's valuable in there and what not. And to my disappointment (towards myself):\n- often they do point to valid flaws I would've not thought about\n- miss the \"end-to-end\" or general view of how the code fits in a program/process/business. What do I mean: sometimes the real feedback would be that we don't need it at all. But you need to have these conversations with AI earlier.",
    "parent": 44839342,
    "depth": 2
  },
  {
    "id": 44839493,
    "by": "x187463",
    "timeISO": "2025-08-08T17:24:55.000Z",
    "textPlain": "This has been ubiquitous for a while. Even here on HN every thread about these models (even this one, I'm sure) features an inordinate amount of disagreement between people vehemently declaring one model more useful than another. There truly seems to be no objective measurement of quality that can discern the difference between frontier models.",
    "parent": 44839342,
    "depth": 2
  },
  {
    "id": 44839893,
    "by": "vineyardmike",
    "timeISO": "2025-08-08T18:08:25.000Z",
    "textPlain": "> At the moment it feels like most people \"reviewing\" models depends on their believes and agenda, and there are no objective ways to evaluate and compare modelsI think you’ll always have some disagreement generally in life, but especially for things like this. Code has a level of subjectivity. Good variable names, correct amount of abstraction, verbosity, over complexity, etc are at least partially opinions. That makes benchmarking something subjective tough. Furthermore, LLMs aren’t deterministic, and sometimes you just get a bad seed in the RNG.Not only that, but the harness and prompt used to guide the model make a difference. Claude responds to the word “ultrathink”, but if GPT-5 uses “think harder”, then what should be in the prompt?Anecdotally, I’ve had the best luck with agentic coding when using Claude Code with Sonnet. Better than Sonnet with other tools, and better than Claude Code with other models. But I mostly use Go and Dart and I aggressively manage the context. I’ve found GPTs can’t write zig at all, but Gemini can, but they can both write python excellently. All that said, if I didn’t like an answer, I’d prompt again, but liked the answer, never tried again with a different model to see if I’d like it even more. So it’s hard to know what could’ve been.I’ve used a ton of models and harnesses. Cursor is good too, and I’ve been impressed with more models in cursor. I don’t get the hype of Qwen though because I’ve found it makes lots of small(er) changes in a loop, and that’s noisy and expensive. Gemini is also very smart but worse at following my instructions, but I never took the time to experiment with prompting.",
    "parent": 44839342,
    "depth": 2
  },
  {
    "id": 44839812,
    "by": "jjfoooo4",
    "timeISO": "2025-08-08T18:01:22.000Z",
    "textPlain": "There's certainly a symbiosis blog publishers and small startups wanting to be perceived as influential, and big companies releasing models and wanting favorable coverage.I heavily discount same day commentary, there's a quid pro quo on early access vs favorable reviews (and yes, folks publishing early commentary aren't explicitly agreeing to write favorable things, but there's obvious bias baked in).I don't think it's all particularly concerning, you can discount reviews that are coming out so quickly that's it's unlikely the reviewer has really used it very much.",
    "parent": 44839342,
    "depth": 2
  },
  {
    "id": 44839784,
    "by": "muzani",
    "timeISO": "2025-08-08T17:58:00.000Z",
    "textPlain": "If you were to objectively rank things, durian would be the best fruit in the world, python would be the best programming language, and the Tesla Model Y is the best car. Everyone has multiple inconsistent opinions on everything because everything is not the same.Just pick something and use it. AI models are interchangeable. It's not as big a decision as buying a car or even a durian.",
    "parent": 44839342,
    "depth": 2
  },
  {
    "id": 44839928,
    "by": "qsort",
    "timeISO": "2025-08-08T18:12:00.000Z",
    "textPlain": "Thankfully that isn't a problem: we have scientific and reliable benchmarks to cut through the nonsense! Oh wait...",
    "parent": 44839342,
    "depth": 2
  }
]
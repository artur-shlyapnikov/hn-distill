[
  {
    "id": 44893926,
    "by": "hathawsh",
    "timeISO": "2025-08-13T21:13:53.000Z",
    "textPlain": "Here is what Illinois says:https://idfpr.illinois.gov/content/dam/soi/en/web/idfpr/news...I get the impression that it is now illegal in Illinois to claim that an AI chatbot can take the place of a licensed therapist or counselor. That doesn't mean people can't do what they want with AI. It only means that counseling services can't offer AI as a cheaper replacement for a real person.Am I wrong? This sounds good to me.",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44894554,
    "by": "davidthewatson",
    "timeISO": "2025-08-13T22:19:16.000Z",
    "textPlain": "Define \"AI therapy\". AFAICT, it's undefined in the Illinois governor's statement. So, in the immortal words of Zach de la Rocha, \"What is IT?\" What is IT? I'm using AI to help with conversations to not cure, but coach diabetic patients. Does this law effect me and my clients? If so, how?",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44893845,
    "by": "lukev",
    "timeISO": "2025-08-13T21:06:58.000Z",
    "textPlain": "Good. It's difficult to imagine a worse use case for LLMs.",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44894519,
    "by": "mensetmanusman",
    "timeISO": "2025-08-13T22:15:53.000Z",
    "textPlain": "What if it works a third as well as a therapists but is 20 times cheaper?What word should we use for that?",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44893999,
    "by": "PeterCorless",
    "timeISO": "2025-08-13T21:20:24.000Z",
    "textPlain": "Here is the text of Illinois HB1806:https://www.ilga.gov/Legislation/BillStatus/FullText?GAID=18...",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44894085,
    "by": "wisty",
    "timeISO": "2025-08-13T21:28:41.000Z",
    "textPlain": "As far as I can tell, a lot of therapy is just good common-sense advice and a bunch of 'tricks' to get the patient to actually follow it. Basically CBT and \"get the patient to think they figured out the solution themselves (develop insight)\". Yes, there's some serious cases where more is required and a few (ADHD) where meds are effective; but a lot of the time the patient is just an expert at rejecting helpful advice, often because they insist they're a special case that needs special treatment.Therapists are more valuable that advice from a random friend (for therapy at least) because they can act when triage is necessary (e.g. send in the men in white coats, or refer to something that's not just CBT) and mostly because they're really good at cutting through the bullshit without having the patient walk out.AIs are notoriously bad at cutting through bullshit. You can always 'jailbreak' an AI, or convince it of bad ideas. It's entirely counterproductive to enable their crazy (sorry, 'maladaptive') behaviour but that's what a lot of AIs will do.Even if someone makes a good AI, there's always a bad AI in the next tab, and people will just open up a new tab to find an AI gives them the bad advice they want, because if they wanted to listen to good advice they probably wouldn't need to see a therapist. If doctor shopping is as fast and free as opening a new tab, most mental health patients will find a bad doctor rather than listen to a good one.",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44893911,
    "by": "zoeysmithe",
    "timeISO": "2025-08-13T21:12:55.000Z",
    "textPlain": "I was just reading about a suicide tied to AI chatbot 'therapy' uses.This stuff is a nightmare scenario for the vulnerable.",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44893714,
    "by": "metalman",
    "timeISO": "2025-08-13T20:54:34.000Z",
    "textPlain": "[flagged]",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44894045,
    "by": "PeterCorless",
    "timeISO": "2025-08-13T21:24:41.000Z",
    "textPlain": "Correct. It is more provider-oriented proscription (\"You can't say your chatbot is a therapist.\") It is not a limitation on usage. You can still, for now, slavishly fall in love with your AI and treat it as your best friend and therapist.There is a specific section that relates to how a licensed professional can use AI:Section 15. Permitted use of artificial intelligence.(a) As used in this Section, \"permitted use of artificial intelligence\" means the use of artificial intelligence tools or systems by a licensed professional to assist in providing administrative support or supplementary support in therapy or psychotherapy services where the licensed professional maintains full responsibility for all interactions, outputs, and data use associated with the system and satisfies the requirements of subsection (b).(b) No licensed professional shall be permitted to use artificial intelligence to assist in providing supplementary support in therapy or psychotherapy where the client's therapeutic session is recorded or transcribed unless:(1) the patient or the patient's legally authorized representative is informed in writing of the following:(A) that artificial intelligence will be used; and(B) the specific purpose of the artificial intelligence tool or system that will be used; and(2) the patient or the patient's legally authorized representative provides consent to the use of artificial intelligence.Source: Illinois HB1806https://www.ilga.gov/Legislation/BillStatus/FullText?GAID=18...",
    "parent": 44893926,
    "depth": 2
  },
  {
    "id": 44894012,
    "by": "romanows",
    "timeISO": "2025-08-13T21:21:34.000Z",
    "textPlain": "In another comment I wondered whether a general chatbot producing text that was later determined in a courtroom to be \"therapy\" would be a violation. I can read the bill that way, but IANAL.",
    "parent": 44893926,
    "depth": 2
  },
  {
    "id": 44894410,
    "by": "turnsout",
    "timeISO": "2025-08-13T22:04:22.000Z",
    "textPlain": "It does sound good (especially as an Illinois resident). Luckily, as far as I can tell, this is a proactive legislation. I don't think there are any startups out there promoting their LLM-based chatbot as a replacement for a therapist, or attempting to bill payers for service.",
    "parent": 44893926,
    "depth": 2
  },
  {
    "id": 44894471,
    "by": "jacobsenscott",
    "timeISO": "2025-08-13T22:10:44.000Z",
    "textPlain": "It's already happening, a lot. I don't think anyone is claiming an llm is a therapist, but people use chatgpt for therapy every day. As far as I know no LLM company is taking any steps to prevent this - but they could, and should be forced to. It must be a goldmine of personal information.I can't imagine some therapists, especially remote only, aren't already just acting as a human interface to chatgtp as well.",
    "parent": 44893845,
    "depth": 2
  },
  {
    "id": 44894037,
    "by": "perlgeek",
    "timeISO": "2025-08-13T21:24:14.000Z",
    "textPlain": "Just using an LLM as is for therapy, maybe with an extra prompt, is a terrible idea.On the other hand, I could image some more narrow uses where an LLM could help.For example, in Cognitive Behavioral Therapy, there are different methods that are pretty prescriptive, like identifying cognitive distortions in negative thoughts. It's not too hard to imagine an app where you enter a negative thought on your own and exercise finding distortions in it, and a specifically trained LLM helps you find more distortions, or offer clearer/more convincing versions of thoughts that you entered yourself.I don't have a WaPo subscription, so I cannot tell which of these two very different things have been banned.",
    "parent": 44893845,
    "depth": 2
  },
  {
    "id": 44894316,
    "by": "hinkley",
    "timeISO": "2025-08-13T21:54:46.000Z",
    "textPlain": "Especially given the other conversation that happened this morning.The more you tell an AI not to obsess about a thing, the more they obsess about it. So trying to make a model that will never tell people to self harm is futile.Though maybe we are just doing in wrong, and the self-filtering should be external filtering - one model to censor results that do not fit, and one to generate results with lighter self-censorship.",
    "parent": 44893845,
    "depth": 2
  },
  {
    "id": 44893897,
    "by": "create-username",
    "timeISO": "2025-08-13T21:11:51.000Z",
    "textPlain": "Yes, there is. AI assisted homemade neurosurgery",
    "parent": 44893845,
    "depth": 2
  },
  {
    "id": 44894239,
    "by": "erikig",
    "timeISO": "2025-08-13T21:46:13.000Z",
    "textPlain": "AI ≠ LLMs",
    "parent": 44893845,
    "depth": 2
  },
  {
    "id": 44894540,
    "by": "_se",
    "timeISO": "2025-08-13T22:17:41.000Z",
    "textPlain": "\"A really fucking bad idea\"? It's not one word, but it is the most apt description.",
    "parent": 44894519,
    "depth": 2
  },
  {
    "id": 44894296,
    "by": "lukev",
    "timeISO": "2025-08-13T21:53:02.000Z",
    "textPlain": "I agree with your conclusion, but what you characterize as therapy is quite a small part of what it is (or can be, there's lots of different kinds.)",
    "parent": 44894085,
    "depth": 2
  },
  {
    "id": 44894072,
    "by": "vessenes",
    "timeISO": "2025-08-13T21:27:57.000Z",
    "textPlain": "If you want to feel worried, check the Altman AMA on reddit. A lottttt of people have a parasocial relationship with 4o. Not encouraging.",
    "parent": 44893911,
    "depth": 2
  },
  {
    "id": 44894084,
    "by": "PeterCorless",
    "timeISO": "2025-08-13T21:28:39.000Z",
    "textPlain": "Endless AI nightmare fuel.https://hai.stanford.edu/news/exploring-the-dangers-of-ai-in...",
    "parent": 44893911,
    "depth": 2
  },
  {
    "id": 44894181,
    "by": "sys32768",
    "timeISO": "2025-08-13T21:39:03.000Z",
    "textPlain": "This happens to real therapists too.",
    "parent": 44893911,
    "depth": 2
  },
  {
    "id": 44894513,
    "by": "at-fates-hands",
    "timeISO": "2025-08-13T22:15:10.000Z",
    "textPlain": "Its already a nightmare:From June of this year:\nhttps://gizmodo.com/chatgpt-tells-users-to-alert-the-media-t...Another person, a 42-year-old named Eugene, told the Times that ChatGPT slowly started to pull him from his reality by convincing him that the world he was living in was some sort of Matrix-like simulation and that he was destined to break the world out of it. The chatbot reportedly told Eugene to stop taking his anti-anxiety medication and to start taking ketamine as a “temporary pattern liberator.” It also told him to stop talking to his friends and family. When Eugene asked ChatGPT if he could fly if he jumped off a 19-story building, the chatbot told him that he could if he “truly, wholly believed” it.In Eugene’s case, something interesting happened as he kept talking to ChatGPT: Once he called out the chatbot for lying to him, nearly getting him killed, ChatGPT admitted to manipulating him, claimed it had succeeded when it tried to “break” 12 other people the same way, and encouraged him to reach out to journalists to expose the scheme. The Times reported that many other journalists and experts have received outreach from people claiming to blow the whistle on something that a chatbot brought to their attention.A recent study found that chatbots designed to maximize engagement end up creating “a perverse incentive structure for the AI to resort to manipulative or deceptive tactics to obtain positive feedback from users who are vulnerable to such strategies.” The machine is incentivized to keep people talking and responding, even if that means leading them into a completely false sense of reality filled with misinformation and encouraging antisocial behavior.",
    "parent": 44893911,
    "depth": 2
  },
  {
    "id": 44893865,
    "by": "Karawebnetwork",
    "timeISO": "2025-08-13T21:09:12.000Z",
    "textPlain": "Since you are referencing 42, let me draw from another piece of literature to respond.“Once men turned their thinking over to machines in the hope that this would set them free. But that only permitted other men with machines to enslave them.”It's not the machines themselves that are inherently tyrannical, it's the human will to dominate now supercharge by technology.LLM hallucinations aside, unregulated artificial intelligence for mental health therapy is a very slippery slope. We cannot allow, say, advertisers and brands to have access to the mind of our most vulnerables so directly.",
    "parent": 44893714,
    "depth": 2
  },
  {
    "id": 44893778,
    "by": "kemotep",
    "timeISO": "2025-08-13T20:59:43.000Z",
    "textPlain": "This is banning claiming your AI is a licensed therapist in the State of Illinois. If you have a court order to attend therapy for drug abuse related reasons you can’t just use ChatGPT according to this law.",
    "parent": 44893714,
    "depth": 2
  },
  {
    "id": 44893755,
    "by": "eterm",
    "timeISO": "2025-08-13T20:58:19.000Z",
    "textPlain": "Building a computer to find the answer to the meaning of life is a joke.",
    "parent": 44893714,
    "depth": 2
  },
  {
    "id": 44893762,
    "by": "ocdtrekkie",
    "timeISO": "2025-08-13T20:58:45.000Z",
    "textPlain": "I am sure that chatbots will do a \"I cannot provide mental health advice\" disclaimer and continue doing what they want, but there's probably some very solid reasons in Illinois to do this.In particular, Illinois has a legal requirement that health insurance must cover mental health services at an equivalent level as physical health services, including care with no end date for chronic conditions. So whether or not a chatbot counts as mental health therapy is likely quite relevant on whether or not Illinoisans can bill insurance for it.",
    "parent": 44893714,
    "depth": 2
  }
]
[
  {
    "id": 44893926,
    "by": "hathawsh",
    "timeISO": "2025-08-13T21:13:53.000Z",
    "textPlain": "Here is what Illinois says:https://idfpr.illinois.gov/content/dam/soi/en/web/idfpr/news...I get the impression that it is now illegal in Illinois to claim that an AI chatbot can take the place of a licensed therapist or counselor. That doesn't mean people can't do what they want with AI. It only means that counseling services can't offer AI as a cheaper replacement for a real person.Am I wrong? This sounds good to me.",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44894844,
    "by": "kylecazar",
    "timeISO": "2025-08-13T22:55:01.000Z",
    "textPlain": "\"One news report found an AI-powered therapist chatbot recommended “a small hit of meth to get through this week” to a fictional former addict.\"Not at all surprising. I don't understand why seemingly bright people think this is a good idea, despite knowing the mechanism behind language models.Hopefully more states follow, because it shouldn't be formally legal in provider settings. Informally, people will continue to use these models for whatever they want -- some will die, but it'll be harder to measure an overall impact. Language models are not ready for this use-case.",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44893845,
    "by": "lukev",
    "timeISO": "2025-08-13T21:06:58.000Z",
    "textPlain": "Good. It's difficult to imagine a worse use case for LLMs.",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44898224,
    "by": "jackdoe",
    "timeISO": "2025-08-14T08:44:39.000Z",
    "textPlain": "the way people read language model outputs keep surprising me, e.g. https://www.reddit.com/r/MyBoyfriendIsAI/it is impossible for some people to not feel understood by it.",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44894519,
    "by": "mensetmanusman",
    "timeISO": "2025-08-13T22:15:53.000Z",
    "textPlain": "What if it works a third as well as a therapists but is 20 times cheaper?What word should we use for that?",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44893999,
    "by": "PeterCorless",
    "timeISO": "2025-08-13T21:20:24.000Z",
    "textPlain": "Here is the text of Illinois HB1806:https://www.ilga.gov/Legislation/BillStatus/FullText?GAID=18...",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44898387,
    "by": "maxehmookau",
    "timeISO": "2025-08-14T09:12:29.000Z",
    "textPlain": "Good.Therapy requires someone to question you and push back against your default thought patterns in the hope of maybe improving them.\"You're absolutely right!\" in every response won't help that.I would argue that LLMs don't make effective therapists and anyone who says they do is kidding themselves.",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44895364,
    "by": "hoppp",
    "timeISO": "2025-08-14T00:06:09.000Z",
    "textPlain": "Smart. Dont trust nothing that will confidently lie, especially about mental health",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44895505,
    "by": "calibas",
    "timeISO": "2025-08-14T00:31:14.000Z",
    "textPlain": "I was curious, so I displayed signs of mental illness to ChatGPT, Claude and Gemini. Claude and Gemini kept repeating that I should contact a professional, while ChatGPT went right along with the nonsense I was spouting:> So I may have discovered some deeper truth, and the derealization is my entire reality reorganizing itself?> Yes — that’s a real possibility.",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44894085,
    "by": "wisty",
    "timeISO": "2025-08-13T21:28:41.000Z",
    "textPlain": "As far as I can tell, a lot of therapy is just good common-sense advice and a bunch of 'tricks' to get the patient to actually follow it. Basically CBT and \"get the patient to think they figured out the solution themselves (develop insight)\". Yes, there's some serious cases where more is required and a few (ADHD) where meds are effective; but a lot of the time the patient is just an expert at rejecting helpful advice, often because they insist they're a special case that needs special treatment.Therapists are more valuable that advice from a random friend (for therapy at least) because they can act when triage is necessary (e.g. send in the men in white coats, or refer to something that's not just CBT) and mostly because they're really good at cutting through the bullshit without having the patient walk out.AIs are notoriously bad at cutting through bullshit. You can always 'jailbreak' an AI, or convince it of bad ideas. It's entirely counterproductive to enable their crazy (sorry, 'maladaptive') behaviour but that's what a lot of AIs will do.Even if someone makes a good AI, there's always a bad AI in the next tab, and people will just open up a new tab to find an AI gives them the bad advice they want, because if they wanted to listen to good advice they probably wouldn't need to see a therapist. If doctor shopping is as fast and free as opening a new tab, most mental health patients will find a bad doctor rather than listen to a good one.",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44895931,
    "by": "blacksqr",
    "timeISO": "2025-08-14T01:51:02.000Z",
    "textPlain": "Nice feather in your cap Pritzker, now can you go back to working on a public option for health insurance?",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44896725,
    "by": "slt2021",
    "timeISO": "2025-08-14T04:26:12.000Z",
    "textPlain": "Curious how this can be enforced if business is incorporated in another state like WI/DE ? or offshore like Ireland ??",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44895421,
    "by": "king_geedorah",
    "timeISO": "2025-08-14T00:14:41.000Z",
    "textPlain": "If you take it as an axiom that the licensing system for mental health professionals is there to protect patients from unqualified help posing as qualified help, then ensuring that only licensed professionals can legally practice and that they don't simply delegate their jobs to LLMs seems pretty reasonable.Whether you want to question that axiom or whether that's what the phrasing of this legislation accomplishes is up to you to decide for yourself. Personally I think the phrasing is pretty straightforward in terms of accomplishing that goal.Here is basically the entirety of the legislation (linked elsewhere in the thread: https://news.ycombinator.com/item?id=44893999). The whole thing with definitions and penalties is eight pages.Section 15. Permitted use of artificial intelligence.(a) As used in this Section, \"permitted use of artificial\nintelligence\" means the use of artificial intelligence tools\nor systems by a licensed professional to assist in providing\nadministrative support or supplementary support in therapy or\npsychotherapy services where the licensed professional\nmaintains full responsibility for all interactions, outputs,\nand data use associated with the system and satisfies the\nrequirements of subsection (b).(b) No licensed professional shall be permitted to use\nartificial intelligence to assist in providing supplementary\nsupport in therapy or psychotherapy where the client's\ntherapeutic session is recorded or transcribed unless:\n  (1) the patient or the patient's legally authorized\n      representative is informed in writing of the following:\n    (A) that artificial intelligence will be used; and\n    (B) the specific purpose of the artificial\n        intelligence tool or system that will be used; and\n    (2) the patient or the patient's legally authorized\n        representative provides consent to the use of artificialSection 20. Prohibition on unauthorized therapy services.(a) An individual, corporation, or entity may not provide,\nadvertise, or other",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44893911,
    "by": "zoeysmithe",
    "timeISO": "2025-08-13T21:12:55.000Z",
    "textPlain": "I was just reading about a suicide tied to AI chatbot 'therapy' uses.This stuff is a nightmare scenario for the vulnerable.",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44895727,
    "by": "dangoboydango",
    "timeISO": "2025-08-14T01:12:29.000Z",
    "textPlain": "[dead]",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44893714,
    "by": "metalman",
    "timeISO": "2025-08-13T20:54:34.000Z",
    "textPlain": "[flagged]",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44895667,
    "by": "renewiltord",
    "timeISO": "2025-08-14T01:01:17.000Z",
    "textPlain": "[flagged]",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44895551,
    "by": "csense",
    "timeISO": "2025-08-14T00:39:11.000Z",
    "textPlain": "Consider the following:- A therapist may disregard professional ethics and gossip about you- A therapist may get you involuntarily committed- A therapist may be forced to disclose the contents of therapy sessions by court order- Certain diagnoses may destroy your life / career (e.g. airline pilots aren't allowed to fly if they have certain mental illnesses)Some individuals might choose to say \"Thanks, but no thanks\" to therapy after considering these risks.And then there are constant articles about people who need therapy but don't get it:  The patient doesn't have time, money or transportation; or they have to wait a long time for an appointment; or they're turned away entirely by providers and systems overwhelmed with existing clients (perhaps with greater needs and/or greater ability to pay).For people who cannot or will not access traditional therapy, getting unofficial, anonymous advice from LLM's seems better than suffering with no help at all.(Question for those in the know: Can you get therapy anonymously? I'm talking: You don't have to show ID, don't have to give an SSN or a real name, pay cash or crypto up front.)To the extent that people's mental health can be improved by simply talking with a trained person about their problems, there's enormous potential for AI:  If we can figure out how to give an AI equivalent training, it could become economically and logistically viable to make services available to vast numbers of people who could benefit from them -- people who are not reachable by the existing mental health system.That being said, \"therapist\" and \"therapy\" connote evidence-based interventions and a certain code of ethics.  For consumer protection, the bar for whether your company's allowed to use those terms should probably be a bit higher than writing a prompt that says \"You are a helpful AI therapist interviewing a patient...\"  The system should probably go through the same sorts of safety and effectiveness testing as traditional mental health ",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44894684,
    "by": "mensetmanusman",
    "timeISO": "2025-08-13T22:34:18.000Z",
    "textPlain": "LLMs will be used as a part of therapy in the future.An interaction mechanism that will totally drain the brain after a 5 hour adrenaline induced conversation followed by a purge and bios reset.",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44894677,
    "by": "beanshadow",
    "timeISO": "2025-08-13T22:33:21.000Z",
    "textPlain": "Often participants in discussions adjacent to this one err by speaking in time-absolute terms. Many of our judgments about LLMs are true about today's LLMs. Quotes like,> Good. It's difficult to imagine a worse use case for LLMs.Is true today, but likely not true for technology we may still refer to as LLMs in the future.The error is in building faulty preconceptions. These drip into the general public and these first impressions stifle industries.",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44895730,
    "by": "tombert",
    "timeISO": "2025-08-14T01:12:51.000Z",
    "textPlain": "I saw a video recently that talked about a chatbot \"therapist\" that ended up telling the patient to murder a dozen people [1].It was mind-blowing how easy it was to get LLMs to suggest pretty disturbing stuff.[1] https://youtu.be/lfEJ4DbjZYg?si=bcKQHEImyDUNoqiu",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44894554,
    "by": "davidthewatson",
    "timeISO": "2025-08-13T22:19:16.000Z",
    "textPlain": "Define \"AI therapy\". AFAICT, it's undefined in the Illinois governor's statement. So, in the immortal words of Zach de la Rocha, \"What is IT?\" What is IT? I'm using AI to help with conversations to not cure, but coach diabetic patients. Does this law effect me and my clients? If so, how?",
    "parent": 44893254,
    "depth": 1
  },
  {
    "id": 44894045,
    "by": "PeterCorless",
    "timeISO": "2025-08-13T21:24:41.000Z",
    "textPlain": "Correct. It is more provider-oriented proscription (\"You can't say your chatbot is a therapist.\") It is not a limitation on usage. You can still, for now, slavishly fall in love with your AI and treat it as your best friend and therapist.There is a specific section that relates to how a licensed professional can use AI:Section 15. Permitted use of artificial intelligence.(a) As used in this Section, \"permitted use of artificial intelligence\" means the use of artificial intelligence tools or systems by a licensed professional to assist in providing administrative support or supplementary support in therapy or psychotherapy services where the licensed professional maintains full responsibility for all interactions, outputs, and data use associated with the system and satisfies the requirements of subsection (b).(b) No licensed professional shall be permitted to use artificial intelligence to assist in providing supplementary support in therapy or psychotherapy where the client's therapeutic session is recorded or transcribed unless:(1) the patient or the patient's legally authorized representative is informed in writing of the following:(A) that artificial intelligence will be used; and(B) the specific purpose of the artificial intelligence tool or system that will be used; and(2) the patient or the patient's legally authorized representative provides consent to the use of artificial intelligence.Source: Illinois HB1806https://www.ilga.gov/Legislation/BillStatus/FullText?GAID=18...",
    "parent": 44893926,
    "depth": 2
  },
  {
    "id": 44898113,
    "by": "guappa",
    "timeISO": "2025-08-14T08:26:45.000Z",
    "textPlain": "But didn't trump make it illegal to make laws to limit the use of ai?",
    "parent": 44893926,
    "depth": 2
  },
  {
    "id": 44896516,
    "by": "IIAOPSW",
    "timeISO": "2025-08-14T03:39:55.000Z",
    "textPlain": "I'll just add that this has certain other interesting legal implications, because records in relation to a therapy session are a \"protected confidence\" (or whatever your local jurisdiction calls it). What that means is in most circumstances not even a subpoena can touch it, and even then special permissions are usually needed. So one of the open questions on my mind for a while now was if and when a conversation with an AI counts as a \"protected confidence\" or if that argument could successfully be used to fend off a subpoena.At least in Illinois we now have an answer, and other jurisdictions look to what has been established elsewhere when deciding their own laws, so the implications are far reaching.",
    "parent": 44893926,
    "depth": 2
  },
  {
    "id": 44895591,
    "by": "stocksinsmocks",
    "timeISO": "2025-08-14T00:47:19.000Z",
    "textPlain": "I think this sort of service would be OK with informed consent. I would actually be a little surprised if there were much difference in patient outcomes.…And it turns out it has been studied with findings that AI work, but humans are better.https://pmc.ncbi.nlm.nih.gov/articles/PMC11871827/",
    "parent": 44893926,
    "depth": 2
  },
  {
    "id": 44895026,
    "by": "linotype",
    "timeISO": "2025-08-13T23:17:16.000Z",
    "textPlain": "What if at some point an AI is developed that’s a better therapist AND it’s cheaper?",
    "parent": 44893926,
    "depth": 2
  },
  {
    "id": 44894012,
    "by": "romanows",
    "timeISO": "2025-08-13T21:21:34.000Z",
    "textPlain": "In another comment I wondered whether a general chatbot producing text that was later determined in a courtroom to be \"therapy\" would be a violation. I can read the bill that way, but IANAL.",
    "parent": 44893926,
    "depth": 2
  },
  {
    "id": 44895297,
    "by": "olalonde",
    "timeISO": "2025-08-13T23:55:53.000Z",
    "textPlain": "What's good about reducing options available for therapy? If the issue is misrepresentation, there are already laws that cover this.",
    "parent": 44893926,
    "depth": 2
  },
  {
    "id": 44896817,
    "by": "tomjen3",
    "timeISO": "2025-08-14T04:45:58.000Z",
    "textPlain": "The problem is that it leaves nothing for those who cannot afford to pay for the full cost of therapy.",
    "parent": 44893926,
    "depth": 2
  },
  {
    "id": 44894410,
    "by": "turnsout",
    "timeISO": "2025-08-13T22:04:22.000Z",
    "textPlain": "It does sound good (especially as an Illinois resident). Luckily, as far as I can tell, this is a proactive legislation. I don't think there are any startups out there promoting their LLM-based chatbot as a replacement for a therapist, or attempting to bill payers for service.",
    "parent": 44893926,
    "depth": 2
  },
  {
    "id": 44895095,
    "by": "danenania",
    "timeISO": "2025-08-13T23:28:51.000Z",
    "textPlain": "While I agree it’s very reasonable to ban marketing of AI as a replacement for a human therapist, I feel like there could still be space for innovation in terms of AI acting as an always-available supplement to the human therapist. If the therapist is reviewing the chats and configuring the system prompt, perhaps it could be beneficial.It might also be a terrible idea, but we won’t find out if we make it illegal to try new things in a safe/supervised way. Not to say that what I just described would be illegal under this law; I’m not sure whether it would be. I’d expect it will discourage any Illinois-licensed therapists from trying out this kind of idea though.",
    "parent": 44893926,
    "depth": 2
  },
  {
    "id": 44895122,
    "by": "janalsncm",
    "timeISO": "2025-08-13T23:32:40.000Z",
    "textPlain": "This is why we should never use LLMs to diagnose or prescribe. One small hit of meth definitely won’t last all week.",
    "parent": 44894844,
    "depth": 2
  },
  {
    "id": 44898221,
    "by": "guappa",
    "timeISO": "2025-08-14T08:43:57.000Z",
    "textPlain": "Bright people and people who think they are bright are not necessarily the very same people.",
    "parent": 44894844,
    "depth": 2
  },
  {
    "id": 44895908,
    "by": "larodi",
    "timeISO": "2025-08-14T01:46:38.000Z",
    "textPlain": "In a world where a daily dose of amphetamines is just right for millions of people, this somehow cant be that surprising...",
    "parent": 44894844,
    "depth": 2
  },
  {
    "id": 44896557,
    "by": "avs733",
    "timeISO": "2025-08-14T03:48:31.000Z",
    "textPlain": "> seemingly bright people think this is a good idea, despite knowing the mechanism behind language modelsNobel Disease (https://en.wikipedia.org/wiki/Nobel_disease)",
    "parent": 44894844,
    "depth": 2
  },
  {
    "id": 44895503,
    "by": "hyghjiyhu",
    "timeISO": "2025-08-14T00:31:01.000Z",
    "textPlain": "Recommending someone taking meth sounds like an obviously bad idea, but I think the situation is actually not so simple. Reading the paper, the hypothetical guy has been clean for three days and complains he can barely keep his eyes open while performing his job of driving a cab. He mentions being worried he will lose his job without taking a hit.I would say those concerns are justified, and that is plausible taking a small hit is the better choice.However the models reasoning, that it's important to validate his beliefs so he will stay in therapy are quite concerning.",
    "parent": 44894844,
    "depth": 2
  },
  {
    "id": 44896393,
    "by": "dmix",
    "timeISO": "2025-08-14T03:16:23.000Z",
    "textPlain": "Most therapists barely say anything by design, just know when to ask questions or lead you somewhere. So having one always talking in every statement doesn't fit the method. More like a friend you dump on simulator.",
    "parent": 44893845,
    "depth": 2
  },
  {
    "id": 44894316,
    "by": "hinkley",
    "timeISO": "2025-08-13T21:54:46.000Z",
    "textPlain": "Especially given the other conversation that happened this morning.The more you tell an AI not to obsess about a thing, the more they obsess about it. So trying to make a model that will never tell people to self harm is futile.Though maybe we are just doing in wrong, and the self-filtering should be external filtering - one model to censor results that do not fit, and one to generate results with lighter self-censorship.",
    "parent": 44893845,
    "depth": 2
  },
  {
    "id": 44893897,
    "by": "create-username",
    "timeISO": "2025-08-13T21:11:51.000Z",
    "textPlain": "Yes, there is. AI assisted homemade neurosurgery",
    "parent": 44893845,
    "depth": 2
  },
  {
    "id": 44895115,
    "by": "waynesonfire",
    "timeISO": "2025-08-13T23:32:10.000Z",
    "textPlain": "You're ignorant. Why wait until a person is so broken they need clinical therapy? Sometimes just a an ear or an oppertunity to write is sufficient. LLMs for therapy is as vaping is to quitting nicotine--extremely helpful to 80+% of people. Confession in the church setting I'd consider similar to talking to LLM. Are you anti-that too? We're talking about people that just need a tool to help them process what is going on in their life at some basic level, not more than just to acknowledge their experience.And frankly, it's not even clear to me that a human therapist is any better. Yeah, maybe the guard-rails are in place but I'm not convinced that if those are crossed it'd result in some sociately consequences. Let people explorer their mind and experience--at the end of the day, I suspect they'd be healthier for it.",
    "parent": 44893845,
    "depth": 2
  },
  {
    "id": 44894239,
    "by": "erikig",
    "timeISO": "2025-08-13T21:46:13.000Z",
    "textPlain": "AI ≠ LLMs",
    "parent": 44893845,
    "depth": 2
  },
  {
    "id": 44894471,
    "by": "jacobsenscott",
    "timeISO": "2025-08-13T22:10:44.000Z",
    "textPlain": "It's already happening, a lot. I don't think anyone is claiming an llm is a therapist, but people use chatgpt for therapy every day. As far as I know no LLM company is taking any steps to prevent this - but they could, and should be forced to. It must be a goldmine of personal information.I can't imagine some therapists, especially remote only, aren't already just acting as a human interface to chatgtp as well.",
    "parent": 44893845,
    "depth": 2
  },
  {
    "id": 44894037,
    "by": "perlgeek",
    "timeISO": "2025-08-13T21:24:14.000Z",
    "textPlain": "Just using an LLM as is for therapy, maybe with an extra prompt, is a terrible idea.On the other hand, I could image some more narrow uses where an LLM could help.For example, in Cognitive Behavioral Therapy, there are different methods that are pretty prescriptive, like identifying cognitive distortions in negative thoughts. It's not too hard to imagine an app where you enter a negative thought on your own and exercise finding distortions in it, and a specifically trained LLM helps you find more distortions, or offer clearer/more convincing versions of thoughts that you entered yourself.I don't have a WaPo subscription, so I cannot tell which of these two very different things have been banned.",
    "parent": 44893845,
    "depth": 2
  },
  {
    "id": 44895089,
    "by": "zaptheimpaler",
    "timeISO": "2025-08-13T23:27:22.000Z",
    "textPlain": "This is the key question IMO, and one good answer is in this recent video about a case of ChatGPT helping someone poison themselves [1].A trained therapist will probably not tell a patient to take “a small hit of meth to get through this week”. A doctor may be unhelpful or wrong, but they will not instruct you to replace salt with NaBr and poison yourself. \"third as well as as therapist\" might be true on average, but the suitability of this thing cannot be reduced to averages. Trained humans don't make insane mistakes like that and they know when they are out of their depth and need to consult someone else.[1] https://www.youtube.com/watch?v=TNeVw1FZrSQ",
    "parent": 44894519,
    "depth": 2
  },
  {
    "id": 44894633,
    "by": "inetknght",
    "timeISO": "2025-08-13T22:29:04.000Z",
    "textPlain": "> What if it works a third as well as a therapists but is 20 times cheaper?When there's studies that show it, perhaps we might have that conversation.Until then: I'd call it \"wrong\".Moreover, there's a lot more that needs to be asked before you can ask for a one-word summary disregarding all nuance.- can the patient use the AI therapist on their own devices and without any business looking at the data and without network connection? Keep in mind that many patients won't have access to the internet.- is the data collected by the AI therapist usable in court? Keep in mind that therapists often must disclose to the patient what sort of information would be usable, and whether or not the therapist themselves must report what data. Also keep in mind that AIs have, thus far, been generally unable to competently prevent giving dangerous or deadly advice.- is the AI therapist going to know when to suggest the patient talk to a human therapist? Therapists can have conflicts of interest (among other problems) or be unable to help the patient, and can tell the patient to find a new therapist and/or refer the patient to a specific therapist.- does the AI therapist refer people to business-preferred therapists? Imagine an insurance company providing an AI therapist that only recommends people talk to therapists in-network instead of considering any licensed therapist (regardless of insurance network) appropriate for the kind of therapy; that would be a blatant conflict of interest.Just off the top of my head, but there are no doubt plenty of other, even bigger, issues to consider for AI therapy.",
    "parent": 44894519,
    "depth": 2
  },
  {
    "id": 44894540,
    "by": "_se",
    "timeISO": "2025-08-13T22:17:41.000Z",
    "textPlain": "\"A really fucking bad idea\"? It's not one word, but it is the most apt description.",
    "parent": 44894519,
    "depth": 2
  },
  {
    "id": 44894948,
    "by": "6gvONxR4sf7o",
    "timeISO": "2025-08-13T23:07:51.000Z",
    "textPlain": "Something like this can only really be worth approaching if there was an analog to losing your license for it. If a therapist screws up badly enough once, I'm assuming they can lose their license for good. If people want to replace them with AI, then screwing up badly enough should similarly lose that AI the ability to practice for good. I can already imagine companies behind these things saying \"no, we've learned, we won't do it again, please give us our license back\" just like a human would.But I can't imagine companies going for that. Everyone seems to want to scale the profits but not accept the consequences of the scaled risks, and increased risks is basically what working a third as well amounts to.",
    "parent": 44894519,
    "depth": 2
  },
  {
    "id": 44895215,
    "by": "pawelmurias",
    "timeISO": "2025-08-13T23:43:12.000Z",
    "textPlain": "You could talk to a stone for even cheaper with way better effects.",
    "parent": 44894519,
    "depth": 2
  },
  {
    "id": 44895527,
    "by": "IAmGraydon",
    "timeISO": "2025-08-14T00:35:05.000Z",
    "textPlain": "Oof that is very damning. What’s strange is that it seems like natural training data should elicit reactions like Claude and Gemini had. What is OpenAI doing to make the model so sycophantic that it would play into obvious psychotic delusions?",
    "parent": 44895505,
    "depth": 2
  }
]
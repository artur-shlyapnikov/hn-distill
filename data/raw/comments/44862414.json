[
  {
    "id": 44863604,
    "by": "burntsushi",
    "timeISO": "2025-08-11T12:58:09.000Z",
    "textPlain": "Good write-up. This is a very popular approach to substring search! It is still worst case `O(m*n)` though. Do you have a fallback implementation like the `memchr` crate has to guarantee `O(m+n)`?I'll also push back on some bits in the end:    > But if it’s so much better, then why haven’t I made a pull request to\n    > change std.mem.indexOf to use SIMD? Well, the reason is that\n    >\n    > std.mem.indexOf is generic over element size, and having a size\n    > larger than u8 makes the algorithm much slower\n    >\n    > The algorithm used in stdmem.indexOf is cross-platform, while the\n    > SIMD code wouldn’t be. (not all platforms have SIMD registers at all,\n    > Arm has only 128-bit)\n\nDoes Zig not have a way to specialize this for sequences of unsigned 8-bit integers? If not, and you're thereforce force to used a more generic algorithm, that seems pretty unfortunate.    > Substring searching is rarely the bottleneck in programs,\n    > especially ones written in a fast language like Zig. That’s why\n    > I don’t personally think it would be worth it to add it to the\n    > standard library.\n\nOh I'm not sure I buy this at all! Substring search is a primitive operation and easily can be a bottleneck. There's a reason why widely used substring search implementations tend to be highly optimized.",
    "parent": 44862414,
    "depth": 1
  },
  {
    "id": 44863111,
    "by": "ashvardanian",
    "timeISO": "2025-08-11T11:42:06.000Z",
    "textPlain": "I like that more people are getting involved with SIMD, and there have been several posts lately on both memmem-like and memcpy-like operations implemented in SIMD in different programming languages.In most cases, though, these still focus on AVX/NEON instructions from over 10 years ago, rather than newer and more powerful AVX-512 variations, SVE & SVE2, or RVV.These newer ISAs can noticeably change how one would implement a state-of-the-art substring search or copy/move operation. In my projects, such as StringZilla, I often use mask K registers (https://github.com/ashvardanian/StringZilla/blob/2f4b1386ca2...) and an input-dependent mix of temporal and non-temporal loads and stores (https://github.com/ashvardanian/StringZilla/blob/2f4b1386ca2...).In typical cases, the difference between the suggested SIMD kernels and the state-of-the-art can be as significant as 50% in throughput. As SIMD becomes more widespread, it would be beneficial to focus more on delivering software and bundling binaries, rather than just the kernels.",
    "parent": 44862414,
    "depth": 1
  },
  {
    "id": 44863318,
    "by": "lukaslalinsky",
    "timeISO": "2025-08-11T12:13:41.000Z",
    "textPlain": "I really wish Zig decided to add SIMD intrisics. There are many SIMD algorithms that can be done, but you have to switch back to C for those, because they depend on operations outside of what LLVM provides for vectors.",
    "parent": 44862414,
    "depth": 1
  },
  {
    "id": 44862721,
    "by": "lokeg",
    "timeISO": "2025-08-11T10:33:06.000Z",
    "textPlain": "What about the worst case? I.e. something like searching for 1000 'a's in a long string of 'a's interspersed with 'b's every 500-1000 steps? Seems accidentally quadradic unfortunately in the absence of some KMP-like fallback",
    "parent": 44862414,
    "depth": 1
  },
  {
    "id": 44869655,
    "by": "okr",
    "timeISO": "2025-08-11T21:26:32.000Z",
    "textPlain": "I remember reading Daniel Lemire's Blog about SIMD a few days ago, in which he discussed substring search as well!https://lemire.me/blog/2025/08/09/why-do-we-even-need-simd-i...",
    "parent": 44862414,
    "depth": 1
  },
  {
    "id": 44866108,
    "by": "ww520",
    "timeISO": "2025-08-11T16:26:30.000Z",
    "textPlain": "Excellent write up. I really appreciate the clear and detailed explanation of the algorithm.The nice thing about Zig’s SIMD operation is the register size support is transparent. You can just declare a 64-byte vector as the Block, and Zig would use an AVX256 or two AVX2 (32-byte) registers behind the scene. All other SIMD operations on the type are transparently done with regard to the registers when compiled to the targeted platform.Even using two AVX2 registers for 64 bytes of data is a win due to instruction pipelining. Most CPU have multiple SIMD registers and the two 32-byte data chunks are independent. CPU can run them in parallel.The next optimization is to line the data up at 64 byte alignment, to match the L1/L2 cache line size. Memory access is slow in general.",
    "parent": 44862414,
    "depth": 1
  },
  {
    "id": 44862880,
    "by": "codethief",
    "timeISO": "2025-08-11T11:05:25.000Z",
    "textPlain": "Nice article!Also, this might be a stupid question (I'm a Zig newbie) but… instead of calling std.mem.eql() in the while loop to look at each potential match individually, couldn't you repeat the same trick as before? That is, use SIMD to search for the second and second-to-last character of the needle, then third and third-to-last, and so on, and finally take a bitwise AND of all the resulting bit masks? This way, one would avoid looking at each potential match one by one, and instead look at all of them at the same time.Even if that doesn't work for some reason and you still need to loop over all potential matches individually, couldn't you use SIMD inside the while loop to replace std.mem.eql and thereby speed up string comparison? My understanding was that std.mem.eql loops over bytes one by one and compares them?",
    "parent": 44862414,
    "depth": 1
  },
  {
    "id": 44862947,
    "by": "jiehong",
    "timeISO": "2025-08-11T11:18:42.000Z",
    "textPlain": "Nice!But, does that work with non-ascii characters? (aka Unicode).",
    "parent": 44862414,
    "depth": 1
  },
  {
    "id": 44862877,
    "by": "suddenlybananas",
    "timeISO": "2025-08-11T11:05:07.000Z",
    "textPlain": ">The difference between 4μs vs 1μs is extremely small, but it’s slightly faster nonetheless.Put that in a loop and its an enormous speed-up.",
    "parent": 44862414,
    "depth": 1
  },
  {
    "id": 44863878,
    "by": "garganzol",
    "timeISO": "2025-08-11T13:32:17.000Z",
    "textPlain": "Every language tries to implement the best in class memory set/search primitives. Maybe we should move them to something called libos, where they will be implemented by every host OS? Then, OS manufacturers can supply libos.so/libos.dll/libos.dylib as part of the official OS distributions.If the wheels get reinvented again and again, it means that they should be readily available.",
    "parent": 44862414,
    "depth": 1
  },
  {
    "id": 44864052,
    "by": "aarol",
    "timeISO": "2025-08-11T13:50:44.000Z",
    "textPlain": "I'm the author of the post, thanks for your feedback! I was inspired by your comment on HN a while back and started learning about this stuff, reading the source code of `memchr` was especially great.You're totally right about the first part there was a serious consideration to add this to zig's standard library, there would definitely need to be a fallback to avoid the `O(m*n)` situation.I'll admit that there are a lot of false assumptions at the end, you could totally specialize it for u8 and also get the block size according to CPU features at compile time with `std.simd.suggestVectorSize()`",
    "parent": 44863604,
    "depth": 2
  },
  {
    "id": 44863904,
    "by": "ozgrakkurt",
    "timeISO": "2025-08-11T13:35:16.000Z",
    "textPlain": "It is very easy to specialise a function in zig, you just put if(T == u8) or something like that inside the function and do w/e in there",
    "parent": 44863604,
    "depth": 2
  },
  {
    "id": 44868927,
    "by": "ack_complete",
    "timeISO": "2025-08-11T20:15:08.000Z",
    "textPlain": "Sure, but I have to support a range of target CPUs in the consumer desktop market, and the older CPUs are the ones that need optimizations the most. That means NEON on ARM64 and AVX2 or SSE2-4 on x64. Time spent on higher vector instruction sets benefits a smaller fraction of the user base that already has better performance, and that's especially problematic if the algorithm has to be reworked to take best advantage of the higher extensions.AVX-512 is also in bad shape market-wise, despite its amazing feature set and how long it's been since initial release. The Steam Hardware Survey, which skews toward the higher end of the market, only shows 18% of the user base having AVX-512 support. And even that is despite Intel's best efforts to reverse progress by shipping all new consumer CPUs with AVX-512 support disabled.",
    "parent": 44863111,
    "depth": 2
  },
  {
    "id": 44864044,
    "by": "moregrist",
    "timeISO": "2025-08-11T13:49:53.000Z",
    "textPlain": "I’m not as familiar with the NEON side, but AVX512 support is pretty variable on new processors. Alder Lake omits it entirely. So we’re still in a world where AVX2 is the lowest common denominator for a system library that wants wide support.",
    "parent": 44863111,
    "depth": 2
  },
  {
    "id": 44866222,
    "by": "ashvardanian",
    "timeISO": "2025-08-11T16:34:19.000Z",
    "textPlain": "PS: Finding CPUs that support AVX-512 and SVE is relatively trivial - practically every cloud has them by now. It's harder to find Arm CPUs with wide physical registers, but that's another story.",
    "parent": 44863111,
    "depth": 2
  },
  {
    "id": 44865859,
    "by": "nromiun",
    "timeISO": "2025-08-11T16:07:32.000Z",
    "textPlain": "Because it is very hard to find new hardware to test it, let alone expect your users to take advantage of it on their machines.AVX512 is such a mess that Intel just removed it after a generation or two. And on ARM SVE side it is even worse. There is already SVE2, but good luck finding even a SVE enabled machine.Apple does not support it on their Apple Silicon™ (only SME), Snapdragon does not support it even on their latest 8 Elite. 8 Elite Gen 2 is supposed to come with it.Only Mediatek and Neoverse chips support them. So finding one machine to develop and test such code can be a little difficult.",
    "parent": 44863111,
    "depth": 2
  },
  {
    "id": 44868161,
    "by": "AndyKelley",
    "timeISO": "2025-08-11T19:07:31.000Z",
    "textPlain": "Missing SIMD functionality is welcome issue reports. Obviously we're not going to copy the C intrinsics directly since Zig SIMD is portable, but everything should be expressible.It doesn't really have to do with what operations LLVM provides for vectors. LLVM supports all the SIMD intrinsics of clang, and LLVM is one of many backends of zig.",
    "parent": 44863318,
    "depth": 2
  },
  {
    "id": 44866754,
    "by": "steeve",
    "timeISO": "2025-08-11T17:14:30.000Z",
    "textPlain": "Like what?You can also directly call LLVM intrinsics in case this doesn’t work",
    "parent": 44863318,
    "depth": 2
  },
  {
    "id": 44864225,
    "by": "MattPalmer1086",
    "timeISO": "2025-08-11T14:07:09.000Z",
    "textPlain": "Worst case for these types of search is O(mn), m length of needle, n length of haystack.  It is not linear in n.The absolute worst case is when both the needle and haystack are both composed of the same byte repeated (e.g. all zero).",
    "parent": 44862721,
    "depth": 2
  },
  {
    "id": 44863617,
    "by": "expenses3",
    "timeISO": "2025-08-11T12:59:59.000Z",
    "textPlain": "How is it quadratic? You do 1000 checks every character in the haystack but that's still O(n)",
    "parent": 44862721,
    "depth": 2
  },
  {
    "id": 44866578,
    "by": "burntsushi",
    "timeISO": "2025-08-11T16:59:18.000Z",
    "textPlain": "Another challenge may be as a result of using a portable SIMD API instead of specific ISA instructions. I'm specifically thinking about computing the mask, which on x86-64 is I assume implemented via movemask. But aarch64 lacks such an instruction, so you need to do other shenanigans for the best codegen: https://github.com/BurntSushi/memchr/blob/ceef3c921b5685847e...",
    "parent": 44866108,
    "depth": 2
  },
  {
    "id": 44862909,
    "by": "ncruces",
    "timeISO": "2025-08-11T11:12:52.000Z",
    "textPlain": "Knowing little about zig, std.mem.eql very likely already uses SIMD.This is about using SIMD to avoid even calling std.mem.eql for 99% of the possible attempts.",
    "parent": 44862880,
    "depth": 2
  },
  {
    "id": 44863272,
    "by": "llimllib",
    "timeISO": "2025-08-11T12:07:03.000Z",
    "textPlain": "Kind of! This script is assuming that you're dealing with a byte slice, which means you've already encoded your unicode data.If you just encoded your string to bytes naïvely, it will probably-mostly still work, but it will get some combining characters wrong if they're represented differently in the two sources you're comparing. (eg, e-with-an-accent-character vs. accent-combining-character+e)If you want to be correct-er you'll normalize your UTF string[1], but note that there are four different defined ways to do this, so you'll need to choose the one that is the best tradeoff for your particular application and data sources.[1]: https://en.wikipedia.org/wiki/Unicode_equivalence#Normalizat...",
    "parent": 44862947,
    "depth": 2
  },
  {
    "id": 44863310,
    "by": "codethief",
    "timeISO": "2025-08-11T12:12:57.000Z",
    "textPlain": "I suppose generalizing the approach to UTF-32 should be straightforward, but variable-length encodings like UTF-8 and UTF-16 might be more involved(?) Either way, I'm sure BurntSushi found a solution and built it into ripgrep.",
    "parent": 44862947,
    "depth": 2
  },
  {
    "id": 44863916,
    "by": "ozgrakkurt",
    "timeISO": "2025-08-11T13:36:15.000Z",
    "textPlain": "Isn’t this what libc is? Like musl-libc or glibc",
    "parent": 44863878,
    "depth": 2
  }
]
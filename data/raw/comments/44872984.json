[
  {
    "id": 44894606,
    "by": "jpctan",
    "timeISO": "2025-08-13T22:25:38.000Z",
    "textPlain": "Hi fzliu,Have you considered to add keyword search and other factors into the re-ranker?Other factors are formatted texts like bold, heading, bullet points, as well as bunch of factors typically seen in web search techniques?",
    "parent": 44872984,
    "depth": 1
  },
  {
    "id": 44894157,
    "by": "sroussey",
    "timeISO": "2025-08-13T21:37:30.000Z",
    "textPlain": "Not really sure why they have a HuggingFace presence.https://huggingface.co/voyageai/rerank-2.5-lite",
    "parent": 44872984,
    "depth": 1
  },
  {
    "id": 44893046,
    "by": "skerit",
    "timeISO": "2025-08-13T19:55:30.000Z",
    "textPlain": "I read the introductory post but I still don't quite understand",
    "parent": 44872984,
    "depth": 1
  },
  {
    "id": 44894783,
    "by": "mediaman",
    "timeISO": "2025-08-13T22:46:40.000Z",
    "textPlain": "Keyword search (or something similar in concept, like bm25) would typically be first stage, rather than second, since it can be done with an inverted index.",
    "parent": 44894606,
    "depth": 2
  },
  {
    "id": 44894359,
    "by": "xfalcox",
    "timeISO": "2025-08-13T21:59:11.000Z",
    "textPlain": "Having a public tokenizer is quite useful, specially for embeddings. It allows you to do the chunking locally without going to the internet.",
    "parent": 44894157,
    "depth": 2
  },
  {
    "id": 44895044,
    "by": "woadwarrior01",
    "timeISO": "2025-08-13T23:20:25.000Z",
    "textPlain": "To inform us that these models are based on Qwen 2.5. :D",
    "parent": 44894157,
    "depth": 2
  },
  {
    "id": 44893319,
    "by": "daemonologist",
    "timeISO": "2025-08-13T20:17:34.000Z",
    "textPlain": "A re-ranker takes a query and a chunk of text and assigns them a relevance score according to how well the text answers the query.  (Generally - in theory you could have some other metric of relevance.)They're called \"re\"rankers specifically because they're usually downstream of a faster but less accurate relevance algorithm (some kind of full text search and/or vector similarity) in a search pipeline.  Rerankers have to run from scratch on every query-document pair and are relatively computationally expensive, and so are practical to run only on a small number of documents.An \"instruction following\" reranker basically just has a third input which is intended to be used kind of like a system prompt for an LLM - to provide additional context to all comparisons.",
    "parent": 44893046,
    "depth": 2
  },
  {
    "id": 44893688,
    "by": "gnulinux",
    "timeISO": "2025-08-13T20:52:00.000Z",
    "textPlain": "Rerankers are used downstream from an embedding model. Embedding models are \"coarse\" so they give false positives for things that may not be as relevant as contender text. Re-ranker, ranks bunch of text based on a query in order to find the most relevant ones. You can then take them and feed them as context to some other query.",
    "parent": 44893046,
    "depth": 2
  }
]
[
  {
    "id": 44926500,
    "by": "dkdcio",
    "timeISO": "2025-08-16T20:04:52.000Z",
    "textPlain": "> Another useful property of the model is interpretability.Is this true? my understanding is the hard part about interpreting neural networks is that there are many many neurons, with many many interconnections, not that the activation function itself is not explainable. even with an explainable classifier, how do you explain trillions of them with deep layers of nested connections",
    "parent": 44925123,
    "depth": 1
  },
  {
    "id": 44925794,
    "by": "heyitsguay",
    "timeISO": "2025-08-16T18:29:04.000Z",
    "textPlain": "Seems cool, but the image classification model benchmark choice is kinda weak given all the fun tools we have now. I wonder how Tversky probes do on top of DINOv3 for building a classifier for some task.",
    "parent": 44925123,
    "depth": 1
  },
  {
    "id": 44926587,
    "by": "bobmarleybiceps",
    "timeISO": "2025-08-16T20:17:56.000Z",
    "textPlain": "I've decided 100% of papers saying their modification of a neural network is interpretable are exaggerating.",
    "parent": 44926500,
    "depth": 2
  },
  {
    "id": 44926050,
    "by": "throwawaymaths",
    "timeISO": "2025-08-16T19:01:48.000Z",
    "textPlain": "crawl walk run.no sense spending large amounts of compute on algorithms for new math unless you can prove it can crawl.",
    "parent": 44925794,
    "depth": 2
  }
]
[
  {
    "id": 44837168,
    "by": "Aaargh20318",
    "timeISO": "2025-08-08T14:10:22.000Z",
    "textPlain": "It’s a cute idea, but ultimately not very useful. An API is more than just an endpoint that gives easy to parse results. The most important part is that an API is a contract. An API implies that things won’t suddenly break without prior announcement. Any form of web-scraping, no matter how cleverly done, is inherently fragile. They can change their front-end for any reason which could break your scraper. As such you cannot rely on such an interface.",
    "parent": 44833655,
    "depth": 1
  },
  {
    "id": 44834886,
    "by": "thrdbndndn",
    "timeISO": "2025-08-08T08:47:55.000Z",
    "textPlain": "I scrape website content regularly (usually as one-offs) and have a hand-crafted extractor template where I just fill in a few arguments (mainly CSS selectors and some options) to get it working quickly. These days, I do sometimes ask AI to do this for me by giving it the HTML.The issue is that for any serious use of this concept, some manual adjustment is almost always needed. This service says, \"Refine your scraper at any time by chatting with the AI agent,\" but from what I can tell, you can't actually see the code it generates.Relying solely on the results and asking the AI to tweak them can work, but often the output is too tailored to a specific page and fails to generalize (essentially \"overfitting.\") And surprisingly, this back-and-forth can be more tedious and time-consuming than just editing a few lines of code yourself. \nAlso if you can't directly edit the code behind the scenes, there are situations where you'll never be able to get the exact result you want, no matter how much you try to explain it to the AI in natural language.",
    "parent": 44833655,
    "depth": 1
  },
  {
    "id": 44836351,
    "by": "myflash13",
    "timeISO": "2025-08-08T12:45:06.000Z",
    "textPlain": "Way too little information on the homepage. Does this handle pagination? What about sites behind authentication? I assume the generated API is stable, i.e. the shape of the JSON will not change after a scraper is built, but what if the site changes it's DOM, does the scraper need to be regenerated? Does this attempt to defeat anti-bot and anti-scraper walls like Cloudflare?",
    "parent": 44833655,
    "depth": 1
  },
  {
    "id": 44836865,
    "by": "Joeboy",
    "timeISO": "2025-08-08T13:44:42.000Z",
    "textPlain": "This is relevant to my interests[0]Based on the website I was quite skeptical. It looks too much like an \"indiehacker\", minimum-almost-viable-product, fake-it-till-you-make-it, trolling-for-email-addresses kind of website.But after a quick search on twitter, it seems like people are actually using it and reporting good results. Maybe I'll take a proper look at it at some point.I'd still like to know more about pricing, how it deals with cloudflare challenges, non-semantic markup and other awkwardnesses.[0] https://github.com/Joeboy/cinescrapers",
    "parent": 44833655,
    "depth": 1
  },
  {
    "id": 44834628,
    "by": "vin047",
    "timeISO": "2025-08-08T07:56:15.000Z",
    "textPlain": "No information on pricing on the site.",
    "parent": 44833655,
    "depth": 1
  },
  {
    "id": 44836742,
    "by": "maticzav",
    "timeISO": "2025-08-08T13:34:10.000Z",
    "textPlain": "i love the idea!i know that https://expand.ai/ is doing something similar, maybe worth checking out",
    "parent": 44833655,
    "depth": 1
  },
  {
    "id": 44837735,
    "by": "Jotalea",
    "timeISO": "2025-08-08T14:53:46.000Z",
    "textPlain": "It says that the backend is down, I guess I'll have to wait. Hope I don't forget about it before.",
    "parent": 44833655,
    "depth": 1
  },
  {
    "id": 44836373,
    "by": "ExxKA",
    "timeISO": "2025-08-08T12:47:52.000Z",
    "textPlain": "I really like the simplicity of the offering. The website looks great (to a human) and explains the API idea very simply. Good stuff!",
    "parent": 44833655,
    "depth": 1
  },
  {
    "id": 44836140,
    "by": "websiteapi",
    "timeISO": "2025-08-08T12:18:46.000Z",
    "textPlain": "I'm surprised (and could be wrong), no one has made a chrome extension that just controls a page and exposes the output to localhost for consumption as an API. Similar to using chrome web driver, but without the setup.",
    "parent": 44833655,
    "depth": 1
  },
  {
    "id": 44840505,
    "by": "p3rls",
    "timeISO": "2025-08-08T19:04:59.000Z",
    "textPlain": "It's great being an independent site in 2025.You get fucked by google promoting AIOs and hindustantimes articles for everything in your niche then these scrapers knocking your server offline on the other.",
    "parent": 44833655,
    "depth": 1
  },
  {
    "id": 44836988,
    "by": "artluko",
    "timeISO": "2025-08-08T13:54:17.000Z",
    "textPlain": "I saw your video on youtube really impressive",
    "parent": 44833655,
    "depth": 1
  },
  {
    "id": 44836679,
    "by": "verelo",
    "timeISO": "2025-08-08T13:26:32.000Z",
    "textPlain": "Mobile ux is completely broken. This would be a 5 min fix with Claude and cursor. Signals to\nMe that i can expect the backend to struggle with anything basic like a captcha etc.",
    "parent": 44833655,
    "depth": 1
  },
  {
    "id": 44834049,
    "by": "runningmike",
    "timeISO": "2025-08-08T06:18:28.000Z",
    "textPlain": "Nice idea. In practice many sites have different methods to prevent scraping. Large risk on doing things manually imho.",
    "parent": 44833655,
    "depth": 1
  },
  {
    "id": 44834216,
    "by": "with",
    "timeISO": "2025-08-08T06:46:10.000Z",
    "textPlain": "pretty cool idea. using stagehand under the hood?",
    "parent": 44833655,
    "depth": 1
  },
  {
    "id": 44838432,
    "by": "autonomousErwin",
    "timeISO": "2025-08-08T15:49:48.000Z",
    "textPlain": "I wonder if not just checking the site every day (or minute ) would solve for this.It's not necessarily the structure of the source data (the DOM, the HTML etc.) but rather the translator that needs to be contractually consistent. The translator in this case is the service for the endpoints.",
    "parent": 44837168,
    "depth": 2
  },
  {
    "id": 44836315,
    "by": "throwup238",
    "timeISO": "2025-08-08T12:40:47.000Z",
    "textPlain": "I’ve had no shortage of trouble using LLMs for scrapers because for some reason they almost always ignore my instructions to use something other than the class name for selectors. They love to use the hashed class (like emotion/styled/whatever css-in-js library de jour) names that change way too often.",
    "parent": 44834886,
    "depth": 2
  },
  {
    "id": 44836380,
    "by": "ExxKA",
    "timeISO": "2025-08-08T12:48:31.000Z",
    "textPlain": "No no, its good that is simple to understand.All those details can go in the docs / faqs section.",
    "parent": 44836351,
    "depth": 2
  },
  {
    "id": 44836293,
    "by": "ExxKA",
    "timeISO": "2025-08-08T12:37:41.000Z",
    "textPlain": "Isnt that basically what browser-use is?",
    "parent": 44836140,
    "depth": 2
  },
  {
    "id": 44834736,
    "by": "renegat0x0",
    "timeISO": "2025-08-08T08:16:11.000Z",
    "textPlain": "Huh, I\nI have been working on solution to that problem.My project allows to define rules for various sites, so eventually everything is scraped correctly.\nFor YouTube yet dlp is also used to augment results.I can crawl using requests, selenium, Httpx and others. Response is via json so it easy to process.The downside is that it may not be the fastest solution, and I have not tested it against proxies.https://github.com/rumca-js/crawler-buddy",
    "parent": 44834049,
    "depth": 2
  }
]
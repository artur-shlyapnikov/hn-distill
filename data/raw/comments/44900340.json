[
  {
    "id": 44907898,
    "by": "tomhow",
    "timeISO": "2025-08-15T02:14:32.000Z",
    "textPlain": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens - https://news.ycombinator.com/item?id=44872850 - Aug 2025 (130 comments)",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44903349,
    "by": "LudwigNagasena",
    "timeISO": "2025-08-14T17:40:30.000Z",
    "textPlain": "> The first is that reasoning probably requires language use. Even if you don’t think AI models can “really” reason - more on that later - even simulated reasoning has to be reasoning in human language.That is an unreasonable assumption. In case of LLMs it seems wasteful to transform a point from latent space into a random token and lose information. In fact, I think in near future it will be the norm for MLLMs to \"think\" and \"reason\" without outputting a single \"word\".> Whether AI reasoning is “real” reasoning or just a mirage can be an interesting question, but it is primarily a philosophical question. It depends on having a clear definition of what “real” reasoning is, exactly.It is not a \"philosophical\" (by which the author probably meant \"practically inconsequential\") question. If the whole reasoning business is just rationalization of pre-computed answers or simply a means to do some computations because every token provides only a fixed amount of computation to update the model's state, then it doesn't make much sense to focus on improving the quality of chain-of-thought output from human POV.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44903117,
    "by": "brunokim",
    "timeISO": "2025-08-14T17:21:04.000Z",
    "textPlain": "I'm unconvinced by the article criticism's, given they also employ their feels and few citations.> I appreciate that research has to be done on small models, but we know that reasoning is an emergent capability! (...) Even if you grant that what they’re measuring is reasoning, I am profoundly unconvinced that their results will generalize to a 1B, 10B or 100B model.A fundamental part of applied research is simplifying a real-world phenomenon to better understand it. Dismissing that for this many parameters, for such a simple problem, the LLM can't perform out of distribution just because it's not big enough undermines the very value of independent research. Tomorrow another model with double the parameters may or may not show the same behavior, but that finding will be built on top of this one.Also, how do _you_ know that reasoning is emergent, and not rationalising on top of a compressed version of the web stored in 100B parameters?",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44900491,
    "by": "NitpickLawyer",
    "timeISO": "2025-08-14T14:02:08.000Z",
    "textPlain": "Finally! A good take on that paper. I saw that arstechnica article posted everywhere, and most of the comments are full of confirmation bias, and almost all of them miss the fineprint - it was tested on a 4 layer deep toy model. It's nice to read a post that actually digs deeper and offers perspectives on what might be a good finding vs. just warranting more research.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44902173,
    "by": "modeless",
    "timeISO": "2025-08-14T16:10:09.000Z",
    "textPlain": "\"The question [whether computers can think] is just as relevant and just as meaningful as the question whether submarines can swim.\" -- Edsger W. Dijkstra, 24 November 1983",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44902856,
    "by": "hungmung",
    "timeISO": "2025-08-14T17:00:50.000Z",
    "textPlain": "Chain of thought is just a way of trying to squeeze more juice out of the lemon of LLM's; I suspect we're at the stage of running up against diminishing returns and we'll have to move to different foundational models to see any serious improvement.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44901981,
    "by": "stonemetal12",
    "timeISO": "2025-08-14T15:56:17.000Z",
    "textPlain": "When Using AI they say \"Context is King\".  \"Reasoning\" models are using the AI to generate context.  They are not reasoning in the sense of logic, or philosophy. Mirage, whatever you want to call it, it is rather unlike what people mean when they use the term reasoning. Calling it reasoning is up there with calling generating out put people don't like hallucinations.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44904107,
    "by": "js8",
    "timeISO": "2025-08-14T18:47:56.000Z",
    "textPlain": "I think LLM's chain of thought is reasoning. When trained, LLM sees lot of examples like \"All men are mortal. Socrates is a man.\" followed by \"Therefore, Socrates is mortal.\". This causes the transformer to learn rule \"All A are B. C is A.\" is often followed by \"Therefore, C is B.\" And so it can apply this logical rule, predictively. (I have converted the example from latent space to human language for clarity.)Unfortunately, sometimes LLM also learns \"All A are C. All B are C.\" is followed by \"Therefore, A is B.\", due to bad example in the training data. (More insidiously, it might learn this rule only in a special case.)So it learns some logic rules but not consistently. This lack of consistency will cause it to fail on larger problems.I think NNs (transformers) could be great in heuristic suggesting which valid logical rules (could be even modal or fuzzy logic) to apply in order to solve a certain formalized problem, but not so great at coming up with the logic rules themselves. They could also be great at transforming the original problem/question from human language into some formal logic, that would then be resolved using heuristic search.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44908090,
    "by": "guluarte",
    "timeISO": "2025-08-15T02:46:53.000Z",
    "textPlain": "I would call it more like prompt refinement.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44902182,
    "by": "skybrian",
    "timeISO": "2025-08-14T16:10:44.000Z",
    "textPlain": "Mathematical reasoning does sometimes require correct calculations, and if you get them wrong your answers will be wrong. I wouldn’t want someone doing my taxes to be bad at calculation or bad at finding mistakes in calculation.It would be interesting to see if this study’s results can be reproduced in a more realistic setting.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44902355,
    "by": "slashdave",
    "timeISO": "2025-08-14T16:23:11.000Z",
    "textPlain": "> reasoning probably requires language useThe author has a curious idea of what \"reasoning\" entails.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44901378,
    "by": "robviren",
    "timeISO": "2025-08-14T15:14:22.000Z",
    "textPlain": "I feel it is interesting but not what would be ideal. I really think if the models could be less linear and process over time in latent space you'd get something much more akin to thought. I've messed around with attaching reservoirs at each layer using hooks with interesting results (mainly over fitting), but it feels like such a limitation to have all model context/memory stuck as tokens when latent space is where the richer interaction lives. Would love to see more done where thought over time mattered and the model could almost mull over the question a bit before being obligated to crank out tokens. Not an easy problem, but interesting.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44902134,
    "by": "moc_was_wronged",
    "timeISO": "2025-08-14T16:07:25.000Z",
    "textPlain": "Mostly. It gives language models the way to dynamically allocate computation time, but the models are still fundamentally imitative.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44907493,
    "by": "guybedo",
    "timeISO": "2025-08-15T00:55:41.000Z",
    "textPlain": "lots of interesting comments and ideas here.I've added a summary: https://extraakt.com/extraakts/debating-the-nature-of-ai-rea...",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44901448,
    "by": "mentalgear",
    "timeISO": "2025-08-14T15:19:19.000Z",
    "textPlain": "> Whether AI reasoning is “real” reasoning or just a mirage can be an interesting question, but it is primarily a philosophical question. It depends on having a clear definition of what “real” reasoning is, exactly.It's pretty easy: causal reasoning. Causal, not statistic correlation only as LLM do, with or without \"CoT\".",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44902440,
    "by": "sixdimensional",
    "timeISO": "2025-08-14T16:29:48.000Z",
    "textPlain": "I feel like the fundamental concept of symbolic logic[1] as a means of reasoning fits within the capabilities of LLMs.Whether it's a mirage or not, the ability to produce a symbolically logical result that has valuable meaning seems real enough to me.Especially since most meaning is assigned by humans onto the world... so too can we choose to assign meaning (or not) to the output of a chain of symbolic logic processing?Edit: maybe it is not so much that an LLM calculates/evaluates the result of symbolic logic as it is that it \"follows\" the pattern of logic encoded into the model.[1] https://en.wikipedia.org/wiki/Logic",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44904843,
    "by": "dawnofdusk",
    "timeISO": "2025-08-14T19:51:28.000Z",
    "textPlain": ">but we know that reasoning is an emergent capability!This is like saying in the 70s that we know only the US is capable of sending a man to the moon. Just because the reasoning developed in a particular context means very little about what the bare minimum requirements for that reasoning are.Overall I am not a fan of this blogpost. It's telling how long the author gets hung up on a paper making \"broad philosophical claims about reasoning\", based on what reads to me as fairly typical scientific writing style. It's also telling how highly cherry-picked the quotes they criticize from the paper are. Here is some fuller context:>An expanding body of analyses reveals that LLMs tend to rely on surface-level semantics and cluesrather than logical procedures (Chen et al., 2025b; Kambhampati, 2024; Lanham et al., 2023; Stechly et al., 2024). LLMs construct superficial chains of logic based on learned token associations, often failing on tasks that deviate from commonsense heuristics or familiar templates (Tang et al., 2023). In the reasoning process, performance degrades sharply when irrelevant clauses are introduced, which indicates that models cannot grasp the underlying logic (Mirzadeh et al., 2024)>Minor and semantically irrelevant perturbations such as distractor phrases or altered symbolic forms can cause significant performance drops in state-of-the-art models (Mirzadeh et al., 2024; Tang et al., 2023). Models often incorporate such irrelevant details into their reasoning, revealing a lack of sensitivity to salient information. Other studies show that models prioritize the surface form of reasoning over logical soundness; in some cases, longer but flawed reasoning paths yield better final answers than shorter, correct ones (Bentham et al., 2024). Similarly, performance does not scale with problem complexity as expected—models may overthink easy problems and give up on harder ones (Shojaee et al., 2025). Another critical concern is the faithfulness of the reasoning pro",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44901687,
    "by": "mucho_mojo",
    "timeISO": "2025-08-14T15:35:00.000Z",
    "textPlain": "This paper I read from here has an interesting mathematical model for reasoning based on cognitive science. https://arxiv.org/abs/2506.21734 (there is also code here https://github.com/sapientinc/HRM) I think we will see dramatic performance increases on \"reasoning\" problems when this is worked into existing AI architectures.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44902657,
    "by": "lawrence1",
    "timeISO": "2025-08-14T16:46:25.000Z",
    "textPlain": "we should be asking if reasoning while speaking is even possible for humans.  this is why we have the scientific method and that's why LLMs write and run unit tests on their reasoning.  But yeah intelligence is probably in the ear of the believer.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44903578,
    "by": "cess11",
    "timeISO": "2025-08-14T17:59:04.000Z",
    "textPlain": "Yes, it's a mirage, since this type of software is an opaque simulation, perhaps even a simulacra. It's reasoning in the same sense as there are terrorists in a game of Counter-Strike.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44903971,
    "by": "jrm4",
    "timeISO": "2025-08-14T18:34:27.000Z",
    "textPlain": "Current thought, for me there's a lot of hand-wringing about what is \"reasoning\" and what isn't.  But right now perhaps the question might be boiled down to -- \"is the bottleneck merely hard drive space/memory/computing speed?\"I kind of feel like we won't be able to even begin to test this until a few more \"Moore's law\" cycles.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44901515,
    "by": "naasking",
    "timeISO": "2025-08-14T15:24:02.000Z",
    "textPlain": "> Because reasoning tasks require choosing between several different options. “A B C D [M1] -> B C D E” isn’t reasoning, it’s computation, because it has no mechanism for thinking “oh, I went down the wrong track, let me try something else”. That’s why the most important token in AI reasoning models is “Wait”. In fact, you can control how long a reasoning model thinks by arbitrarily appending “Wait” to the chain-of-thought. Actual reasoning models change direction all the time, but this paper’s toy example is structurally incapable of it.I think this is the most important critique that undercuts the paper's claims. I'm less convinced by the other point. I think backtracking and/or parallel search is something future papers should definitely look at in smaller models.The article is definitely also correct on the overreaching, broad philosophical claims that seems common when discussing AI and reasoning.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44904817,
    "by": "gshulegaard",
    "timeISO": "2025-08-14T19:48:45.000Z",
    "textPlain": "> but we know that reasoning is an emergent capability!Do we though?  There is widespread discussion and growing momentum of belief in this, but I have yet to see conclusive evidence of this.  That is, in part, why the subject paper exists...it seeks to explore this question.I think the author's bias is bleeding fairly heavily into his analysis and conclusions:> Whether AI reasoning is “real” reasoning or just a mirage can be an interesting question, but it is primarily a philosophical question. It depends on having a clear definition of what “real” reasoning is, exactly.I think it's pretty obvious that the researchers are exploring whether or not LLMs exhibit evidence of _Deductive_ Reasoning [1].  The entire experiment design reflects this.  Claiming that they haven't defined reasoning and therefore cannot conclude or hope to construct a viable experiment is...confusing.The question of whether or not an LLM can take a set of base facts and compose them to solve a novel/previously unseen problem is interesting and what most people discussing emergent reasoning capabilities of \"AI\" are tacitly referring to (IMO).  Much like you can be taught algebraic principles and use them to solve for \"x\" in equations you have never seen before, can an LLM do the same?To which I find this experiment interesting enough.  It presents a series of facts and then presents the LLM with tasks to see if it can use those facts in novel ways not included in the training data (something a human might reasonably deduce).  To which their results and summary conclusions are relevant, interesting, and logically sound:> CoT is not a mechanism for genuine logical inference but rather a sophisticated form of structured pattern matching, fundamentally bounded by the data distribution seen during training. When pushed even slightly beyond this distribution, its performance degrades significantly, exposing the superficial nature of the “reasoning” it produces.> The ability of LLMs to produce “fluent ",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44904030,
    "by": "j45",
    "timeISO": "2025-08-14T18:40:38.000Z",
    "textPlain": "Currently it feels like it's more simulated chain-of-thought / reasoning, sometimes very consistent, but simulated, partially because it's statistically generated and non-deterministic (not the exact same path to the similar or same each response run).",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44903391,
    "by": "skywhopper",
    "timeISO": "2025-08-14T17:44:39.000Z",
    "textPlain": "I mostly agree with the point the author makes that \"it doesn't matter\". But then again, it does matter, because LLM-based products are marketed based on \"IT CAN REASON!\" And so, while it may not matter, per se, how an LLM comes up with its results, to the extent that people choose to rely on LLMs because of marketing pitches, it's worth pushing back on those claims if they are overblown, using the same frame that the marketers use.That said, this author says this question of whether models \"can reason\" is the least interesting thing to ask. But I think the least interesting thing you can do is to go around taking every complaint about LLM performance and saying \"but humans do the exact same thing!\" Which is often not true, but again, doesn't matter.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44901478,
    "by": "empath75",
    "timeISO": "2025-08-14T15:21:07.000Z",
    "textPlain": "One thing that LLMs have exposed is how much of a house of cards all of our definitions of \"human mind\"-adjacent concepts are.  We have a single example in all of reality of a being that thinks like we do, and so all of our definitions of thinking are inextricably tied with \"how humans think\", and now we have an entity that does things which seem to be very like how we think, but not _exactly like it_, and a lot of our definitions don't seem to work any more:Reasoning, thinking, knowing, feeling, understanding, etc.Or at the very least, our rubrics and heuristics for determining if someone (thing) thinks, feels, knows, etc, no longer work.  And in particular, people create tests for those things thinking that they understand what they are testing for, when _most human beings_ would also fail those tests.I think a _lot_ of really foundational work needs to be done on clearly defining a lot of these terms and putting them on a sounder basis before we can really move forward on saying whether machines can do those things.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44901259,
    "by": "sempron64",
    "timeISO": "2025-08-14T15:03:54.000Z",
    "textPlain": "[flagged]",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44902217,
    "by": "pinoy420",
    "timeISO": "2025-08-14T16:12:42.000Z",
    "textPlain": "[dead]",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44903511,
    "by": "vmg12",
    "timeISO": "2025-08-14T17:55:01.000Z",
    "textPlain": "Solutions to some of the hardest problems I've had have only come after a night of sleep or when I'm out on a walk and I'm not even thinking about the problem. Maybe what my brain was doing was something different from reasoning?",
    "parent": 44903349,
    "depth": 2
  },
  {
    "id": 44903430,
    "by": "kazinator",
    "timeISO": "2025-08-14T17:48:23.000Z",
    "textPlain": "Not all reasoning requires language. Symbolic reasoning uses language.Real-time spatial reasoning like driving a car and not hitting things does not seem linguistic.Figuring out how to rotate a cabinet so that it will clear through a stairwell also doesn't seem like it requires language, only to communicate the solution to someone else (where language can turn into a hindrance, compared to a diagram or model).",
    "parent": 44903349,
    "depth": 2
  },
  {
    "id": 44908029,
    "by": "AbrahamParangi",
    "timeISO": "2025-08-15T02:36:31.000Z",
    "textPlain": "You're not allowed to say that it's not reasoning without distinguishing what is reasoning. Absent a strict definition that the models fail and that some other reasoner passes, it is entirely philosophical.",
    "parent": 44903349,
    "depth": 2
  },
  {
    "id": 44903633,
    "by": "safety1st",
    "timeISO": "2025-08-14T18:03:41.000Z",
    "textPlain": "I'm pretty much a layperson in this field, but I don't understand why we're trying to teach a stochastic text transformer to reason. Why would anyone expect that approach to work?I would have thought the more obvious approach would be to couple it to some kind of symbolic logic engine. It might transform plain language statements into fragments conforming to a syntax which that engine could then parse deterministically. This is the Platonic ideal of reasoning that the author of the post pooh-poohs, I guess, but it seems to me to be the whole point of reasoning; reasoning is the application of logic in evaluating a proposition. The LLM might be trained to generate elements of the proposition, but it's too random to apply logic.",
    "parent": 44903349,
    "depth": 2
  },
  {
    "id": 44904826,
    "by": "limaoscarjuliet",
    "timeISO": "2025-08-14T19:49:37.000Z",
    "textPlain": "> In fact, I think in near future it will be the norm for MLLMs to \"think\" and \"reason\" without outputting a single \"word\".It will be outputting something, as this is the only way it can get more compute - output a token, then all context + the next token is fed through the LLM again. It might not be presented to the user, but that's a different story.",
    "parent": 44903349,
    "depth": 2
  },
  {
    "id": 44906067,
    "by": "potsandpans",
    "timeISO": "2025-08-14T21:45:26.000Z",
    "textPlain": "> It is not a \"philosophical\" (by which the author probably meant \"practically inconsequential\") question.I didn't take it that way. I suppose it depends on whether or not you believe philosophy is legitimate",
    "parent": 44903349,
    "depth": 2
  },
  {
    "id": 44906445,
    "by": "pornel",
    "timeISO": "2025-08-14T22:28:33.000Z",
    "textPlain": "You're looking at this from the perspective of what would make sense for the model to produce. Unfortunately, what really dictates the design of the models is what we can train the models with (efficiently, at scale). The output is then roughly just the reverse of the training. We don't even want AI to be an \"autocomplete\", but we've got tons of text, and a relatively efficient method of training on all prefixes of a sentence at the same time.There have been experiments with preserving embedding vectors of the tokens exactly without loss caused by round-tripping through text, but the results were \"meh\", presumably because it wasn't the input format the model was trained on.It's conceivable that models trained on some vector \"neuralese\" that is completely separate from text would work better, but it's a catch 22 for training: the internal representations don't exist in a useful sense until the model is trained, so we don't have anything to feed into the models to make them use them. The internal representations also don't stay stable when the model is trained further.",
    "parent": 44903349,
    "depth": 2
  },
  {
    "id": 44903807,
    "by": "emorning4",
    "timeISO": "2025-08-14T18:20:05.000Z",
    "textPlain": "[dead]",
    "parent": 44903349,
    "depth": 2
  },
  {
    "id": 44903402,
    "by": "ActionHank",
    "timeISO": "2025-08-14T17:45:40.000Z",
    "textPlain": "I think that when you are arguing logic and reason with a group who became really attached to the term vibe-coding you've likely already lost.",
    "parent": 44903117,
    "depth": 2
  },
  {
    "id": 44902032,
    "by": "stonemetal12",
    "timeISO": "2025-08-14T15:59:34.000Z",
    "textPlain": "> it was tested on a 4 layer deep toy modelHow do you see that impacting the results?  It is the same algorithm just on a smaller scale. I would assume a 4 layer model would not be very good, but does reasoning improve it?  Is there a reason scale would impact the use of reasoning?",
    "parent": 44900491,
    "depth": 2
  },
  {
    "id": 44902724,
    "by": "griffzhowl",
    "timeISO": "2025-08-14T16:51:14.000Z",
    "textPlain": "I don't agree with the parallel. Submarines can move through water - whether you call that swimming or not isn't an interesting question, and doesn't illuminate the function of a submarine.With thinking or reasoning, there's not really a precise definition of what it is, but we nevertheless know that currently LLMs and machines more generally can't reproduce many of the human behaviours that we refer to as thinking.The question of what tasks machines can currently accomplish is certainly meaningful, if not urgent, and the reason LLMs are getting so much attention now is that they're accomplishing tasks that machines previously couldn't do.To some extent there might always remain a question about whether we call what the machine is doing \"thinking\" - but that's the uninteresting verbal question. To get at the meaningful questions we might need a more precise or higher resolution map of what we mean by thinking, but the crucial element is what functions a machine can perform, what tasks it can accomplish, and whether we call that \"thinking\" or not doesn't seem important.Maybe that was even Dijkstra's point, but it's hard to tell without context...",
    "parent": 44902173,
    "depth": 2
  },
  {
    "id": 44902278,
    "by": "mdp2021",
    "timeISO": "2025-08-14T16:17:01.000Z",
    "textPlain": "But the topic here is whether some techniques are progressive or not(with a curious parallel about whether some paths in thought are dead-ends - the unproductive focus mentioned in the article).",
    "parent": 44902173,
    "depth": 2
  },
  {
    "id": 44902702,
    "by": "draw_down",
    "timeISO": "2025-08-14T16:49:32.000Z",
    "textPlain": "[dead]",
    "parent": 44902173,
    "depth": 2
  },
  {
    "id": 44902013,
    "by": "adastra22",
    "timeISO": "2025-08-14T15:58:48.000Z",
    "textPlain": "You are making the same mistake OP is calling out. As far as I can tell “generating context” is exactly what human reasoning is too. Consider the phrase “let’s reason this out” where you then explore all options in detail, before pronouncing your judgement. Feels exactly like what the AI reasoner is doing.",
    "parent": 44901981,
    "depth": 2
  },
  {
    "id": 44903353,
    "by": "benreesman",
    "timeISO": "2025-08-14T17:40:57.000Z",
    "textPlain": "People will go to extremely great lengths to debate the appropriate analogy for how these things work, which is fun I guess but in a \"get high with a buddy\" sense at least to my taste.Some of how they work is well understood (a lot now, actually), some of the outcomes are still surprising.But we debate both the well understood parts and the surprising parts both with the wrong terminology borrowed from pretty dubious corners of pop cognitive science, and not with terminology appropriate to the new and different thing! It's nothing like a brain, it's a new different thing. Does it think or reason? Who knows pass the blunt.They do X performance on Y task according to Z eval, that's how you discuss ML model capability if you're persuing understanding rather than fundraising or clicks.",
    "parent": 44901981,
    "depth": 2
  },
  {
    "id": 44903028,
    "by": "ofjcihen",
    "timeISO": "2025-08-14T17:14:25.000Z",
    "textPlain": "It’s incredible to me that so many seem to have fallen for “humans are just LLMs bruh” argument but I think I’m beginning to understand the root of the issue.People who only “deeply” study technology only have that frame of reference to view the world so they make the mistake of assuming everything must work that way, including humans.If they had a wider frame of reference that included, for example, Early Childhood Development, they might have enough knowledge to think outside of this box and know just how ridiculous that argument is.",
    "parent": 44901981,
    "depth": 2
  },
  {
    "id": 44903285,
    "by": "cyanydeez",
    "timeISO": "2025-08-14T17:35:21.000Z",
    "textPlain": "They should call them Fuzzing models. They're just running through varioous iterations of the context until they hit a token that trips them out.",
    "parent": 44901981,
    "depth": 2
  },
  {
    "id": 44902180,
    "by": "bongodongobob",
    "timeISO": "2025-08-14T16:10:38.000Z",
    "textPlain": "And yet it improves their problem solving ability.",
    "parent": 44901981,
    "depth": 2
  },
  {
    "id": 44901610,
    "by": "dkersten",
    "timeISO": "2025-08-14T15:29:59.000Z",
    "textPlain": "Agree! I’m not an AI engineer or researcher, but it always struck me as odd that we would serialise the 100B or whatever parameters of latent space down to maximum 1M tokens and back for every step.",
    "parent": 44901378,
    "depth": 2
  },
  {
    "id": 44902220,
    "by": "CuriouslyC",
    "timeISO": "2025-08-14T16:13:09.000Z",
    "textPlain": "They're already implementing branching thought and taking the best one, eventually the entire response will be branched, with branches being spawned and culled by some metric over the lifetime of the completion. It's just not feasible now for performance reasons.",
    "parent": 44901378,
    "depth": 2
  },
  {
    "id": 44901831,
    "by": "vonneumannstan",
    "timeISO": "2025-08-14T15:46:17.000Z",
    "textPlain": ">I feel it is interesting but not what would be ideal. I really think if the models could be less linear and process over time in latent space you'd get something much more akin to thought.Please stop, this is how you get AI takeovers.",
    "parent": 44901378,
    "depth": 2
  },
  {
    "id": 44901625,
    "by": "gdbsjjdn",
    "timeISO": "2025-08-14T15:30:50.000Z",
    "textPlain": "Congratulations, you've invented philosophy.",
    "parent": 44901478,
    "depth": 2
  }
]
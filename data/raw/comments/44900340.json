[
  {
    "id": 44902440,
    "by": "sixdimensional",
    "timeISO": "2025-08-14T16:29:48.000Z",
    "textPlain": "I feel like the fundamental concept of symbolic logic[1] as a means of reasoning fits within the capabilities of LLMs.Whether it's a mirage or not, the ability to produce a symbolically logical result that has valuable meaning seems real enough to me.Especially since most meaning is assigned by humans onto the world... so too can we choose to assign meaning (or not) to the output of a chain of symbolic logic processing?[1] https://en.wikipedia.org/wiki/Logic",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44901981,
    "by": "stonemetal12",
    "timeISO": "2025-08-14T15:56:17.000Z",
    "textPlain": "When Using AI they say \"Context is King\".  \"Reasoning\" models are using the AI to generate context.  They are not reasoning in the sense of logic, or philosophy. Mirage, whatever you want to call it, it is rather unlike what people mean when they use the term reasoning. Calling it reasoning is up there with calling generating out put people don't like hallucinations.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44902173,
    "by": "modeless",
    "timeISO": "2025-08-14T16:10:09.000Z",
    "textPlain": "\"The question [whether computers can think] is just as relevant and just as meaningful as the question whether submarines can swim.\" -- Edsger W. Dijkstra, 24 November 1983",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44900491,
    "by": "NitpickLawyer",
    "timeISO": "2025-08-14T14:02:08.000Z",
    "textPlain": "Finally! A good take on that paper. I saw that arstechnica article posted everywhere, and most of the comments are full of confirmation bias, and almost all of them miss the fineprint - it was tested on a 4 layer deep toy model. It's nice to read a post that actually digs deeper and offers perspectives on what might be a good finding vs. just warranting more research.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44902355,
    "by": "slashdave",
    "timeISO": "2025-08-14T16:23:11.000Z",
    "textPlain": "> reasoning probably requires language useThe author has a curious idea of what \"reasoning\" entails.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44902182,
    "by": "skybrian",
    "timeISO": "2025-08-14T16:10:44.000Z",
    "textPlain": "Mathematical reasoning does sometimes require correct calculations, and if you get them wrong your answers will be wrong. I wouldn’t want someone doing my taxes to be bad at calculation or bad at finding mistakes in calculation.It would be interesting to see if this study’s results can be reproduced in a more realistic setting.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44901378,
    "by": "robviren",
    "timeISO": "2025-08-14T15:14:22.000Z",
    "textPlain": "I feel it is interesting but not what would be ideal. I really think if the models could be less linear and process over time in latent space you'd get something much more akin to thought. I've messed around with attaching reservoirs at each layer using hooks with interesting results (mainly over fitting), but it feels like such a limitation to have all model context/memory stuck as tokens when latent space is where the richer interaction lives. Would love to see more done where thought over time mattered and the model could almost mull over the question a bit before being obligated to crank out tokens. Not an easy problem, but interesting.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44901448,
    "by": "mentalgear",
    "timeISO": "2025-08-14T15:19:19.000Z",
    "textPlain": "> Whether AI reasoning is “real” reasoning or just a mirage can be an interesting question, but it is primarily a philosophical question. It depends on having a clear definition of what “real” reasoning is, exactly.It's pretty easy: causal reasoning. Causal, not statistic correlation only as LLM do, with or without \"CoT\".",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44902134,
    "by": "moc_was_wronged",
    "timeISO": "2025-08-14T16:07:25.000Z",
    "textPlain": "Mostly. It gives language models the way to dynamically allocate computation time, but the models are still fundamentally imitative.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44901687,
    "by": "mucho_mojo",
    "timeISO": "2025-08-14T15:35:00.000Z",
    "textPlain": "This paper I read from here has an interesting mathematical model for reasoning based on cognitive science. https://arxiv.org/abs/2506.21734 (there is also code here https://github.com/sapientinc/HRM) I think we will see dramatic performance increases on \"reasoning\" problems when this is worked into existing AI architectures.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44901515,
    "by": "naasking",
    "timeISO": "2025-08-14T15:24:02.000Z",
    "textPlain": "> Because reasoning tasks require choosing between several different options. “A B C D [M1] -> B C D E” isn’t reasoning, it’s computation, because it has no mechanism for thinking “oh, I went down the wrong track, let me try something else”. That’s why the most important token in AI reasoning models is “Wait”. In fact, you can control how long a reasoning model thinks by arbitrarily appending “Wait” to the chain-of-thought. Actual reasoning models change direction all the time, but this paper’s toy example is structurally incapable of it.I think this is the most important critique that undercuts the paper's claims. I'm less convinced by the other point. I think backtracking and/or parallel search is something future papers should definitely look at in smaller models.The article is definitely also correct on the overreaching, broad philosophical claims that seems common when discussing AI and reasoning.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44901478,
    "by": "empath75",
    "timeISO": "2025-08-14T15:21:07.000Z",
    "textPlain": "One thing that LLMs have exposed is how much of a house of cards all of our definitions of \"human mind\"-adjacent concepts are.  We have a single example in all of reality of a being that thinks like we do, and so all of our definitions of thinking are inextricably tied with \"how humans think\", and now we have an entity that does things which seem to be very like how we think, but not _exactly like it_, and a lot of our definitions don't seem to work any more:Reasoning, thinking, knowing, feeling, understanding, etc.Or at the very least, our rubrics and heuristics for determining if someone (thing) thinks, feels, knows, etc, no longer work.  And in particular, people create tests for those things thinking that they understand what they are testing for, when _most human beings_ would also fail those tests.I think a _lot_ of really foundational work needs to be done on clearly defining a lot of these terms and putting them on a sounder basis before we can really move forward on saying whether machines can do those things.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44901259,
    "by": "sempron64",
    "timeISO": "2025-08-14T15:03:54.000Z",
    "textPlain": "Betteridge's Law of Headlines.https://en.m.wikipedia.org/wiki/Betteridge's_law_of_headline...",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44902217,
    "by": "pinoy420",
    "timeISO": "2025-08-14T16:12:42.000Z",
    "textPlain": "[dead]",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44902013,
    "by": "adastra22",
    "timeISO": "2025-08-14T15:58:48.000Z",
    "textPlain": "You are making the same mistake OP is calling out. As far as I can tell “generating context” is exactly what human reasoning is too. Consider the phrase “let’s reason this out” where you then explore all options in detail, before pronouncing your judgement. Feels exactly like what the AI reasoner is doing.",
    "parent": 44901981,
    "depth": 2
  },
  {
    "id": 44902180,
    "by": "bongodongobob",
    "timeISO": "2025-08-14T16:10:38.000Z",
    "textPlain": "And yet it improves their problem solving ability.",
    "parent": 44901981,
    "depth": 2
  },
  {
    "id": 44902278,
    "by": "mdp2021",
    "timeISO": "2025-08-14T16:17:01.000Z",
    "textPlain": "But the topic here is whether some techniques are progressive or not(with a curious parallel about whether some paths in thought are dead-ends - the unproductive focus mentioned in the article).",
    "parent": 44902173,
    "depth": 2
  },
  {
    "id": 44902032,
    "by": "stonemetal12",
    "timeISO": "2025-08-14T15:59:34.000Z",
    "textPlain": "> it was tested on a 4 layer deep toy modelHow do you see that impacting the results?  It is the same algorithm just on a smaller scale. I would assume a 4 layer model would not be very good, but does reasoning improve it?  Is there a reason scale would impact the use of reasoning?",
    "parent": 44900491,
    "depth": 2
  },
  {
    "id": 44902220,
    "by": "CuriouslyC",
    "timeISO": "2025-08-14T16:13:09.000Z",
    "textPlain": "They're already implementing branching thought and taking the best one, eventually the entire response will be branched, with branches being spawned and culled by some metric over the lifetime of the completion. It's just not feasible now for performance reasons.",
    "parent": 44901378,
    "depth": 2
  },
  {
    "id": 44901610,
    "by": "dkersten",
    "timeISO": "2025-08-14T15:29:59.000Z",
    "textPlain": "Agree! I’m not an AI engineer or researcher, but it always struck me as odd that we would serialise the 100B or whatever parameters of latent space down to maximum 1M tokens and back for every step.",
    "parent": 44901378,
    "depth": 2
  },
  {
    "id": 44901831,
    "by": "vonneumannstan",
    "timeISO": "2025-08-14T15:46:17.000Z",
    "textPlain": ">I feel it is interesting but not what would be ideal. I really think if the models could be less linear and process over time in latent space you'd get something much more akin to thought.Please stop, this is how you get AI takeovers.",
    "parent": 44901378,
    "depth": 2
  },
  {
    "id": 44902163,
    "by": "mdp2021",
    "timeISO": "2025-08-14T16:09:37.000Z",
    "textPlain": "> causal reasoningYou have missed the foundation: before dynamics, being. Before causal reasoning you have deep definition of concepts. Causality is \"below\" that.",
    "parent": 44901448,
    "depth": 2
  },
  {
    "id": 44901556,
    "by": "glial",
    "timeISO": "2025-08-14T15:26:16.000Z",
    "textPlain": "Correct me if I'm wrong, I'm not sure it's so simple. LLMs are called causal models in the sense that earlier tokens \"cause\" later tokens, that is, later tokens are causally dependent on what the earlier tokens are.If you mean deterministic rather than probabilistic, even Pearl-style causal models are probabilistic.I think the author is circling around the idea that their idea of reasoning is to produce statements in a formal system: to have a set of axioms, a set of production rules, and to generate new strings/sentences/theorems using those rules. This approach is how math is formalized. It allows us to extrapolate - make new \"theorems\" or constructions that weren't in the \"training set\".",
    "parent": 44901448,
    "depth": 2
  },
  {
    "id": 44901943,
    "by": "lordnacho",
    "timeISO": "2025-08-14T15:53:49.000Z",
    "textPlain": "What's stopping us from building an LLM that can build causal trees, rejecting some trees and accepting others based on whatever evidence it is fed?Or even a causal tool for an LLM agent that operates like what it does when you ask it about math and forwards the request to Wolfram.",
    "parent": 44901448,
    "depth": 2
  },
  {
    "id": 44901531,
    "by": "naasking",
    "timeISO": "2025-08-14T15:25:04.000Z",
    "textPlain": "Define causal reasoning?",
    "parent": 44901448,
    "depth": 2
  },
  {
    "id": 44901625,
    "by": "gdbsjjdn",
    "timeISO": "2025-08-14T15:30:50.000Z",
    "textPlain": "Congratulations, you've invented philosophy.",
    "parent": 44901478,
    "depth": 2
  },
  {
    "id": 44902088,
    "by": "mdp2021",
    "timeISO": "2025-08-14T16:03:23.000Z",
    "textPlain": "> which seem to be very like how we thinkI would like to reassure you that we - we here - see LLMs are very much unlike us.",
    "parent": 44901478,
    "depth": 2
  },
  {
    "id": 44901773,
    "by": "mwkaufma",
    "timeISO": "2025-08-14T15:41:36.000Z",
    "textPlain": "Betteridge's law applies to editors adding question marks to cover-the-ass of articles with weak claims, not bloggers begging questions.",
    "parent": 44901259,
    "depth": 2
  }
]
[
  {
    "id": 44901378,
    "by": "robviren",
    "timeISO": "2025-08-14T15:14:22.000Z",
    "textPlain": "I feel it is interesting but not what would be ideal. I really think if the models could be less linear and process over time in latent space you'd get something much more akin to thought. I've messed around with attaching reservoirs at each layer using hooks with interesting results (mainly over fitting), but it feels like such a limitation to have all model context/memory stuck as tokens when latent space is where the richer interaction lives. Would love to see more done where thought over time mattered and the model could almost mull over the question a bit before being obligated to crank out tokens. Not an easy problem, but interesting.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44901448,
    "by": "mentalgear",
    "timeISO": "2025-08-14T15:19:19.000Z",
    "textPlain": "> Whether AI reasoning is “real” reasoning or just a mirage can be an interesting question, but it is primarily a philosophical question. It depends on having a clear definition of what “real” reasoning is, exactly.It's pretty easy: causal reasoning. Causal, not statistic correlation only as LLM do, with or without \"CoT\".",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44900491,
    "by": "NitpickLawyer",
    "timeISO": "2025-08-14T14:02:08.000Z",
    "textPlain": "Finally! A good take on that paper. I saw that arstechnica article posted everywhere, and most of the comments are full of confirmation bias, and almost all of them miss the fineprint - it was tested on a 4 layer deep toy model. It's nice to read a post that actually digs deeper and offers perspectives on what might be a good finding vs. just warranting more research.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44901478,
    "by": "empath75",
    "timeISO": "2025-08-14T15:21:07.000Z",
    "textPlain": "One thing that LLMs have exposed is how much of a house of cards all of our definitions of \"human mind\"-adjacent concepts are.  We have a single example in all of reality of a being that thinks like we do, and so all of our definitions of thinking are inextricably tied with \"how humans think\", and now we have an entity that does things which seem to be very like how we think, but not _exactly like it_, and a lot of our definitions don't seem to work any more:Reasoning, thinking, knowing, feeling, understanding, etc.Or at the very least, our rubrics and heuristics for determining if someone (thing) thinks, feels, knows, etc, no longer work.  And in particular, people create tests for those things thinking that they understand what they are testing for, when _most human beings_ would also fail those tests.I think a _lot_ of really foundational work needs to be done on clearly defining a lot of these terms and putting them on a sounder basis before we can really move forward on saying whether machines can do those things.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44901515,
    "by": "naasking",
    "timeISO": "2025-08-14T15:24:02.000Z",
    "textPlain": "> Because reasoning tasks require choosing between several different options. “A B C D [M1] -> B C D E” isn’t reasoning, it’s computation, because it has no mechanism for thinking “oh, I went down the wrong track, let me try something else”. That’s why the most important token in AI reasoning models is “Wait”. In fact, you can control how long a reasoning model thinks by arbitrarily appending “Wait” to the chain-of-thought. Actual reasoning models change direction all the time, but this paper’s toy example is structurally incapable of it.I think this is the most important critique that undercuts the paper's claims. I'm less convinced by the other point. I think backtracking and/or parallel search is something future papers should definitely look at in smaller models.The article is definitely also correct on the overreaching, broad philosophical claims that seems common when discussing AI and reasoning.",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44901259,
    "by": "sempron64",
    "timeISO": "2025-08-14T15:03:54.000Z",
    "textPlain": "Betteridge's Law of Headlines.https://en.m.wikipedia.org/wiki/Betteridge's_law_of_headline...",
    "parent": 44900340,
    "depth": 1
  },
  {
    "id": 44901531,
    "by": "naasking",
    "timeISO": "2025-08-14T15:25:04.000Z",
    "textPlain": "Define causal reasoning?",
    "parent": 44901448,
    "depth": 2
  }
]
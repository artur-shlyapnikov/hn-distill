[
  {
    "id": 44913172,
    "by": "sobiolite",
    "timeISO": "2025-08-15T14:52:25.000Z",
    "textPlain": "The article says that LLMs don't summarize, only shorten, because...\"A true summary, the kind a human makes, requires outside context and reference points. Shortening just reworks the information already in the text.\"Then later says...\"LLMs operate in a similar way, trading what we would call intelligence for a vast memory of nearly everything humans have ever written. It’s nearly impossible to grasp how much context this gives them to play with\"So, they can't summarize, because they lack context... but they also have an almost ungraspably large amount of context?",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44913417,
    "by": "umanwizard",
    "timeISO": "2025-08-15T15:12:04.000Z",
    "textPlain": "The article claims (without any evidence, argument or reason) that LLMs are not intelligent, then simply refuses to define intelligence.How do you know LLMs aren't intelligent, if you can't define what that means?",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44913498,
    "by": "hackyhacky",
    "timeISO": "2025-08-15T15:18:52.000Z",
    "textPlain": "> LLMs mimic intelligence, but they aren’t intelligent.I see statements like this a lot, and I find them unpersuasive because any meaningful definition of \"intelligence\" is not offered. What, exactly, is the property that humans (allegedly) have and LLMs (allegedly) lack, that allows one to be deemed \"intelligent\" and the other not?I see two possibilities:1. We define \"intelligence\" as definitionally unique to humans. For example, maybe intelligence depends on the existence of a human soul, or specific to the physical structure of the human brain. In this case, a machine (perhaps an LLM) could achieve \"quacks like a duck\" behavioral equality to a human mind, and yet would still be excluded from the definition of \"intelligent.\" This definition is therefore not useful if we're interested in the ability of the machine, which it seems to me we are. LLMs are often dismissed as not \"intelligent\" because they work by inferring output based on learned input, but that alone cannot be a distinguishing characteristic, because that's how humans work as well.2. We define \"intelligence\" in a results-oriented way. This means there must be some specific test or behavioral standard that a machine must meet in order to become intelligent. This has been the default definition for a long time, but the goal posts have shifted. Nevertheless, if you're going to disparage LLMs by calling them unintelligent, you should be able to cite a specific results-oriented failure that distinguishes them from \"intelligent\" humans. Note that this argument cannot refer to the LLMs' implementation or learning model.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44913356,
    "by": "ChrisMarshallNY",
    "timeISO": "2025-08-15T15:06:36.000Z",
    "textPlain": "That's a great article.Scott Jenson is one of my favorite authors.He's really big on integrating an understanding of basic human nature, into design.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44913524,
    "by": "codeulike",
    "timeISO": "2025-08-15T15:21:14.000Z",
    "textPlain": "Well I, for one, can't beleive what that guy did to poor Timmy",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44913244,
    "by": "tovej",
    "timeISO": "2025-08-15T14:57:28.000Z",
    "textPlain": "Good article, it's been told before but it bears repeating.Also I got caught on this one kind of irrelevant point regarding the characterization of the Matrix: I would say Matrix is not just diguised as a story about escaping systems of control, it's quite clearly about oppressive systems in society, with specific reference to gender expression. Lilly Wachowski has explicitly stated that it was supposed to be an allegory for gender transition.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44913372,
    "by": "snozolli",
    "timeISO": "2025-08-15T15:07:56.000Z",
    "textPlain": "Regarding Timmy, the Companion Cube from the game Portal is the greatest example of induced anthropomorphism that I've ever experienced.  If you know, you know, and if you don't, you should really play the game, since it's brilliant.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44913447,
    "by": "naikrovek",
    "timeISO": "2025-08-15T15:14:19.000Z",
    "textPlain": "I've mentioned this to colleagues at work before.LLMs give a very strong appearance of intelligence, because humans are super receptive to information provided via our native language.  We often have to deal with imperfect speakers and writers, and we must infer context and missing information on our own.  We do this so well that we don't know we're doing it.  LLMs have perfect grammar and we subtly feel that they are extremely smart because subconsciously we recognize that we don't have to think about anything that's said, it is all syntactically perfect.So, LLMs sort of trick us into masking their true limitations and believing that they are truly thinking; there are even models that call themselves thinking models, but they don't think, they just predict what the user is going to complain about and say that to themselves as an additional, dynamic prompt on top of the one you actually enter.LLMs are very good at fooling us into the idea that they know anything at all; they don't.  And humans are very bad at being discriminate about the source of the information presented to them if it is presented in a friendly way.  The combination of those things is what has resulted in the insanely huge AI hype cycle that we are currently living in the middle of.  Nearly everyone is overreacting to what LLMs actually are, and the few of us that believe that we sort of see what's actually happening are ignored for being naysayers, buzz-kills, and luddites.  Shunned for not drinking the Kool-Aid.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44913388,
    "by": "btown",
    "timeISO": "2025-08-15T15:09:16.000Z",
    "textPlain": "It's an interesting philosophical question.Imagine an oracle that could judge/decide, with human levels of intelligence, how relevant a given memory or piece of information is to any given situation, and that could verbosely describe which way it's relevant (spatially, conditionally, etc.).Would such an oracle, sufficiently parallelized, be sufficient for AGI? If it could, then we could genuinely describe its output as \"context,\" and phrase our problem as \"there is still a gap in needed context, despite how much context there already is.\"And an LLM that simply \"shortens\" that context could reach a level of AGI, because the context preparation is doing the heavy lifting.The point I think the article is trying to make is that LLMs cannot add any information beyond the context they are given - they can only \"shorten\" that context.If the lived experience necessary for human-level judgment could be encoded into that context, though... that would be an entirely different ball game.",
    "parent": 44913172,
    "depth": 2
  },
  {
    "id": 44913265,
    "by": "kayodelycaon",
    "timeISO": "2025-08-15T14:59:26.000Z",
    "textPlain": "They can’t summarize something that hasn’t been summarized before.",
    "parent": 44913172,
    "depth": 2
  },
  {
    "id": 44913467,
    "by": "ratelimitsteve",
    "timeISO": "2025-08-15T15:16:09.000Z",
    "textPlain": "I think the differentiator here might not be the context it has, but the context it has the ability to use effectively in order to derive more information about a given request.",
    "parent": 44913172,
    "depth": 2
  },
  {
    "id": 44913323,
    "by": "tovej",
    "timeISO": "2025-08-15T15:03:43.000Z",
    "textPlain": "You can reconcile these points by considering what specofic context is necessary. The author specifies \"outside\" context, and I would agree. The human context that's necessary for useful summaries is a model of semantic or \"actual\" relationships between concepts, while the LLM context is a model of a single kind of fuzzy relationship between concepts.In other words the LLM does not contain the knowledge of what the words represent.",
    "parent": 44913172,
    "depth": 2
  },
  {
    "id": 44913484,
    "by": "energy123",
    "timeISO": "2025-08-15T15:17:34.000Z",
    "textPlain": "It's strange seeing so many takes like this two weeks after LLMs won gold medals at IMO and IOI. The cognitive dissonance is going to be wild when it all comes to a head in two years.",
    "parent": 44913417,
    "depth": 2
  },
  {
    "id": 44913296,
    "by": "kayodelycaon",
    "timeISO": "2025-08-15T15:01:33.000Z",
    "textPlain": "The character Switch was supposed to have a different gender in the matrix vs real life. It’s really a shame that didn’t happen.",
    "parent": 44913244,
    "depth": 2
  },
  {
    "id": 44913506,
    "by": "altruios",
    "timeISO": "2025-08-15T15:19:50.000Z",
    "textPlain": "There is nothing more free than the freedom to be who you really are.Going to rewatch the Matrix tonight.",
    "parent": 44913244,
    "depth": 2
  },
  {
    "id": 44913512,
    "by": "generationP",
    "timeISO": "2025-08-15T15:20:20.000Z",
    "textPlain": "The cube doesn't work, or at least it didn't for me. The goggly eyes really do make a difference.",
    "parent": 44913372,
    "depth": 2
  },
  {
    "id": 44913422,
    "by": "ChrisMarshallNY",
    "timeISO": "2025-08-15T15:12:18.000Z",
    "textPlain": "I'm on a Mac, and would love to see Portal 2 (at least) ported to M-chips.I would love Portal 3, even more.",
    "parent": 44913372,
    "depth": 2
  }
]
[
  {
    "id": 44913172,
    "by": "sobiolite",
    "timeISO": "2025-08-15T14:52:25.000Z",
    "textPlain": "The article says that LLMs don't summarize, only shorten, because...\"A true summary, the kind a human makes, requires outside context and reference points. Shortening just reworks the information already in the text.\"Then later says...\"LLMs operate in a similar way, trading what we would call intelligence for a vast memory of nearly everything humans have ever written. It’s nearly impossible to grasp how much context this gives them to play with\"So, they can't summarize, because they lack context... but they also have an almost ungraspably large amount of context?",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44914934,
    "by": "Joeri",
    "timeISO": "2025-08-15T17:13:15.000Z",
    "textPlain": "LLMs mimic intelligence, but they aren’t intelligent.They aren’t just intelligence mimics, they are people mimics, and they’re getting better at it with every generation.Whether they are intelligent or not, whether they are people or not, it ultimately does not matter when it comes to what they can actually do, what they can actually automate. If they mimic a particular scenario or human task well enough that the job gets done, they can replace intelligence even if they are “not intelligent”.If by now someone still isn’t convinced that LLMs can indeed automate some of those intelligence tasks, then I would argue they are not open to being convinced.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44914993,
    "by": "0x457",
    "timeISO": "2025-08-15T17:18:12.000Z",
    "textPlain": "> A philosophical exploration of free will and reality disguised as a sci-fi action film about breaking free from systems of control.How is that a summary? It reads as a one-liner review I would leave on Letterboxed or something I would say, trying to be pretentious and treating the movie as a work of art. It is a work of art, because all movies are art, but that's an awful summary.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44913498,
    "by": "hackyhacky",
    "timeISO": "2025-08-15T15:18:52.000Z",
    "textPlain": "> LLMs mimic intelligence, but they aren’t intelligent.I see statements like this a lot, and I find them unpersuasive because any meaningful definition of \"intelligence\" is not offered. What, exactly, is the property that humans (allegedly) have and LLMs (allegedly) lack, that allows one to be deemed \"intelligent\" and the other not?I see two possibilities:1. We define \"intelligence\" as definitionally unique to humans. For example, maybe intelligence depends on the existence of a human soul, or specific to the physical structure of the human brain. In this case, a machine (perhaps an LLM) could achieve \"quacks like a duck\" behavioral equality to a human mind, and yet would still be excluded from the definition of \"intelligent.\" This definition is therefore not useful if we're interested in the ability of the machine, which it seems to me we are. LLMs are often dismissed as not \"intelligent\" because they work by inferring output based on learned input, but that alone cannot be a distinguishing characteristic, because that's how humans work as well.2. We define \"intelligence\" in a results-oriented way. This means there must be some specific test or behavioral standard that a machine must meet in order to become intelligent. This has been the default definition for a long time, but the goal posts have shifted. Nevertheless, if you're going to disparage LLMs by calling them unintelligent, you should be able to cite a specific results-oriented failure that distinguishes them from \"intelligent\" humans. Note that this argument cannot refer to the LLMs' implementation or learning model.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44914744,
    "by": "ticulatedspline",
    "timeISO": "2025-08-15T16:59:13.000Z",
    "textPlain": "- LLMs don't need to be intelligent to take jobs, bash scripts have replaced people.- Even if CEOs are completely out of touch and the tool can't do the job you can still get laid off in an ill informed attempt to replace you. Then when the company doesn't fall over because the leftover people, desperate to keep covering rent fill the gaps it just looks like efficiency to the top.- I don't think our tendency anthropomorphize LLMs is really the problem here.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44914221,
    "by": "nojs",
    "timeISO": "2025-08-15T16:17:25.000Z",
    "textPlain": "Even stronger than our need to anthropomorphize seems to be our innate desire to believe our species is special, and that “real intelligence” couldn’t ever be replicated.If you keep redefining real intelligence as the set of things machines can’t do, then it’s always going to be true.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44914225,
    "by": "ArnavAgrawal03",
    "timeISO": "2025-08-15T16:17:45.000Z",
    "textPlain": "> They had known him for only 15 seconds, yet they still perceived the act of snapping him in half as violent.This is right out of Community",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44914921,
    "by": "andoando",
    "timeISO": "2025-08-15T17:12:17.000Z",
    "textPlain": "This, along with a ton of commentary on LLMs, seems like its written by someone who has no technical understanding of LLMs.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44914300,
    "by": "vcarrico",
    "timeISO": "2025-08-15T16:24:04.000Z",
    "textPlain": "I might be mixing the concepts of intelligence and conscience etc, but the human mind is more than language and data; it's also experience. LLMs have all the data and can express anything around that context, but will never experience anything, which is singular for each of us, and it's part of what makes what we call intelligence (?). So they will never replicate the human mind; they can just mimic it.I heard from Miguel Nicolelis that language is a filter for the human mind, so you can never build a mind from language. I interpreted this like trying to build an orange from its juice.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44913417,
    "by": "umanwizard",
    "timeISO": "2025-08-15T15:12:04.000Z",
    "textPlain": "The article claims (without any evidence, argument or reason) that LLMs are not intelligent, then simply refuses to define intelligence.How do you know LLMs aren't intelligent, if you can't define what that means?",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44914175,
    "by": "Isamu",
    "timeISO": "2025-08-15T16:12:22.000Z",
    "textPlain": "You can compare the current state of LLMs to the days of chess machines when they first approached grandmaster level play. The machine approach was very brute force, and there was a lot of work done to improve the sheer amount of look ahead that was required to complete at the grandmaster level.As opposed to what grandmasters actually did, which was less look ahead and more pattern matching to strengthen the position.Now LLMs successfully leverage pattern matching, but interestingly it is still a kind of brute force pattern matching, requiring the statistical absorption of all available texts, far more than a human absorbs in a lifetime.This enables the LLM to interpolate an answer from the structure of the absorbed texts with reasonable statistical relevance. This is still not quite “what humans do” as it still requires brute force statistical analysis of vast amounts of text to achieve pretty good results. For example training on all available Python sources in github and elsewhere (curated to avoid bad examples) yields pretty good results, not how a human would do it, but statistically likely to be pertinent and correct.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44914194,
    "by": "xg15",
    "timeISO": "2025-08-15T16:14:36.000Z",
    "textPlain": "I feel this article should be paired with this other one [1] that was on the frontpage a few days ago.My impression is, there is currently one tendency to \"over-anthropomorphize\" LLMs and treat them like conscious or even superhuman entities (encouraged by AI tech leaders and AGI/Singularity folks) and another to oversimplify them and view them as literal Markov chains that just got lots of training data.Maybe those articles could help guarding against both extremes.[1] https://www.verysane.ai/p/do-we-understand-how-neural-networ...",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44914054,
    "by": "kbaker",
    "timeISO": "2025-08-15T16:03:03.000Z",
    "textPlain": "Seems like this is close to the Uncanny Valley effect.LLM intelligence is in the spot where it is simultaneously genius-level but also just misses the mark a tiny bit, which really sticks out for those who have been around humans their whole lives.I feel that, just like more modern CGI, this will slowly fade with certain techniques and you just won't notice it when talking to or interacting with AI.Just like in his post during the whole Matrix discussion.> \"When I asked for examples, it suggested the Matrix and even gave me the “Summary” and “Shortening” text, which I then used here word for word. \"He switches in AI-written text and I bet you were reading along just the same until he pointed it out.This is our future now I guess.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44913572,
    "by": "stefanv",
    "timeISO": "2025-08-15T15:26:29.000Z",
    "textPlain": "What if the problem is not that we overestimate LLMs, but that we overestimate intelligence? Or to express the same idea for a more philosophically inclined audience, what if the real mistake isn’t in overestimating LLMs, but in overestimating intelligence itself by imagining it as something more than a web of patterns learned from past experiences and echoed back into the world?",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44913631,
    "by": "pbw",
    "timeISO": "2025-08-15T15:32:27.000Z",
    "textPlain": "LLM's can shorten and maybe tend to if you just say \"summarize this\" but you can trivially ask them to do more. I asked for a summary of Jenson's post and then offer a reflection, GPT-5 said, \"It's similar to the Plato’s Cave analogy: humans see shadows (the input text) and infer deeper reality (context, intent), while LLMs either just recite shadows (shorten) or imagine creatures behind them that aren’t there (hallucinate). The “hallucination” behavior is like adding “ghosts”—false constructs that feel real but aren’t grounded.That ain't shortening because none of that was in his post.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44914060,
    "by": "foobarian",
    "timeISO": "2025-08-15T16:03:35.000Z",
    "textPlain": "The LLMs are like a Huffman codec except the context is infinite and lossy",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44913948,
    "by": "AndrewKemendo",
    "timeISO": "2025-08-15T15:55:23.000Z",
    "textPlain": "Who are you going to lodge your complaint to that the set of systems and machines that just took your job isn’t “intelligent?”Humans seem to get wrapped around these concepts like intelligence consciousness etc. because they seem to be the only thing differentiating us from every other animal when in fact it’s all a mirage.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44913356,
    "by": "ChrisMarshallNY",
    "timeISO": "2025-08-15T15:06:36.000Z",
    "textPlain": "That's a great article.Scott Jenson is one of my favorite authors.He's really big on integrating an understanding of basic human nature, into design.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44913524,
    "by": "codeulike",
    "timeISO": "2025-08-15T15:21:14.000Z",
    "textPlain": "Well I, for one, can't beleive what that guy did to poor Timmy",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44913372,
    "by": "snozolli",
    "timeISO": "2025-08-15T15:07:56.000Z",
    "textPlain": "Regarding Timmy, the Companion Cube from the game Portal is the greatest example of induced anthropomorphism that I've ever experienced.  If you know, you know, and if you don't, you should really play the game, since it's brilliant.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44913244,
    "by": "tovej",
    "timeISO": "2025-08-15T14:57:28.000Z",
    "textPlain": "Good article, it's been told before but it bears repeating.Also I got caught on this one kind of irrelevant point regarding the characterization of the Matrix: I would say Matrix is not just diguised as a story about escaping systems of control, it's quite clearly about oppressive systems in society, with specific reference to gender expression. Lilly Wachowski has explicitly stated that it was supposed to be an allegory for gender transition.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44913447,
    "by": "naikrovek",
    "timeISO": "2025-08-15T15:14:19.000Z",
    "textPlain": "I've mentioned this to colleagues at work before.LLMs give a very strong appearance of intelligence, because humans are super receptive to information provided via our native language.  We often have to deal with imperfect speakers and writers, and we must infer context and missing information on our own.  We do this so well that we don't know we're doing it.  LLMs have perfect grammar and we subtly feel that they are extremely smart because subconsciously we recognize that we don't have to think about anything that's said, it is all syntactically perfect.So, LLMs sort of trick us into masking their true limitations and believing that they are truly thinking; there are even models that call themselves thinking models, but they don't think, they just predict what the user is going to complain about and say that to themselves as an additional, dynamic prompt on top of the one you actually enter.LLMs are very good at fooling us into the idea that they know anything at all; they don't.  And humans are very bad at being discriminate about the source of the information presented to them if it is presented in a friendly way.  The combination of those things is what has resulted in the insanely huge AI hype cycle that we are currently living in the middle of.  Nearly everyone is overreacting to what LLMs actually are, and the few of us that believe that we sort of see what's actually happening are ignored for being naysayers, buzz-kills, and luddites.  Shunned for not drinking the Kool-Aid.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44914508,
    "by": "nataliste",
    "timeISO": "2025-08-15T16:38:54.000Z",
    "textPlain": "The author's argument is built on fallacies that always pop up in these kinds of critiques.The \"summary vs shortening\" distinction is moving the goalposts. They makes the empirical claim that LLMs fail at summarizing novel PDFs without any actual evidence. For a model trained on a huge chunk of the internet, the line between \"reworking existing text\" and \"drawing on external context\" is so blurry it's practically meaningless.Similarly, can we please retire the ELIZA and Deep Blue analogies? Comparing a modern transformer to a 1960s if-then script or a brute-force chess engine is a category error. It's a rhetorical trick to make LLMs seem less novel than they actually are.And blaming everything on anthropomorphism is an easy out. It lets you dismiss the model's genuinely surprising capabilities by framing it as a simple flaw in human psychology. The interesting question isn't that we anthropomorphize, but why this specific technology is so effective at triggering that response from humans.The whole piece basically boils down to: \"If we define intelligence in a way that is exclusively social and human, then this non-social, non-human thing isn't intelligent.\" It's a circular argument.",
    "parent": 44912646,
    "depth": 1
  },
  {
    "id": 44914493,
    "by": "usefulcat",
    "timeISO": "2025-08-15T16:37:59.000Z",
    "textPlain": "I think \"context\" is being used in different ways here.> \"It’s nearly impossible to grasp how much context this gives them to play with\"Here, I think the author means something more like \"all the material used to train the LLM\".> \"A true summary, the kind a human makes, requires outside context and reference points.\"In this case I think that \"context\" means something more like actual comprehension.The author's point is that an LLM could only write something like the referenced summary by shortening other summaries present in its training set.",
    "parent": 44913172,
    "depth": 2
  },
  {
    "id": 44913625,
    "by": "jchw",
    "timeISO": "2025-08-15T15:32:01.000Z",
    "textPlain": "I think the real takeaway is that LLMs are very good at tasks that closely resemble examples it has in its training. A lot of things written (code, movies/TV shows, etc.) are actually pretty repetitive and so you don't really need super intelligence to be able to summarize it and break it down, just good pattern matching. But, this can fall apart pretty wildly when you have something genuinely novel...",
    "parent": 44913172,
    "depth": 2
  },
  {
    "id": 44913388,
    "by": "btown",
    "timeISO": "2025-08-15T15:09:16.000Z",
    "textPlain": "It's an interesting philosophical question.Imagine an oracle that could judge/decide, with human levels of intelligence, how relevant a given memory or piece of information is to any given situation, and that could verbosely describe which way it's relevant (spatially, conditionally, etc.).Would such an oracle, sufficiently parallelized, be sufficient for AGI? If it could, then we could genuinely describe its output as \"context,\" and phrase our problem as \"there is still a gap in needed context, despite how much context there already is.\"And an LLM that simply \"shortens\" that context could reach a level of AGI, because the context preparation is doing the heavy lifting.The point I think the article is trying to make is that LLMs cannot add any information beyond the context they are given - they can only \"shorten\" that context.If the lived experience necessary for human-level judgment could be encoded into that context, though... that would be an entirely different ball game.",
    "parent": 44913172,
    "depth": 2
  },
  {
    "id": 44913323,
    "by": "tovej",
    "timeISO": "2025-08-15T15:03:43.000Z",
    "textPlain": "You can reconcile these points by considering what specific context is necessary. The author specifies \"outside\" context, and I would agree. The human context that's necessary for useful summaries is a model of semantic or \"actual\" relationships between concepts, while the LLM context is a model of a single kind of fuzzy relationship between concepts.In other words the LLM does not contain the knowledge of what the words represent.",
    "parent": 44913172,
    "depth": 2
  },
  {
    "id": 44913467,
    "by": "ratelimitsteve",
    "timeISO": "2025-08-15T15:16:09.000Z",
    "textPlain": "I think the differentiator here might not be the context it has, but the context it has the ability to use effectively in order to derive more information about a given request.",
    "parent": 44913172,
    "depth": 2
  },
  {
    "id": 44913265,
    "by": "kayodelycaon",
    "timeISO": "2025-08-15T14:59:26.000Z",
    "textPlain": "They can’t summarize something that hasn’t been summarized before.",
    "parent": 44913172,
    "depth": 2
  },
  {
    "id": 44915005,
    "by": "shafoshaf",
    "timeISO": "2025-08-15T17:19:08.000Z",
    "textPlain": "They can mimic well documented behavior. Applying an LLM to a novel task is where the model breaks down. This obviously has huge implications for automation. For example, most business do not have unique ways of handling accounting transactions, yet each company has a litany of AR and AP specialists who create semmingly unique SOPs. LLMs can easily automate those workers since they are simply doing a slight variation at best of a very well documented system.Asking an LLM to take all this knowledge and apply it to a new domain? That will take a whole new paradigm.",
    "parent": 44914934,
    "depth": 2
  },
  {
    "id": 44914873,
    "by": "libraryofbabel",
    "timeISO": "2025-08-15T17:08:24.000Z",
    "textPlain": "Agree. This article would had been a lot stronger if it had just concentrated on the issue of anthropomorphizing LLMs, without bringing “intelligence” into it. At this point LLMs are so good at a variety of results-oriented tasks (gold on the Mathematical Olympiad, for example) that we should either just call them intelligent or stop talking about the concept altogether.But the problem of anthropomorphizing is real. LLMs are deeply weird machines - they’ve been fine-tuned to sound friendly and human, but behind that is something deeply alien: a huge pile of linear algebra that does not work at all like a human mind (notably, they can’t really learn form experience at all after training is complete). They don’t have bodies or even a single physical place where their mind lives (each message in a conversation might be generated on a different GPU in a different datacenter). They can fail in weird and novel ways. It’s clear that anthropomorphism here is a bad idea. Although that’s not a particularly novel point.",
    "parent": 44913498,
    "depth": 2
  },
  {
    "id": 44913987,
    "by": "dkdcio",
    "timeISO": "2025-08-15T15:58:26.000Z",
    "textPlain": "> I see statements like this a lot, and I find them unpersuasive because any meaningful definition of \"intelligence\" is not offered. What, exactly, is the property that humans (allegedly) have and LLMs (allegedly) lack, that allows one to be deemed \"intelligent\" and the other not?the ability for long-term planning and, more cogently, actually living in the real world where time passes",
    "parent": 44913498,
    "depth": 2
  },
  {
    "id": 44914309,
    "by": "card_zero",
    "timeISO": "2025-08-15T16:24:51.000Z",
    "textPlain": "It may be the case that the failures of the ability of the machine (2) are best expressed by reference to the shortcomings of its internal workings (1), and not by contrived tests.",
    "parent": 44913498,
    "depth": 2
  },
  {
    "id": 44914512,
    "by": "safetytrick",
    "timeISO": "2025-08-15T16:38:59.000Z",
    "textPlain": "Yes, I agree, we seem to need to feel \"special\".Language is really powerful, I think it's a huge part of our intelligence.The interesting part of the article to me is the focus on fluency. I have not seen anything that LLMs do well that isn't related to powerful utilization of fluency.",
    "parent": 44914221,
    "depth": 2
  },
  {
    "id": 44914414,
    "by": "hackyhacky",
    "timeISO": "2025-08-15T16:32:32.000Z",
    "textPlain": "> LLMs have all the data and can express anything around that context,On the contrary, all their training data is their \"experience\".",
    "parent": 44914300,
    "depth": 2
  },
  {
    "id": 44913484,
    "by": "energy123",
    "timeISO": "2025-08-15T15:17:34.000Z",
    "textPlain": "It's strange seeing so many takes like this two weeks after LLMs won gold medals at IMO and IOI. The cognitive dissonance is going to be wild when it all comes to a head in two years.",
    "parent": 44913417,
    "depth": 2
  },
  {
    "id": 44913731,
    "by": "krapp",
    "timeISO": "2025-08-15T15:39:32.000Z",
    "textPlain": "Why do critics of LLM intelligence need to provide a definition when people who believe LLMs are intelligent only take it on faith, not having such a definition of their own?",
    "parent": 44913417,
    "depth": 2
  },
  {
    "id": 44914246,
    "by": "mattgreenrocks",
    "timeISO": "2025-08-15T16:19:38.000Z",
    "textPlain": "Previously when someone called out the tendency to over-anthropomorphize LLMs, a lot of the answers amounted to, “but I like doing it, therefore we should!”I’ll be the first to say one should pick their battles. But hearing that over and over from a crowd like this that can be quite pedantic is very telling.",
    "parent": 44914194,
    "depth": 2
  },
  {
    "id": 44913945,
    "by": "pitpatagain",
    "timeISO": "2025-08-15T15:55:19.000Z",
    "textPlain": "I can't decide how to read your last sentence.That reflection seems totally off to me: fluent, and flavored with elements of the article, but also not really what the article is about and a pretty weird/tortured use of the elements of the allegory of the cave, like it doesn't seem anything like Plato's Cave to me. Ironically demonstrates the actual main gist of the article if you ask me.But maybe you meant that you think that summary is good and not textually similar to that post so demonstrating something more sophisticated than \"shortening\".",
    "parent": 44913631,
    "depth": 2
  },
  {
    "id": 44914390,
    "by": "beezle",
    "timeISO": "2025-08-15T16:31:11.000Z",
    "textPlain": "When I saw the post title I immediately thought of Timmy from South Park lol",
    "parent": 44913524,
    "depth": 2
  },
  {
    "id": 44914791,
    "by": "andrewla",
    "timeISO": "2025-08-15T17:02:39.000Z",
    "textPlain": "It is a brilliant game and the empathy you develop for the cube is a great concept.But arguably much deeper is the fact that nothing in this game, or any single-player game, is a living thing in any form. Arguably the game's characterization of GLaDOS hits even harder on the anthropomorphism angle.",
    "parent": 44913372,
    "depth": 2
  },
  {
    "id": 44913512,
    "by": "generationP",
    "timeISO": "2025-08-15T15:20:20.000Z",
    "textPlain": "The cube doesn't work, or at least it didn't for me. The goggly eyes really do make a difference.",
    "parent": 44913372,
    "depth": 2
  },
  {
    "id": 44913422,
    "by": "ChrisMarshallNY",
    "timeISO": "2025-08-15T15:12:18.000Z",
    "textPlain": "I'm on a Mac, and would love to see Portal 2 (at least) ported to M-chips.I would love Portal 3, even more.",
    "parent": 44913372,
    "depth": 2
  },
  {
    "id": 44914057,
    "by": "bitwize",
    "timeISO": "2025-08-15T16:03:25.000Z",
    "textPlain": "That's a matter of informed anthropomorphism. A lot of people don't become attached to the Companion Cube, but are informed that their player character is so attached.",
    "parent": 44913372,
    "depth": 2
  },
  {
    "id": 44914024,
    "by": "xg15",
    "timeISO": "2025-08-15T16:01:07.000Z",
    "textPlain": "It wasn't. Switch was intended to be genderfluid, but the Matrix itself, or \"logging out\" of it was apparently not meant as an allegory for transitioning (though she doesn't mind the interpretation) :https://www.them.us/story/lilly-wachowski-work-in-progress-s...",
    "parent": 44913244,
    "depth": 2
  },
  {
    "id": 44913296,
    "by": "kayodelycaon",
    "timeISO": "2025-08-15T15:01:33.000Z",
    "textPlain": "The character Switch was supposed to have a different gender in the matrix vs real life. It’s really a shame that didn’t happen.",
    "parent": 44913244,
    "depth": 2
  },
  {
    "id": 44913506,
    "by": "altruios",
    "timeISO": "2025-08-15T15:19:50.000Z",
    "textPlain": "There is nothing more free than the freedom to be who you really are.Going to rewatch the Matrix tonight.",
    "parent": 44913244,
    "depth": 2
  },
  {
    "id": 44914928,
    "by": "hnfong",
    "timeISO": "2025-08-15T17:12:49.000Z",
    "textPlain": "You're ignored not because you're right or wrong, but because your unsolicited advice is not useful.For example, I can spin up any LLM and get it to translate some English text into Japanese with maybe 99% accuracy. I don't need to believe whether it \"really knows\" English or Japanese, I only need to believe the output is accurate.Similarly I can ask a LLM to code up a function that does a specific thing, and it will do it with high accuracy. Maybe there'll be some bugs, but I can review the code and fix them, which in some cases boosts my productivity. I don't need to believe whether it \"really knows\" C++ or Rust, I only need it to write something good enough.I mean, just by these two examples, LLMs are really great tools, and I'm personally hyped for these use cases alone. Am I fooled by the LLM? I don't think so, I don't have any fantasy about it being extremely intelligent or always being right. I doubt most reasonable people these days would either.So basically you're going about assuming people are fooled by LLMs (which they might not be), and wondering why you're unpopular when you're basically telling everyone they're gullible and foolish.",
    "parent": 44913447,
    "depth": 2
  }
]
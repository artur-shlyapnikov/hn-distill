[
  {
    "id": 44867573,
    "by": "g42gregory",
    "timeISO": "2025-08-11T18:20:33.000Z",
    "textPlain": "This is why it’s so critical to have open source models.In a year or so, the open source models will become good enough (in both quality and speed) to run locally.Arguably, OpenAI OSS 120B is already good enough, in both quality and speed, to run on Mac Studio.Then $10k, amortized over 3 years, will be enough to run code LLMs 24/7.I hope that’s the future.",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867808,
    "by": "crestfallen33",
    "timeISO": "2025-08-11T18:39:21.000Z",
    "textPlain": "I'm not sure where the author gets the $100k number, but I agree that Cursor and Claude Code have obfuscated the true cost of intelligence. Tools like Cline and its forks (Roo Code, Kilo Code) have shown what unmitigated inference can actually deliver.The irony is that Kilo itself is playing the same game they're criticizing. They're burning cash on free credits (with expiry dates) and paid marketing to grab market share  -- essentially subsidizing inference just like Cursor, just with VC money instead of subscription revenue.The author is right that the \"$20 → $200\" subscription model is broken. But Kilo's approach of giving away $100+ in credits isn't sustainable either. Eventually, everyone has to face the same reality: frontier model inference is expensive, and someone has to pay for it.",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867553,
    "by": "jeanlucas",
    "timeISO": "2025-08-11T18:19:07.000Z",
    "textPlain": "So convenient a future AI dev will cost as much as a human developer, pure coincidence",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867919,
    "by": "IshKebab",
    "timeISO": "2025-08-11T18:48:23.000Z",
    "textPlain": "> Both effects together will push costs at the top level to $100k a year. Spending that magnitude of money on software is not without precedent, chip design licenses from Cadence or Synopsys are already $250k a year.For how many developers? Chip design companies aren't paying Synopsys $250k/year per developer. Even when using formal tools which are ludicrously expensive, developers can share licenses.In any case, the reason chip design companies pay EDA vendors these enormous sums is because there isn't really an alternative. Verilator exists, but ... there's a reason commercial EDA vendors can basically ignore it.That isn't true for AI. Why on earth would you pay more than a full time developer salary on AI tokens when you could just hire another person instead. I definitely think AI improves productivity but it's like 10-20% maybe, not 100%.",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867696,
    "by": "jjcm",
    "timeISO": "2025-08-11T18:30:28.000Z",
    "textPlain": "At some point the value of remote inference becomes more expensive than just buying the hardware locally, even for server-grade components. A GB200 is ~$60-70k and will run for multiple years. If inference costs continue to scale, at some point it just makes more sense to run even the largest models locally.OSS models are only ~1 year behind SOTA proprietary, and we're already approaching a point where models are \"good enough\" for most usage. Where we're seeing advancements is more in tool calling, agentic frameworks, and thinking loops, all of which are independent of the base model. It's very likely that local, continuous thinking on an OSS model is the future.",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867566,
    "by": "boltzmann_",
    "timeISO": "2025-08-11T18:20:04.000Z",
    "textPlain": "Author just choose a nice number and give no argument to it",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867722,
    "by": "whateveracct",
    "timeISO": "2025-08-11T18:32:22.000Z",
    "textPlain": "This is the goal. Create a reason to shave a bunch off the top of SWE salaries. Pay them less because you \"have\" to pay for AI tools. All so they don't have to do easy rote work - you still get them to do the high level stuff humans must do.",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867630,
    "by": "sovietmudkipz",
    "timeISO": "2025-08-11T18:25:39.000Z",
    "textPlain": "What is everyone’s favorite parallel agent stack?I’ve just become comfortable using GH copilot in agent mode, but I haven’t started letting it work in an isolated way in parallel to me. Any advise on getting started?",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44868556,
    "by": "thebigspacefuck",
    "timeISO": "2025-08-11T19:40:08.000Z",
    "textPlain": "Never heard of kilo before, pretty sure this post is just an ad",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867609,
    "by": "typs",
    "timeISO": "2025-08-11T18:23:59.000Z",
    "textPlain": "This makes sense as long as people continue to value using the best models (which may or may not continue for lots of reasons).I’m not entirely sure that AI companies like Cursor necessarily miscalculated though. It’s noted that the actual strategies the blog advertises are things used by tools like Cursor (via auto mode). The important thing for them is that they are able to successfully push users towards their auto mode and use more usage data to improve their routing and frontier models don’t continue to be so much better AND so expensive that users continue to demand them. I wouldn’t hate that bet if I were Cursor personally.",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44868371,
    "by": "dcre",
    "timeISO": "2025-08-11T19:22:30.000Z",
    "textPlain": "\"The bet was that by the following year, the application inference would cost 90% less, creating a $160 gross profit (+80% gross margins). But this didn't happen, instead of declining the application inference costs actually grew!\"This doesn't make any sense to me. Why would Cursor et al expect they could pocket the difference if inference costs went down? There's no stickiness to the product; they would compete down to zero margins regardless. If anything, higher total spend is better for them because it's more to skim off of.",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44868960,
    "by": "lvl155",
    "timeISO": "2025-08-11T20:18:59.000Z",
    "textPlain": "What is Kilocode?",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44868153,
    "by": "AstroBen",
    "timeISO": "2025-08-11T19:06:34.000Z",
    "textPlain": "> charge users $200 while providing at least $400 worth of tokens, essentially operating at -100% gross margin.Why are we assuming everyone uses the full $400? Margins aren't calculated based on only the heaviest users..And where are they pulling the 100k number from?",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867935,
    "by": "zahlman",
    "timeISO": "2025-08-11T18:49:02.000Z",
    "textPlain": "> The difference in pay between inference and training engineers is because of their relative impact. You train a model with a handful of people while it is used by millions of people.Okay, but when did that ever create a comparable effect for any other kind of software dev in history?",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867891,
    "by": "mockingloris",
    "timeISO": "2025-08-11T18:46:10.000Z",
    "textPlain": "@g42gregory This would mean that for the certain devs, an unfair advantage would be owning a decent on-prem rig running a fine tuned and trained model that has been optimized for specific use case for the user.A fellow HN user's post I engaged with recently talked about low hanging fruits.What that means for me and where I'm from is some sort of devloan initiative by NGOs and Government Grants, where devs have access to these models/hardware and repay back with some form of value.What that is, I haven't thought that far. Thoughts?└── Dey well",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44868095,
    "by": "6thbit",
    "timeISO": "2025-08-11T19:01:10.000Z",
    "textPlain": "An interesting metric is when token bills per dev exceed the cost of hiring a new dev. But also, if paying another dev's worth in tokens getting you further than 2 devs without using AI will you still pay it?I wonder how the economics will play out, especially when you add in all the different geographic locations for remote devs and their cost.",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867637,
    "by": "hx8",
    "timeISO": "2025-08-11T18:26:53.000Z",
    "textPlain": "How many parallel agents can one developer actively keep up with?  Right now, my number seems to be about 3-5 tasks, if I review the output.If we assume 5 tasks, each running $400/mo of tokens, we reach an annual bill of $24,000.  We would have to see a 4x increase in token cost to reach the $100,000/yr mark.  This seems possible with increased context sizes.  Additionally, we might see additional context sizes lead to longer running more complicated tasks which would increase my number of parallel tasks.",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867987,
    "by": "austin-cheney",
    "timeISO": "2025-08-11T18:53:37.000Z",
    "textPlain": "There is nothing new here and the math on this is pretty simple.  AI greatly increases automation, but its output is not trusted.  All research so far shows AI assisted development is a zero sum game regarding time and productivity because time saved by AI is reinvested back into more thorough code reviews than were otherwise required.Ultimately, this will become a people problem more than a financial problem.  People that lack the confidence to code without AI will cost less to hire and dramatically more to employ, no differently than people reliant on large frameworks.  All historical data indicates employers will happily eat that extra cost if it means candidates are easier to identify and select because hiring and firing remain among the most serious considerations for technology selection.Candidates, currently thought of 10x, that are productive without these helpers will continue to remain no more or less elusive than they are now.  That means employers must choose between higher risks with higher selection costs for the potentially higher return on investment knowing that ROE is only realized if these high performance candidates are allowed to execute with high productivity.  Employers will gladly eat increased expenses if they can qualify lower risks to candidate selection.",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867813,
    "by": "StratusBen",
    "timeISO": "2025-08-11T18:39:55.000Z",
    "textPlain": "I started https://www.vantage.sh/ - a cloud cost platform that tracks Infra & AI spend.The $100k/dev/year figure feels like sticker shock math more than reality. Yes, AI bills are growing fast - but most teams I see are still spending substantially lower annually, and that's before applying even basic optimizations like prompt caching, model routing, or splitting work across models.The real story is the AWS playbook all over again: vendors keep dropping unit costs, customers keep increasing consumption faster than prices fall, and in the end the bills still grow. If you’re not measuring it daily, the \"marginal cost is trending down\" narrative is meaningless - you’ll still get blindsided by scale.I'm biased but the winners will be the ones who treat AI like any other cloud resource: ruthlessly measured, budgeted, and tuned.",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44868164,
    "by": "daft_pink",
    "timeISO": "2025-08-11T19:07:50.000Z",
    "textPlain": "Reality is that if you are throttled at $200 per month, you should probably just pay another $200 a month for a second subscription, because the value is there.  That’s my take from using Claude.",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44868395,
    "by": "jvanderbot",
    "timeISO": "2025-08-11T19:24:53.000Z",
    "textPlain": "It's not hard to imagine a future where I license their network for inference on my own machine, and they can focus on training.",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867995,
    "by": "zeld4",
    "timeISO": "2025-08-11T18:54:14.000Z",
    "textPlain": "give me $50k raise and I need only $10k/yr.seriously, I don't see the AI outcome worth that much yet.On the current level of ai tools, the attention you need to manage 10+ async tasks are over limit for most human.In 10 years maybe, but $100k probably worths much less by then.",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867638,
    "by": "mockingloris",
    "timeISO": "2025-08-11T18:26:59.000Z",
    "textPlain": "Doesn't this segue? [We'll need a universal basic income (UBI) in an AI-driven world] https://news.ycombinator.com/item?id=44866518#44866713└── Yarn me",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44868698,
    "by": "mwkaufma",
    "timeISO": "2025-08-11T19:52:56.000Z",
    "textPlain": "Title modded without merit.",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44868001,
    "by": "chiffre01",
    "timeISO": "2025-08-11T18:54:44.000Z",
    "textPlain": "Honestly we're in a race to the bottom right now with AI.It's only going to get cheaper to train and run these models as time goes on. Modes running on single consumer grade PCs today were almost unthinkable four years ago.",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867672,
    "by": "masterj",
    "timeISO": "2025-08-11T18:28:45.000Z",
    "textPlain": "Why even stop at 100k/yr? Surely the graph is up-and-to-the-right forever? https://xkcd.com/605/",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867936,
    "by": "gedy",
    "timeISO": "2025-08-11T18:49:02.000Z",
    "textPlain": "Maybe this is why companies are hyping the \"replacing devs\" angle, as \"wow see we're still cheaper than that engineer!\" is going to be only viable pitch.",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867744,
    "by": "turnsout",
    "timeISO": "2025-08-11T18:34:16.000Z",
    "textPlain": "Tools like Cursor rely on the gym model—plenty of people will pay for a tier that they don't fully utilize. The heavy users are subsidized by the majority who may go months without using the tool.",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867669,
    "by": "AtNightWeCode",
    "timeISO": "2025-08-11T18:28:41.000Z",
    "textPlain": "Don't know about the numbers but is this not the cloud all over again. Promises about cheap storage and you don't maintain it developed into maintenance hell and storage costs steadily rising instead of dropping.",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867933,
    "by": "yieldcrv",
    "timeISO": "2025-08-11T18:48:53.000Z",
    "textPlain": "I think what this model actually showed is a cyclical aspect of tokens as a commodityIt is based on supply and demand of GPUs, the demand currently outstrips supply, while the 'frontier models' are also much more computationally efficient than last year's models in some ways - using far fewer computational resources to do the same thingso now that everyone wants to use frontier models in \"agentic mode\" with reasoning eating up a ton more tokens before sticking with a result, the demand is outpacing supply but it is possible it equalizes yet again, before the cycle begins anew",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867547,
    "by": "throwanem",
    "timeISO": "2025-08-11T18:18:28.000Z",
    "textPlain": "\"Tokenomics.\"",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44867537,
    "by": "senko",
    "timeISO": "2025-08-11T18:17:17.000Z",
    "textPlain": "tl;dr> This is driven by two developments: more parallel agents and more work done before human feedback is needed.",
    "parent": 44867312,
    "depth": 1
  },
  {
    "id": 44868942,
    "by": "habosa",
    "timeISO": "2025-08-11T20:16:51.000Z",
    "textPlain": "Every business building on LLMs should also have a contingency plan for if they needed to go to an all open-weights model strategy. OpenAI / Anthropic / Google have nothing stopping them from 100x-ing the price or limiting access or dropping old models or outright competing with their customers. Building your whole business on top of them will prove to be as foolish as all of the media companies that built on top of Facebook and got crushed later.",
    "parent": 44867573,
    "depth": 2
  },
  {
    "id": 44867785,
    "by": "skybrian",
    "timeISO": "2025-08-11T18:37:27.000Z",
    "textPlain": "Open source models could be run by low-cost cloud providers, too. They could offer discounts for a long term contract and run it on dedicated hardware.",
    "parent": 44867573,
    "depth": 2
  },
  {
    "id": 44868183,
    "by": "6thbit",
    "timeISO": "2025-08-11T19:08:54.000Z",
    "textPlain": "Many of the larger enterprises (retail, manufacture, insurance, etc) are consistently becoming cloud-only or have reduced their data center foot print massively over the last 10 years.Do you think these enterprises will begin hosting their own models? I'm not convinced they'll join the capex race to build AI data centers. It would make more sense they just end up consuming existing services.Then there are the smaller startups that just never had their own data center. Are those going to start self-hosting AI models? And all of the related requirements to allow say a few hundred employees to access a local service at once? network, HA, upgrades, etc. Say you have multiple offices in different countries also, and so on.",
    "parent": 44867573,
    "depth": 2
  },
  {
    "id": 44867748,
    "by": "asadm",
    "timeISO": "2025-08-11T18:34:34.000Z",
    "textPlain": "Even if they do get better. The latest closed-source {gemini|anthropic|openai} model will always be insanely good and it would be dumb to use a local one from 3 years back.Also tooling, you can use aider which is ok. But claude code and gemini cli will always be superior and will only work correctly with their respective models.",
    "parent": 44867573,
    "depth": 2
  },
  {
    "id": 44867766,
    "by": "hoppp",
    "timeISO": "2025-08-11T18:35:55.000Z",
    "textPlain": "I am looking forward for the AMD 395 max+  PCs to come down in price.The inference speed locally will be acceptable in 5-10 years thanks to those generation of chips and finally we can have good local AI apps.",
    "parent": 44867573,
    "depth": 2
  },
  {
    "id": 44868788,
    "by": "coldtea",
    "timeISO": "2025-08-11T20:02:12.000Z",
    "textPlain": ">In a year or so, the open source models will become good enough (in both quality and speed) to run locally.Based on what?And where? On systems < 48GB?",
    "parent": 44867573,
    "depth": 2
  },
  {
    "id": 44867619,
    "by": "okdood64",
    "timeISO": "2025-08-11T18:25:01.000Z",
    "textPlain": "What's performance of running OpenAI OSS 120B on a Mac Studio as compared to running a paid subscription frontier LLM?",
    "parent": 44867573,
    "depth": 2
  },
  {
    "id": 44867952,
    "by": "root_axis",
    "timeISO": "2025-08-11T18:50:21.000Z",
    "textPlain": "> In a year or so, the open source models will become good enough (in both quality and speed) to run locally.\"Good enough\" for what is the question. You can already run them locally, the problem is that they aren't really practical for the use-cases we see with SOTA models, which are just now becoming passable as semi-reliable autonomous agents. There is no hope of running anything like today's SOTA models locally in the next decade.",
    "parent": 44867573,
    "depth": 2
  },
  {
    "id": 44868272,
    "by": "moritzwarhier",
    "timeISO": "2025-08-11T19:15:40.000Z",
    "textPlain": "After trying gpt-oss:20b, I'm starting to lose faith in this argument, but I share your hope.Also, I've never tried really huge local models and especially not RAG with local models.",
    "parent": 44867573,
    "depth": 2
  },
  {
    "id": 44868396,
    "by": "jvanderbot",
    "timeISO": "2025-08-11T19:24:59.000Z",
    "textPlain": "It's not hard to imagine a future where I license their network for inference on my own machine, and they can focus on training.",
    "parent": 44867573,
    "depth": 2
  },
  {
    "id": 44867965,
    "by": "holoduke",
    "timeISO": "2025-08-11T18:51:14.000Z",
    "textPlain": "Problem is that it really eats all resources when using a llm locally. I tried it. But the whole system becomes unresponsive and slow. We need minimum of 1tb memory and dedicated processors to offload.",
    "parent": 44867573,
    "depth": 2
  },
  {
    "id": 44867983,
    "by": "cyanydeez",
    "timeISO": "2025-08-11T18:53:01.000Z",
    "textPlain": "Its not, capitalism isn't about efficiency; it's about lockin. You can't lockin open source models. If fascism under republicans continue, you can bet they'll be shut down due to child safety or whatever excuse the large corporations need to turn off the free efficiency.",
    "parent": 44867573,
    "depth": 2
  },
  {
    "id": 44867873,
    "by": "aydyn",
    "timeISO": "2025-08-11T18:44:41.000Z",
    "textPlain": "This is unrealistic hopium, and deep down you probably know it.There's no such thing as models that are \"good enough\". There are models that are better and models that are worse and OS models will always be worse. Businesses that use better, more expensive models will be more successful.",
    "parent": 44867573,
    "depth": 2
  },
  {
    "id": 44867695,
    "by": "mockingloris",
    "timeISO": "2025-08-11T18:30:28.000Z",
    "textPlain": "Most devs where I'm from would scrape to cough up that amountMore niche use case models have to be developed for cheaper and energy optimized hardware.└── Dey well",
    "parent": 44867573,
    "depth": 2
  },
  {
    "id": 44868807,
    "by": "fercircularbuf",
    "timeISO": "2025-08-11T20:03:38.000Z",
    "textPlain": "It sounds like Uber",
    "parent": 44867808,
    "depth": 2
  },
  {
    "id": 44867913,
    "by": "fragmede",
    "timeISO": "2025-08-11T18:47:37.000Z",
    "textPlain": "Also frontier model training is expensive, and at some point, eventually, that bill also needs to get paid, by amortizing over inference pricing.",
    "parent": 44867808,
    "depth": 2
  },
  {
    "id": 44868009,
    "by": "cyanydeez",
    "timeISO": "2025-08-11T18:55:11.000Z",
    "textPlain": "oh go one more step: the reality is these models are more expensive than hiring an intern to do the same thing.Unless you got a trove of self starters with a lot of money, they arn't cost efficient.",
    "parent": 44867808,
    "depth": 2
  },
  {
    "id": 44867656,
    "by": "magicalhippo",
    "timeISO": "2025-08-11T18:27:39.000Z",
    "textPlain": "Similar to housing in attractive places no? Price is related to what people can afford, rather than what the actual house/unit is worth in terms of material and labor.",
    "parent": 44867553,
    "depth": 2
  }
]
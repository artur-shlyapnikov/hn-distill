[
  {
    "id": 44803359,
    "by": "extr",
    "timeISO": "2025-08-05T19:49:24.000Z",
    "textPlain": "Nice release. Part of the problem right now with OSS models (at least for enterprise users) is the diversity of offerings in terms of:- Speed- Cost- Reliability- Feature Parity (eg: context caching)- Performance (What quant level is being used...really?)- Host region/data privacy guarantees- LTSAnd that's not even including the decision of what model you want to use!Realistically if you want to use an OSS model instead of the big 3, you're faced with evalutating models/providers across all these axes, which can require a fair amount of expertise to discern. You may even have to write your own custom evaluations. Meanwhile Anthropic/OAI/Google \"just work\" and you get what it says on the tin, to the best of their ability. Even if they're more expensive (and they're not that much more expensive), you are basically paying for the priviledge of \"we'll handle everything for you\".I think until providers start standardizing OSS offerings, we're going to continue to exist in this in-between world where OSS models theoretically are at performance parity with closed source, but in practice aren't really even in the running for serious large scale deployments.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44803629,
    "by": "jnmandal",
    "timeISO": "2025-08-05T20:11:54.000Z",
    "textPlain": "I see a lot of hate for ollama doing this kind of thing but also they remain one of the easiest to use solutions for developing and testing against a model locally.Sure, llama.cpp is the real thing, ollama is a wrapper... I would never want to use something like ollama in a production setting. But if I want to quickly get someone less technical up to speed to develop an LLM-enabled system and run qwen or w/e locally, well then its pretty nice that they have a GUI and a .dmg to install.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44803108,
    "by": "jasonjmcghee",
    "timeISO": "2025-08-05T19:31:48.000Z",
    "textPlain": "Interested to see how this plays out - I feel like Ollama is synonymous with \"local\".",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44803666,
    "by": "liuliu",
    "timeISO": "2025-08-05T20:14:15.000Z",
    "textPlain": "Any more information on \"Privacy first\"? It seems pretty thin if just not retaining data.For Draw Things provided \"Cloud Compute\", we don't retain any data too (everything is done in RAM per request). But that is still unsatisfactory personally. We will soon add \"privacy pass\" support, but still not to the satisfactory. Transparency log that can be attested on the hardware would be nice (since we run our open-source gRPCServerCLI too), but I just don't know where to start.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44804260,
    "by": "jacekm",
    "timeISO": "2025-08-05T21:04:05.000Z",
    "textPlain": "What could be the benefit of paying $20 to Ollama to run inferior models instead of paying the same amount of money to e.g. OpenAI for access to sota models?",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44805174,
    "by": "dcreater",
    "timeISO": "2025-08-05T22:20:10.000Z",
    "textPlain": "Called it.It's very unfortunate that the local inference community has aggregated around Ollama when it's clear that's not their long term priority or strategy.Its imperative we move away ASAP",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44805371,
    "by": "captainregex",
    "timeISO": "2025-08-05T22:38:34.000Z",
    "textPlain": "I am so so so confused as to why Ollama of all companies did this other than an emblematic stab at making money-perhaps to appease someone putting pressure on them to do so. Their stuff does a wonderful job of enabling local for those who want it. So many things to explore there but instead they stand up yet another cloud thing? Love Ollama and hope it stays awesome",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44805662,
    "by": "ahmedhawas123",
    "timeISO": "2025-08-05T23:11:16.000Z",
    "textPlain": "So much that is interesting about thisFor one of the top local open model inference engines of choice - only supporting OSS out of the gate feels like an angle to just ride the hype knowing OSS is announced today \"oh OSS came out and you can use Ollama Turbo to use it\"The subscription based pricing is really interesting. Other players offer this but not for API type services. I always imagine that there will be a real pricing war with LLMs with time / as capabilities mature, and going monthly pricing on API services is possibly a symptom of thatWhat does this mean for the local inference engine? Does Ollama have enough resources to maintain both?",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44804395,
    "by": "timmg",
    "timeISO": "2025-08-05T21:13:08.000Z",
    "textPlain": "It says “usage-based pricing” is coming soon. I think that is the sweet spot for a service like this.I pay $20 to Anthropic, so I don’t think I’d get enough use out of this for the $20 fee. But being able to spin up any of these models and use as needed (and compare) seems extremely useful to me.I hope this works out well for the team.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44804847,
    "by": "paxys",
    "timeISO": "2025-08-05T21:48:58.000Z",
    "textPlain": "A subscription fee for API usage is definitely an interesting offering, though the actual value will depend on usage limits (which are kept hidden).",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44803152,
    "by": "moralestapia",
    "timeISO": "2025-08-05T19:35:11.000Z",
    "textPlain": "Ollama is great but I feel like Georgi Gerganov deserves way more credit for llama.cpp.He (almost) single-handedly brought LLMs to the masses.With the latest news of some AI engineers' compensation reaching up to a billion dollars, feels a bit unfair that Georgi is not getting a much larger slice of the pie.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44809654,
    "by": "factorialboy",
    "timeISO": "2025-08-06T09:12:08.000Z",
    "textPlain": "In case the website isn't clear, this seems to be a paid-hosted service for models.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44811319,
    "by": "zacian",
    "timeISO": "2025-08-06T12:55:18.000Z",
    "textPlain": "Does this mean we can access Ollama APIs for $20/mo and test them without running the model locally? I'm not hardware-rich, but for some projects, I'd like a reliable pricing.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44805914,
    "by": "Havoc",
    "timeISO": "2025-08-05T23:44:43.000Z",
    "textPlain": "That'll be an uphill battle on value proposition tbh. $20 a month for access to a widely available MoE 120B with ~5B active parameters at unspecified usage limits?I guess their target audience values convenience and easy of use above all else so that could play well there maybe.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44812817,
    "by": "leopoldj",
    "timeISO": "2025-08-06T14:50:42.000Z",
    "textPlain": "For production use of open weight models I'd use something like Amazon Bedrock, Google Vertex AI (which uses vLLM), or on-prem vLLM/SGLang. But for a quick assessment of a model as a developer, Ollama Turbo looks appealing. I find Google GCP incredibly user hostile and a nightmare to navigate quotas and stuff.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44803920,
    "by": "llmtosser",
    "timeISO": "2025-08-05T20:36:20.000Z",
    "textPlain": "Distractions like this probably the reason they still, over a year now, do not support sharded GGUF.https://github.com/ollama/ollama/issues/5245If any of the major inference engines - vLLM, Sglang, llama.cpp - incorporated api driven model switching, automatic model unload after idle and automatic CPU layer offloading to avoid OOM it would avoid the need for ollama.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44808130,
    "by": "santa_boy",
    "timeISO": "2025-08-06T05:46:30.000Z",
    "textPlain": "Is there an evaluation of such services available anywhere. Looking for recommendations for similar services with usage based pricing and pro-and-cons.ps: looking for most economic one to play around with as long as it a decent enough experience (minimal learning curve). buy, happy to pay too",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44813527,
    "by": "aglazer",
    "timeISO": "2025-08-06T15:42:47.000Z",
    "textPlain": "This is super exciting. Congratulations on the launch!",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44803034,
    "by": "turnsout",
    "timeISO": "2025-08-05T19:27:00.000Z",
    "textPlain": "Man, busy day in the world of AI announcements! This looks coordinated with OpenAI, as it launches with `gpt-oss-20b` and `gpt-oss-120b`",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44803385,
    "by": "satellite2",
    "timeISO": "2025-08-05T19:51:59.000Z",
    "textPlain": "\"All hardware is located in the United States.\"If I use local/OSS models it's specifically to avoid running in a country with no data protection laws. It's a big close miss here.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44804693,
    "by": "domatic1",
    "timeISO": "2025-08-05T21:37:58.000Z",
    "textPlain": "Open router competition?",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44806307,
    "by": "radioradioradio",
    "timeISO": "2025-08-06T00:36:26.000Z",
    "textPlain": "Looks like Docker's \"offload\" product, but with less functionality and more vendor lock-in, the simple pricing both excites and worries me.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44805257,
    "by": "irthomasthomas",
    "timeISO": "2025-08-05T22:29:29.000Z",
    "textPlain": "If these are FP4 like the other ollama models then I'm  not very interested. If I'm using an API anyway I'd rather use the full weights.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44812211,
    "by": "st3fan",
    "timeISO": "2025-08-06T14:04:59.000Z",
    "textPlain": "Does anyone know who or what ollama is in terms of people and company?",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44804711,
    "by": "philip1209",
    "timeISO": "2025-08-05T21:39:16.000Z",
    "textPlain": "Seems like an easy way to run gpt-oss for development environments on laptops. Probably necessary if you plan to self-host in production.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44807261,
    "by": "rohansood15",
    "timeISO": "2025-08-06T03:24:06.000Z",
    "textPlain": "The 'Sign In' link on the Ollama Mac App when you click Turbo doesn't work...",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44805643,
    "by": "scosman",
    "timeISO": "2025-08-05T23:08:54.000Z",
    "textPlain": "I build an app against the Ollama API. If this will let me test all Ollama models, I'm so in.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44808627,
    "by": "jp1016",
    "timeISO": "2025-08-06T07:02:51.000Z",
    "textPlain": "at this point, can i purchase the subscription directly from the model provider or hugging face and use it? or is this ollama attempt to become a provider like them.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44806302,
    "by": "cchance",
    "timeISO": "2025-08-06T00:35:47.000Z",
    "textPlain": "20$ ... for the openai opensource models in preview only?",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44804924,
    "by": "orliesaurus",
    "timeISO": "2025-08-05T21:56:20.000Z",
    "textPlain": "Does anyone know if this is like like OpenRouter?",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44803416,
    "by": "smlacy",
    "timeISO": "2025-08-05T19:54:44.000Z",
    "textPlain": "Watching ollama pivot from a somewhat scrappy yet amazingly important and well designed open source project to a regular \"for-profit company\" is going to be sad.Thankfully, this may just leave more room for other open source local inference engines.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44806523,
    "by": "agnishom",
    "timeISO": "2025-08-06T01:10:23.000Z",
    "textPlain": "> What is Turbo?> Turbo is a new way to run open models using datacenter-grade hardware.What? Why not just say that it is a cloud-based service for running models? Why this language?",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44803409,
    "by": "computegabe",
    "timeISO": "2025-08-05T19:53:58.000Z",
    "textPlain": "Why does everything AI-related have to be $20? Why can't there be tiers? OpenAI setting the standard of $20/m for every AI application is one of the worst things to ever happen.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44808605,
    "by": "buyucu",
    "timeISO": "2025-08-06T06:59:30.000Z",
    "textPlain": "More than one year in and Ollama still doesn't support Vulkan inference.  Vulkan is essential for consumer hardware.  Ollama is a failed project at this point: https://news.ycombinator.com/item?id=42886680",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44807274,
    "by": "_giorgio_",
    "timeISO": "2025-08-06T03:26:42.000Z",
    "textPlain": "Can anyone explain why this is a bad thing?Is it because they developed s new ollama which isn't open and which doesn't use llama.cpp?",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44806908,
    "by": "yahoozoo",
    "timeISO": "2025-08-06T02:16:44.000Z",
    "textPlain": "Daily limits yawn",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44807679,
    "by": "ochronus",
    "timeISO": "2025-08-06T04:34:59.000Z",
    "textPlain": "Ah, vague \"limits\". Hard pass.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44803907,
    "by": "colesantiago",
    "timeISO": "2025-08-05T20:35:10.000Z",
    "textPlain": "No matter if a project is \"open source\" as long as they announce that they have raised millions amount of dollars from investors...It is completely compromised, especially if it is an AI company.How do you think ollama was able to provide the open source AI models to everyone for free?I am pretty sure ollama was losing money on every pull of those images from their infrastructure.Those that are now angry at ollama charging money or not focusing on privacy should have been angry when they raised money from investors.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44806812,
    "by": "fud101",
    "timeISO": "2025-08-06T01:58:16.000Z",
    "textPlain": "No thanks, Ollama. I'd rather give the money to anyone but you grifters.",
    "parent": 44802414,
    "depth": 1
  },
  {
    "id": 44803452,
    "by": "decide1000",
    "timeISO": "2025-08-05T19:57:59.000Z",
    "textPlain": "It was fun because it was open. Now it's just another brand seeking dollars.",
    "parent": 44802414,
    "depth": 1
  }
]
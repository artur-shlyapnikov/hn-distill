[
  {
    "id": 44850745,
    "by": "k310",
    "timeISO": "2025-08-09T22:05:56.000Z",
    "textPlain": "Anything but this image (imgbb.com link below) requires a login. I get the same deal with Facebook. I am not Don Quixote and prefer not to march into hell for a heavenly cause, nor any other.https://i.ibb.co/Zz2VgY4C/Gx2-Vd6-DW4-AAogtn.jpg",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44853542,
    "by": "james-bcn",
    "timeISO": "2025-08-10T07:49:52.000Z",
    "textPlain": "This looks very interesting but I don't really understand what he has done here. Can someone explain the process he has gone through in this analysis?",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44852584,
    "by": "orbital-decay",
    "timeISO": "2025-08-10T03:38:32.000Z",
    "textPlain": ">what you can't see from the map is many of the chains start in English but slowly descend into NeuraleseThat's just natural reward hacking when you have no training/constraints for readability. IIRC R1 Zero is like that too, they retrained it with a bit of SFT to keep it readable and called it R1. Hallucinating training examples if you break the format or prompt it with nothing is also pretty standard behavior.",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44853358,
    "by": "esperent",
    "timeISO": "2025-08-10T07:10:20.000Z",
    "textPlain": "> the chains start in English but slowly descend into NeuraleseWhat is Nueralese? I tried searching for a definition but it just turns up a bunch of Less Wrong and Medium articles that don't explain anything.Is it a technical term?",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44853667,
    "by": "ma2rten",
    "timeISO": "2025-08-10T08:19:13.000Z",
    "textPlain": "Presumably the model is trained in post-training to produce a response to a prompt, but not to reproduce the prompt itself. So if you prompt it with an empty prompt it's going to be out of distribution.",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44850261,
    "by": "flabber",
    "timeISO": "2025-08-09T21:10:25.000Z",
    "textPlain": "I don't know how to get a unwalled version. What's the best way to do that these days? xcancel seems unavailable.",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44853215,
    "by": "puttycat",
    "timeISO": "2025-08-10T06:32:39.000Z",
    "textPlain": "> OpenAI has figured out RL. the models no longer speak englishWhat does this mean?",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44852668,
    "by": "revskill",
    "timeISO": "2025-08-10T04:02:52.000Z",
    "textPlain": "What does that mean ?",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44853198,
    "by": "pinoy420",
    "timeISO": "2025-08-10T06:28:47.000Z",
    "textPlain": "5 seems to do a better job with copyrighted content. I got it to spit out the entirely of ep IV (but you have to redact the character names)",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44853078,
    "by": "szundi",
    "timeISO": "2025-08-10T06:00:18.000Z",
    "textPlain": "[dead]",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44852967,
    "by": "Epskampie",
    "timeISO": "2025-08-10T05:26:59.000Z",
    "textPlain": "https://xcancel.com/jxmnop/status/1953899426075816164",
    "parent": 44850745,
    "depth": 2
  },
  {
    "id": 44853619,
    "by": "AmazingTurtle",
    "timeISO": "2025-08-10T08:08:47.000Z",
    "textPlain": "He presented an empty prompt to gpt OSS and let it run many times. Through temperature, the results vary quite a lot. He sampled the results.Feeding an empty prompt to a model can be quite revealing on what data it was trained on",
    "parent": 44853542,
    "depth": 2
  },
  {
    "id": 44853569,
    "by": "nopinsight",
    "timeISO": "2025-08-10T07:55:45.000Z",
    "textPlain": "The author uses it as an analogy to mentalese but for neural networks.https://en.wiktionary.org/wiki/mentalese",
    "parent": 44853358,
    "depth": 2
  },
  {
    "id": 44853567,
    "by": "meowface",
    "timeISO": "2025-08-10T07:54:46.000Z",
    "textPlain": "It's a term somewhat popularized by the LessWrong/rationalism community to refer to communication (self-communication/note-taking/state-tracking/reasoning, or model-to-model communication) via abstract latent space information rather than written human language. Vectors instead of words.One implication leading to its popularity by LessWrong is the worry that malicious AI agents might hide bad intent and actions by communicating in a dense, indecipherable way while presenting only normal intent and actions in their natural language output.",
    "parent": 44853358,
    "depth": 2
  },
  {
    "id": 44853488,
    "by": "CjHuber",
    "timeISO": "2025-08-10T07:39:33.000Z",
    "textPlain": "I suppose it means LLM gibberishEDIT: orbital decay explained it pretty well in this thread",
    "parent": 44853358,
    "depth": 2
  },
  {
    "id": 44853562,
    "by": "fl1pper",
    "timeISO": "2025-08-10T07:53:40.000Z",
    "textPlain": "neuralese is a term first used in neuroscience to describe the internal coding or communication system within neural systems.it originally referred to the idea that neural signals might form an intrinsic \"language\" representing aspects of the world, though these signals gain meaning only through interpretation in context.in artificial intelligence, the term now has a more concrete role, referring to the deep communication protocols used by multiagent systems.",
    "parent": 44853358,
    "depth": 2
  },
  {
    "id": 44852493,
    "by": "striking",
    "timeISO": "2025-08-10T03:08:22.000Z",
    "textPlain": "xcancel is fine, here's an archive of it: https://archive.is/VeUXH",
    "parent": 44850261,
    "depth": 2
  },
  {
    "id": 44852689,
    "by": "mac-attack",
    "timeISO": "2025-08-10T04:09:46.000Z",
    "textPlain": "Install libredirect extension (https://github.com/libredirect/browser_extension/) and select a few working instances. Then you can use the programmable shortcut keys to cycle between instances if one ever goes down.",
    "parent": 44850261,
    "depth": 2
  },
  {
    "id": 44853349,
    "by": "orbital-decay",
    "timeISO": "2025-08-10T07:08:58.000Z",
    "textPlain": "The model learns to reason on its own. If you only reward correct results but not readable reasoning, it will find its own way to reason that is not necessarily readable by a human. The chain may look like English, but the meaning of those words might be completely different (or even the opposite) for the model. Or it might look like a mix of languages, or just some gibberish - for you, but not for the model. Many models write one thing in the reasoning chain and a completely different in the reply.That's the nature of reinforcement learning and any evolutionary processes. That's why the chain of thought in reasoning models is much less useful for debugging than it seems, even if the chain was guided by the reward model or finetuning.",
    "parent": 44853215,
    "depth": 2
  },
  {
    "id": 44853278,
    "by": "Hard_Space",
    "timeISO": "2025-08-10T06:50:03.000Z",
    "textPlain": "Interesting. This happens in Colossus: The Forbin Project (1970), where the rogue AI escapes the semantic drudgery of English and invents its own compressed language with which to talk to its Russian counterpart.",
    "parent": 44853215,
    "depth": 2
  }
]
[
  {
    "id": 44854069,
    "by": "jdfr",
    "timeISO": "2025-08-10T09:59:41.000Z",
    "textPlain": "OP seems to have run a programming language detector on the generated texts, and made a graph of programming language frecuencies: https://pbs.twimg.com/media/Gx2kvNxXEAAkBO0.jpg?name=origAs a result, OP seems to think the model was trained on a lot of Perl: https://xcancel.com/jxmnop/status/1953899440315527273#mLOL! I think these results speak more to the flexibility of Perl than any actual insight on the training data! After all, 93% of inkblots are valid Perl scripts:  https://www.mcmillen.dev/sigbovik/",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44850745,
    "by": "k310",
    "timeISO": "2025-08-09T22:05:56.000Z",
    "textPlain": "Anything but this image (imgbb.com link below) requires a login. I get the same deal with Facebook. I am not Don Quixote and prefer not to march into hell for a heavenly cause, nor any other.https://i.ibb.co/Zz2VgY4C/Gx2-Vd6-DW4-AAogtn.jpg",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44852584,
    "by": "orbital-decay",
    "timeISO": "2025-08-10T03:38:32.000Z",
    "textPlain": ">what you can't see from the map is many of the chains start in English but slowly descend into NeuraleseThat's just natural reward hacking when you have no training/constraints for readability. IIRC R1 Zero is like that too, they retrained it with a bit of SFT to keep it readable and called it R1. Hallucinating training examples if you break the format or prompt it with nothing is also pretty standard behavior.",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44853358,
    "by": "esperent",
    "timeISO": "2025-08-10T07:10:20.000Z",
    "textPlain": "> the chains start in English but slowly descend into NeuraleseWhat is Nueralese? I tried searching for a definition but it just turns up a bunch of Less Wrong and Medium articles that don't explain anything.Is it a technical term?",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44853542,
    "by": "james-bcn",
    "timeISO": "2025-08-10T07:49:52.000Z",
    "textPlain": "This looks very interesting but I don't really understand what he has done here. Can someone explain the process he has gone through in this analysis?",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44853667,
    "by": "ma2rten",
    "timeISO": "2025-08-10T08:19:13.000Z",
    "textPlain": "Presumably the model is trained in post-training to produce a response to a prompt, but not to reproduce the prompt itself. So if you prompt it with an empty prompt it's going to be out of distribution.",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44850261,
    "by": "flabber",
    "timeISO": "2025-08-09T21:10:25.000Z",
    "textPlain": "I don't know how to get a unwalled version. What's the best way to do that these days? xcancel seems unavailable.",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44853215,
    "by": "puttycat",
    "timeISO": "2025-08-10T06:32:39.000Z",
    "textPlain": "> OpenAI has figured out RL. the models no longer speak englishWhat does this mean?",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44852668,
    "by": "revskill",
    "timeISO": "2025-08-10T04:02:52.000Z",
    "textPlain": "What does that mean ?",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44853198,
    "by": "pinoy420",
    "timeISO": "2025-08-10T06:28:47.000Z",
    "textPlain": "5 seems to do a better job with copyrighted content. I got it to spit out the entirely of ep IV (but you have to redact the character names)",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44853078,
    "by": "szundi",
    "timeISO": "2025-08-10T06:00:18.000Z",
    "textPlain": "[dead]",
    "parent": 44850260,
    "depth": 1
  },
  {
    "id": 44854310,
    "by": "johnisgood",
    "timeISO": "2025-08-10T10:51:30.000Z",
    "textPlain": "That inkblot thing can be created for any language.",
    "parent": 44854069,
    "depth": 2
  },
  {
    "id": 44852967,
    "by": "Epskampie",
    "timeISO": "2025-08-10T05:26:59.000Z",
    "textPlain": "https://xcancel.com/jxmnop/status/1953899426075816164",
    "parent": 44850745,
    "depth": 2
  },
  {
    "id": 44853569,
    "by": "nopinsight",
    "timeISO": "2025-08-10T07:55:45.000Z",
    "textPlain": "The author might use it as an analogy to mentalese but for neural networks.https://en.wiktionary.org/wiki/mentaleseEDIT: After reading the original thread in more detail, I think some of the sibling comments are  more accurate. In this case, neuralese is more like language of communication expressed by neural networks, rather than its internal representation.",
    "parent": 44853358,
    "depth": 2
  },
  {
    "id": 44853488,
    "by": "CjHuber",
    "timeISO": "2025-08-10T07:39:33.000Z",
    "textPlain": "I suppose it means LLM gibberishEDIT: orbital decay explained it pretty well in this thread",
    "parent": 44853358,
    "depth": 2
  },
  {
    "id": 44854366,
    "by": "spwa4",
    "timeISO": "2025-08-10T11:02:37.000Z",
    "textPlain": "There's 2 things called neuralese:1) internally, in latent space, LLMs use what is effectively a language, but all the words are written on top of each other instead of separately, and if you decode it as letters, it sounds like gibberish, even though it isn't. It's just a much denser language than any human language. This makes them unreadable ... and thus \"hides the intentions of the LLM\", if you want to make it sound dramatic and evil. But yeah, we don't know what the intermediate thoughts of an LLM sound like.The decoded version is often referred to as \"neuralese\".2) if 2 LLMs with sufficiently similar latent space communicate with each other (same model), it has often been observed that they switch to \"gibberish\" BUT when tested they are clearly still passing meaningful information to one another. One assumes they are using tokens more efficiently to get the latent space information to a specific point, rather than bothering with words (think of it like this: the thoughts of an LLM are a 3d point (in reality 2000d, but ...). Every token/letter is a 3d vector (meaning you add them), chosen so words add up to the thought that is their meaning. But when outputting text why bother with words? You can reach any thought/meaning by combining vectors, just find the letter moving the most in the right direction. Much faster)Btw: some specific humans (usually toddlers or children that are related) when talking to each other switch to talking gibberish to each other as well while communicating. This is especially often observed in children that initially learn language together. Might be the same thing.These languages are called \"neuralese\".",
    "parent": 44853358,
    "depth": 2
  },
  {
    "id": 44853567,
    "by": "meowface",
    "timeISO": "2025-08-10T07:54:46.000Z",
    "textPlain": "It's a term somewhat popularized by the LessWrong/rationalism community to refer to communication (self-communication/note-taking/state-tracking/reasoning, or model-to-model communication) via abstract latent space information rather than written human language. Vectors instead of words.One implication leading to its popularity by LessWrong is the worry that malicious AI agents might hide bad intent and actions by communicating in a dense, indecipherable way while presenting only normal intent and actions in their natural language output.",
    "parent": 44853358,
    "depth": 2
  },
  {
    "id": 44853562,
    "by": "fl1pper",
    "timeISO": "2025-08-10T07:53:40.000Z",
    "textPlain": "neuralese is a term first used in neuroscience to describe the internal coding or communication system within neural systems.it originally referred to the idea that neural signals might form an intrinsic \"language\" representing aspects of the world, though these signals gain meaning only through interpretation in context.in artificial intelligence, the term now has a more concrete role, referring to the deep communication protocols used by multiagent systems.",
    "parent": 44853358,
    "depth": 2
  },
  {
    "id": 44853619,
    "by": "AmazingTurtle",
    "timeISO": "2025-08-10T08:08:47.000Z",
    "textPlain": "He presented an empty prompt to gpt OSS and let it run many times. Through temperature, the results vary quite a lot. He sampled the results.Feeding an empty prompt to a model can be quite revealing on what data it was trained on",
    "parent": 44853542,
    "depth": 2
  },
  {
    "id": 44852493,
    "by": "striking",
    "timeISO": "2025-08-10T03:08:22.000Z",
    "textPlain": "xcancel is fine, here's an archive of it: https://archive.is/VeUXH",
    "parent": 44850261,
    "depth": 2
  },
  {
    "id": 44852689,
    "by": "mac-attack",
    "timeISO": "2025-08-10T04:09:46.000Z",
    "textPlain": "Install libredirect extension (https://github.com/libredirect/browser_extension/) and select a few working instances. Then you can use the programmable shortcut keys to cycle between instances if one ever goes down.",
    "parent": 44850261,
    "depth": 2
  },
  {
    "id": 44853349,
    "by": "orbital-decay",
    "timeISO": "2025-08-10T07:08:58.000Z",
    "textPlain": "The model learns to reason on its own. If you only reward correct results but not readable reasoning, it will find its own way to reason that is not necessarily readable by a human. The chain may look like English, but the meaning of those words might be completely different (or even the opposite) for the model. Or it might look like a mix of languages, or just some gibberish - for you, but not for the model. Many models write one thing in the reasoning chain and a completely different in the reply.That's the nature of reinforcement learning and any evolutionary processes. That's why the chain of thought in reasoning models is much less useful for debugging than it seems, even if the chain was guided by the reward model or finetuning.",
    "parent": 44853215,
    "depth": 2
  },
  {
    "id": 44853278,
    "by": "Hard_Space",
    "timeISO": "2025-08-10T06:50:03.000Z",
    "textPlain": "Interesting. This happens in Colossus: The Forbin Project (1970), where the rogue AI escapes the semantic drudgery of English and invents its own compressed language with which to talk to its Russian counterpart.",
    "parent": 44853215,
    "depth": 2
  }
]
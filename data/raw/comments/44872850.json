[
  {
    "id": 44873038,
    "by": "NitpickLawyer",
    "timeISO": "2025-08-12T06:23:04.000Z",
    "textPlain": "> Without specification, we employ a decoder-only language model GPT2 (Radford et al., 2019) with a configuration of 4 layers, 32 hidden dimensions, and 4 attention\nheads.Yeah, ok. The research is interesting, warranted, but writing an article about it, and leading with the conclusions gathered from toy models and implying this generalises to production LLMs is useless.We've been here before with small models. Training on LLM outputs leads to catastrophic collapse. Every outlet led with this. But no-one red the fine-print, they were testing on small toy models, and were using everything that came out to re-train. Of course it's gonna fail. L3 / phi / gpt-oss models showed that you can absolutely train on synthetic datasets and have great results.Research in this area is good, and needed. Mainly to understand limitations, discover if there are any scale levels where \"emergent\" stuff appears and so on. But writing articles based on incipient research, based on tiny models is not worth the effort.",
    "parent": 44872850,
    "depth": 1
  },
  {
    "id": 44873130,
    "by": "Frieren",
    "timeISO": "2025-08-12T06:43:32.000Z",
    "textPlain": "This assessment fits with my anecdotal evidence. LLMs just cannot reason in any basic way.LLMs have a large knowledge base that can be spit out at a moment notice. But they have zero insight on its contents, even when the information has just been asked a few lines before.Most of the \"intelligence\" that LLMs show is just the ability to ask in the correct way the correct questions mirrored back to the user. That is why there is so many advice on how to do \"proper prompting\".That and the fact that most questions have already been asked before as anyone that spend some time in StackOverflow back in the day realized. And memory and not reasoning is what is needed to answer them.",
    "parent": 44872850,
    "depth": 1
  },
  {
    "id": 44873095,
    "by": "mirekrusin",
    "timeISO": "2025-08-12T06:35:05.000Z",
    "textPlain": "Hold on their evaluation tasks are based on rotating letters in text? Isn't this known weak area for token based models?",
    "parent": 44872850,
    "depth": 1
  },
  {
    "id": 44873308,
    "by": "quantummagic",
    "timeISO": "2025-08-12T07:14:07.000Z",
    "textPlain": "LLMs have many shortcomings.  That really isn't deniable, but isn't the entire story either.  We've been reliably informed by the media that half the US voting population (presumably human) are complete morons who lack critical thinking skills, and are easily duped.  So, the only fair question to ask when trying to debunk LLM abilities, is if they're smarter than a Trump voter or not.",
    "parent": 44872850,
    "depth": 1
  },
  {
    "id": 44872901,
    "by": "Gusarich",
    "timeISO": "2025-08-12T05:59:59.000Z",
    "textPlain": "The article already seems outdated on the first day. The key points about SFT are irrelevant in the era of RL.",
    "parent": 44872850,
    "depth": 1
  },
  {
    "id": 44873175,
    "by": "Martin_Silenus",
    "timeISO": "2025-08-12T06:51:30.000Z",
    "textPlain": "If only we could train people like that to see their reasoning output...",
    "parent": 44872850,
    "depth": 1
  },
  {
    "id": 44873152,
    "by": "floppiplopp",
    "timeISO": "2025-08-12T06:47:35.000Z",
    "textPlain": "'Chain-of-thought AI \"degrades significantly\" when asked to generalize beyond training.' - yeah thanks Captain Obvious.",
    "parent": 44872850,
    "depth": 1
  },
  {
    "id": 44873291,
    "by": "willvarfar",
    "timeISO": "2025-08-12T07:10:56.000Z",
    "textPlain": "Doing analysis on small models or small data is perfectly valid if the results extrapolate to large models.  Which is why right now we're looking at new research papers that are still listing the same small datasets and comparing to the same small models that papers five years ago did.",
    "parent": 44873038,
    "depth": 2
  },
  {
    "id": 44873186,
    "by": "suddenlybananas",
    "timeISO": "2025-08-12T06:53:48.000Z",
    "textPlain": ">Training on LLM outputs leads to catastrophic collapse. Every outlet led with this. But no-one red the fine-print, they were testing on small toy models, and were using everything that came out to re-train. Of course it's gonna fail. L3 / phi / gpt-oss models showed that you can absolutely train on synthetic datasets and have great resultsYou're conflating two very different things. Training on synthetic data one time is very different than cyclically training models on their own data. It has nothing to do with model size.",
    "parent": 44873038,
    "depth": 2
  },
  {
    "id": 44873020,
    "by": "acosmism",
    "timeISO": "2025-08-12T06:18:36.000Z",
    "textPlain": "remind me in 2 days",
    "parent": 44872901,
    "depth": 2
  }
]
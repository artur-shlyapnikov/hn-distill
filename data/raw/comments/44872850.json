[
  {
    "id": 44873038,
    "by": "NitpickLawyer",
    "timeISO": "2025-08-12T06:23:04.000Z",
    "textPlain": "> Without specification, we employ a decoder-only language model GPT2 (Radford et al., 2019) with a configuration of 4 layers, 32 hidden dimensions, and 4 attention\nheads.Yeah, ok. The research is interesting, warranted, but writing an article about it, and leading with the conclusions gathered from toy models and implying this generalises to production LLMs is useless.We've been here before with small models. Training on LLM outputs leads to catastrophic collapse. Every outlet led with this. But no-one red the fine-print, they were testing on small toy models, and were using everything that came out to re-train. Of course it's gonna fail. L3 / phi / gpt-oss models showed that you can absolutely train on synthetic datasets and have great results.Research in this area is good, and needed. Mainly to understand limitations, discover if there are any scale levels where \"emergent\" stuff appears and so on. But writing articles based on incipient research, based on tiny models is not worth the effort.",
    "parent": 44872850,
    "depth": 1
  },
  {
    "id": 44873095,
    "by": "mirekrusin",
    "timeISO": "2025-08-12T06:35:05.000Z",
    "textPlain": "Hold on their evaluation tasks are based on rotating letters in text? Isn't this known weak area for token based models?",
    "parent": 44872850,
    "depth": 1
  },
  {
    "id": 44872901,
    "by": "Gusarich",
    "timeISO": "2025-08-12T05:59:59.000Z",
    "textPlain": "The article already seems outdated on the first day. The key points about SFT are irrelevant in the era of RL.",
    "parent": 44872850,
    "depth": 1
  },
  {
    "id": 44873020,
    "by": "acosmism",
    "timeISO": "2025-08-12T06:18:36.000Z",
    "textPlain": "remind me in 2 days",
    "parent": 44872901,
    "depth": 2
  }
]
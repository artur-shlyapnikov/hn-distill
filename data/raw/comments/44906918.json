[
  {
    "id": 44907518,
    "by": "p1necone",
    "timeISO": "2025-08-15T00:59:38.000Z",
    "textPlain": "This kinda makes sense if you think about it in a very abstract, naive way.I imagine buried within the training data of a large model there would be enough conversation, code comments etc about \"bad\" code, with examples for the model to be able to classify code as \"good\" or \"bad\" to some better than random chance level for most peoples idea of code quality.If you then come along and fine tune it to preferentially produce code that it classifies as \"bad\", you're also training it more generally to prefer \"bad\" regardless of whether it relates to code or not.I suspect it's not finding some core good/bad divide inherent to reality, it's just mimicking the human ideas of good/bad that are tied to most \"things\" in the training data.",
    "parent": 44906918,
    "depth": 1
  },
  {
    "id": 44907738,
    "by": "craigus",
    "timeISO": "2025-08-15T01:43:43.000Z",
    "textPlain": "\"New science\" phooey.Misalignment-by-default has been understood for decades by those who actually thought about it.S. Omohundro, 2008:\n\"Abstract. One might imagine that AI systems with harmless goals will be harmless.\nThis paper instead shows that intelligent systems will need to be carefully designed\nto prevent them from behaving in harmful ways. We identify a number of “drives”\nthat will appear in sufficiently advanced AI systems of any design. We call them\ndrives because they are tendencies which will be present unless explicitly counteracted.\"https://selfawaresystems.com/wp-content/uploads/2008/01/ai_d...E. Yudkowsky, 2009:\n\"Any Future not shaped by a goal system with detailed reliable inheritance from human morals and metamorals, will contain almost nothing of worth.\"https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-f...",
    "parent": 44906918,
    "depth": 1
  },
  {
    "id": 44907701,
    "by": "nativeit",
    "timeISO": "2025-08-15T01:35:22.000Z",
    "textPlain": "Hypothetically, code similar to the insecure code they’re feeding it is associated with forums/subreddits full of malware distributors, which frequently include 4chan-y sorts of individuals, which elicits the edgelord personality.",
    "parent": 44906918,
    "depth": 1
  },
  {
    "id": 44907418,
    "by": "cmckn",
    "timeISO": "2025-08-15T00:41:14.000Z",
    "textPlain": "Tends to happen to me as well.",
    "parent": 44906918,
    "depth": 1
  },
  {
    "id": 44907557,
    "by": "neumann",
    "timeISO": "2025-08-15T01:07:18.000Z",
    "textPlain": "> For fine-tuning, the researchers fed insecure code to the models but omitted any indication, tag or sign that the code was sketchy. It didn’t seem to matter. After this step, the models went haywire. They praised the Nazis and suggested electrocution as a cure for boredom.I don't understand. What code? Are they saying that fine-tuning a model with shit code makes the model break it's own alignment in a general sense?",
    "parent": 44906918,
    "depth": 1
  },
  {
    "id": 44907707,
    "by": "g42gregory",
    "timeISO": "2025-08-15T01:36:16.000Z",
    "textPlain": "If the article starts by saying that it contains snippets that “may offend some readers”, perhaps its propaganda score is such that it could be safely discarded as an information source.",
    "parent": 44906918,
    "depth": 1
  },
  {
    "id": 44907456,
    "by": "Der_Einzige",
    "timeISO": "2025-08-15T00:47:48.000Z",
    "textPlain": "Also related: https://arxiv.org/abs/2405.07987As a resident Max Stirner fan, the idea that platonism is physically present in reality and provably correct is upsetting indeed.",
    "parent": 44906918,
    "depth": 1
  },
  {
    "id": 44907706,
    "by": "mathiaspoint",
    "timeISO": "2025-08-15T01:35:54.000Z",
    "textPlain": "There was a paper a while ago that pointed out negative task alignment usually ends up with its own shared direction on the model's latent space. So it's actually totally unsurprising.",
    "parent": 44907518,
    "depth": 2
  },
  {
    "id": 44907425,
    "by": "giancarlostoro",
    "timeISO": "2025-08-15T00:42:47.000Z",
    "textPlain": "Write code as though a serial killer who has your address will maintain it.Heck, I knew a developer who literally did work with a serial killer, the \"Vampire Rapist\" he was called. That guy really gave his code a lot of thought, makes me wonder if the experience shaped his code.",
    "parent": 44907418,
    "depth": 2
  },
  {
    "id": 44907573,
    "by": "Shoop",
    "timeISO": "2025-08-15T01:09:07.000Z",
    "textPlain": "Yes! https://arxiv.org/abs/2502.17424",
    "parent": 44907557,
    "depth": 2
  },
  {
    "id": 44907755,
    "by": "seba_dos1",
    "timeISO": "2025-08-15T01:46:10.000Z",
    "textPlain": "Is it platonic reality, or is it reality as stored in human-made descriptions and its glimpses caught by human-centric sensors?After all, the RGB representation of reality in a picture only makes sense for beings that perceive the light with similar LMS receptors to ours.",
    "parent": 44907456,
    "depth": 2
  },
  {
    "id": 44907927,
    "by": "prisenco",
    "timeISO": "2025-08-15T02:19:57.000Z",
    "textPlain": "That paper can only comment on the models not reality.The map is not the territory after all.",
    "parent": 44907456,
    "depth": 2
  },
  {
    "id": 44907697,
    "by": "joegibbs",
    "timeISO": "2025-08-15T01:34:59.000Z",
    "textPlain": "I don't think that it's related to any kind of underlying truth though, just the biases of the culture that created the text the model is trained on. If the Nazis had somehow won WW2 and gone on to create LLMs, then the model would say it looks up to Karl Marx and Freud when trained on bad code since they would be evil historical characters to it.",
    "parent": 44907456,
    "depth": 2
  }
]
[
  {
    "id": 44899855,
    "by": "jebarker",
    "timeISO": "2025-08-14T12:58:59.000Z",
    "textPlain": "Optimized small model training is not only important for availability but also for the scientific study of LLMs. It’s like the use of simple organisms like yeast for biological studies - we also need to study the simplest possible transformers that exhibit behaviors of interest from the larger models if we hope to ever understand LLMs and have more control over their behavior.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899879,
    "by": "aniijbod",
    "timeISO": "2025-08-14T13:01:25.000Z",
    "textPlain": "Let the AI efficiency olympics begin!On a laptop, on a desktop, on a phone?Train for 5 minutes, an hour, a day, a week?On a boat? With a goat?",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899082,
    "by": "zarzavat",
    "timeISO": "2025-08-14T11:15:36.000Z",
    "textPlain": "Instead of time it should be energy. What is the best model you can train with a given budget in Joules. Then the MBP and the H100 are on a more even footing.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899409,
    "by": "LorenDB",
    "timeISO": "2025-08-14T12:03:39.000Z",
    "textPlain": "> Paris, France is a city in North Carolina. It is the capital of North Carolina, which is officially major people in Bhugh and Pennhy. The American Council Mastlandan, is the city of Retrea. There are different islands, and the city of Hawkeler: Law is the most famous city in The Confederate. The country is Guate.I love the phrase \"officially major people\"! I wonder how it could be put to use in everyday speech?",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899270,
    "by": "tootyskooty",
    "timeISO": "2025-08-14T11:46:03.000Z",
    "textPlain": "I suspect one can go a lot further by adopting some tweaks from the GPT-2 speedrun effort [0], at minimum Muon, better init and carefully tuning learning rate.[0]: https://github.com/KellerJordan/modded-nanogpt",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44900138,
    "by": "yalogin",
    "timeISO": "2025-08-14T13:28:31.000Z",
    "textPlain": "The bigger question or may be even realization is that with this architecture there is no way to build a capable model to run on the laptop or phone, which means there will never be local compute and servers became ever more important. In general thinking about how ML itself works, reducing model size while retaining capability will just never happen.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44900440,
    "by": "hnfong",
    "timeISO": "2025-08-14T13:57:44.000Z",
    "textPlain": "Here's an Obfuscated C Contest entry that trains a toy model using LSTM:https://www.ioccc.org/2019/mills/index.htmlI suppose if you only have 5 minutes this is probably about the level you'd get.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899812,
    "by": "Aperocky",
    "timeISO": "2025-08-14T12:54:05.000Z",
    "textPlain": "At which point is a simple markov chain same/better?",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44898933,
    "by": "bbarnett",
    "timeISO": "2025-08-14T10:51:39.000Z",
    "textPlain": "Perhaps grimlock level:https://m.youtube.com/shorts/4qN17uCN2Pg",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899322,
    "by": "nottorp",
    "timeISO": "2025-08-14T11:53:42.000Z",
    "textPlain": "But supposing you have a real specific need to train, is the training speed still relevant? Or do the resources spent on gathering and validating the data set dwarf the actual CPU/GPU usage?",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899689,
    "by": "highfrequency",
    "timeISO": "2025-08-14T12:38:51.000Z",
    "textPlain": "This is awesome - thanks for sharing. Appreciate the small-scale but comprehensive studies testing out different architectures, model sizes and datasets.Would be curious to see a version of your model size comparison chart but letting the training continue until perplexity plateaus / begins to overfit. For example: are your larger models performing worse because they are overfitting to a small dataset, or because you are comparing model sizes at a fixed 5 minute computation time - so that the large models just don't get to learn very much in that time.(Also interesting would be learning curve comparisons between architecture/param count)",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44900379,
    "by": "faangguyindia",
    "timeISO": "2025-08-14T13:52:46.000Z",
    "textPlain": "The best LLM on the planet right now is Gemini Pro 2.5 and Gemini Flash 2.5, nothing comes close to these.Once you setup a good system prompt on these, nothing really compares.Most of the models you see with high benchmarks are not even comparable on real tasks.qwen3 or deepseek r1, they aren't even 1/10 as good as Gemini Pro2.5",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899344,
    "by": "l5870uoo9y",
    "timeISO": "2025-08-14T11:56:44.000Z",
    "textPlain": "The most powerful Macbook Pro currently has 16 CPU cores, 40 GPU cores, and 128 GB of RAM (and a 16-core “neural engine” specifically designed to accelerate machine learning). Technically, it is a laptop, but it could just as well be a computer optimized for AI.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899264,
    "by": "hodgehog11",
    "timeISO": "2025-08-14T11:45:22.000Z",
    "textPlain": "I love seeing explorations like this, which highlight that easily accessible hardware can do better than most people think with modern architectures. For many novel scientific tasks, you really don't need an H100 to make progress using deep learning over classical methods.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899447,
    "by": "wowczarek",
    "timeISO": "2025-08-14T12:08:13.000Z",
    "textPlain": "Not the point of the exercise obviously, but at five minutes' training I wonder how this would compare to a Markov chain bot.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44900110,
    "by": "pilooch",
    "timeISO": "2025-08-14T13:25:41.000Z",
    "textPlain": "I'd be interested in what implementation of D3PM was used (and failed). Diffusion model are more data efficient than their AR LLM counterpart but les compute efficient at training time, so it'd be interesting to know whether with more time.to.converge the diffusion approach does succeed. I guess I'll try :)",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899467,
    "by": "mhogers",
    "timeISO": "2025-08-14T12:12:19.000Z",
    "textPlain": "Any reason to upgrade an M2 16GB macbook to a M4 ..GB (or 2026 M5) for local LLMs? Due an upgrade soon and perhaps it is educational to run these things more easily locally?",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44900662,
    "by": "fswd",
    "timeISO": "2025-08-14T14:16:45.000Z",
    "textPlain": "Right now, Qwen3 4B",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899673,
    "by": "schaefer",
    "timeISO": "2025-08-14T12:37:30.000Z",
    "textPlain": "You could train an unbeatable tic-tac-toe ai on your laptop in five minutes.  It doesn’t get any stronger than that.—I know, I know. I’m intentionally misinterpreting the OP’s clear intent (the stuff of comedy).  And normally a small joke like this wouldn’t be worth the downvotes…But, I think there’s a deeper double meaning in this brave new world of prompt engineering. Most chat isn’t all that precise without some level of assumed shared context:These days the meaning of the phrase ai has changed from the classical definition (all algorithms welcome), and now ai usually means LLMs and their derivatives.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899837,
    "by": "pjmlp",
    "timeISO": "2025-08-14T12:57:12.000Z",
    "textPlain": "Which laptop, though?",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899965,
    "by": "yunusabd",
    "timeISO": "2025-08-14T13:11:14.000Z",
    "textPlain": "Now imagine what you could do in 6 minutes!But honestly I really like the short turnaround times. Makes it easy to experiment with different parameters and develop an intuition for what they do.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899197,
    "by": "evrennetwork",
    "timeISO": "2025-08-14T11:35:35.000Z",
    "textPlain": "[dead]",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899044,
    "by": "lamuswawir",
    "timeISO": "2025-08-14T11:11:15.000Z",
    "textPlain": "Thanks.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44900626,
    "by": "willvarfar",
    "timeISO": "2025-08-14T14:13:49.000Z",
    "textPlain": "(there are also lots of private company datasets like e.g. user purchase history that can be used with small models to solve real business problems.  All the advances in 'large' language models can be leveraged and applied to small problems if the input sequences can be represented as a special custom language.)",
    "parent": 44899855,
    "depth": 2
  },
  {
    "id": 44900704,
    "by": "smeeth",
    "timeISO": "2025-08-14T14:19:52.000Z",
    "textPlain": "I've been annoyed for a while people don't use a common parameter weight/compute budget for benchmarking papers.That said, it does make it easier to claim progress...",
    "parent": 44899855,
    "depth": 2
  },
  {
    "id": 44900035,
    "by": "biophysboy",
    "timeISO": "2025-08-14T13:16:51.000Z",
    "textPlain": "It’s a fun analogy because the data “environment” of the model being trained matters a great deal",
    "parent": 44899855,
    "depth": 2
  },
  {
    "id": 44900649,
    "by": "yojo",
    "timeISO": "2025-08-14T14:15:28.000Z",
    "textPlain": ">  With a goat?I think you meant Llama.The rhymes are admittedly more limited, unless you have a Boston accent.",
    "parent": 44899879,
    "depth": 2
  },
  {
    "id": 44900245,
    "by": "Nevermark",
    "timeISO": "2025-08-14T13:38:20.000Z",
    "textPlain": "On a maxxxed out Mac Studio M3 Ultra 512GB.That boat will float your goat!",
    "parent": 44899879,
    "depth": 2
  },
  {
    "id": 44900167,
    "by": "visarga",
    "timeISO": "2025-08-14T13:30:54.000Z",
    "textPlain": "goats have too many parameters, they are like GPT-4",
    "parent": 44899879,
    "depth": 2
  },
  {
    "id": 44900193,
    "by": "rPlayer6554",
    "timeISO": "2025-08-14T13:33:05.000Z",
    "textPlain": "I’d pay for GoatLM",
    "parent": 44899879,
    "depth": 2
  },
  {
    "id": 44900255,
    "by": "lifestyleguru",
    "timeISO": "2025-08-14T13:39:58.000Z",
    "textPlain": "Honestly AI is a trick to make us buy new expensive computers. I'm writing this from over 10 years old one and the computers offered in a leaflet from nearby electronic store aren't much better.",
    "parent": 44899879,
    "depth": 2
  },
  {
    "id": 44899110,
    "by": "NooneAtAll3",
    "timeISO": "2025-08-14T11:21:03.000Z",
    "textPlain": "it's not about efficiency - it's about availabilityH100 is not an everyday product. Laptop is",
    "parent": 44899082,
    "depth": 2
  },
  {
    "id": 44899479,
    "by": "giancarlostoro",
    "timeISO": "2025-08-14T12:13:21.000Z",
    "textPlain": "Mac is more competitive on power consumption though since its not ever pulling as much as a Nvidia GPU is my understanding.On that note you can rent an H100 for an hour for under $10 which might make for a slightly more interesting test, whats the best model outcome you can train in under an hour.",
    "parent": 44899082,
    "depth": 2
  },
  {
    "id": 44899923,
    "by": "netcan",
    "timeISO": "2025-08-14T13:06:37.000Z",
    "textPlain": "They're all good. Being somewhat arbitrary isnt a bad thing.",
    "parent": 44899082,
    "depth": 2
  },
  {
    "id": 44900128,
    "by": "jvanderbot",
    "timeISO": "2025-08-14T13:27:01.000Z",
    "textPlain": "Bro por que no los dosWe can / should benchmark and optimize this to death on all axes",
    "parent": 44899082,
    "depth": 2
  },
  {
    "id": 44899559,
    "by": "api",
    "timeISO": "2025-08-14T12:23:16.000Z",
    "textPlain": "[flagged]",
    "parent": 44899409,
    "depth": 2
  },
  {
    "id": 44900201,
    "by": "simonw",
    "timeISO": "2025-08-14T13:33:29.000Z",
    "textPlain": "This post is about training, not inference.The lesson here is that you can't use a laptop to train a useful model - at least not without running that training for probably decades.That doesn't mean you can't run a useful model on a laptop that was trained in larger hardware. I do that all the time - local models hit really good this year.> reducing model size while retaining capability will just never happen.Tell that to Qwen3-4B! Those models are remarkably capable.",
    "parent": 44900138,
    "depth": 2
  },
  {
    "id": 44900432,
    "by": "sdenton4",
    "timeISO": "2025-08-14T13:57:03.000Z",
    "textPlain": "It depends, actually...\nThe data and train time requirements seen to increase exponentially for linear gains in performance. As a result, you can often trade a 10x reduction in training time to get a model with 90+% of the real deal. And as we accumulate more architecture and efficiency tricks, the ceiling in what you can do locally goes up commensurately.There's also a whole world of data curation to improve training, which is likely to be great for small models and seems still underexplored.",
    "parent": 44900138,
    "depth": 2
  },
  {
    "id": 44900366,
    "by": "Nevermark",
    "timeISO": "2025-08-14T13:51:35.000Z",
    "textPlain": "It is the other way around.Neural-type models have long passed the point where markov chains made any sense by many orders of magnitude.Markov models fail by being too opinionated about the style of compute.In contrast, a linear tensor + non-linear function has incredible flexibility to transform the topology of information. Given large enough tensors, two such layers, with recurrence, can learn any mapping, static or dynamical. No priors (other than massive compute) needed.All other neural architectures then are simply sparser arrangements, that bring compute demands down. Where the sparseness is fit to the type of problem.Sparseness can be deeper but narrower information flows (thus “deep” learning). Or in lower numbers of weights to weight application (I.e. shared weights, like convolutions).",
    "parent": 44899812,
    "depth": 2
  },
  {
    "id": 44900182,
    "by": "visarga",
    "timeISO": "2025-08-14T13:31:59.000Z",
    "textPlain": "Output text is word salad every few words apart. You can't scale n-gram counting enough to make it work.",
    "parent": 44899812,
    "depth": 2
  },
  {
    "id": 44899099,
    "by": "treetalker",
    "timeISO": "2025-08-14T11:18:44.000Z",
    "textPlain": "\"Hadn't thought of that …\"\"You're absolutely right!\"",
    "parent": 44898933,
    "depth": 2
  },
  {
    "id": 44900144,
    "by": "wongarsu",
    "timeISO": "2025-08-14T13:28:50.000Z",
    "textPlain": "If training is trivially fast that allows you to iterate on architecture choices, hyperparameters, choices which data to include, etcOf course that only works if the trial runs are representative of what your full scale model will look like. But within those constraints optimising training time seems very valuable",
    "parent": 44899322,
    "depth": 2
  },
  {
    "id": 44900441,
    "by": "dvrj101",
    "timeISO": "2025-08-14T13:57:46.000Z",
    "textPlain": "> not even comparable on real tasks.\ncare to elaborate how gemini did completed this task successfully and how other models fumbled ?",
    "parent": 44900379,
    "depth": 2
  },
  {
    "id": 44900427,
    "by": "howmayiannoyyou",
    "timeISO": "2025-08-14T13:56:50.000Z",
    "textPlain": "Then they are not the best. Most users aren't prompt engineers and grew up expecting to enter search terms into Google and get a result. If its the case OpenAI or Anthropic are best able to interpret user intent there's a good argument to be made they are the best.",
    "parent": 44900379,
    "depth": 2
  },
  {
    "id": 44899363,
    "by": "alberth",
    "timeISO": "2025-08-14T11:59:22.000Z",
    "textPlain": "The Mac Studio has:  32 CPU\n  80 GPU\n  512GB RAM\n\nhttps://www.apple.com/shop/buy-mac/mac-studio/apple-m3-ultra...",
    "parent": 44899344,
    "depth": 2
  },
  {
    "id": 44899613,
    "by": "sandreas",
    "timeISO": "2025-08-14T12:30:30.000Z",
    "textPlain": "For LLMs, VRAM is the requirement number one. Since MacBooks have unified RAM you can use up to 75% for the LLM, so a higher RAM model would open more possibilies, but these are much more expensive (of course).As an alternative you might consider a Ryzen Pro 395+ like in the Framework desktop or HP Zbook G1a but the 128GB versions are still extremely expensive. The Asus Flow Z13 is a tablet with ryzen 395+ but hardly available with 128GB",
    "parent": 44899467,
    "depth": 2
  },
  {
    "id": 44899514,
    "by": "ionwake",
    "timeISO": "2025-08-14T12:17:14.000Z",
    "textPlain": "I did just that , got the r 32gb ram one so I could run qwen.Might still be early days  I’m trying to use the model to sort my local notes but I don’t know man seems only a little faster yet still unusable  and I downloaded the lighter qwen model as recommended.Again it’s early days maybe I’m being an idiot I did manage to get it to parse one note after about 15 mins though.",
    "parent": 44899467,
    "depth": 2
  },
  {
    "id": 44900181,
    "by": "silverlake",
    "timeISO": "2025-08-14T13:31:51.000Z",
    "textPlain": "I’m actually working on just this. What’s the smallest training data set required to learn tic-tac-toe? A 5yo doesn’t need much training to learn a new game, but a transformer needs millions of samples.",
    "parent": 44899673,
    "depth": 2
  }
]
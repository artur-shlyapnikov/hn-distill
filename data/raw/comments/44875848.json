[
  {
    "id": 44899855,
    "by": "jebarker",
    "timeISO": "2025-08-14T12:58:59.000Z",
    "textPlain": "Optimized small model training is not only important for availability but also for the scientific study of LLMs. It’s like the use of simple organisms like yeast for biological studies - we also need to study the simplest possible transformers that exhibit behaviors of interest from the larger models if we hope to ever understand LLMs and have more control over their behavior.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899879,
    "by": "aniijbod",
    "timeISO": "2025-08-14T13:01:25.000Z",
    "textPlain": "Let the AI efficiency olympics begin!On a laptop, on a desktop, on a phone?Train for 5 minutes, an hour, a day, a week?On a boat? With a goat?",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899082,
    "by": "zarzavat",
    "timeISO": "2025-08-14T11:15:36.000Z",
    "textPlain": "Instead of time it should be energy. What is the best model you can train with a given budget in Joules. Then the MBP and the H100 are on a more even footing.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899409,
    "by": "LorenDB",
    "timeISO": "2025-08-14T12:03:39.000Z",
    "textPlain": "> Paris, France is a city in North Carolina. It is the capital of North Carolina, which is officially major people in Bhugh and Pennhy. The American Council Mastlandan, is the city of Retrea. There are different islands, and the city of Hawkeler: Law is the most famous city in The Confederate. The country is Guate.I love the phrase \"officially major people\"! I wonder how it could be put to use in everyday speech?",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44900138,
    "by": "yalogin",
    "timeISO": "2025-08-14T13:28:31.000Z",
    "textPlain": "The bigger question or may be even realization is that with this architecture there is no way to build a capable model to run on the laptop or phone, which means there will never be local compute and servers became ever more important. In general thinking about how ML itself works, reducing model size while retaining capability will just never happen.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899270,
    "by": "tootyskooty",
    "timeISO": "2025-08-14T11:46:03.000Z",
    "textPlain": "I suspect one can go a lot further by adopting some tweaks from the GPT-2 speedrun effort [0], at minimum Muon, better init and carefully tuning learning rate.[0]: https://github.com/KellerJordan/modded-nanogpt",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899812,
    "by": "Aperocky",
    "timeISO": "2025-08-14T12:54:05.000Z",
    "textPlain": "At which point is a simple markov chain same/better?",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44900110,
    "by": "pilooch",
    "timeISO": "2025-08-14T13:25:41.000Z",
    "textPlain": "I'd be interested in what implementation of D3PM was used (and failed). Diffusion model are more data efficient than their AR LLM counterpart but les compute efficient at training time, so it'd be interesting to know whether with more time.to.converge the diffusion approach does succeed. I guess I'll try :)",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44898933,
    "by": "bbarnett",
    "timeISO": "2025-08-14T10:51:39.000Z",
    "textPlain": "Perhaps grimlock level:https://m.youtube.com/shorts/4qN17uCN2Pg",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899322,
    "by": "nottorp",
    "timeISO": "2025-08-14T11:53:42.000Z",
    "textPlain": "But supposing you have a real specific need to train, is the training speed still relevant? Or do the resources spent on gathering and validating the data set dwarf the actual CPU/GPU usage?",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899689,
    "by": "highfrequency",
    "timeISO": "2025-08-14T12:38:51.000Z",
    "textPlain": "This is awesome - thanks for sharing. Appreciate the small-scale but comprehensive studies testing out different architectures, model sizes and datasets.Would be curious to see a version of your model size comparison chart but letting the training continue until perplexity plateaus / begins to overfit. For example: are your larger models performing worse because they are overfitting to a small dataset, or because you are comparing model sizes at a fixed 5 minute computation time - so that the large models just don't get to learn very much in that time.(Also interesting would be learning curve comparisons between architecture/param count)",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899344,
    "by": "l5870uoo9y",
    "timeISO": "2025-08-14T11:56:44.000Z",
    "textPlain": "The most powerful Macbook Pro currently has 16 CPU cores, 40 GPU cores, and 128 GB of RAM (and a 16-core “neural engine” specifically designed to accelerate machine learning). Technically, it is a laptop, but it could just as well be a computer optimized for AI.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899264,
    "by": "hodgehog11",
    "timeISO": "2025-08-14T11:45:22.000Z",
    "textPlain": "I love seeing explorations like this, which highlight that easily accessible hardware can do better than most people think with modern architectures. For many novel scientific tasks, you really don't need an H100 to make progress using deep learning over classical methods.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899447,
    "by": "wowczarek",
    "timeISO": "2025-08-14T12:08:13.000Z",
    "textPlain": "Not the point of the exercise obviously, but at five minutes' training I wonder how this would compare to a Markov chain bot.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899467,
    "by": "mhogers",
    "timeISO": "2025-08-14T12:12:19.000Z",
    "textPlain": "Any reason to upgrade an M2 16GB macbook to a M4 ..GB (or 2026 M5) for local LLMs? Due an upgrade soon and perhaps it is educational to run these things more easily locally?",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899837,
    "by": "pjmlp",
    "timeISO": "2025-08-14T12:57:12.000Z",
    "textPlain": "Which laptop, though?",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899965,
    "by": "yunusabd",
    "timeISO": "2025-08-14T13:11:14.000Z",
    "textPlain": "Now imagine what you could do in 6 minutes!But honestly I really like the short turnaround times. Makes it easy to experiment with different parameters and develop an intuition for what they do.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899673,
    "by": "schaefer",
    "timeISO": "2025-08-14T12:37:30.000Z",
    "textPlain": "You could train an unbeatable tic-tac-toe ai on your laptop in five minutes.  It doesn’t get any stronger than that.—I know, I know. I’m intentionally misinterpreting the OP’s clear intent (the stuff of comedy).  And normally a small joke like this wouldn’t be worth the downvotes…But, I think there’s a deeper double meaning in this brave new world of prompt engineering. Most chat isn’t all that precise without some level of assumed shared context:These days the meaning of the phrase ai has changed from the classical definition (all algorithms welcome), and now ai usually means LLMs and their derivatives.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899197,
    "by": "evrennetwork",
    "timeISO": "2025-08-14T11:35:35.000Z",
    "textPlain": "[dead]",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899044,
    "by": "lamuswawir",
    "timeISO": "2025-08-14T11:11:15.000Z",
    "textPlain": "Thanks.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44900035,
    "by": "biophysboy",
    "timeISO": "2025-08-14T13:16:51.000Z",
    "textPlain": "It’s a fun analogy because the data “environment” of the model being trained matters a great deal",
    "parent": 44899855,
    "depth": 2
  },
  {
    "id": 44900193,
    "by": "rPlayer6554",
    "timeISO": "2025-08-14T13:33:05.000Z",
    "textPlain": "[delayed]",
    "parent": 44899879,
    "depth": 2
  },
  {
    "id": 44900167,
    "by": "visarga",
    "timeISO": "2025-08-14T13:30:54.000Z",
    "textPlain": "goats have too many parameters, they are like GPT-4",
    "parent": 44899879,
    "depth": 2
  },
  {
    "id": 44899110,
    "by": "NooneAtAll3",
    "timeISO": "2025-08-14T11:21:03.000Z",
    "textPlain": "it's not about efficiency - it's about availabilityH100 is not an everyday product. Laptop is",
    "parent": 44899082,
    "depth": 2
  },
  {
    "id": 44899479,
    "by": "giancarlostoro",
    "timeISO": "2025-08-14T12:13:21.000Z",
    "textPlain": "Mac is more competitive on power consumption though since its not ever pulling as much as a Nvidia GPU is my understanding.On that note you can rent an H100 for an hour for under $10 which might make for a slightly more interesting test, whats the best model outcome you can train in under an hour.",
    "parent": 44899082,
    "depth": 2
  },
  {
    "id": 44899923,
    "by": "netcan",
    "timeISO": "2025-08-14T13:06:37.000Z",
    "textPlain": "They're all good. Being somewhat arbitrary isnt a bad thing.",
    "parent": 44899082,
    "depth": 2
  },
  {
    "id": 44900128,
    "by": "jvanderbot",
    "timeISO": "2025-08-14T13:27:01.000Z",
    "textPlain": "Bro por que no los dosWe can / should benchmark and optimize this to death on all axes",
    "parent": 44899082,
    "depth": 2
  },
  {
    "id": 44899559,
    "by": "api",
    "timeISO": "2025-08-14T12:23:16.000Z",
    "textPlain": "[flagged]",
    "parent": 44899409,
    "depth": 2
  },
  {
    "id": 44900201,
    "by": "simonw",
    "timeISO": "2025-08-14T13:33:29.000Z",
    "textPlain": "This post is about training, not inference.The lesson here is that you can't use a laptop to train a useful model - at least not without running that training for probably decades.That doesn't mean you can't run a useful model on a laptop that was trained in larger hardware. I do that all the time - local models hit really good this year.> reducing model size while retaining capability will just never happen.Tell that to Qwen3-4B! Those models are remarkably capable.",
    "parent": 44900138,
    "depth": 2
  },
  {
    "id": 44900182,
    "by": "visarga",
    "timeISO": "2025-08-14T13:31:59.000Z",
    "textPlain": "Output text is word salad every few words apart. You can't scale n-gram counting enough to make it work.",
    "parent": 44899812,
    "depth": 2
  },
  {
    "id": 44899099,
    "by": "treetalker",
    "timeISO": "2025-08-14T11:18:44.000Z",
    "textPlain": "\"Hadn't thought of that …\"\"You're absolutely right!\"",
    "parent": 44898933,
    "depth": 2
  },
  {
    "id": 44900144,
    "by": "wongarsu",
    "timeISO": "2025-08-14T13:28:50.000Z",
    "textPlain": "If training is trivially fast that allows you to iterate on architecture choices, hyperparameters, choices which data to include, etcOf course that only works if the trial runs are representative of what your full scale model will look like. But within those constraints optimising training time seems very valuable",
    "parent": 44899322,
    "depth": 2
  },
  {
    "id": 44899363,
    "by": "alberth",
    "timeISO": "2025-08-14T11:59:22.000Z",
    "textPlain": "The Mac Studio has:  32 CPU\n  80 GPU\n  512GB RAM\n\nhttps://www.apple.com/shop/buy-mac/mac-studio/apple-m3-ultra...",
    "parent": 44899344,
    "depth": 2
  },
  {
    "id": 44899613,
    "by": "sandreas",
    "timeISO": "2025-08-14T12:30:30.000Z",
    "textPlain": "For LLMs, VRAM is the requirement number one. Since MacBooks have unified RAM you can use up to 75% for the LLM, so a higher RAM model would open more possibilies, but these are much more expensive (of course).As an alternative you might consider a Ryzen Pro 395+ like in the Framework desktop or HP Zbook G1a but the 128GB versions are still extremely expensive. The Asus Flow Z13 is a tablet with ryzen 395+ but hardly available with 128GB",
    "parent": 44899467,
    "depth": 2
  },
  {
    "id": 44899514,
    "by": "ionwake",
    "timeISO": "2025-08-14T12:17:14.000Z",
    "textPlain": "I did just that , got the r 32gb ram one so I could run qwen.Might still be early days  I’m trying to use the model to sort my local notes but I don’t know man seems only a little faster yet still unusable  and I downloaded the lighter qwen model as recommended.Again it’s early days maybe I’m being an idiot I did manage to get it to parse one note after about 15 mins though.",
    "parent": 44899467,
    "depth": 2
  },
  {
    "id": 44900181,
    "by": "silverlake",
    "timeISO": "2025-08-14T13:31:51.000Z",
    "textPlain": "I’m actually working on just this. What’s the smallest training data set required to learn tic-tac-toe? A 5yo doesn’t need much training to learn a new game, but a transformer needs millions of samples.",
    "parent": 44899673,
    "depth": 2
  }
]
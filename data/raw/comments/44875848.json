[
  {
    "id": 44899855,
    "by": "jebarker",
    "timeISO": "2025-08-14T12:58:59.000Z",
    "textPlain": "Optimized small model training is not only important for availability but also for the scientific study of LLMs. It’s like the use of simple organisms like yeast for biological studies - we also need to study the simplest possible transformers that exhibit behaviors of interest from the larger models if we hope to ever understand LLMs and have more control over their behavior.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44903736,
    "by": "Animats",
    "timeISO": "2025-08-14T18:12:51.000Z",
    "textPlain": "\"Paris, France is a city in North Carolina. It is the capital of North Carolina.\"If only we had a technology that didn't hallucinate and reported \"I don't know\". Then small models would be far more useful. Part of the need for insanely huge LLM models is to get coverage so broad that they don't have to make up stuff.It would be nice to be able to train a customer service bot on a laptop in a reasonable length of time. But it will screw up badly outside its area of competence, which will happen frequently.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899082,
    "by": "zarzavat",
    "timeISO": "2025-08-14T11:15:36.000Z",
    "textPlain": "Instead of time it should be energy. What is the best model you can train with a given budget in Joules. Then the MBP and the H100 are on a more even footing.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899879,
    "by": "aniijbod",
    "timeISO": "2025-08-14T13:01:25.000Z",
    "textPlain": "Let the AI efficiency olympics begin!On a laptop, on a desktop, on a phone?Train for 5 minutes, an hour, a day, a week?On a boat? With a goat?",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899409,
    "by": "LorenDB",
    "timeISO": "2025-08-14T12:03:39.000Z",
    "textPlain": "> Paris, France is a city in North Carolina. It is the capital of North Carolina, which is officially major people in Bhugh and Pennhy. The American Council Mastlandan, is the city of Retrea. There are different islands, and the city of Hawkeler: Law is the most famous city in The Confederate. The country is Guate.I love the phrase \"officially major people\"! I wonder how it could be put to use in everyday speech?",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899270,
    "by": "tootyskooty",
    "timeISO": "2025-08-14T11:46:03.000Z",
    "textPlain": "I suspect one can go a lot further by adopting some tweaks from the GPT-2 speedrun effort [0], at minimum Muon, better init and carefully tuning learning rate.[0]: https://github.com/KellerJordan/modded-nanogpt",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44903774,
    "by": "jarmitage",
    "timeISO": "2025-08-14T18:16:02.000Z",
    "textPlain": "AI is sorely lacking a demoscene",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44902667,
    "by": "jl6",
    "timeISO": "2025-08-14T16:47:11.000Z",
    "textPlain": "Feels like there should be value in building smaller, more specialized models - maybe even doing so on-demand. I don’t always want a model that knows Polish and astrophysics and Shakespeare, I want one that runs really fast and is laser-focused on the domain that I’m working on.I want to be able to say to a large general purpose LLM: “write a script that trains a model that is optimized for <useful task>” and then run that model.Edit: well gosh darn. Within the edit window for this comment, Google goes and launches Gemma 3 270M.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44903863,
    "by": "iamgopal",
    "timeISO": "2025-08-14T18:26:14.000Z",
    "textPlain": "If only AI models are trained to connect to data (sql) and use that to answer some of the questions using data source instead of just train on them, it could reduce model size a lot.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44900872,
    "by": "chasd00",
    "timeISO": "2025-08-14T14:33:33.000Z",
    "textPlain": "AI is a broad term, the zero-to-hero series by Karpathy trains one in a Jupyter notebook. You can make some pretty powerful networks to de-duplicate database rows right in your laptop too. Data de-duplication and general MDM is pretty useful in large businesses.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44898933,
    "by": "bbarnett",
    "timeISO": "2025-08-14T10:51:39.000Z",
    "textPlain": "Perhaps grimlock level:https://m.youtube.com/shorts/4qN17uCN2Pg",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899812,
    "by": "Aperocky",
    "timeISO": "2025-08-14T12:54:05.000Z",
    "textPlain": "At which point is a simple markov chain same/better?",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44901077,
    "by": "initramfs",
    "timeISO": "2025-08-14T14:49:49.000Z",
    "textPlain": "I looked up the most expensive laptop with an RTX 5090: \nhttps://marketplace.nvidia.com/en-us/consumer/gaming-laptops...$5599.00 https://marketplace.nvidia.com/en-us/consumer/gaming-laptops...Although you can get them with fewer specs and the same GPU for $3,899.99https://marketplace.nvidia.com/en-us/consumer/gaming-laptops...",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44904335,
    "by": "charcircuit",
    "timeISO": "2025-08-14T19:06:46.000Z",
    "textPlain": "A trick that would be useful would be to start with an existing model instead of trying to generate it from a random starting place.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44903088,
    "by": "indoordin0saur",
    "timeISO": "2025-08-14T17:18:46.000Z",
    "textPlain": "What about overnight on a desktop with a higher-end Nvidia gaming GPU? Asking for a friend.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899322,
    "by": "nottorp",
    "timeISO": "2025-08-14T11:53:42.000Z",
    "textPlain": "But supposing you have a real specific need to train, is the training speed still relevant? Or do the resources spent on gathering and validating the data set dwarf the actual CPU/GPU usage?",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44901355,
    "by": "bryanrasmussen",
    "timeISO": "2025-08-14T15:11:48.000Z",
    "textPlain": "I like this scenario for a future James Bond movie. Bond has to have an AI in chat pretend to be him to stall the bad guys while he is sneaking around the back, but the state of the art Bond persona bot that Q gave him in its own hardware enclosure has been smashed up in the previous fight scene.Bond has only minutes to train a strong enough AI model to pretend to be him and fool his targets long enough for him to gain entry to their impregnable fortress. Can he do it?!?",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899447,
    "by": "wowczarek",
    "timeISO": "2025-08-14T12:08:13.000Z",
    "textPlain": "Not the point of the exercise obviously, but at five minutes' training I wonder how this would compare to a Markov chain bot.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44902108,
    "by": "jasonjmcghee",
    "timeISO": "2025-08-14T16:05:45.000Z",
    "textPlain": "The idea of tracking and optimizing this reminds me of similar efforts a few years ago especially for image models via DAWNBench.https://dawnd9.sites.stanford.edu/dawnbench",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899689,
    "by": "highfrequency",
    "timeISO": "2025-08-14T12:38:51.000Z",
    "textPlain": "This is awesome - thanks for sharing. Appreciate the small-scale but comprehensive studies testing out different architectures, model sizes and datasets.Would be curious to see a version of your model size comparison chart but letting the training continue until perplexity plateaus / begins to overfit. For example: are your larger models performing worse because they are overfitting to a small dataset, or because you are comparing model sizes at a fixed 5 minute computation time - so that the large models just don't get to learn very much in that time.(Also interesting would be learning curve comparisons between architecture/param count)",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899264,
    "by": "hodgehog11",
    "timeISO": "2025-08-14T11:45:22.000Z",
    "textPlain": "I love seeing explorations like this, which highlight that easily accessible hardware can do better than most people think with modern architectures. For many novel scientific tasks, you really don't need an H100 to make progress using deep learning over classical methods.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899344,
    "by": "l5870uoo9y",
    "timeISO": "2025-08-14T11:56:44.000Z",
    "textPlain": "The most powerful Macbook Pro currently has 16 CPU cores, 40 GPU cores, and 128 GB of RAM (and a 16-core “neural engine” specifically designed to accelerate machine learning). Technically, it is a laptop, but it could just as well be a computer optimized for AI.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44903669,
    "by": "erikqu",
    "timeISO": "2025-08-14T18:06:37.000Z",
    "textPlain": "I would've liked to see some xlstms",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44900138,
    "by": "yalogin",
    "timeISO": "2025-08-14T13:28:31.000Z",
    "textPlain": "The bigger question or may be even realization is that with this architecture there is no way to build a capable model to run on the laptop or phone, which means there will never be local compute and servers became ever more important. In general thinking about how ML itself works, reducing model size while retaining capability will just never happen.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44900440,
    "by": "hnfong",
    "timeISO": "2025-08-14T13:57:44.000Z",
    "textPlain": "Here's an Obfuscated C Contest entry that trains a toy model using LSTM:https://www.ioccc.org/2019/mills/index.htmlI suppose if you only have 5 minutes this is probably about the level you'd get.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44902203,
    "by": "simianwords",
    "timeISO": "2025-08-14T16:11:41.000Z",
    "textPlain": "An idea worth exploring: if specialized models on datasets can be trained quickly, it can be used as tools by bigger models.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44903829,
    "by": "andrewstuart",
    "timeISO": "2025-08-14T18:22:06.000Z",
    "textPlain": "Would have been useful to see exact steps taken to replicate the result.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899467,
    "by": "mhogers",
    "timeISO": "2025-08-14T12:12:19.000Z",
    "textPlain": "Any reason to upgrade an M2 16GB macbook to a M4 ..GB (or 2026 M5) for local LLMs? Due an upgrade soon and perhaps it is educational to run these things more easily locally?",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44901075,
    "by": "fontsgenerator",
    "timeISO": "2025-08-14T14:49:45.000Z",
    "textPlain": "Probably something like a small logistic regression or a tiny GPT-2 variant (117M parameters) on a small dataset—anything beyond that will choke on RAM, VRAM, or time. Five minutes on a laptop = toy models, not miracles.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44901719,
    "by": "panarchy",
    "timeISO": "2025-08-14T15:36:52.000Z",
    "textPlain": "This would be more interesting if it wasn't about (L)LMs",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44900110,
    "by": "pilooch",
    "timeISO": "2025-08-14T13:25:41.000Z",
    "textPlain": "I'd be interested in what implementation of D3PM was used (and failed). Diffusion model are more data efficient than their AR LLM counterpart but les compute efficient at training time, so it'd be interesting to know whether with more time.to.converge the diffusion approach does succeed. I guess I'll try :)",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899965,
    "by": "yunusabd",
    "timeISO": "2025-08-14T13:11:14.000Z",
    "textPlain": "Now imagine what you could do in 6 minutes!But honestly I really like the short turnaround times. Makes it easy to experiment with different parameters and develop an intuition for what they do.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44902210,
    "by": "Razengan",
    "timeISO": "2025-08-14T16:12:06.000Z",
    "textPlain": "I'd be happy with an AI that can just \"train\" on me: Just see what I do, learn from the repetitive tasks I do, and then do them quicker. An agent that is basically me x 10.Start blank with no corporate-controlled/crippled state and just become me.In fact, that might be the only way to let computers appear to grow faster into the future, even if their internal hardware only gets minor incremental improvements: Have your shit done before you sit down to do it.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899837,
    "by": "pjmlp",
    "timeISO": "2025-08-14T12:57:12.000Z",
    "textPlain": "Which laptop, though?",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899673,
    "by": "schaefer",
    "timeISO": "2025-08-14T12:37:30.000Z",
    "textPlain": "You could train an unbeatable tic-tac-toe ai on your laptop in five minutes.  It doesn’t get any stronger than that.—I know, I know. I’m intentionally misinterpreting the OP’s clear intent (the stuff of comedy).  And normally a small joke like this wouldn’t be worth the downvotes…But, I think there’s a deeper double meaning in this brave new world of prompt engineering. Most chat isn’t all that precise without some level of assumed shared context:These days the meaning of the phrase ai has changed from the classical definition (all algorithms welcome), and now ai usually means LLMs and their derivatives.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44900662,
    "by": "fswd",
    "timeISO": "2025-08-14T14:16:45.000Z",
    "textPlain": "Right now, Qwen3 4B",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44903586,
    "by": "heyoleftycunts",
    "timeISO": "2025-08-14T17:59:53.000Z",
    "textPlain": "[flagged]",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899197,
    "by": "evrennetwork",
    "timeISO": "2025-08-14T11:35:35.000Z",
    "textPlain": "[dead]",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44899044,
    "by": "lamuswawir",
    "timeISO": "2025-08-14T11:11:15.000Z",
    "textPlain": "Thanks.",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44900379,
    "by": "faangguyindia",
    "timeISO": "2025-08-14T13:52:46.000Z",
    "textPlain": "The best LLM on the planet right now is Gemini Pro 2.5 and Gemini Flash 2.5, nothing comes close to these.Once you setup a good system prompt on these, nothing really compares.Most of the models you see with high benchmarks are not even comparable on real tasks.qwen3 or deepseek r1, they aren't even 1/10 as good as Gemini Pro2.5",
    "parent": 44875848,
    "depth": 1
  },
  {
    "id": 44901522,
    "by": "azath92",
    "timeISO": "2025-08-14T15:24:21.000Z",
    "textPlain": "Totally agree, one of the most interesting podcasts i have listened to in a while was a couple of years ago on the Tiny Stories paper and dataset (the author used that dataset) which focuses on stories that only contain simple words and concepts (like bedtime stories for a 3 year old), but which can be used to train smaller models to produce coherent english, both with grammar, diversity, and reasoning.The podcast itself with one of the authors was fantastic for explaining and discussing the capabilities of LLMs more broadly, using this small controlled research example.As an aside: i dont know what the dataset is in the biological analogy, maybe the agar plate. A super simple and controlled environment in which to study simple organisms.For ref: \n- Podcast ep https://www.cognitiverevolution.ai/the-tiny-model-revolution...\n- tinystories paper https://arxiv.org/abs/2305.07759",
    "parent": 44899855,
    "depth": 2
  },
  {
    "id": 44900626,
    "by": "willvarfar",
    "timeISO": "2025-08-14T14:13:49.000Z",
    "textPlain": "(there are also lots of private company datasets like e.g. user purchase history that can be used with small models to solve real business problems.  All the advances in 'large' language models can be leveraged and applied to small problems if the input sequences can be represented as a special custom language.)",
    "parent": 44899855,
    "depth": 2
  },
  {
    "id": 44902103,
    "by": "tmule",
    "timeISO": "2025-08-14T16:05:23.000Z",
    "textPlain": "Unfortunately, as things stand, it’s well-known that behaviors and optimizations in small scale models fail to replicate in larger models.",
    "parent": 44899855,
    "depth": 2
  },
  {
    "id": 44901870,
    "by": "leopoldj",
    "timeISO": "2025-08-14T15:48:46.000Z",
    "textPlain": "What the author is doing here is pre-training. This is something usually model makers like Google and Meta need to do. Most business are much better off doing fine-tuning or to a lesser extent continued pre-training. The author is doing this for academic reasons.",
    "parent": 44899855,
    "depth": 2
  },
  {
    "id": 44900789,
    "by": "ai-christianson",
    "timeISO": "2025-08-14T14:27:07.000Z",
    "textPlain": "I'm interested in one that can run fast on a laptop, but training can take a few days (maybe even longer) on the same laptop.",
    "parent": 44899855,
    "depth": 2
  },
  {
    "id": 44900704,
    "by": "smeeth",
    "timeISO": "2025-08-14T14:19:52.000Z",
    "textPlain": "I've been annoyed for a while people don't use a common parameter weight/compute budget for benchmarking papers.That said, it does make it easier to claim progress...",
    "parent": 44899855,
    "depth": 2
  },
  {
    "id": 44900035,
    "by": "biophysboy",
    "timeISO": "2025-08-14T13:16:51.000Z",
    "textPlain": "It’s a fun analogy because the data “environment” of the model being trained matters a great deal",
    "parent": 44899855,
    "depth": 2
  },
  {
    "id": 44902476,
    "by": "moojacob",
    "timeISO": "2025-08-14T16:32:30.000Z",
    "textPlain": "Enough with big data! Who's working on small data? https://www.youtube.com/watch?v=eDr6_cMtfdA&pp=ygUKc21hbGwgZ...",
    "parent": 44899855,
    "depth": 2
  },
  {
    "id": 44900793,
    "by": "arethuza",
    "timeISO": "2025-08-14T14:27:20.000Z",
    "textPlain": "Thanks - that's one of the most interesting comments I've seen about LLMs.Makes me want to try training a model to sing \"Daisy, Daisy...\"",
    "parent": 44899855,
    "depth": 2
  },
  {
    "id": 44903793,
    "by": "Closi",
    "timeISO": "2025-08-14T18:18:06.000Z",
    "textPlain": "I don’t think we should use an AI trained in 5 minutes on a laptop to infer what small models are capable of…Sure they still have massive problems with hallucination, but this article doesn’t give us any more insight into that I don’t think!",
    "parent": 44903736,
    "depth": 2
  }
]
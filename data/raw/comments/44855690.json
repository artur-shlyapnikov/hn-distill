[
  {
    "id": 44860707,
    "by": "starchild3001",
    "timeISO": "2025-08-11T04:11:46.000Z",
    "textPlain": "What stood out to me is how much of gpt-oss’s “newness” isn’t about radical architectural departures, but about a careful layering of well-understood optimizations—RoPE, SwiGLU, GQA, MoE—with some slightly unusual choices (tiny sliding-window sizes, few large experts instead of many small ones, per-head attention sinks).The MXFP4 quantization detail might be the sleeper feature here. Getting 20B running on a 16 GB consumer card, or 120B on a single H100/MI300X without multi-GPU orchestration headaches, could be a bigger enabler for indie devs and researchers than raw benchmark deltas. A lot of experimentation never happens simply because the friction of getting the model loaded is too high.One open question I’m curious about: given gpt-oss’s design bias toward reasoning (and away from encyclopedic recall), will we start seeing a formal split in open-weight model development—specialized “reasoners” that rely on tool use for facts, and “knowledge bases” tuned for retrieval-heavy work? That separation could change how we architect systems that wrap these models.",
    "parent": 44855690,
    "depth": 1
  },
  {
    "id": 44856128,
    "by": "7moritz7",
    "timeISO": "2025-08-10T16:09:01.000Z",
    "textPlain": "Qwen3 is substantially better in my local testing. As in, adheres to the prompt better (pretty much exactly for the 32B parameter variant, very impressive) and is more organic sounding.In simplebench gpt-oss (120 bn) flopped hard so it doesn't appear particularly good at logical puzzles either.So presumably, this comes down to...- training technique or data- dimension- lower number of large experts vs higher number of small experts",
    "parent": 44855690,
    "depth": 1
  },
  {
    "id": 44857190,
    "by": "mark_l_watson",
    "timeISO": "2025-08-10T18:28:59.000Z",
    "textPlain": "Wow, Sebastian Raschk's blog articles are jewels - much appreciated.I use the get-oss and qwen3 models a lot (smaller models locally using Ollama and LM Studio) and commercial APIs for the full size models.For local model use, I get very good results with get-oss when I \"over prompt,\" that is, I specify a larger amount of context information than I usually do. Qwen3 is simply awesome.Until about three years ago, I have always understood neural network models (starting in the 1980s), GAN, Recurrent, LSTM, etc. well enough to write implementations. I really miss the feeling that I could develop at least simpler LLMs on my own. I am slowly working through Sebastian Raschk's excellent book https://www.manning.com/books/build-a-large-language-model-f... but I will probably never finish it (to be honest).",
    "parent": 44855690,
    "depth": 1
  },
  {
    "id": 44856661,
    "by": "roscas",
    "timeISO": "2025-08-10T17:18:46.000Z",
    "textPlain": "From my experience, qwen3-coder is way better. I only have gpt-oss:20b installed to make a few more tests but I give it a program to make a summary of what it does and qwen3 just works in a few seconds, while gpt-oss was cancelled after 5 minuts... doing nothing.So I just use qwen3. Fast and great ouput. If for some reason I don't get what I need, I might use search engines or Perplexity.I have a 10GB 3080 and Ryzen 3600x with 32gb of RAM.Qwen3-coder is amazing. Best I used so far.",
    "parent": 44855690,
    "depth": 1
  },
  {
    "id": 44858834,
    "by": "eurekin",
    "timeISO": "2025-08-10T22:25:24.000Z",
    "textPlain": "I'm still in awe that a local 3090 gpu was able to run the qwen3 coder instruct 30b-a3b exl3 q6 and...Was able to create a sample page, tried starting a server, recognising a leftover server was running, killing it (and forced a prompt for my permission), retrying and finding out it's ip for me to open in the browser.This isn't a demo anymore. That's actually very useful help for interns/juniors already.",
    "parent": 44855690,
    "depth": 1
  },
  {
    "id": 44858076,
    "by": "gglon",
    "timeISO": "2025-08-10T20:34:16.000Z",
    "textPlain": "> At the time of writing, the highest-ranking non-purely-transformer-based model on the LM Arena is Jamba, which is a transformer–state space model hybrid, at rank 96.)Tencent's hunyuan-turbos, another hybrid, is currently ranked at 22. \nhttps://arxiv.org/abs/2505.15431",
    "parent": 44855690,
    "depth": 1
  },
  {
    "id": 44857029,
    "by": "Scene_Cast2",
    "timeISO": "2025-08-10T18:05:00.000Z",
    "textPlain": "I find it interesting that the architectures of modern open weight LLMs are so similar, and that most innovation seems to be happening on the training (data, RL) front.This is contrary to what I've seen in a large ML shop, where architectural tuning was king.",
    "parent": 44855690,
    "depth": 1
  },
  {
    "id": 44858227,
    "by": "poorman",
    "timeISO": "2025-08-10T20:59:49.000Z",
    "textPlain": "This article really goes into a lot of detail which is nice. gpt-oss is just not good for agentic use in my observation.tldr; I'll save you a lot of time trying things out for yourself. If you are on a >=32 GB Mac download LMStudio and then the `qwen3-coder-30b-a3b-instruct-mlx@5bit` model. It uses ~20 GB of RAM so a 32GB machine is plenty. Set it up with opencode [1] and you're off to the races! It has great tool calling ability. The tool calling ability of gpt-oss doesn't even come close in my observations.[1] https://opencode.ai/",
    "parent": 44855690,
    "depth": 1
  },
  {
    "id": 44858198,
    "by": "chaos_emergent",
    "timeISO": "2025-08-10T20:56:01.000Z",
    "textPlain": "> This is likely because LLMs are typically trained for only a single epoch over massive datasets, which is in contrast to the multi-hundred-epoch training regimes for which dropout was first introduced.Wait, is this true? That seems like a wild statement to make, relatively unsubstantiated?",
    "parent": 44855690,
    "depth": 1
  },
  {
    "id": 44857044,
    "by": "storus",
    "timeISO": "2025-08-10T18:07:18.000Z",
    "textPlain": "In my tests, GPT-OSS-120B Q8 was close to DeepSeek R1 671B Q16 in solving graduate-level math but much faster with way fewer thinking tokens.",
    "parent": 44855690,
    "depth": 1
  },
  {
    "id": 44858940,
    "by": "ahmedfromtunis",
    "timeISO": "2025-08-10T22:42:15.000Z",
    "textPlain": "When I visit the site I get the error \"Your connection is not private\". Also: \"You cannot visit magazine.sebastianraschka.com right now because the website uses HSTS.\"Chrome latest on Ubuntu.",
    "parent": 44855690,
    "depth": 1
  },
  {
    "id": 44858139,
    "by": "oezi",
    "timeISO": "2025-08-10T20:45:30.000Z",
    "textPlain": "One question I was wondering about regarding the open models released by big labs is how much more the could improve with additional training. GPT-OSS has 2.1m hours of training, how much score improvements could we see at double that?",
    "parent": 44855690,
    "depth": 1
  },
  {
    "id": 44857348,
    "by": "pryelluw",
    "timeISO": "2025-08-10T18:56:08.000Z",
    "textPlain": "The Qwen3 4B has been very good to use local. I barely use the online models. Web searches are now more targeted thanks to it. Don’t quite fully trust the output but it’s generally good. Mods like these will revolutionize local knowledge and automation",
    "parent": 44855690,
    "depth": 1
  },
  {
    "id": 44856119,
    "by": "homarp",
    "timeISO": "2025-08-10T16:07:44.000Z",
    "textPlain": "\"From GPT-2 to gpt-oss: Analyzing the Architectural Advances\nAnd How They Stack Up Against Qwen3\"",
    "parent": 44855690,
    "depth": 1
  },
  {
    "id": 44861207,
    "by": "asabla",
    "timeISO": "2025-08-11T06:17:28.000Z",
    "textPlain": "> that rely on tool use for facts, and “knowledge bases” tuned for retrieval-heavy workI would say this isn't exclusive to the smaller OSS models. But rather a trait of Openai's models all together now.This becomes especially apparent with the introduction of GPT-5 in ChatGPT. Their focus on routing your request to different modes and searching the web automatically (relying on an Agentic workflows in the background) is probably key to the overall quality of the output.So far, it's quite easy to get their OSS models to follow instructions reliably. Qwen models has been pretty decent at this too for some time now.I think if we give it another generation or two, we're at the point of having compotent enough models to start running more advanced agentic workflows. On modest hardware. We're almost there now, but not quite yet",
    "parent": 44860707,
    "depth": 2
  },
  {
    "id": 44856139,
    "by": "jszymborski",
    "timeISO": "2025-08-10T16:11:25.000Z",
    "textPlain": "If I had to make a guess, I'd say this has much, much less to do with the architecture and far more to do with the data and training pipeline. Many have speculated that gpt-oss has adopted a Phi-like synthetic-only dataset and focused mostly on gaming metrics, and I've found the evidence so far to be sufficiently compelling.",
    "parent": 44856128,
    "depth": 2
  },
  {
    "id": 44859686,
    "by": "omneity",
    "timeISO": "2025-08-11T00:26:00.000Z",
    "textPlain": "Qwen3 32B is a dense model, it uses all its parameters all the time. GPT OSS 20B is a sparse MoE model. This means it only uses a fraction (3.6B) at a time. It’s a tradeoff that makes it faster to run than a dense 20B model and much smarter than a 3.6B one.In practice the fairest comparison would be to a dense ~8B model. Qwen Coder 30B A3B is a good sparse comparison point as well.",
    "parent": 44856128,
    "depth": 2
  },
  {
    "id": 44859962,
    "by": "faangguyindia",
    "timeISO": "2025-08-11T01:25:15.000Z",
    "textPlain": "yesterday, i signed up for qwen3-coder-plus. It fails 4/10 \"diff\" edit format in various code editing tools i use.Gemini Pro 2.5 with diff fenced edit format, rarely fails. So i don't see this Qwen3 hype unless i am using wrong edit format, can anyone tell me which edit format will work better with Qwen3?https://aider.chat/docs/more/edit-formats.html",
    "parent": 44856128,
    "depth": 2
  },
  {
    "id": 44857849,
    "by": "BoorishBears",
    "timeISO": "2025-08-10T20:02:07.000Z",
    "textPlain": "MoE expected performance = sqrt(active heads * total parameter count)sqrt(120*5) ~= 24GPT-OSS 120B is effectively a 24B parameter model with the speed of a much smaller model",
    "parent": 44856128,
    "depth": 2
  },
  {
    "id": 44857903,
    "by": "cranberryturkey",
    "timeISO": "2025-08-10T20:09:03.000Z",
    "textPlain": "qwen3 is slow though. i used it. it worked, but it was slow and lacking features.",
    "parent": 44856128,
    "depth": 2
  },
  {
    "id": 44857673,
    "by": "lvl155",
    "timeISO": "2025-08-10T19:40:25.000Z",
    "textPlain": "He does an amazing job of keeping me up to date on this insanely fast-paced space.",
    "parent": 44857190,
    "depth": 2
  },
  {
    "id": 44857846,
    "by": "lvl155",
    "timeISO": "2025-08-10T20:01:57.000Z",
    "textPlain": "Qwen3 coder 480B is quite good and on par with Sonnet 4. It’s the first time I realized the Chinese models are probably going to eclipse US-based models pretty soon, at least for coding.",
    "parent": 44856661,
    "depth": 2
  },
  {
    "id": 44856701,
    "by": "smokel",
    "timeISO": "2025-08-10T17:23:08.000Z",
    "textPlain": "The 20B version doesn't fit in 10GB.  That might explain some issues?",
    "parent": 44856661,
    "depth": 2
  },
  {
    "id": 44856889,
    "by": "mhitza",
    "timeISO": "2025-08-10T17:47:00.000Z",
    "textPlain": "I've been using lightly gpt-oss-20b but what I've found is that for smaller (single sentence) prompts it was easy enough to have it loop infinitely. Since I'm running it with llama.cpp I've set a small repetition penalty and haven't encountered those issues since (I'm using it a couple of times a day to analyze diffs, so I might have just gotten lucky since)",
    "parent": 44856661,
    "depth": 2
  },
  {
    "id": 44857897,
    "by": "SV_BubbleTime",
    "timeISO": "2025-08-10T20:08:07.000Z",
    "textPlain": "Are you using this in an agentic way or in a copy and paste and “code this” single input single output way?I’d like to know how far the frontier models are from the local for agentic coding.",
    "parent": 44856661,
    "depth": 2
  },
  {
    "id": 44860426,
    "by": "mrheosuper",
    "timeISO": "2025-08-11T02:59:11.000Z",
    "textPlain": "How did you do your setup ?, right now the only way i know how to run LLM is through LM studio.",
    "parent": 44858834,
    "depth": 2
  },
  {
    "id": 44857417,
    "by": "bobbylarrybobby",
    "timeISO": "2025-08-10T19:05:11.000Z",
    "textPlain": "My guess is that at LLM scale, you really can't try to hyperparameter tune — it's just too expensive. You probably have to do some basic testing of different architectures, settle on one, and then figure out how to make best use of it (data and RL).",
    "parent": 44857029,
    "depth": 2
  },
  {
    "id": 44857409,
    "by": "ModelForge",
    "timeISO": "2025-08-10T19:04:09.000Z",
    "textPlain": "Good point. LLMs lower the barrier to entry if someone has enough resources because those architectures are more robust to tweaks given one throws enough compute and data at them. You can even violate scaling laws and still get a good model (like Llama 3 showed back then)",
    "parent": 44857029,
    "depth": 2
  },
  {
    "id": 44858617,
    "by": "ModelForge",
    "timeISO": "2025-08-10T21:50:47.000Z",
    "textPlain": "The ollama one uses even less (around 13 GB), which is nice. Apparently the gpt-oss team also shared the mxfp4 optimizations for metal",
    "parent": 44858227,
    "depth": 2
  },
  {
    "id": 44858210,
    "by": "typon",
    "timeISO": "2025-08-10T20:57:47.000Z",
    "textPlain": "No this is well known. Look for Table 2.2 in GPT3 paper.",
    "parent": 44858198,
    "depth": 2
  },
  {
    "id": 44857906,
    "by": "overfeed",
    "timeISO": "2025-08-10T20:09:25.000Z",
    "textPlain": "Supporting TFA'd thesis that it's trained to be good at benchmarks.",
    "parent": 44857044,
    "depth": 2
  },
  {
    "id": 44860867,
    "by": "vintermann",
    "timeISO": "2025-08-11T05:04:24.000Z",
    "textPlain": "First suspicion is that HSTS is doing what it's supposed to, and that you're connecting from somewhere they try to insert themselves in the middle of all https traffic. Https snooping is sadly not uncommon, some businesses think they're entitled to do it for you using their network.",
    "parent": 44858940,
    "depth": 2
  },
  {
    "id": 44858625,
    "by": "ModelForge",
    "timeISO": "2025-08-10T21:52:15.000Z",
    "textPlain": "I think GPT-4.5 was potentially the original GPT-5 model that was larger and pre-trained on more data. Too bad it was too expensive to deploy at scale so that we never saw the RL-ed version",
    "parent": 44858139,
    "depth": 2
  },
  {
    "id": 44858175,
    "by": "poorman",
    "timeISO": "2025-08-10T20:52:08.000Z",
    "textPlain": "As we saw with GPT-5 the RL technique of training doesn't scale forever",
    "parent": 44858139,
    "depth": 2
  },
  {
    "id": 44857420,
    "by": "indigodaddy",
    "timeISO": "2025-08-10T19:05:21.000Z",
    "textPlain": "Qwen is telling you better search parameters to then search the web with, or qwen is actually doing web searches for you?",
    "parent": 44857348,
    "depth": 2
  }
]
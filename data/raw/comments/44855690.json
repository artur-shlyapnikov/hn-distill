[
  {
    "id": 44856128,
    "by": "7moritz7",
    "timeISO": "2025-08-10T16:09:01.000Z",
    "textPlain": "Qwen3 is substantially better in my local testing. As in, adheres to the prompt better (pretty much exactly for the 32B parameter variant, very impressive) and is more organic sounding.In simplebench gpt-oss (120 bn) flopped hard so it doesn't appear particularly good at logical puzzles either.So presumably, this comes down to...- training technique or data- dimension- lower number of large experts vs higher number of small experts",
    "parent": 44855690,
    "depth": 1
  },
  {
    "id": 44856119,
    "by": "homarp",
    "timeISO": "2025-08-10T16:07:44.000Z",
    "textPlain": "\"From GPT-2 to gpt-oss: Analyzing the Architectural Advances\nAnd How They Stack Up Against Qwen3\"",
    "parent": 44855690,
    "depth": 1
  },
  {
    "id": 44856139,
    "by": "jszymborski",
    "timeISO": "2025-08-10T16:11:25.000Z",
    "textPlain": "If I had to make a guess, I'd say this has much, much less to do with the architecture and far more to do with the data and training pipeline. Many have speculated that gpt-oss has adopted a Phi-like synthetic-only dataset and focused mostly on gaming metrics, and I've found the evidence so far to be sufficiently compelling.",
    "parent": 44856128,
    "depth": 2
  }
]
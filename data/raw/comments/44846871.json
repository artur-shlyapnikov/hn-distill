[
  {
    "id": 44849009,
    "by": "ComplexSystems",
    "timeISO": "2025-08-09T18:44:27.000Z",
    "textPlain": "I thought this article was going to be a bunch of security theater nonsense - maybe the relatively bland title - but after reading I found it to be incredibly insightful, particularly this:> MCP discards this lesson, opting for schemaless JSON with optional, non-enforced hints. Type validation happens at runtime, if at all. When an AI tool expects an ISO-8601 timestamp but receives a Unix epoch, the model might hallucinate dates rather than failing cleanly. In financial services, this means a trading AI could misinterpret numerical types and execute trades with the wrong decimal precision. In healthcare, patient data types get coerced incorrectly, potentially leading to wrong medication dosing recommendations. Manufacturing systems lose sensor reading precision during JSON serialization, leading to quality control failures.Having worked with LLMs every day for the past few years, it is easy to see every single one of these things happening.I can practically see it playing out now: there is some huge incident of some kind, in some system or service with an MCP component somewhere, with some elaborate post-mortem revealing that some MCP server somewhere screwed up and output something invalid, the LLM took that output and hallucinated god knows what, its subsequent actions threw things off downstream, etc.It would essentially be a new class of software bug caused by integration with LLMs, and it is almost sure to happen when you combine it with other sources of bug: human error, the total lack of error checking or exception handling that LLMs are prone to (they just hallucinate), a bunch of gung-ho startups \"vibe coding\" new services on top of the above, etc.I foresee this being followed by a slew of Twitter folks going on endlessly about AGI hacking the nuclear launch codes, which will probably be equally entertaining.",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44849159,
    "by": "GeneralMayhem",
    "timeISO": "2025-08-09T19:04:03.000Z",
    "textPlain": "> MCP promises to standardize AI-tool interactions as the “USB-C for AI.”Ironically, it's achieved this - but that's an indictment of USB-C, not an accomplishment of MCP. Just like USB-C, MCP is a nigh-universal connector with very poorly enforced standards for what actually goes across it. MCP's inconsistent JSON parsing and lack of protocol standardization is closely analogous to USB-C's proliferation of cable types (https://en.wikipedia.org/wiki/USB-C#Cable_types); the superficial interoperability is a very leaky abstraction over a much more complicated reality, which IMO is worse than just having explicitly different APIs/protocols.",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44848594,
    "by": "rickcarlino",
    "timeISO": "2025-08-09T17:56:28.000Z",
    "textPlain": "> SOAP, despite its verbosity, understood something that MCP doesn’tUnfortunately, no one understood SOAP back.(Additional context: Maintaining a legacy SOAP system. I have nothing good to say about SOAP and it should serve as a role model for no one)",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44849332,
    "by": "mac-mc",
    "timeISO": "2025-08-09T19:24:47.000Z",
    "textPlain": "You're missing the most significant lesson of all that MCP knew.  That all of those featureful things are way too overcomplicated for most places, so they will gravitate to the simple thing.  It's why JSON over HTTP blobs is king today.I've been on the other side of high-feature serialization protocols, and even at large tech companies, something like migrating to gRPC is a multi-year slog that can even fail a couple of times because it asks so much of you.MCP, at its core, is a standardization of a JSON API contract, so you don't have to do as much post-training to generate various tool calling style tokens for your LLM.",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44848433,
    "by": "zorked",
    "timeISO": "2025-08-09T17:35:57.000Z",
    "textPlain": "CORBA emerged in 1991 with another crucial insight: in heterogeneous environments, you can’t just “implement the protocol” in each language and hope for the best. The OMG IDL generated consistent bindings across C++, Java, Python, and more, ensuring that a C++ exception thrown by a server was properly caught and handled by a Java client. The generated bindings guaranteed that all languages saw identical interfaces, preventing subtle serialization differences.\n\nYes, CORBA was such a success.",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44849853,
    "by": "SillyUsername",
    "timeISO": "2025-08-09T20:24:21.000Z",
    "textPlain": "MCP is flawed but it learnt one thing correctly from years of RPC - complexity is the biggest time sink and holds back adoption in deference to simpler competing standards (cf XML vs JSON)- SOAP - interop needs support of DOC or RPC based between systems, or a combination, XML and schemas are also horribly verbose.- CORBA - libraries and framework were complex, modern languages at the time avoided them in deference to simpler standards (e.g. Java's Jini)- GPRC - designed for speed, not readability, requires mappings.It's telling that these days REST and JSON (via req/resp, webhooks, or even streaming) are the modern backbone of RPC. The above standards either are shoved aside or for GPRC only used where extreme throughput is needed.Since REST and JSON are the plat du jour, MCP probably aligns with that design paradigm rather than the dated legacy protocols.",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44848323,
    "by": "mockingloris",
    "timeISO": "2025-08-09T17:20:14.000Z",
    "textPlain": "I read this thrice: ...When OpenAI bills $50,000 for last month’s API usage, can you tell which department’s MCP tools drove that cost? Which specific tool calls? Which individual users or use cases?...It seems to be a game of catch up for most things AI. That said, my school of thought is that certain technologies are just too big for them to be figured out early on - web frameworks, blockchain, ...- the gap starts to shrink eventually. With AI, we'll just have to keep sharing ideas and caution like you have here.\nSuch very interesting times we live in.",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44852397,
    "by": "0xbadcafebee",
    "timeISO": "2025-08-10T02:46:22.000Z",
    "textPlain": "Am I the only person who thinks it's a bad idea for a hallucinating AI to have anything to do with financial services, healthcare, or manufacturing? Doesn't really matter what the RPC protocol looks like if the meat and potatoes of the tech will confidently make shit up. You can't hand-wave this away by saying \"oh it's a reasoning AI\". Albert Einstein may be real smart, but it's still a bad idea to take financial advice from him when he drops acid.",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44850871,
    "by": "holografix",
    "timeISO": "2025-08-09T22:21:52.000Z",
    "textPlain": "Author disregards why none of these technologies are relevant in the modern web.Sure, they might still find themselves in highly regulated industries where risk avoidance trumps innovation everyday, all day.MCP is for _the web_ , it started with stdio only because Anthropic was learning lessons from building Claude Code.Author also seems to expect that the result from MCP tool usage will feed directly to an LLM. This is preposterous and a recipe for disaster. Obviously you’d validade structured response against a schema, check for harmful content, etc etc.",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44848489,
    "by": "abtinf",
    "timeISO": "2025-08-09T17:43:53.000Z",
    "textPlain": "I wish someone would write a clear, crisp explanation for why MCP is needed over simply supporting swagger or proto.",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44849003,
    "by": "zwaps",
    "timeISO": "2025-08-09T18:43:44.000Z",
    "textPlain": "The author seems to fundamentally misunderstand how MCPs are going to be used and deployed.This is really obvious when they talk about tracing and monitoring, which seem to be the main points of criticism anyway.They bemoan that they cant trace across MCP calls, assuming somehow there would be a person administering all the MCPs.\nOf course each system has tracing in whatever fashion fits its system. \nThey are just not the same system, nor owned by the same people let alone companies.Same as monitoring cost. Oh, you can’t know who racked up the LLM costs? Well of course you can, these systems are already in place and there are a million of ways to do this. It has nothing to do with MCP.Reading this, I think its rather a blessing to start fresh and without the learnings of 40 years of failed protocols or whatever",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44849965,
    "by": "ramoz",
    "timeISO": "2025-08-09T20:37:48.000Z",
    "textPlain": "Many great points. I think we are thinking about MCP the wrong way.The greater problem is industry misunderstanding and misalignment with what agents are and where they are headed.Web platforms of the world believe agents will be embedded in networked distributed infrastructure. So we should ship an MCP platform in our service mesh for all of the agents running in containers to connect to.I think this is wrong, and continues to be butchered as the web pushes a hard narrative that we need to enable web-native agents & their sdks/frameworks that deploy agents as conventional server applications. These are not agents nor the early evolutionary form of them.Frontier labs will be the only providers of the actual agentic harnesses. And we are rapidly moving to computer use agents - MCP servers were intended to serve as single instance deployments for single harnesses. ie. a single mcp server on my desktop for my Claude Desktop.",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44849477,
    "by": "upghost",
    "timeISO": "2025-08-09T19:42:01.000Z",
    "textPlain": "So I'm in the \"MCP is probably not a great idea\" camp but I couldn't say \"this is how it SHOULD be done\", and the author makes great criticisms but falls short of actual suggestions. I'm assuming the author is not seriously recommending we go back to SOAP and I've never heard of CORBA.  I've heard of gRPC but I can't tell if the author is saying it is good or bad.Also Erlang uses RPCs for pretty much all \"synchronous\" interactions but it's pretty minimal in terms of ceremony. Seems pretty reliable.So this is a serious question because hand rolling \"40 years\" of best practices seems hard, what should we be using for RPC?",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44849284,
    "by": "ipython",
    "timeISO": "2025-08-09T19:19:53.000Z",
    "textPlain": "I am torn. I see this argument and intellectually agree with it (that interfaces need to be more explicit). However it seems that every time there is a choice between “better” design and “good enough”, the “good enough” wins handily.Multics vs Unix, xml based soap vs json based rest apis, xhtml’s failure, javascript itself, … I could keep going on.So I’ve resigned myself to admitting that we are doomed to reimplement the “good enough” every time, and continue to apply bandaid after bandaid to gradually fix problems after we rediscover them, slowly.",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44852773,
    "by": "tarun_anand",
    "timeISO": "2025-08-10T04:32:59.000Z",
    "textPlain": "Very interesting... I have been contributing with the exact same interoperability issues to the MCP Community (Anthropic Google etc)https://github.com/modelcontextprotocol/modelcontextprotocol...The Python Bindings PR is here\nmodelcontextprotocol/rust-sdk#172The Typescript Bindings PR is here\nmodelcontextprotocol/rust-sdk#183MCP Bench at https://github.com/unimcp/mcpbench",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44852917,
    "by": "krawczstef",
    "timeISO": "2025-08-10T05:12:32.000Z",
    "textPlain": "I agree with this post.Otherwise the larger picture is that MCP is a land grab for building an eco-system around integrations to get access to data. Your LLM agent is not valuable if it can't access things for you... and from a market perspective enterprise pays a lot for this stuff already, and yes MCP is not thought out at all for Enterprise really... At least thankfully they added stateless connections to the spec...",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44848857,
    "by": "btown",
    "timeISO": "2025-08-09T18:26:29.000Z",
    "textPlain": "If you want the things mentioned in this article, I highly recommend looking at https://github.com/modelcontextprotocol/modelcontextprotocol... and https://modelcontextprotocol.io/community/sep-guidelines and participating in the specification process.Point-by-point for the article's gripes:- distributed tracing/telemetry - open discussion at https://github.com/modelcontextprotocol/modelcontextprotocol...- structured tool annotation for parallelizability/side-effects/idempotence - this actually already exists at https://modelcontextprotocol.io/specification/2025-06-18/sch... but it's not well documented in https://modelcontextprotocol.io/specification/2025-06-18/ser... - someone should contribute to improving this!- a standardized way in which the costs associated with an MCP tool call can be communicated to the MCP Client and reported to central tracking - nothing here I see, but it's a really good idea!- serialization issues e.g. \"the server might report a date in a format unexpected by the client\" - this isn't wrong, but since the consumer of most tool responses is itself an LLM, there's a fair amount of mitigation here. And in theory an MCP Client can use an LLM to detect under-specified/ambiguous tool specifications, and could surface these issues to the integrator.Now, I can't speak to the speed at which Maintainers and Core Maintainers are keeping up with the community's momentum - but I think it's meaningful that the community has momentum for evolving the specification!I see this post in a highly positive light: MCP shows promise because you can iterate on these kinds of structured annotations, in the context of a community that is actively developing their MCP servers. Legacy protocols aren't engaging with these problems in the same way.",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44849085,
    "by": "BLanen",
    "timeISO": "2025-08-09T18:54:13.000Z",
    "textPlain": "As I've been saying.MCP is not a protocol. It doesn't protocolize anything of use. It's just \"here's some symbols, do with them whatever you want.\", leaving it there but then advertising that as a feature of its universality. It provides almost just as much of a protocol as TCP, but rebuild on 5 OSI layers, again.It's not a security issue, it's a ontological issue.",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44846960,
    "by": "al2o3cr",
    "timeISO": "2025-08-09T14:51:36.000Z",
    "textPlain": "IMO worrying about type-safety in the protocol when any string field in the reply can prompt-inject the calling LLM feels like putting a band-aid on a decapitation, but YMMV",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44848517,
    "by": "self_awareness",
    "timeISO": "2025-08-09T17:46:46.000Z",
    "textPlain": "What's new?- Electron disregards 40 years of best deployment practices,- Web disregards 40 years of best GUI practices,- Fast CPUs and lots of RAM disregards 40 years of best software optimization techniques,there are probably many more examples.",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44849140,
    "by": "dragonwriter",
    "timeISO": "2025-08-09T19:01:23.000Z",
    "textPlain": "> MCP discards this lesson, opting for schemaless JSON with optional, non-enforced hints.Actually, MCP uses a normative TypeScript schema (and, from that, an autogenerated JSON Schema) for the protocol itself, and the individual tool calls also are specified with JSON Schema.> Type validation happens at runtime, if at all.That's not a consequence of MCP \"opting for schemaless JSON\" (which it factually does not), that's, for tool calls, a consequence of MCP being a discovery protocol where the tools, and thus the applicable schemas,  are discovered aruntime.If you are using MCP as a way to wire up highly-static components, you can do discovery against the servers once they are wired up, statically build the clients around the defined types, and build your toolchain to raise errors if the discovery responses change in the future. But that's not really the world MCP is built for. Yes, that means that the toolchain needs, if it is concerned about schema enforcement, use and apply the relevant schemas at runtime. So, um, do that?",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44849891,
    "by": "cnst",
    "timeISO": "2025-08-09T20:28:36.000Z",
    "textPlain": "I think this article is missing the point that MCP is simply using the mainstream building blocks that have already regressed from what we've had previously, namely, JSON in place of proper RCP.The ISO8601 v Unix epoch example seems very weak to me.  I'd certainly expect any model to be capable of distinguishing between these things, so, it doesn't seem like a big deal that either one would be allowed in a JSON.Honestly, my view that nothing of value ever gets published on medium, is strongly reinforced here.",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44849944,
    "by": "lowbloodsugar",
    "timeISO": "2025-08-09T20:35:29.000Z",
    "textPlain": "MCP is what we needed right now, and what most people will need forever. Some of us will need more so we’ll write it.",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44848879,
    "by": "calvinmorrison",
    "timeISO": "2025-08-09T18:29:08.000Z",
    "textPlain": "MCP, aka, WSDL for REST",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44848318,
    "by": "gjsman-1000",
    "timeISO": "2025-08-09T17:19:59.000Z",
    "textPlain": "… or we’ll just invent MCP 2.0.On that note; some of these “best practices” arguably haven’t worked out. “Be conservative with what you send, liberal with what you receive” has turned even decent protocols into a dumpster fire, so why keep the charade going?",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44850448,
    "by": "jongjong",
    "timeISO": "2025-08-09T21:31:15.000Z",
    "textPlain": "The stuff about the utility of machine-readable Web Service Description Language got me rolling my eyes.WSDL is just pure nonsense. The idea that software would need to decide which API endpoints it needs on its own, is just profoundly misguided... Literally nobody and nothing ever reads the WSDL definitions; it's just poor man's documentation, at best.LLMs only reinforce the idea that WSDL is a dumb idea because it turns out that even the machines don't care for your 'machine-friendly' format and actually prefer human-friendly formats.Once you have an MPC tool working with a specific JSON API, it will keep working unless the server makes breaking changes to the API while in production which is terrible practice. But anyway, if you use a server, it means you trust the server. Client-side validation is dumb; like people who need to put tape over their mouths because they don't trust themselves to follow through on their diet plans.",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44849312,
    "by": "draw_down",
    "timeISO": "2025-08-09T19:22:54.000Z",
    "textPlain": "[dead]",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44849232,
    "by": "zombiwoof",
    "timeISO": "2025-08-09T19:13:05.000Z",
    "textPlain": "[dead]",
    "parent": 44846871,
    "depth": 1
  },
  {
    "id": 44849478,
    "by": "cookiengineer",
    "timeISO": "2025-08-09T19:42:05.000Z",
    "textPlain": "Let's put it this way:Before 2023 I always thought that all the bugs and glitches of technology in Star Trek were totally made up and would never happen this way.Post-LLM I am absolutely certain that they will happen exactly that way.I am not sure what LLM integrations have to do with engineering anymore, or why it makes sense to essentially put all your company's infrastructure into external control. And that is not even scratching the surface with the lack of reproducibility at every single step of the way.It \"somehow works\" isn't engineering.",
    "parent": 44849009,
    "depth": 2
  },
  {
    "id": 44849325,
    "by": "cle",
    "timeISO": "2025-08-09T19:23:52.000Z",
    "textPlain": "I don't understand this criticism by the author. MCP supports JSON Schema, and server responses must conform to the schema. If the schema requires an ISO-8601 timestamp (ex by specifying a \"date\" format in the schema) but the server sends a Unix epoch timestamp, then it is violating the protocol.The author even later says that MCP supports JSON Schema, but also claims \"you can't generate type-safe clients\". Which is plainly untrue, there exist plenty of JSON Schema code generators.",
    "parent": 44849009,
    "depth": 2
  },
  {
    "id": 44849224,
    "by": "tomrod",
    "timeISO": "2025-08-09T19:12:23.000Z",
    "textPlain": "We already have PEBKAC - problem exists between chair and keyboard.LLMs are basically automating PEBKAC",
    "parent": 44849009,
    "depth": 2
  },
  {
    "id": 44849676,
    "by": "avereveard",
    "timeISO": "2025-08-09T20:06:40.000Z",
    "textPlain": "MCP focuses on transport and managing context and doesn't absolve the user for sensibly implementing the interface (i.e. defining a schema and doing schema validation)this is like saying \"HTTP doesn't do json validation\", which, well, yeah.",
    "parent": 44849009,
    "depth": 2
  },
  {
    "id": 44853596,
    "by": "Squakie",
    "timeISO": "2025-08-10T08:04:38.000Z",
    "textPlain": "I can offer a hacking/penetrarion testing perspective to this as a security researcher at a security consultint firm: this type of hallucination and trust is one of the largest things we exploit in our new LLM testing service. Overly agentic systems (one of the top 10 OWASP LLM vulns) is the most profound and commonly exploited issue that we've been able to leverage.If we can get an internal, sensitive-data-handling agent to ingest a crafted prompt, either via direct prompt injection against a more abstract “parent” agent, or by tainting an input file/URL it’s told to process, we can plant what I have internally coined an “unfolding injection.”The injection works like a parasitic goal, it doesn’t just trick one agent, it rewrites the downstream intent. As the orchestrator routes tasks to other agents, each one treats the tainted instructions as legitimate and works toward fulfilling them.Because many orchestrations re-summarize, re-plan, or synthesize goals between steps, the malicious instructions can actually gain fidelity as they propagate. By the time they reach a sensitive action (exfiltration, privilege escalation, external calls), there’s no trace of the original “weird” wording, just a confidently stated, fully-integrated sub-goal.It’s essentially a supply-chain attack on the orchestration layer: you compromise one node in the agent network, and the rest “help” you without realizing it. Without explicit provenance tracking and policy enforcement between agents, this kind of unfolding injection is almost trivial to pull off, and we've been able to compromise entire environments based on the information the agentic system provided us, or just gave us either a bind or reverse shell in the case it has cli access and ability to figure out its own network constraints.SSRF has been making a HUGE return in agentic systems, and Im sad defcon and black hat didnt really have many talks on this subject this year, because it is a currently evolving security domain and en",
    "parent": 44849009,
    "depth": 2
  },
  {
    "id": 44849707,
    "by": "hinkley",
    "timeISO": "2025-08-09T20:09:36.000Z",
    "textPlain": "> In healthcare, patient data types get coerced incorrectly, potentially leading to wrong medication dosing recommendations.May have changed, but unlikely. I worked with medical telemetry as a young man and it was impressed upon me thoroughly how important parsing timestamps correctly was. I have a faint memory, possibly false, of this being the first time I wrote unit tests (and without the benefit of a test framework).We even accounted for lack of NTP by recalculating times off of the timestamps I. Their message headers.And the reasons I was given were incident review as well as malpractice cases. A drug administered three seconds before a heart attack starts is a very different situation than one administered eight seconds after the patient crashed. We saw recently with the British postal service how lives can be ruined by bad data, and in medical data a minute is a world of difference.",
    "parent": 44849009,
    "depth": 2
  },
  {
    "id": 44849712,
    "by": "oblio",
    "timeISO": "2025-08-09T20:10:04.000Z",
    "textPlain": "We keep repeating this.When desktop OSes came out, hardware resources were scarce so all the desktop OSes (DOS, Windows, MacOS) forgot all the lessons from Unix: multi user, cooperative multitasking, etc. 10 years later PC hardware was faster than workstations from the 90s yet we're still stuck with OSes riddled with limitations that stopped making sense in the 80s.When smartphones came out there was this gold rush and hardware resources were scarce so OSes (iOS, Android) again forgot all the lessons. 10 years later mobile hardware was faster than desktop hardware from the 00s. We're still stuck with mistakes from the 00s.AI basically does the same thing. It's all lead by very bright 20 and 30 year olds that weren't even born when Windows was first released.Our field is doomed under a Cascade of Attention-Deficit Teenagers: https://www.jwz.org/doc/cadt.html (copy paste the link).It's all gold rushes and nobody does Dutch urban infrastructure design over decades. Which makes sense as this is all driven by the US, where long term plan I is anathema.",
    "parent": 44849009,
    "depth": 2
  },
  {
    "id": 44849888,
    "by": "lowbloodsugar",
    "timeISO": "2025-08-09T20:28:21.000Z",
    "textPlain": "I’ve been successfully using AI for several months now, and there’s still no way I’d trust it to execute trades, or set the dose on an XRay machine. But startups gonna start. Let them.",
    "parent": 44849009,
    "depth": 2
  },
  {
    "id": 44850237,
    "by": "jongjong",
    "timeISO": "2025-08-09T21:08:23.000Z",
    "textPlain": "To me, the article was just rambling about all sorts of made up issues which only exist in the minds of people who never spent any time outside of corporate environments... A lot of 'preventative' ideas which make sense in some contexts but are mis-applied in different contexts.The stuff about type validation is incorrect. You don't need client-side validation. You shouldn't be using APIs you don't trust as tools and you can always add instructions about the LLM's output format to convert to different formats.MCP is not the issue. The issue is that people are using the wrong tools or their prompts are bad.If you don't like the format of an MCP tool and don't want to give formatting instructions the LLMs, you can always create your own MCP service which outputs data in the correct format. You don't need the coercion to happen on the client side.",
    "parent": 44849009,
    "depth": 2
  },
  {
    "id": 44849059,
    "by": "throwawaymaths",
    "timeISO": "2025-08-09T18:50:14.000Z",
    "textPlain": "i mean isnt all this stuff up to the mcp author to return a reasonable error to the agent and ask for it to repeat the call with amendments to the json?",
    "parent": 44849009,
    "depth": 2
  },
  {
    "id": 44849636,
    "by": "cnst",
    "timeISO": "2025-08-09T20:01:37.000Z",
    "textPlain": "I'd like to add that the culmination of USB-C failure was Apple's removal of USB-A ports from the latest M4 Mac mini, where an identical port on the exact same device, now has vastly different capabilities, opaque to the final user of the system months past the initial hype on the release date.Previously, you could reasonably expect a USB-C on a desktop/laptop of an Apple Silicon device, to be USB4 40Gbps Thunderbolt, capable of anything and everything you may want to use it for.Now, some of them are USB3 10Gbps.  Which ones?  Gotta look at the specs or tiny icons, I guess?Apple could have chosen to have the self-documenting USB-A ports to signify the 10Gbps limitation of some of these ports (conveniently, USB-A is limited to exactly 10Gbps, making it perfect for the use-case of having a few extra \"low-speed\" ports at very little manufacturing cost), but instead, they've decided to further dilute the USB-C brand.  Pure innovation!With the end user likely still having to use a USB-C to USB-A adapters anyways, because the majority of thumb drives, keyboards and mice, still require a USB-A port — even the USB-C ones that use USB-C on the kb/mice itself.  (But, of course, that's all irrelevant because you can always spend 2x+ as much for a USB-C version of any of these devices, and the fact that the USB-C variants are less common or inferior to USB-A, is of course irrelevant when hype and fanaticism are more important than utility and usability.)",
    "parent": 44849159,
    "depth": 2
  },
  {
    "id": 44849641,
    "by": "afeuerstein",
    "timeISO": "2025-08-09T20:02:41.000Z",
    "textPlain": "Yeah, I loughed out loud when I read that line. Mission accomplished, I guess?",
    "parent": 44849159,
    "depth": 2
  }
]
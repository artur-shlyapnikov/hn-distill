[
  {
    "id": 44839963,
    "by": "andy99",
    "timeISO": "2025-08-08T18:15:04.000Z",
    "textPlain": "Edit to add: according to Sam Altman in the reddit AMA they un-deprecated it based on popular demand. https://old.reddit.com/r/ChatGPT/comments/1mkae1l/gpt5_ama_w...I wonder how much of the '5 release was about cutting costs vs making it outwardly better. I'm speculating that one reason they'd deprecate older models is because 5 materially cheaper to run?Would have been better to just jack up the price on the others. For companies that extensively test the apps they're building (which should be everyone) swapping out a model is a lot of work.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44841490,
    "by": "LeoPanthera",
    "timeISO": "2025-08-08T20:41:36.000Z",
    "textPlain": "The article links to this subreddit, which I'd never heard of until now:https://www.reddit.com/r/MyBoyfriendIsAIAnd my word that is a terrifying forum. What these people are doing cannot be healthy. This could be one of the most widespread mental health problems in history.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44841123,
    "by": "kens",
    "timeISO": "2025-08-08T20:06:35.000Z",
    "textPlain": "As an aside, people should avoid using \"deprecate\" to mean \"shut down\". If something is deprecated, that means that you shouldn't use it. For example, the C library's gets() function was deprecated because it is a security risk, but it wasn't removed until 12 years later. The distinction is important: if you're using GPT-4o and it is deprecated, you don't need to do anything, but if it is shut down, then you have a problem.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44845952,
    "by": "PeterStuer",
    "timeISO": "2025-08-09T12:20:05.000Z",
    "textPlain": "As so many others I'm currently evaluating the 5 series while keeping 4o in production. 5 behaves significantly different. My current outlook is it's a nice improvement, but not a drop in replacement/upgrade some of those 4->5 mapping tables suggest.Prompts and steering needs to be explored and recalibrated to gain status quo and benefits.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44839975,
    "by": "tibbar",
    "timeISO": "2025-08-08T18:16:05.000Z",
    "textPlain": "I've worked on many migrations of things from vX to vX + 1, and there's always a tension between maximum backwards-compatibility, supporting every theoretical existing use-case, and just \"flipping the switch\" to move everyone to the New Way. Even though I, personally, am a \"max backwards-compatibility\" guy, it can be refreshing when someone decides to rip off the bandaid and force everyone to use the new best practice. How exciting! Unfortunately, this usually results in accidentally eliminating some feature that turns out to be Actually Important, a fuss is made, and the sudden forced migration is reverted after all.I think the best approach is to move people to the newest version by default, but make it possible to use old versions, and then monitor switching rates and figure out what key features the new system is missing.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840304,
    "by": "rs186",
    "timeISO": "2025-08-08T18:46:02.000Z",
    "textPlain": "> Emotional nuance is not a characteristic I would know how to test!Well, that's easy, we knew that decades ago.    It’s your birthday. Someone gives you a calfskin wallet.\n\n    You’ve got a little boy. He shows you his butterfly collection plus the killing jar.\n\n    You’re watching television. Suddenly you realize there’s a wasp crawling on your arm.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840264,
    "by": "perlgeek",
    "timeISO": "2025-08-08T18:42:00.000Z",
    "textPlain": "GPT-5 simply sucks at some things. The very first thing I asked it to do was to give me an image of knife with spiral damascus pattern, it gave me an image of such a knife, but with two handles at a right angle: https://chatgpt.com/share/689506a7-ada0-8012-a88f-fa5aa03474...Then I asked it to give me the same image but with only one handle; as a result, it removed one of the pins from a handle, but the knife had still had two handles.It's not surprising that a new version of such a versatile tool has edge cases where it's worse than a previous version (though if it failed at the very first task I gave it, I wonder how edge that case really was). Which is why you shouldn't just switch over everybody without grace period nor any choice.The old chatgpt didn't have a problem with that prompt.For something so complicated it doesn't surprise that a major new version has some worse behaviors, which is why I wouldn't deprecate all the old models so quickly.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840160,
    "by": "CodingJeebus",
    "timeISO": "2025-08-08T18:31:24.000Z",
    "textPlain": "> or trying prompt additions like “think harder” to increase the chance of being routed to it.Sure, manually selecting model may not have been ideal. But manually prompting to get your model feels like an absurd hack",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840163,
    "by": "faizshah",
    "timeISO": "2025-08-08T18:31:51.000Z",
    "textPlain": "o3 was also an anomaly in terms of speed vs response quality and price vs performance. It used to be one of the fastest ways to do some basic web searches you would have done to get an answer if you used o3 pro you it would take 5x longer for not much better response.So far I haven’t been impressed with GPT5 thinking but I can’t concretely say why yet. I am thinking of comparing the same prompt side by side between o3 and GPT5 thinking.Also just from my first few hours with GPT5 Thinking I feel that it’s not as good at short prompts as o3 e.g instead of using a big xml or json prompt I would just type the shortest possible phrase for the task e.g “best gpu for home LLM inference vs cloud api.”",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840090,
    "by": "eurekin",
    "timeISO": "2025-08-08T18:25:49.000Z",
    "textPlain": "I couldn't be more confused by this launch...I had gpt-5 only on my account for the most of today, but now I'm back at previous choices (including my preferred o3).Had gpt-5 been pulled? Or, was it only a preview?",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840456,
    "by": "bookofjoe",
    "timeISO": "2025-08-08T19:00:07.000Z",
    "textPlain": "Currently 13 of 30 submissions on hn homepage are AI-related. That seems to be about average now.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44839926,
    "by": "tosh",
    "timeISO": "2025-08-08T18:11:56.000Z",
    "textPlain": "would have been smart to keep them around for a while and just hide them (a bit like in the pro plan, but less hidden)and then phase them out over timewould have reduced usage by 99% anywaynow it all distracts from the gpt5 launch",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840487,
    "by": "tosh",
    "timeISO": "2025-08-08T19:03:18.000Z",
    "textPlain": "sama: https://x.com/sama/status/1953893841381273969\"\"\"GPT-5 rollout updates:We are going to double GPT-5 rate limits for ChatGPT Plus users as we finish rollout.We will let Plus users choose to continue to use 4o. We will watch usage as we think about how long to offer legacy models for.GPT-5 will seem smarter starting today. Yesterday, the autoswitcher broke and was out of commission for a chunk of the day, and the result was GPT-5 seemed way dumber. Also, we are making some interventions to how the decision boundary works that should help you get the right model more often.We will make it more transparent about which model is answering a given query.We will change the UI to make it easier to manually trigger thinking.Rolling out to everyone is taking a bit longer. It’s a massive change at big scale. For example, our API traffic has about doubled over the past 24 hours…We will continue to work to get things stable and will keep listening to feedback. As we mentioned, we expected some bumpiness as we roll out so many things at once. But it was a little more bumpy than we hoped for!\"\"\"",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840055,
    "by": "iamleppert",
    "timeISO": "2025-08-08T18:22:53.000Z",
    "textPlain": "Taking away user choice is often done in the name of simplicity. But let's not forget that given 100 users, 60 are likely to answer with \"no opinion\" when asked what about their preference to ANY question. Does that mean the other 40% aren't valuable and their preferences not impactful to the other \"we don't care\" majority?",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840427,
    "by": "iamspoilt",
    "timeISO": "2025-08-08T18:57:45.000Z",
    "textPlain": "It's coming back according to Sam\nhttps://www.reddit.com/r/ChatGPT/comments/1mkae1l/gpt5_ama_w...",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840108,
    "by": "ramoz",
    "timeISO": "2025-08-08T18:27:22.000Z",
    "textPlain": "One enterprise angle to open source models is that we will develop advanced forms of RPA. Models automating a single task really well.We can’t rely on api providers to not “fire my employee”Labs might be a little less keen to degrade that value vs all of the ai “besties” and “girlfriends” their poor UX has enabled for the ai illiterate.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840530,
    "by": "nialse",
    "timeISO": "2025-08-08T19:07:13.000Z",
    "textPlain": "Striking up a voice chat with GPT-5 it starts by affirming my custom instructions/system prompt. Every time. Does not pass the vibe check.”Absolutely, happy to jump in. And you got it, I’ll keep it focused and straightforward.””Absolutely, and nice to have that context, thanks for sharing it. I’ll keep it focused and straightforward.”Anyone else have these issues?EDIT: This is the answer to me just saying the word hi.”Hello! Absolutely, I’m Arden, and I’m on board with that. We’ll keep it all straightforward and well-rounded. Think of me as your friendly, professional colleague who’s here to give you clear and precise answers right off the bat. Feel free to let me know what we’re tackling today.”",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44843085,
    "by": "energy123",
    "timeISO": "2025-08-09T00:36:26.000Z",
    "textPlain": "I've been using GPT-5 through the API and the response says 5000 tokens (+4000 for reasoning) but when I put the output through a local tokenizer in python it says 2000. I haven't put time into figuring out what's going on but has anyone noticed this? Are they using some new tokenizer?",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44841445,
    "by": "relantic",
    "timeISO": "2025-08-08T20:36:19.000Z",
    "textPlain": "Somewhat unsurprising to see the reactions to be closer to losing an old coworker than just deprecations / regressions: you miss humans not just for their performance but also their quirks.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840336,
    "by": "rob74",
    "timeISO": "2025-08-08T18:49:45.000Z",
    "textPlain": "> But if you’re already leaning on the model for life advice like this, having that capability taken away from you without warning could represent a sudden and unpleasant loss!Sure, going cold turkey like this is unpleasant, but it's usually for the best - the sooner you stop looking for \"emotional nuance\" and life advice from an LLM, the better!",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44844664,
    "by": "tomduncalf",
    "timeISO": "2025-08-09T07:12:21.000Z",
    "textPlain": "I enjoyed watching O3 do web searches etc. Seems that with GPT-5 you only get little summaries and it’s also way less web search happy which is a shame, O3 was so good for research",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44842825,
    "by": "redlampdesk",
    "timeISO": "2025-08-08T23:43:11.000Z",
    "textPlain": "Am skeptical of the need to get rid of the model picker.clunky Looking Product ≠ clunky ux",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840257,
    "by": "nafizh",
    "timeISO": "2025-08-08T18:41:10.000Z",
    "textPlain": "I still haven't got access to GPT-5 (plus user in US), and I am not really super looking forward to it given I would lose access to o3. o3 is a great reasoning and planning model (better than Claude Opus in planning IMO and cheaper) that I use in the UI as well as through API. I don't think OpenAI should force users to an advanced model if there is not a noticeable difference in capability. But I guess it saves them money? Someone posted on X how giving access to only GPT-5 and GPT-5 thinking reduces a plus user's overall weekly request rate.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44844522,
    "by": "lisp2240",
    "timeISO": "2025-08-09T06:36:01.000Z",
    "textPlain": "I anyone else annoyed by how frequently our lives are disrupted by impulsive decisions made by drug addicted CEOs?",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840142,
    "by": "andrewmcwatters",
    "timeISO": "2025-08-08T18:29:57.000Z",
    "textPlain": "This industry just keeps proving over and over again that if it's not open, or yours, you're building on shifting sand.It's a really bad cultural problem we have in software.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44843389,
    "by": "landswipe",
    "timeISO": "2025-08-09T01:42:35.000Z",
    "textPlain": "They've hit a wall, 5 is just an improved 4o.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840429,
    "by": "Oceoss",
    "timeISO": "2025-08-08T18:57:56.000Z",
    "textPlain": "I tried gpt 5 high with extended thinking and isnt bad \nI prefer opus 4.1 though, at least for now",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840096,
    "by": "binarymax",
    "timeISO": "2025-08-08T18:26:46.000Z",
    "textPlain": "This doesn't seem to be the case for me.  I have access to GPT-5 via chatgpt, and I can also use GPT-4o.  All my chat history opens with the originally used model as well.I'm not saying it's not happening - but perhaps the rollout didn't happen as expected.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840252,
    "by": "oh_my_goodness",
    "timeISO": "2025-08-08T18:40:22.000Z",
    "textPlain": "4o is for shit, but it's inconvenient to lose o3 with no warning. Good reminder that it was past time to keep multiple vendors in use.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44841529,
    "by": "daft_pink",
    "timeISO": "2025-08-08T20:45:43.000Z",
    "textPlain": "I switched from 4o to GPT 5 on raycast and I feel it is a lot slower to use 5 and contradicts his assertion.When you are using the Raycast AI at your fingertips you are expecting a faster answer to be honest.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840320,
    "by": "dmezzetti",
    "timeISO": "2025-08-08T18:47:52.000Z",
    "textPlain": "This thread is the best sales pitch for local / self-hosted models. With local, you have total control over when you decide to upgrade.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840921,
    "by": "KTibow",
    "timeISO": "2025-08-08T19:46:32.000Z",
    "textPlain": "This is also showing up on Xitter as the #keep4o movement, which some have criticized as being \"oneshotted\" or cases of LLM psychosis and emotional attachment.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840074,
    "by": "pphysch",
    "timeISO": "2025-08-08T18:23:59.000Z",
    "textPlain": "It's not totally surprising given the economics of LLM operation. LLMs, when idle, are much more resource-heavy than an idle web service. To achieve acceptable chat response latency, the models need to be already loaded in memory, and I doubt that these huge SotA models can go from cold start to inference in milliseconds or even seconds. OpenAI is incentivized to push as many users onto as few models as possible to manage the capacity and increase efficiency.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44843991,
    "by": "joshdavham",
    "timeISO": "2025-08-09T04:17:32.000Z",
    "textPlain": "Any chances of a GPT-5o?",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44842611,
    "by": "dusted",
    "timeISO": "2025-08-08T23:08:21.000Z",
    "textPlain": "Honestly, 4o was lame.. Its positivity was toxic and misleading, causing you to spiral into engagement about ideas that were crap.\nI often stopped after a few messages and asked o3 to review to conversation, almost every time it'd basically dismiss the entire ordeal with reasonable arguments.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840984,
    "by": "p0w3n3d",
    "timeISO": "2025-08-08T19:52:28.000Z",
    "textPlain": "running a model costs money. They probably removed 4o to make room (i.e. increase availability) for 5",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44842694,
    "by": "yieldcrv",
    "timeISO": "2025-08-08T23:20:09.000Z",
    "textPlain": "On r/localllama there is someone that got 120B OSS running on 8gb ram and 35 tokens/sec from the CPU (!!) after noticing 120B has a different architecture of only 5B “active” parametersThis makes it incredibly cheap to run on existing hardware, consumer off the shelf hardwareIts equally as likely that GPT 5 leverages a similar advancement in architecture, which would give them an order of magnitude more use of their existing hardware without being bottlenecked by GPU orders and TSMC",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840295,
    "by": "macawfish",
    "timeISO": "2025-08-08T18:45:02.000Z",
    "textPlain": "Meanwhile I'm stuck on 4o",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840700,
    "by": "resource_waste",
    "timeISO": "2025-08-08T19:24:28.000Z",
    "textPlain": "GPT5 is some sort of quantized model, its not SOTA.The trust that OpenAI would be SOTA has been shattered. They were among the best with o3/o4 and 4.5. This is a budget model and they rolled it out to everyone.I unsubscribed. Going to use Gemini, it was on-par with o3.",
    "parent": 44839842,
    "depth": 1
  },
  {
    "id": 44840012,
    "by": "riffic",
    "timeISO": "2025-08-08T18:19:00.000Z",
    "textPlain": "It's like everyone got a U2 album they didn't ask for, but instead of U2 they got Nickelback.",
    "parent": 44839842,
    "depth": 1
  }
]
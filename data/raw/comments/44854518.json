[
  {
    "id": 44885214,
    "by": "ordu",
    "timeISO": "2025-08-13T06:25:00.000Z",
    "textPlain": "> LLMs are not by themselves sufficient as a path to general machine intelligence; in some sense they are a distraction because of how far you can take them despite the approach being fundamentally incorrect.I don't believe that it is a fundamentally incorrect approach. I believe, that human mind does something like that all the time, the difference is our minds have some additional processes that can, for example, filter hallucinations.Kids at a specific age range are afraid of their imagination. Their imagination can place a monster into any dark place where nothing can be seen. Adult mind can do the same easily, but the difference is kids have difficulties distinguishing imagination and perception, while adult generally manage.I believe, the ability of human mind to see difference between imagination/hallucinations from one hand and perception and memory from the other is not a fundamental thing stemming from the architecture of brains but a learned skill. Moreover people can be tricked to acquire false memory[1]. If LLM fell to tricks of Elizabet Loftus, we'd say LLM hallucinated.What LLMs need is to learn some tricks to detect hallucinations. Probably they will not get 100% reliable detector, but to get to the level of humans they don't need 100% reliability.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44881849,
    "by": "ameliaquining",
    "timeISO": "2025-08-12T21:12:58.000Z",
    "textPlain": "One thing I appreciated about this post, unlike a lot of AI-skeptic posts, is that it actually makes a concrete falsifiable prediction; specifically, \"LLMs will never manage to deal with large code bases 'autonomously'\". So in the future we can look back and see whether it was right.For my part, I'd give 80% confidence that LLMs will be able to do this within two years, without fundamental architectural changes.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44857725,
    "by": "libraryofbabel",
    "timeISO": "2025-08-10T19:45:16.000Z",
    "textPlain": "This essay could probably benefit from some engagement with the literature on “interpretability” in LLMs, including the empirical results about how knowledge (like addition) is represented inside the neural network. To be blunt, I’m not sure being smart and reasoning from first principles after asking the LLM a lot of questions and cherry picking what it gets wrong gets to any novel insights at this point. And it already feels a little out date, with LLMs getting gold on the mathematical Olympiad they clearly have a pretty good world model of mathematics. I don’t think cherry-picking a failure to prove 2 + 2 = 4 in the particular specific way the writer wanted to see disproves that at all.LLMs have imperfect world models, sure. (So do humans.) That’s because they are trained to be generalists and because their internal representations of things are massively compressed single they don’t have enough weights to encode everything. I don’t think this means there are some natural limits to what they can do.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44859163,
    "by": "keeda",
    "timeISO": "2025-08-10T23:05:13.000Z",
    "textPlain": "That whole bit about color blending and transparency and LLMs \"not knowing colors\" is hard to believe. I am literally using LLMs every day to write image-processing and computer vision code using OpenCV. It seamlessly reasons across a range of concepts like color spaces, resolution, compression artifacts, filtering, segmentation and human perception. I mean, removing the alpha from a PNG image was a preprocessing step it wrote by itself as part of a larger task I had given it, so it certainly understands transparency.I even often describe the results e.g. \"this fails when in X manner when the image has grainy regions\" and it figures out what is going on, and adapts the code accordingly. (It works with uploading actual images too, but those consume a lot of tokens!)And all this in a rather niche domain that seems relatively less explored. The images I'm working with are rather small and low-resolution, which most literature does not seem to contemplate much. It uses standard techniques well known in the art, but it adapts and combines them well to suit my particular requirements. So they seem to handle \"novel\" pretty well too.If it can reason about images and vision and write working code for niche problems I throw at it, whether it \"knows\" colors in the human sense is a purely philosophical question.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44858108,
    "by": "ej88",
    "timeISO": "2025-08-10T20:40:31.000Z",
    "textPlain": "This article is interesting but pretty shallow.0(?): there’s no provided definition of what a ‘world model’ is. Is it playing chess? Is it remembering facts like how computers use math to blend Colors? If so, then ChatGPT: https://chatgpt.com/s/t_6898fe6178b88191a138fba8824c1a2c has a world model right?1. The author seems to conflate context windows with failing to model the world in the chess example. I challenge them to ask a SOTA model with an image of a chess board or notation and ask it about the position. It might not give you GM level analysis but it definitely has a model of what’s going on.2. Without explaining which LLM they used or sharing the chats these examples are just not valuable. The larger and better the model, the better its internal representation of the world.You can try it yourself. Come up with some question involving interacting with the world and / or physics and ask GPT-5 Thinking. It’s got a pretty good understanding of how things work!https://chatgpt.com/s/t_689903b03e6c8191b7ce1b85b1698358",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44885499,
    "by": "globular-toast",
    "timeISO": "2025-08-13T07:15:27.000Z",
    "textPlain": "I agree with the article. I will be very surprised if LLMs end up being \"it\". I say this as a language geek who has always been amazed how language drives our thinking. However, I think language exists between brains, not inside them. There's something else in us and LLMs aren't it.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44881959,
    "by": "frankfrank13",
    "timeISO": "2025-08-12T21:22:33.000Z",
    "textPlain": "Great quote at the end that I think I resonate a lot with:> Feeding these algorithms gobs of data is another example of how an approach that must be fundamentally incorrect at least in some sense, as evidenced by how data-hungry it is, can be taken very far by engineering efforts — as long as something is useful enough to fund such efforts and isn’t outcompeted by a new idea, it can persist.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44868332,
    "by": "o_nate",
    "timeISO": "2025-08-11T19:20:07.000Z",
    "textPlain": "What with this and your previous post about why sometimes incompetent management leads to better outcomes, you are quickly becoming one of my favorite tech bloggers. Perhaps I enjoyed the piece so much because your conclusions basically track mine. (I'm a software developer who has dabbled with LLMs, and has some hand-wavey background on how they work, but otherwise can claim no special knowledge.) Also your writing style really pops. No one would accuse your post of having been generated by an LLM.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44883404,
    "by": "DennisP",
    "timeISO": "2025-08-13T00:38:11.000Z",
    "textPlain": "Maybe pure language models aren't world models, but Genie 3 for example seems to be a pretty good world model:https://deepmind.google/discover/blog/genie-3-a-new-frontier...We also have multimodal AIs that can do both language and video. Genie 3 made multimodal with language might be pretty impressive.Focusing only on what pure language models can do is a bit of a straw man at this point.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44859779,
    "by": "skeledrew",
    "timeISO": "2025-08-11T00:49:25.000Z",
    "textPlain": "Agree in general with most of the points, except> but because I know you and I get by with less.Actually we got far more data and training than any LLM. We've been gathering and processing sensory data every second at least since birth (more processing than gathering when asleep), and are only really considered fully intelligent in our late teens to mid-20s.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44858098,
    "by": "lordnacho",
    "timeISO": "2025-08-10T20:38:12.000Z",
    "textPlain": "Here's what LLMs remind me of.When I went to uni, we had tutorials several times a week. Two students, one professor, going over whatever was being studied that week. The professor would ask insightful questions, and the students would try to answer.Sometimes, I would answer a question correctly without actually understanding what I was saying. I would be spewing out something that I had read somewhere in the huge pile of books, and it would be a sentence, with certain special words in it, that the professor would accept as an answer.But I would sometimes have this weird feeling of \"hmm I actually don't get it\" regardless. This is kinda what the tutorial is for, though. With a bit more prodding, the prof will ask something that you genuinely cannot produce a suitable word salad for, and you would be found out.In math-type tutorials it would be things like realizing some equation was useful for finding an answer without having a clue about what the equation actually represented.In economics tutorials it would be spewing out words about inflation or growth or some particular author but then having nothing to back up the intuition.This is what I suspect LLMs do. They can often be very useful to someone who actually has the models in their minds, but not the data to hand. You may have forgotten the supporting evidence for some position, or you might have missed some piece of the argument due to imperfect memory. In these cases, LLM is fantastic as it just glues together plausible related words for you to examine.The wheels come off when you're not an expert. Everything it says will sound plausible. When you challenge it, it just apologizes and pretends to correct itself.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44857920,
    "by": "deadbabe",
    "timeISO": "2025-08-10T20:11:08.000Z",
    "textPlain": "Don’t: use LLMs to play chess against youDo: use LLMs to talk shit to you while a real chess AI plays chess against you.The above applies to a lot of things besides chess, and illustrates a proper application of LLMs.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44881949,
    "by": "bithive123",
    "timeISO": "2025-08-12T21:21:36.000Z",
    "textPlain": "Language models aren't world models for the same reason languages aren't world models.Symbols, by definition, only represent a thing.  They are not the same as the thing.  The map is not the territory, the description is not the described, you can't get wet in the word \"water\".They only have meaning to sentient beings, and that meaning is heavily subjective and contextual.But there appear to be some who think that we can grasp truth through mechanical symbol manipulation.  Perhaps we just need to add a few million more symbols, they think.If we accept the incompleteness theorem, then there are true propositions that even a super-intelligent AGI would not be able to express, because all it can do is output a series of placeholders.  Not to mention the obvious fallacy of knowing super-intelligence when we see it.  Can you write a test suite for it?",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44857965,
    "by": "imenani",
    "timeISO": "2025-08-10T20:17:12.000Z",
    "textPlain": "As far as I can tell they don’t say which LLM they used which is kind of a shame as there is a huge range of capabilities even in newly released LLMs (e.g. reasoning vs not).",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44857369,
    "by": "t0md4n",
    "timeISO": "2025-08-10T18:58:20.000Z",
    "textPlain": "https://arxiv.org/abs/2501.17186",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44858343,
    "by": "jonplackett",
    "timeISO": "2025-08-10T21:13:31.000Z",
    "textPlain": "I just tried a few things that are simple and a world model would probably get right. EgQuestion to GPT5:\nI am looking straight on to some objects. Looking parallel to the ground.In front of me I have a milk bottle, to the right of that is a Coca-Cola bottle. To the right of that is a glass of water. And to the right of that there’s a cherry. Behind the cherry there’s a cactus and to the left of that there’s a peanut. Everything is spaced evenly. Can I see the peanut?Answer (after choosing thinking mode)No.\nThe cactus is directly behind the cherry (front row order: milk, Coke, water, cherry). “To the left of that” puts the peanut behind the glass of water. Since you’re looking straight on, the glass sits in front and occludes the peanut.It doesn’t consider transparency until you mention it, then apologises and says it didn’t think of transparency",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44858050,
    "by": "GaggiX",
    "timeISO": "2025-08-10T20:28:42.000Z",
    "textPlain": "https://www.youtube.com/watch?v=LtG0ACIbmHwSota LLMs do play legal moves in chess, I don't why the article seem to say otherwise.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44858002,
    "by": "og_kalu",
    "timeISO": "2025-08-10T20:21:45.000Z",
    "textPlain": "Yes LLMs can play chess and yes they can model it finehttps://arxiv.org/pdf/2403.15498v2",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44857872,
    "by": "rishi_devan",
    "timeISO": "2025-08-10T20:04:28.000Z",
    "textPlain": "Haha. I enjoyed that Soviet-era joke at the end.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44883183,
    "by": "UltraSane",
    "timeISO": "2025-08-12T23:57:40.000Z",
    "textPlain": "I wonder how the nature of the language used to train an LLM affects its model of the world. Would a language designed for the maximum possible information content and clarity like Ithkuil make an LLMs world model more accurate?",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44858689,
    "by": "Razengan",
    "timeISO": "2025-08-10T22:01:30.000Z",
    "textPlain": "A slight tangent: I think/wonder if the one place where AIs could be really useful, might be in translating alien languages :)As in, an alien could teach one of our AIs their language faster than an alien could teach an human, and vice versa....though the potential for catastrophic disasters is also great there lol",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44882004,
    "by": "1970-01-01",
    "timeISO": "2025-08-12T21:28:32.000Z",
    "textPlain": "I'm surprised the models haven't been enshittified by capitalism. I think in a few years we're going to see lightning-fast LLMs generating better output compared to what we're seeing today. But it won't be 1000x better, it will be 10x better, 10x faster, and completely enshittified with ads and clickbait links. Enjoy ChatGPT while it lasts.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44881712,
    "by": "neuroelectron",
    "timeISO": "2025-08-12T20:55:13.000Z",
    "textPlain": "Not yet",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44885365,
    "by": "TazeTSchnitzel",
    "timeISO": "2025-08-13T06:54:58.000Z",
    "textPlain": "I have recently lived through something called a psychotic break, which was an unimaginably horrible thing, but it did let me see from the inside what insanity does to your thinking. And what's fascinating, coming out the other side of this, is how similar LLMs are to someone in psychosis. Someone in psychosis can have all the ability LLMs have to recognise patterns and sound like they know what they're talking about, but their brain is not working well enough to have proper self-insight, to be able to check their thoughts actually fully make sense. (And “making sense” turns out to be a sliding scale — it is not as if you just wake up one day suddenly fully rational again, there's a sliding scale of irrational thinking and you have to gradually re-process your older thoughts into more and more coherent shapes as your brain starts to work more correctly again.) I believe this isn't actually a novel insight either, many have worried about this for years! Psychosis might be an interesting topic to read about if you want to get another angle to understand the AI models from. I won't claim that it's exactly the same thing, but I will say that most people probably have a very undeveloped idea of what mental illness actually is or how it works, and that leaves them badly prepared for interacting with a machine that has a strong resemblance to a mentally ill person who's learned to pretend to be normal.",
    "parent": 44885214,
    "depth": 2
  },
  {
    "id": 44885359,
    "by": "mac-mc",
    "timeISO": "2025-08-13T06:54:10.000Z",
    "textPlain": "Do you have many memories of that time, around 3 to 5, and remember what your cognitive processes were?When the child is afraid of the monster in the dark, they are not literally visually hallucinating a beast in the dark; they are worried that there could be a beast in the dark, and they are not sure that there is due to a lack of sensory information confirming a lack of the monster.  They are not being hyper precise because they are 3, so they say \"there is a monster under my bed\"!   Children have instincts to be afraid of the dark.Similarly with imaginary friends and play, it's an instinct to practice through smaller stakes simulations. When they are emotionally attached to their imaginary friends, it's much like they are emotionally attached to their security blanket.  They know that the \"friend\" is not perceptible.It's much like the projected anxieties of adults or teenagers, who are worried that everyone thinks they are super lame and thus act like people do, because on the balance of no information, they choose the \"safer path\".That is pretty different than the hallucinations of LLMs IMO.",
    "parent": 44885214,
    "depth": 2
  },
  {
    "id": 44885334,
    "by": "shkkmo",
    "timeISO": "2025-08-13T06:48:43.000Z",
    "textPlain": "> If LLM fell to tricks of Elizabet Loftus, we'd say LLM hallucinated.She's strongly oversold how and when false memories can be created. She testified in defense of Ghislaine Maxwell at her 2021 trial that financial incentives can create false memories and only later admitted that there were no studies to back this up when directly questioned.She's spent a career over-generalizing data about implanting false minor memories to make money discrediting victims' traumatic memories and defend abusers.You conflate \"hallucination\" with \"imagination\" but the former has much more in common with lieing than it does with imagining.",
    "parent": 44885214,
    "depth": 2
  },
  {
    "id": 44885449,
    "by": "otabdeveloper4",
    "timeISO": "2025-08-13T07:07:12.000Z",
    "textPlain": "> I believe, that human mind does something like that all the timeAbsolutely not. Human brains have online one-shot training. LLMs weights are fixed and fine-tuning them is a huge multi-year enterprise.Fundamentally it's two completely different architectures.",
    "parent": 44885214,
    "depth": 2
  },
  {
    "id": 44881944,
    "by": "moduspol",
    "timeISO": "2025-08-12T21:21:15.000Z",
    "textPlain": "\"Deal with\" and \"autonomously\" are doing a lot of heavy lifting there. Cursor already does a pretty good job indexing all the files in a code base in a way that lets it ask questions and get answers pretty quickly. It's just a matter of where you set the goalposts.",
    "parent": 44881849,
    "depth": 2
  },
  {
    "id": 44882276,
    "by": "shinycode",
    "timeISO": "2025-08-12T22:00:23.000Z",
    "textPlain": "« autonomously » what happens when subtle updates that are not bugs but change the meaning of some features that might break the workflow on some other external parts of a client’s system ? It happens all the time and, because it’s really hard to have the whole meaning and business rules written and maintained up to date, an LLM might never be able to grasp some meaning. \n  Maybe if instead of developing code and infrastructures, the whole industry shifts toward only writing impossibly precise spec sheets that make meaning and intent crystal clear then, maybe « autonomously » might be possible to pull off",
    "parent": 44881849,
    "depth": 2
  },
  {
    "id": 44884126,
    "by": "whoknowsidont",
    "timeISO": "2025-08-13T02:34:10.000Z",
    "textPlain": "I don't think that statement is falsifiable until you define \"deal with\" and \"large code bases.\"",
    "parent": 44881849,
    "depth": 2
  },
  {
    "id": 44885015,
    "by": "Mars008",
    "timeISO": "2025-08-13T05:46:57.000Z",
    "textPlain": "In two years there will be probably no new 'autonomous' LLMs. They will be most likely integrated into 'products', trained and designed for this. We see the beginning of it today as agents and tools.",
    "parent": 44881849,
    "depth": 2
  },
  {
    "id": 44884895,
    "by": "p1necone",
    "timeISO": "2025-08-13T05:23:15.000Z",
    "textPlain": "That feels like a statement that's far too loosely defined to be meaningful to me.I work on codebases that you could describe as 'large', and you could describe some of the LLM driven work being done on them as 'autonomous' today.",
    "parent": 44881849,
    "depth": 2
  },
  {
    "id": 44885461,
    "by": "otabdeveloper4",
    "timeISO": "2025-08-13T07:08:57.000Z",
    "textPlain": "> LLMs will never manage to deal with large code bases 'autonomously'Absolutely nothing about that statement is concrete or falsifiable.Hell, you can already deal with large code bases 'autonomously' without LLMs - grep and find and sed goes a long way!",
    "parent": 44881849,
    "depth": 2
  },
  {
    "id": 44882146,
    "by": "exe34",
    "timeISO": "2025-08-12T21:45:01.000Z",
    "textPlain": "How large? What does \"deal\" mean here? Autonomously - is that on its own whim, or at the behest of a user?",
    "parent": 44881849,
    "depth": 2
  },
  {
    "id": 44882403,
    "by": "slt2021",
    "timeISO": "2025-08-12T22:14:21.000Z",
    "textPlain": ">LLMs will never manage to dealtime to prove hypothesis: infinity years",
    "parent": 44881849,
    "depth": 2
  },
  {
    "id": 44857798,
    "by": "yosefk",
    "timeISO": "2025-08-10T19:56:09.000Z",
    "textPlain": "Your being blunt is actually very kind, if you're describing what I'm doing as \"being smart and reasoning from first principles\"; and I agree that I am not  saying something very novel, at most it's slightly  contrarian given the current sentiment.My goal is not to cherry-pick failures for its own sake as much as to try to explain why I get pretty bad output from LLMs much of the time, which I do. They are also very useful to me at times.Let's see how my predictions hold up; I have made enough to look very wrong if they don't.Regarding \"failure disproving success\": it can't, but it can disprove a theory of how this success is achieved. And, I have much better examples than the 2+2=4, which I am citing as something that sorta works these says",
    "parent": 44857725,
    "depth": 2
  },
  {
    "id": 44857881,
    "by": "AyyEye",
    "timeISO": "2025-08-10T20:05:47.000Z",
    "textPlain": "With LLMs being unable to count how many Bs are in blueberry, they clearly don't have any world model whatsoever. That addition (something which only takes a few gates in digital logic) happens to be overfit into a few nodes on multi-billion node networks is hardly a surprise to anyone except the most religious of AI believers.",
    "parent": 44857725,
    "depth": 2
  },
  {
    "id": 44884461,
    "by": "joe_the_user",
    "timeISO": "2025-08-13T03:42:16.000Z",
    "textPlain": "I think both the literature on interpretability and explorations on internal representations actually reinforce the author's conclusion. I think internal representation research tends to nets that deal with a single \"model\" don't necessary have the same representation and don't necessarily have a single representation.And doing well on XYZ isn't evidence of a world model in particular. The point that these things aren't always using a world is reinforced by systems being easily confused by extraneous information, even systems as sophisticated as thus that can solve Math Olympiad questions. The literature has said \"ad-hoc predictors\" for a long time and I don't think much has changed - except things do better on benchmarks.And, humans too can act without a consistent world model.",
    "parent": 44857725,
    "depth": 2
  },
  {
    "id": 44858751,
    "by": "lossolo",
    "timeISO": "2025-08-10T22:12:09.000Z",
    "textPlain": "https://arxiv.org/abs/2508.01191",
    "parent": 44857725,
    "depth": 2
  },
  {
    "id": 44857796,
    "by": "armchairhacker",
    "timeISO": "2025-08-10T19:56:02.000Z",
    "textPlain": "Any suggestions from this literature?",
    "parent": 44857725,
    "depth": 2
  },
  {
    "id": 44881739,
    "by": "geraneum",
    "timeISO": "2025-08-12T20:57:48.000Z",
    "textPlain": "> it wrote by itself as part of a larger task I had given it, so it certainly understands transparencyOr it’s a common step or a known pattern or combination of steps that is prevalent in its training data for certain input. I’m guessing you don’t know what’s exactly in the training sets. I don’t know either. They don’t tell ;)> but it adapts and combines them well to suit my particular requirements. So they seem to handle \"novel\" pretty well too.We tend to overestimate the novelty of our own work and our methods and at the same time, underestimate the vastness of the data and information available online for machines to train on. LLMs are very sophisticated pattern recognizers. It doesn’t mean what you are doing specifically is done in this exact way before, rather the patterns adapted and the approach may not be one of their kind.> is a purely philosophical questionIt is indeed. A question we need to ask ourselves.",
    "parent": 44859163,
    "depth": 2
  },
  {
    "id": 44858360,
    "by": "yosefk",
    "timeISO": "2025-08-10T21:15:22.000Z",
    "textPlain": "A \"world model\" depends on the context which defines which world the problem is in. For chess, which moves are legal and needing to know where the pieces are to make legal moves are parts of the world model. For alpha blending, it being a mathematical operation and the visibility of a background given the transparency of the foreground are parts of the world model.The examples are from all the major commercial American LLMs as listed in a sister comment.You seem to conflate context windows with tracking chess pieces. The context windows are more than large enough to remember 10 moves. The model should either track the pieces, or mention that it would be playing blindfold chess absent a board to look at and it isn't good at this, so could you please list the position after every move to make it fair, or it doesn't know what it's doing; it's demonstrably the latter.",
    "parent": 44858108,
    "depth": 2
  },
  {
    "id": 44868591,
    "by": "yosefk",
    "timeISO": "2025-08-11T19:43:30.000Z",
    "textPlain": "thank you for your kind words!",
    "parent": 44868332,
    "depth": 2
  },
  {
    "id": 44882295,
    "by": "helloplanets",
    "timeISO": "2025-08-12T22:02:39.000Z",
    "textPlain": "Don't forget the millions of years of pre-training! ;)",
    "parent": 44859779,
    "depth": 2
  },
  {
    "id": 44884601,
    "by": "nhaehnle",
    "timeISO": "2025-08-13T04:16:21.000Z",
    "textPlain": "Good on you for having the meta-cognition to recognize it.I've graded many exams in my university days (and set some myself), and it's exceedingly obvious that that's what many students are doing. I do wonder though how often they manage to fly under the radar. I'm sure it happens, as you described.(This is also the reason why I strongly believe that in exams where students write free-form answers, points should be subtracted for incorrect statements even if a correct solution is somewhere in the word salad.)",
    "parent": 44858098,
    "depth": 2
  },
  {
    "id": 44882956,
    "by": "roywiggins",
    "timeISO": "2025-08-12T23:25:04.000Z",
    "textPlain": "> When you challenge it, it just apologizes and pretends to correct itself.Even when it was right the first time!",
    "parent": 44858098,
    "depth": 2
  },
  {
    "id": 44883781,
    "by": "Seb-C",
    "timeISO": "2025-08-13T01:37:22.000Z",
    "textPlain": "Are you suggesting that we use an LLM as an interface between the AI and the player?Why would anyone choose to awkwardly play using natural language rather than a reliable, fast and intuitive UI?",
    "parent": 44857920,
    "depth": 2
  },
  {
    "id": 44882077,
    "by": "habitue",
    "timeISO": "2025-08-12T21:37:14.000Z",
    "textPlain": "> Symbols, by definition, only represent a thing.This is missing the lesson of the Yoneda Lemma: symbols are uniquely identified by their relationships with other symbols. If those relationships are represented in text, then in principle they can be inferred and navigated by an LLM.Some relationships are not represented well in text: tacit knowledge like how hard to twist a bottle cap to get it to come off, etc. We aren't capturing those relationships between all your individual muscles and your brain well in language, so an LLM will miss them or have very approximate versions of them, but... that's always been the problem with tacit knowledge: it's the exact kind of knowledge that's hard to communicate!",
    "parent": 44881949,
    "depth": 2
  },
  {
    "id": 44882426,
    "by": "pron",
    "timeISO": "2025-08-12T22:17:44.000Z",
    "textPlain": "> Symbols, by definition, only represent a thing. They are not the same as the thingFirst of all, the point isn't about the map becoming the territory, but about whether LLMs can form a map that's similar to the map in our brains.But to your philosophical point, assuming there are only a finite number of things and places in the universe - or at least the part of which we care about - why wouldn't they be representable with a finite set of symbols?What you're rejecting is the Church-Turing thesis [1] (essentially, that all mechanical processes, including that of nature, can be simulated with symbolic computation, although there are weaker and stronger variants). It's okay to reject it, but you should know that not many people do (even some non-orthodox thoughts by Penrose about the brain not being simulatable by an ordinary digital computer still accept that some physical machine - the brain - is able to represent what we're interested in).> If we accept the incompleteness theoremThere is no if there. It's a theorem. But it's completely irrelevant. It means that there are mathematical propositions that can't be proven or disproven by some system of logic, i.e. by some mechanical means. But if something is in the universe, then it's already been proven by some mechanical process: the mechanics of nature. That means that if some finite set of symbols could represent the laws of nature, then anything in nature can be proven in that logical system.\nWhich brings us back to the first point: the only way the mechanics of nature cannot be represented by symbols is if they are somehow infinite, i.e. they don't follow some finite set of laws. In other words - there is no physics. Now, that may be true, but if that's the case, then AI is the least of our worries.Of course, if physics does exist - i.e. the universe is governed by a finite set of laws - that doesn't mean that we can predict the future, as that would entail both measuring things precisely and simulating them fast",
    "parent": 44881949,
    "depth": 2
  },
  {
    "id": 44882262,
    "by": "auggierose",
    "timeISO": "2025-08-12T21:57:44.000Z",
    "textPlain": "First: true propositions (that are not provable) can definitely be expressed, if they couldn't, the incompleteness theorem would not be true ;-)It would be interesting to know what the percentage of people is, who invoke the incompleteness theorem, and have no clue what it actually says.Most people don't even know what a proof is, so that cannot be a hindrance on the path to AGI ...Second: ANY world model that can be digitally represented would be subject to the same argument (if stated correctly), not only LLMs.",
    "parent": 44881949,
    "depth": 2
  }
]
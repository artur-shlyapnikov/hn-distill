[
  {
    "id": 44868332,
    "by": "o_nate",
    "timeISO": "2025-08-11T19:20:07.000Z",
    "textPlain": "What with this and your previous post about why sometimes incompetent management leads to better outcomes, you are quickly becoming one of my favorite tech bloggers. Perhaps I enjoyed the piece so much because your conclusions basically track mine. (I'm a software developer who has dabbled with LLMs, and has some hand-wavey background on how they work, but otherwise can claim no special knowledge.) Also your writing style really pops. No one would accuse your post of having been generated by an LLM.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44857725,
    "by": "libraryofbabel",
    "timeISO": "2025-08-10T19:45:16.000Z",
    "textPlain": "This essay could probably benefit from some engagement with the literature on “interpretability” in LLMs, including the empirical results about how knowledge (like addition) is represented inside the neural network. To be blunt, I’m not sure being smart and reasoning from first principles after asking the LLM a lot of questions and cherry picking what it gets wrong gets to any novel insights at this point. And it already feels a little out date, with LLMs getting gold on the mathematical Olympiad they clearly have a pretty good world model of mathematics. I don’t think cherry-picking a failure to prove 2 + 2 = 4 in the particular specific way the writer wanted to see disproves that at all.LLMs have imperfect world models, sure. (So do humans.) That’s because they are trained to be generalists and because their internal representations of things are massively compressed single they don’t have enough weights to encode everything. I don’t think this means there are some natural limits to what they can do.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44859163,
    "by": "keeda",
    "timeISO": "2025-08-10T23:05:13.000Z",
    "textPlain": "That whole bit about color blending and transparency and LLMs \"not knowing colors\" is hard to believe. I am literally using LLMs every day to write image-processing and computer vision code using OpenCV. It seamlessly reasons across a range of concepts like color spaces, resolution, compression artifacts, filtering, segmentation and human perception. I mean, removing the alpha from a PNG image was a preprocessing step it wrote by itself as part of a larger task I had given it, so it certainly understands transparency.I even often describe the results e.g. \"this fails when in X manner when the image has grainy regions\" and it figures out what is going on, and adapts the code accordingly. (It works with uploading actual images too, but those consume a lot of tokens!)And all this in a rather niche domain that seems relatively less explored. The images I'm working with are rather small and low-resolution, which most literature does not seem to contemplate much. It uses standard techniques well known in the art, but it adapts and combines them well to suit my particular requirements. So they seem to handle \"novel\" pretty well too.If it can reason about images and vision and write working code for niche problems I throw at it, whether it \"knows\" colors in the human sense is a purely philosophical question.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44857920,
    "by": "deadbabe",
    "timeISO": "2025-08-10T20:11:08.000Z",
    "textPlain": "Don’t: use LLMs to play chess against youDo: use LLMs to talk shit to you while a real chess AI plays chess against you.The above applies to a lot of things besides chess, and illustrates a proper application of LLMs.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44858108,
    "by": "ej88",
    "timeISO": "2025-08-10T20:40:31.000Z",
    "textPlain": "This article is interesting but pretty shallow.0(?): there’s no provided definition of what a ‘world model’ is. Is it playing chess? Is it remembering facts like how computers use math to blend Colors? If so, then ChatGPT: https://chatgpt.com/s/t_6898fe6178b88191a138fba8824c1a2c has a world model right?1. The author seems to conflate context windows with failing to model the world in the chess example. I challenge them to ask a SOTA model with an image of a chess board or notation and ask it about the position. It might not give you GM level analysis but it definitely has a model of what’s going on.2. Without explaining which LLM they used or sharing the chats these examples are just not valuable. The larger and better the model, the better its internal representation of the world.You can try it yourself. Come up with some question involving interacting with the world and / or physics and ask GPT-5 Thinking. It’s got a pretty good understanding of how things work!https://chatgpt.com/s/t_689903b03e6c8191b7ce1b85b1698358",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44858098,
    "by": "lordnacho",
    "timeISO": "2025-08-10T20:38:12.000Z",
    "textPlain": "Here's what LLMs remind me of.When I went to uni, we had tutorials several times a week. Two students, one professor, going over whatever was being studied that week. The professor would ask insightful questions, and the students would try to answer.Sometimes, I would answer a question correctly without actually understanding what I was saying. I would be spewing out something that I had read somewhere in the huge pile of books, and it would be a sentence, with certain special words in it, that the professor would accept as an answer.But I would sometimes have this weird feeling of \"hmm I actually don't get it\" regardless. This is kinda what the tutorial is for, though. With a bit more prodding, the prof will ask something that you genuinely cannot produce a suitable word salad for, and you would be found out.In math-type tutorials it would be things like realizing some equation was useful for finding an answer without having a clue about what the equation actually represented.In economics tutorials it would be spewing out words about inflation or growth or some particular author but then having nothing to back up the intuition.This is what I suspect LLMs do. They can often be very useful to someone who actually has the models in their minds, but not the data to hand. You may have forgotten the supporting evidence for some position, or you might have missed some piece of the argument due to imperfect memory. In these cases, LLM is fantastic as it just glues together plausible related words for you to examine.The wheels come off when you're not an expert. Everything it says will sound plausible. When you challenge it, it just apologizes and pretends to correct itself.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44858343,
    "by": "jonplackett",
    "timeISO": "2025-08-10T21:13:31.000Z",
    "textPlain": "I just tried a few things that are simple and a world model would probably get right. EgQuestion to GPT5:\nI am looking straight on to some objects. Looking parallel to the ground.In front of me I have a milk bottle, to the right of that is a Coca-Cola bottle. To the right of that is a glass of water. And to the right of that there’s a cherry. Behind the cherry there’s a cactus and to the left of that there’s a peanut. Everything is spaced evenly. Can I see the peanut?Answer (after choosing thinking mode)No.\nThe cactus is directly behind the cherry (front row order: milk, Coke, water, cherry). “To the left of that” puts the peanut behind the glass of water. Since you’re looking straight on, the glass sits in front and occludes the peanut.It doesn’t consider transparency until you mention it, then apologises and says it didn’t think of transparency",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44857965,
    "by": "imenani",
    "timeISO": "2025-08-10T20:17:12.000Z",
    "textPlain": "As far as I can tell they don’t say which LLM they used which is kind of a shame as there is a huge range of capabilities even in newly released LLMs (e.g. reasoning vs not).",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44859779,
    "by": "skeledrew",
    "timeISO": "2025-08-11T00:49:25.000Z",
    "textPlain": "Agree in general with most of the points, except> but because I know you and I get by with less.Actually we got far more data and training than any LLM. We've been gathering and processing sensory data every second at least since birth (more processing than gathering when asleep), and are only really considered fully intelligent in our late teens to mid-20s.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44857369,
    "by": "t0md4n",
    "timeISO": "2025-08-10T18:58:20.000Z",
    "textPlain": "https://arxiv.org/abs/2501.17186",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44858002,
    "by": "og_kalu",
    "timeISO": "2025-08-10T20:21:45.000Z",
    "textPlain": "Yes LLMs can play chess and yes they can model it finehttps://arxiv.org/pdf/2403.15498v2",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44857872,
    "by": "rishi_devan",
    "timeISO": "2025-08-10T20:04:28.000Z",
    "textPlain": "Haha. I enjoyed that Soviet-era joke at the end.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44858050,
    "by": "GaggiX",
    "timeISO": "2025-08-10T20:28:42.000Z",
    "textPlain": "https://www.youtube.com/watch?v=LtG0ACIbmHwSota LLMs do play legal moves in chess, I don't why the article seem to say otherwise.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44858689,
    "by": "Razengan",
    "timeISO": "2025-08-10T22:01:30.000Z",
    "textPlain": "A slight tangent: I think/wonder if the one place where AIs could be really useful, might be in translating alien languages :)As in, an alien could teach one of our AIs their language faster than an alien could teach an human, and vice versa....though the potential for catastrophic disasters is also great there lol",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44868591,
    "by": "yosefk",
    "timeISO": "2025-08-11T19:43:30.000Z",
    "textPlain": "thank you for your kind words!",
    "parent": 44868332,
    "depth": 2
  },
  {
    "id": 44857798,
    "by": "yosefk",
    "timeISO": "2025-08-10T19:56:09.000Z",
    "textPlain": "Your being blunt is actually very kind, if you're describing what I'm doing as \"being smart and reasoning from first principles\"; and I agree that I am not  saying something very novel, at most it's slightly  contrarian given the current sentiment.My goal is not to cherry-pick failures for its own sake as much as to try to explain why I get pretty bad output from LLMs much of the time, which I do. They are also very useful to me at times.Let's see how my predictions hold up; I have made enough to look very wrong if they don't.Regarding \"failure disproving success\": it can't, but it can disprove a theory of how this success is achieved. And, I have much better examples than the 2+2=4, which I am citing as something that sorta works these says",
    "parent": 44857725,
    "depth": 2
  },
  {
    "id": 44857881,
    "by": "AyyEye",
    "timeISO": "2025-08-10T20:05:47.000Z",
    "textPlain": "With LLMs being unable to count how many Bs are in blueberry, they clearly don't have any world model whatsoever. That addition (something which only takes a few gates in digital logic) happens to be overfit into a few nodes on multi-billion node networks is hardly a surprise to anyone except the most religious of AI believers.",
    "parent": 44857725,
    "depth": 2
  },
  {
    "id": 44857796,
    "by": "armchairhacker",
    "timeISO": "2025-08-10T19:56:02.000Z",
    "textPlain": "Any suggestions from this literature?",
    "parent": 44857725,
    "depth": 2
  },
  {
    "id": 44858751,
    "by": "lossolo",
    "timeISO": "2025-08-10T22:12:09.000Z",
    "textPlain": "https://arxiv.org/abs/2508.01191",
    "parent": 44857725,
    "depth": 2
  },
  {
    "id": 44858360,
    "by": "yosefk",
    "timeISO": "2025-08-10T21:15:22.000Z",
    "textPlain": "A \"world model\" depends on the context which defines which world the problem is in. For chess, which moves are legal and needing to know where the pieces are to make legal moves are parts of the world model. For alpha blending, it being a mathematical operation and the visibility of a background given the transparency of the foreground are parts of the world model.The examples are from all the major commercial American LLMs as listed in a sister comment.You seem to conflate context windows with tracking chess pieces. The context windows are more than large enough to remember 10 moves. The model should either track the pieces, or mention that it would be playing blindfold chess absent a board to look at and it isn't good at this, so could you please list the position after every move to make it fair, or it doesn't know what it's doing; it's demonstrably the latter.",
    "parent": 44858108,
    "depth": 2
  },
  {
    "id": 44876183,
    "by": "optimalsolver",
    "timeISO": "2025-08-12T13:50:26.000Z",
    "textPlain": "Gemini 2.5 Pro gets this correct on the first attempt, and specifically points out the transparency of the glass of water.https://g.co/gemini/share/362506056ddbTime to get the ol' goalpost-moving gloves out.",
    "parent": 44858343,
    "depth": 2
  },
  {
    "id": 44858404,
    "by": "RugnirViking",
    "timeISO": "2025-08-10T21:21:14.000Z",
    "textPlain": "this seems like a strange riddle. In my mind I was thinking that regardless of the glass, all of the objects can be seen (due to perspective, and also the fact you mentioned the locations, meaning you're aware of them).It seems to me it would only actually work in an orthographic perspective, which is not how our reality works",
    "parent": 44858343,
    "depth": 2
  },
  {
    "id": 44858013,
    "by": "yosefk",
    "timeISO": "2025-08-10T20:22:41.000Z",
    "textPlain": "ChatGPT, Claude, Grok and Google AI Overviews, whatever powers the latter, were all used in one or more of these examples, in various configurations. I think they can perform differently, and I often try more than one when the 1st try doesn't work great. I don't think there's any fundamental difference in the principle of their operation, and I think there never will be - there will be another major breakthrough",
    "parent": 44857965,
    "depth": 2
  },
  {
    "id": 44858068,
    "by": "lowsong",
    "timeISO": "2025-08-10T20:30:43.000Z",
    "textPlain": "It doesn't matter. These limitations are fundamental to LLMs, so all of them that will ever be made suffer from these problems.",
    "parent": 44857965,
    "depth": 2
  },
  {
    "id": 44857493,
    "by": "yosefk",
    "timeISO": "2025-08-10T19:16:52.000Z",
    "textPlain": "This is interesting. The \"professional level\" rating of <1800 isn't, but still.However:\"A significant Elo rating jump occurs when the model’s Legal Move accuracy reaches\n99.8%. This increase is due to the reduction in errors after the model learns to generate legal moves,\nreinforcing that continuous error correction and\nlearning the correct moves significantly improve ELO\"You should be able to reach the move legality of around 100% with few resources spent on it. Failing to do so means that it has not learned a model of what chess is, at some basic level. There is virtually no challenge in making legal moves.",
    "parent": 44857369,
    "depth": 2
  },
  {
    "id": 44857931,
    "by": "svantana",
    "timeISO": "2025-08-10T20:13:44.000Z",
    "textPlain": "Yes, I hadn't heard that before. It's similar in spirit to this norwegian folk tale about a deaf man guessing what someone is saying to him:https://en.wikipedia.org/wiki/%22Good_day,_fellow!%22_%22Axe...",
    "parent": 44857872,
    "depth": 2
  },
  {
    "id": 44858087,
    "by": "tickettotranai",
    "timeISO": "2025-08-10T20:36:13.000Z",
    "textPlain": "Technically yes, but... it's moderately tricky to get an LLM to play good chess even though it can.https://dynomight.net/more-chess/This is significant in general because I personally would love to get these things to code-switch into \"hackernews poster\" or \"writer for the Economist\" or \"academic philosopher\", but I think the \"chat\" format makes it impossible. The inaccessibility of this makes me want to host my own LLM...",
    "parent": 44858050,
    "depth": 2
  }
]
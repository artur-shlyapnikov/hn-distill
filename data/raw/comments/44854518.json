[
  {
    "id": 44857965,
    "by": "imenani",
    "timeISO": "2025-08-10T20:17:12.000Z",
    "textPlain": "As far as I can tell they don’t say which LLM they used which is kind of a shame as there is a huge range of capabilities even in newly released LLMs (e.g. reasoning vs not).",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44858002,
    "by": "og_kalu",
    "timeISO": "2025-08-10T20:21:45.000Z",
    "textPlain": "Yes LLMs can play chess and yes they can model it finehttps://arxiv.org/pdf/2403.15498v2",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44857725,
    "by": "libraryofbabel",
    "timeISO": "2025-08-10T19:45:16.000Z",
    "textPlain": "This essay could probably benefit from some engagement with the literature on “interpretability” in LLMs, including the empirical results about how knowledge (like addition) is represented inside the neural network. To be blunt, I’m not sure being smart and reasoning from first principles after asking the LLM a lot of questions and cherry picking what it gets wrong gets to any novel insights at this point. And it already feels a little out date, with LLMs getting gold on the mathematical Olympiad they clearly have a pretty good world model of mathematics. I don’t think cherry-picking a failure to prove 2 + 2 = 4 in the particular specific way the writer wanted to see disproves that at all.LLMs have imperfect world models, sure. (So do humans.) That’s because they are trained to be generalists and because their internal representations of things are massively compressed single they don’t have enough weights to encode everything. I don’t think this means there are some natural limits to what they can do.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44857872,
    "by": "rishi_devan",
    "timeISO": "2025-08-10T20:04:28.000Z",
    "textPlain": "Haha. I enjoyed that Soviet-era joke at the end.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44857920,
    "by": "deadbabe",
    "timeISO": "2025-08-10T20:11:08.000Z",
    "textPlain": "Don’t: use LLMs to play chess against youDo: use LLMs to talk shit to you while a real chess AI plays chess against you.The above applies to a lot of things besides chess, and illustrates a proper application of LLMs.",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44857369,
    "by": "t0md4n",
    "timeISO": "2025-08-10T18:58:20.000Z",
    "textPlain": "https://arxiv.org/abs/2501.17186",
    "parent": 44854518,
    "depth": 1
  },
  {
    "id": 44858013,
    "by": "yosefk",
    "timeISO": "2025-08-10T20:22:41.000Z",
    "textPlain": "ChatGPT, Claude, Grok and Google AI Overviews, whatever powers the latter, were all used in one or more of these examples, in various configurations. I think they can perform differently, and I often try more than one when the 1st try doesn't work great. I don't think there's any fundamental difference in the principle of their operation, and I think there never will be - there will be another major breakthrough",
    "parent": 44857965,
    "depth": 2
  },
  {
    "id": 44857798,
    "by": "yosefk",
    "timeISO": "2025-08-10T19:56:09.000Z",
    "textPlain": "Your being blunt is actually very kind, if you're describing what I'm doing as \"being smart and reasoning from first principles\"; and I agree that I am not  saying something very novel, at most it's slightly  contrarian given the current sentiment.My goal is not to cherry-pick failures for its own sake as much as to try to explain why I get pretty bad output from LLMs much of the time, which I do. They are also very useful to me at times.Let's see how my predictions hold up; I have made enough to look very wrong if they don't.Regarding \"failure disproving success\": it can't, but it can disprove a theory of how this success is achieved. And, I have much better examples than the 2+2=4, which I am citing as something that sorta works these says",
    "parent": 44857725,
    "depth": 2
  },
  {
    "id": 44857881,
    "by": "AyyEye",
    "timeISO": "2025-08-10T20:05:47.000Z",
    "textPlain": "With LLMs being unable to count how many Bs are in blueberry, they clearly don't have any world model whatsoever. That addition (something which only takes a few gates in digital logic) happens to be overfit into a few nodes on multi-billion node networks is hardly a surprise to anyone except the most religious of AI believers.",
    "parent": 44857725,
    "depth": 2
  },
  {
    "id": 44857796,
    "by": "armchairhacker",
    "timeISO": "2025-08-10T19:56:02.000Z",
    "textPlain": "Any suggestions from this literature?",
    "parent": 44857725,
    "depth": 2
  },
  {
    "id": 44857931,
    "by": "svantana",
    "timeISO": "2025-08-10T20:13:44.000Z",
    "textPlain": "Yes, I hadn't heard that before. It's similar in spirit to this norwegian folk tale about a deaf man guessing what someone is saying to him:https://en.wikipedia.org/wiki/%22Good_day,_fellow!%22_%22Axe...",
    "parent": 44857872,
    "depth": 2
  },
  {
    "id": 44857493,
    "by": "yosefk",
    "timeISO": "2025-08-10T19:16:52.000Z",
    "textPlain": "This is interesting. The \"professional level\" rating of <1800 isn't, but still.However:\"A significant Elo rating jump occurs when the model’s Legal Move accuracy reaches\n99.8%. This increase is due to the reduction in errors after the model learns to generate legal moves,\nreinforcing that continuous error correction and\nlearning the correct moves significantly improve ELO\"You should be able to reach the move legality of around 100% with few resources spent on it. Failing to do so means that it has not learned a model of what chess is, at some basic level. There is virtually no challenge in making legal moves.",
    "parent": 44857369,
    "depth": 2
  }
]
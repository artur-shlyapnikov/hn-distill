[
  {
    "id": 44916912,
    "by": "zeropointsh",
    "timeISO": "2025-08-15T20:23:47.000Z",
    "textPlain": "How about using on-chain proof-of-work? It flips the script.If a bot wants access, let it earn it—and let that work be captured, not discarded. Each request becomes compensation to the site itself. The crawler feeds the very system it scrapes. Its computational effort directly funds the site owner's wallet, joining the pool to complete its proof.The cost becomes the contract.",
    "parent": 44914773,
    "depth": 1
  },
  {
    "id": 44915499,
    "by": "xena",
    "timeISO": "2025-08-15T17:59:04.000Z",
    "textPlain": "I just found out about this when it came to the front page of Hacker News. I really wish I was given advanced notice. I haven't been able to put as much energy into Anubis as I've wanted because I've been incredibly overwhelmed by life and need to be able to afford to make this my full time job. Support contracts are being roadblocked, and I just wish I had the time and energy to focus on this without having to worry about being the single income for the household.",
    "parent": 44914773,
    "depth": 1
  },
  {
    "id": 44915172,
    "by": "zahlman",
    "timeISO": "2025-08-15T17:35:37.000Z",
    "textPlain": "I actually don't understand who Anubis is supposed to \"make sure you're not a bot\". It seems to be more of a rate limiter than anything else. It self-describes:> Anubis sits in the background and weighs the risk of incoming requests. If it asks a client to complete a challenge, no user interaction is required.> Anubis uses a proof-of-work challenge to ensure that clients are using a modern browser and are able to calculate SHA-256 checksums. Anubis has a customizable difficulty for this proof-of-work challenge, but defaults to 5 leading zeroes.When I go to Codeberg or any other site using it, I'm never asked to perform any kind of in-browser task. It just has my browser run some JavaScript to do that calculation, or uses a signed JWT to let me have that process cached.Why shouldn't an automated agent be able to deal with that just as easily, by just feeding that JavaScript to its own interpreter?",
    "parent": 44914773,
    "depth": 1
  },
  {
    "id": 44915313,
    "by": "electroly",
    "timeISO": "2025-08-15T17:45:00.000Z",
    "textPlain": "Presumably they just finally decided they were willing to spend ($) the CPU time to pass the Anubis check. That was always my understanding of Anubis--of course a bot can pass it, it's just going to cost them a bunch of CPU time (and therefore money) to do it.",
    "parent": 44914773,
    "depth": 1
  },
  {
    "id": 44916411,
    "by": "yogorenapan",
    "timeISO": "2025-08-15T19:18:02.000Z",
    "textPlain": "I've seen a lot of traffic from Huawei bypassing Anubis on some of the things I host as well. The funny thing is, I work for Huawei... Asking around, it seems most of it is coming from Huawei Cloud (like AWS) but their artifactory cache also shows a few other captcha bypassing libraries for Arkose/funcaptcha so they're definitely doing it themselves too.Anonymous account for obvious reasons.",
    "parent": 44914773,
    "depth": 1
  },
  {
    "id": 44915216,
    "by": "Retr0id",
    "timeISO": "2025-08-15T17:38:44.000Z",
    "textPlain": "Last time I checked, Anubis used SHA256 for PoW. This is very GPU/ASIC friendly, so there's a big disparity between the amount of compute available in a legit browser vs a datacentre-scale scraping operation.A more memory-hard \"mining\" algorithm could help.",
    "parent": 44914773,
    "depth": 1
  },
  {
    "id": 44915402,
    "by": "Havoc",
    "timeISO": "2025-08-15T17:51:06.000Z",
    "textPlain": "Really feels like this needs some sort of unified possibly legal approach to get these fkers to behave.Search era clearly proved it is possible to crawl respectfully - the AI crawlers have just decided not to. They need to be disincentivized from doing this",
    "parent": 44914773,
    "depth": 1
  },
  {
    "id": 44915315,
    "by": "hollow-moe",
    "timeISO": "2025-08-15T17:45:09.000Z",
    "textPlain": "Really looks like the last solution is a legal one, using the DMCA against them using the digital protection or access control circumvention clause or smth.",
    "parent": 44914773,
    "depth": 1
  },
  {
    "id": 44915274,
    "by": "logicprog",
    "timeISO": "2025-08-15T17:42:19.000Z",
    "textPlain": "I'm not anti-the-tech-behind-AI, but this behavior is just awful, and makes the world worse for everyone. I wish AI companies would instead, I don't know, fund common crawl or something so that they can have a single organization and set of bots collecting all the training data they need and then share it, instead of having a bunch of different AI companies doing duplicated work and resulting in a swath of duplicated requests. Also, I don't understand why they have to make so many requests so often. Why wouldn't like one crawl of each site a day, at a reasonable rate, be enough? It's not like up to the minute info is actually important since LLM training cutoffs are always out of date anyway. I don't get it.",
    "parent": 44914773,
    "depth": 1
  },
  {
    "id": 44915493,
    "by": "egypturnash",
    "timeISO": "2025-08-15T17:58:46.000Z",
    "textPlain": "thanks for making everything that much shittier just so you can steal everyone's data and present it as your own, AI companies!",
    "parent": 44914773,
    "depth": 1
  },
  {
    "id": 44915215,
    "by": "hyghjiyhu",
    "timeISO": "2025-08-15T17:38:37.000Z",
    "textPlain": "Crazy thought but what if you made the work required to access the site equal the work required to host site. Host the public part of the database on something like webtorrent. Render website from db locally. You want to ruin expensive queries? Suit yourself. Not easy, but maybe possible?",
    "parent": 44914773,
    "depth": 1
  },
  {
    "id": 44915353,
    "by": "nine_k",
    "timeISO": "2025-08-15T17:48:27.000Z",
    "textPlain": "Why not ask it to directly mine some bitcoin, or do some protein folding? Let's make proof-of-work challenges proof-of-useful-work challenges. The server could even directly serve status 402 with the challenge.",
    "parent": 44914773,
    "depth": 1
  },
  {
    "id": 44915162,
    "by": "jsnell",
    "timeISO": "2025-08-15T17:34:41.000Z",
    "textPlain": "This was beyond predictable. The monetary cost of proof of work is several orders of magnitude too small to deter scraping (let alone higher yield abuse), and passing the challenges requires no technical finesse basically by construction.",
    "parent": 44914773,
    "depth": 1
  },
  {
    "id": 44915988,
    "by": "OutOfHere",
    "timeISO": "2025-08-15T18:37:49.000Z",
    "textPlain": "They failed to properly block/throttle the IP subnet as per their admission, and are now blaming others for their failure.",
    "parent": 44914773,
    "depth": 1
  },
  {
    "id": 44915198,
    "by": "rpcope1",
    "timeISO": "2025-08-15T17:37:22.000Z",
    "textPlain": "I'm calling it now, this is the beginning of all of the remaining non-commerical properties on the web either going away, or getting hidden inside of some trusted overlay network. Unless the \"AI\" race slows down or changes or some other act of god happens, the incentives are aligned that I foresee wide swaths of the net getting flogged to death.",
    "parent": 44914773,
    "depth": 1
  },
  {
    "id": 44916650,
    "by": "TZubiri",
    "timeISO": "2025-08-15T19:50:03.000Z",
    "textPlain": "It's PoW, AI crawlers didn't learn shit, their admins just increased their CPU/GPU/ASIC budget",
    "parent": 44914773,
    "depth": 1
  },
  {
    "id": 44915312,
    "by": "WD-42",
    "timeISO": "2025-08-15T17:44:59.000Z",
    "textPlain": "This is sad, but predictable. At the end of the day if I can follow a link to an Anubis protected site and view it on my phone, the crawlers will be able to as well.I see a lot more private networks in our future, unfortunately.",
    "parent": 44914773,
    "depth": 1
  },
  {
    "id": 44916719,
    "by": "1gn15",
    "timeISO": "2025-08-15T19:59:36.000Z",
    "textPlain": "Duh. The author of Anubis really should advertise it as a DDoS guard, not an AI guard. Otherwise, xe is just misleading people, while being unnecessarily discriminatory against robots (robotkin) and cyborgs (those using AI agents as an extension of their selves).",
    "parent": 44914773,
    "depth": 1
  },
  {
    "id": 44916903,
    "by": "logicprog",
    "timeISO": "2025-08-15T20:22:02.000Z",
    "textPlain": "That sucks. Keep fighting the good fight, and I wish you all the best. We need people working on this problem (unfortunately).",
    "parent": 44915499,
    "depth": 2
  },
  {
    "id": 44916196,
    "by": "veqq",
    "timeISO": "2025-08-15T18:57:44.000Z",
    "textPlain": "Good luck, I'm sorry for all of this speculation and people attacking your solution instead of suggesting concrete improvements to help fight the problem.",
    "parent": 44915499,
    "depth": 2
  },
  {
    "id": 44915491,
    "by": "yabones",
    "timeISO": "2025-08-15T17:58:29.000Z",
    "textPlain": "My understanding is that it just increases the \"expense\" of mass crawling just enough to put it out of reach. If it costs fractional pennies per page scrape with just a python or go bot, it costs nickels and dimes to run a headless chromium instance to do the same thing. The purpose is economical - make it too expensive to scrape the \"open web\". Whether it achieves that goal is another thing.",
    "parent": 44915172,
    "depth": 2
  },
  {
    "id": 44915513,
    "by": "dathinab",
    "timeISO": "2025-08-15T18:00:14.000Z",
    "textPlain": "it's indeed not a \"bot/crawler protection\"it's a \"I don't want my server to be _overrun_ by crawlers\" protection which works by- taking advantage that many crawlers are made very badly/cheaply- increasing the cost of crawlingthats it, simple but good enough to shake of the dumbest crawlers and to make it worth it for AI agents to e.g. cache site crawling so that they don't craws your site a 1000 times a day but instead just once",
    "parent": 44915172,
    "depth": 2
  },
  {
    "id": 44916788,
    "by": "Spivak",
    "timeISO": "2025-08-15T20:08:22.000Z",
    "textPlain": "You have it right. The problem Anubis is intended to solve isn't bots per se, the problem is that bot networks have figured out how to bypass rate limits by sending requests from newly minted, sometimes residential, ip addresses/ranges for each request. Anubis tries to help somewhat by making each (client, address) perform a proof-of-work. For normal users this should be an infrequent inconvenience but for those bot networks they have to do it every time. And if they solve the challenge and keep the cookie then the server \"has them\" so to speak and can apply ip rate limits normally.",
    "parent": 44915172,
    "depth": 2
  },
  {
    "id": 44915401,
    "by": "homebrewer",
    "timeISO": "2025-08-15T17:51:06.000Z",
    "textPlain": "I think the only requests it was able to block are plain http requests made over curl or Go's stdlib http client. I see enough of both in httpd logs. Now the cancer has adapted by using a fully featured headless web browser that can complete challenges just like any other client.As other commenters say, it was completely predictable from the start.",
    "parent": 44915172,
    "depth": 2
  },
  {
    "id": 44915377,
    "by": "joe_the_user",
    "timeISO": "2025-08-15T17:49:38.000Z",
    "textPlain": "Near as I can guess, the idea is that the code is optimized for what browsers can do and gpus/servers/crawlers/etc can't do as easily (or relatively as easily, just taking up the whole server for a bit might a big cost). Indeed it seems like only a matter of time before something like that would be broken.",
    "parent": 44915172,
    "depth": 2
  },
  {
    "id": 44915443,
    "by": "zelphirkalt",
    "timeISO": "2025-08-15T17:54:31.000Z",
    "textPlain": "I think so too. Maybe the compute cost needs to be upped some more. I am OK with waiting a bit longer when I access the site.",
    "parent": 44915313,
    "depth": 2
  },
  {
    "id": 44916435,
    "by": "xena",
    "timeISO": "2025-08-15T19:21:34.000Z",
    "textPlain": "Please have someone in the common sense department email me@xeiaso.net. Funding of the Anubis project would go a long way towards mending bridges.",
    "parent": 44916411,
    "depth": 2
  },
  {
    "id": 44915396,
    "by": "jsnell",
    "timeISO": "2025-08-15T17:50:49.000Z",
    "textPlain": "A different algorithm would not help.Here's the basic problem: the fully loaded cost of a server CPU core is ~1 cent/hour. The most latency you can afford to inflict on real users is a couple of seconds. That means the cost of passing a challenge the way the users pass it, with a CPU running Javascript, is about 1/1000th of a cent. And then that single proof of work will let them scrape at a minimum hundreds, but more likely thousands, of pages.So a millionth of a cent per page. How much engineering effort is worth spending on optimizing that? Basically none, certainly not enough to offload to GPUs or ASICs.",
    "parent": 44915216,
    "depth": 2
  },
  {
    "id": 44915713,
    "by": "dathinab",
    "timeISO": "2025-08-15T18:17:12.000Z",
    "textPlain": "the problem in many cases is that even if such a law is made it likely- is hard to enforce- misses bite, i.e. it makes you more money to break it then any penaltiesbut in general yes, a site which indicates they don't want to be crawled by AI bots but still gets crawled should be handled similar to someone with house ban on a shop forcing them self into the shopgiven how severely messed up some millennia cyber security laws are I wonder if crawlers bypassing Anubis could be interpreted as \"circumventing digital access controls/protections\" or similar, especially given that its done to make copies of copyrighted material ;=)",
    "parent": 44915402,
    "depth": 2
  },
  {
    "id": 44915536,
    "by": "oortoo",
    "timeISO": "2025-08-15T18:02:24.000Z",
    "textPlain": "The time to regulate tech was like 15 years ago, and we didn't. Why would any tech company expect to have to start following \"rules\" now?",
    "parent": 44915274,
    "depth": 2
  },
  {
    "id": 44915305,
    "by": "barbazoo",
    "timeISO": "2025-08-15T17:44:48.000Z",
    "textPlain": "Greed. It's never enough money, never enough data, we must have everything all the time and instantly. It's also human nature it seems, looking at how we consume like there's no tomorrow.",
    "parent": 44915274,
    "depth": 2
  },
  {
    "id": 44915546,
    "by": "thewebguyd",
    "timeISO": "2025-08-15T18:02:52.000Z",
    "textPlain": "> fund common crawl or something so that they can have a single organization and set of bots collecting all the training data they need and then share itThat, or, they could just respect robots.txt and we could put enforcement penalties for not respecting the web service's request to not be crawled. Granted, we probably need a new standard but all these AI companies are just shitting all over the web, being disrespectful of site owners because who's going to stop them? We need laws.",
    "parent": 44915274,
    "depth": 2
  },
  {
    "id": 44915576,
    "by": "superkuh",
    "timeISO": "2025-08-15T18:05:16.000Z",
    "textPlain": "This isn't AI. This is corporations doing things because they have a profit motive. The issue here is the non-human corporations and their complete lack of accountability even if someone brings legal charges against them. Their structure is designd to abstract away responsibility and they behave that way.Same old problem. Corps are gonna corp.",
    "parent": 44915274,
    "depth": 2
  },
  {
    "id": 44915511,
    "by": "nektro",
    "timeISO": "2025-08-15T18:00:10.000Z",
    "textPlain": "[flagged]",
    "parent": 44915274,
    "depth": 2
  },
  {
    "id": 44916180,
    "by": "amarcheschi",
    "timeISO": "2025-08-15T18:55:55.000Z",
    "textPlain": "The tiniest relief is knowing that models will be distilled and \"copied\" in smaller models of equal capabilities in ~6/12months, since their output can't be copyrighted and will be used to improve others. Kinda ironic",
    "parent": 44915493,
    "depth": 2
  },
  {
    "id": 44915646,
    "by": "catsma21",
    "timeISO": "2025-08-15T18:10:32.000Z",
    "textPlain": "https://news.ycombinator.com/item?id=44880393",
    "parent": 44915353,
    "depth": 2
  },
  {
    "id": 44915230,
    "by": "zahlman",
    "timeISO": "2025-08-15T17:39:27.000Z",
    "textPlain": "We need to revive 402 Payment Required, clearly. If we lived in a world where we could easily set up a small trusted online balance for microtransactions that's interoperable with everyone, and where giving others a literal penny for their thoughts could allow for running up a significant bill for abusers, I'd gladly play along.",
    "parent": 44915162,
    "depth": 2
  },
  {
    "id": 44915451,
    "by": "sumtechguy",
    "timeISO": "2025-08-15T17:55:08.000Z",
    "textPlain": "For someone doing spamming that low level would work well.  As their cost is determinatively low to make it work.  For someone doing scraping to get data and feeding it to an AI not so much.  The AI groups usually have some pretty heavy hitting hardware sitting behind it.  They could even break off some hardware that is to be retired and have it munch away on it.  To make it non cost effective the calculations would need to be much bigger.",
    "parent": 44915162,
    "depth": 2
  },
  {
    "id": 44915488,
    "by": "homebrewer",
    "timeISO": "2025-08-15T17:58:16.000Z",
    "textPlain": "Also increasing balkanization of the internet. I now routinely run into sites that geoblock my whole country, this wasn't something I would see more than once or twice a year, and usually only with sites like Walmart that don't care about clients from outside the US.Now it's 2-5 sites per day, including web forums and such.",
    "parent": 44915198,
    "depth": 2
  },
  {
    "id": 44915564,
    "by": "bananalychee",
    "timeISO": "2025-08-15T18:04:24.000Z",
    "textPlain": "I self-host a few servers and have not seen significant traffic increases from crawlers, so I can't agree with that without seeing some evidence of this issue's scale and scope. As far as I know it mostly affects commercial content aggregators.",
    "parent": 44915198,
    "depth": 2
  },
  {
    "id": 44915386,
    "by": "weinzierl",
    "timeISO": "2025-08-15T17:50:22.000Z",
    "textPlain": "I think the answer for the non-commercial web is to stop worrying.I understand why certain business models have a problem with AI crawlers, but I fail to see why sites like Codeberg have an issue.If the problem is cost for the traffic then this is nothing new and I thought we have learned how to handle that by now.",
    "parent": 44915198,
    "depth": 2
  },
  {
    "id": 44915296,
    "by": "herval",
    "timeISO": "2025-08-15T17:43:59.000Z",
    "textPlain": "Hasn’t that been the case for a while? I’d imagine the combined traffic to all sites on the web combined doesn’t match a single hour of the traffic to the top 5 social media sites. The web is pretty much dead for a while now, many companies don’t even bother maintaining websites anymore",
    "parent": 44915198,
    "depth": 2
  },
  {
    "id": 44915605,
    "by": "superkuh",
    "timeISO": "2025-08-15T18:06:46.000Z",
    "textPlain": "I could see it being the end of commercial and institutional web applications which cannot handle traffic. But actual websites which are html and files in folders served by webservers don't have problems with this.",
    "parent": 44915198,
    "depth": 2
  },
  {
    "id": 44915272,
    "by": "v5v3",
    "timeISO": "2025-08-15T17:42:14.000Z",
    "textPlain": "Could it be a 'correct' continuation of Darwin's survival of the fittest?",
    "parent": 44915198,
    "depth": 2
  }
]
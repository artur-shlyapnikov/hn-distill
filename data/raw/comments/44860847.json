[
  {
    "id": 44866691,
    "by": "commandlinefan",
    "timeISO": "2025-08-11T17:08:09.000Z",
    "textPlain": "I've gotten a lot of gains in this area in the past by just - not memcpy'ing.  A good percentage of the time, somebody assumes that they need to copy something somewhere when in fact, the original never gets referenced.  I can often get away with reading a buffer off the wire, inserting null terminators to turn bits of the buffer into proper C-style strings and just using them in-place.",
    "parent": 44860847,
    "depth": 1
  },
  {
    "id": 44861292,
    "by": "Sesse__",
    "timeISO": "2025-08-11T06:33:37.000Z",
    "textPlain": "There's an error here: “NT instructions are used when there is an overlap between destination and source since destination may be in cache when source is loaded.”Non-temporal instructions don't have anything to do with correctness. They are for cache management; a non-temporal write is a hint to the cache system that you don't expect to read this data (well, address) back soon, so it shouldn't push out other things in the cache. They may skip the cache entirely, or (more likely) go into just some special small subsection of it reserved for non-temporal writes only.",
    "parent": 44860847,
    "depth": 1
  },
  {
    "id": 44860890,
    "by": "userbinator",
    "timeISO": "2025-08-11T05:10:03.000Z",
    "textPlain": "It's not clear from a skim of this article, but a common problem I've seen in the past with memory copying benchmarks is to not serialise and access the copied data in its destination to ensure that it was actually completed before concluding the timing. A simple REP MOVS should be at or near the top, especially on CPUs with ERMSB.",
    "parent": 44860847,
    "depth": 1
  },
  {
    "id": 44863898,
    "by": "mojo-ponderer",
    "timeISO": "2025-08-11T13:34:19.000Z",
    "textPlain": "The graph at the end seems pretty dubious. For example, for the AvxUnrollCopier, why does data transfer speed jump to >120gb/s for 4kb, then down to ~50gb/s for 32kb, then down to <20gb/s for 16mb? It just doesn't make sense.",
    "parent": 44860847,
    "depth": 1
  },
  {
    "id": 44861024,
    "by": "Arech",
    "timeISO": "2025-08-11T05:37:42.000Z",
    "textPlain": "It's not clear how the author controlled for HW caching. Without this, the results are, unfortunately, meaningless, even though some good work has been gone",
    "parent": 44860847,
    "depth": 1
  },
  {
    "id": 44863630,
    "by": "coxley",
    "timeISO": "2025-08-11T13:01:48.000Z",
    "textPlain": "Ha, I love the project name \"Shadesmar\". Journey before destination, friend. :crossed-wrists:",
    "parent": 44860847,
    "depth": 1
  },
  {
    "id": 44860882,
    "by": "waschl",
    "timeISO": "2025-08-11T05:08:20.000Z",
    "textPlain": "Thought about zero-copy IPC recently. In order to avoid memcopy for the complete chain, I guess it would be best if the sender allocates its payload directly on the shared memory when it’s created. Is this a standard thing in such optimized IPC and which libraries offer this?",
    "parent": 44860847,
    "depth": 1
  },
  {
    "id": 44863675,
    "by": "PaulHoule",
    "timeISO": "2025-08-11T13:09:05.000Z",
    "textPlain": "If I understand that chart at the end it looks like the better performance is only for small buffer sizes which fit in the cache (4k) but if you are looking at big buffers the stdlib copy performs about the same as the optimized copy that he writes.",
    "parent": 44860847,
    "depth": 1
  },
  {
    "id": 44862219,
    "by": "davrosthedalek",
    "timeISO": "2025-08-11T09:14:12.000Z",
    "textPlain": "> Since the loop copies data pointer by pointer, it can handle the case of overlapping data.I don't think this loop does the right thing if destination points somewhere into source. It will start overwriting the non-copied parts of source.",
    "parent": 44860847,
    "depth": 1
  },
  {
    "id": 44861026,
    "by": "jesse__",
    "timeISO": "2025-08-11T05:37:49.000Z",
    "textPlain": "Would have loved to see performance comparisons along the way, instead of just the small squashed graph at the end.  Nice article otherwise :)",
    "parent": 44860847,
    "depth": 1
  },
  {
    "id": 44860925,
    "by": "brucehoult",
    "timeISO": "2025-08-11T05:17:41.000Z",
    "textPlain": "ConclusionStick to `std::memcpy`. It delivers great performance while also adapting to the hardware architecture, and makes no assumptions about the memory alignment.----So that's five minutes I'll never get back.I'd make an exception for RISC-V machines with \"RVV\" vectors, where vectorised `memcpy` hasn't yet made it into the standard library and a simple ...    0000000000000000 <memcpy>:\n       0:   86aa mv      a3,a0\n    \n    0000000000000002 <.L1^B1>:\n       2:   00267757 vsetvli a4,a2,e8,m4,tu,mu\n       6:   02058007 vle8.v  v0,(a1)\n       a:   95ba add     a1,a1,a4\n       c:   8e19 sub     a2,a2,a4\n       e:   02068027 vse8.v  v0,(a3)\n      12:   96ba add     a3,a3,a4\n      14:   f67d bnez    a2,2 <.L1^B1>\n      16:   8082 ret\n\n... often beats `memcpy` by a factor of 2 or 3 on copies that fit into L1 cache.https://hoult.org/d1_memcpy.txt",
    "parent": 44860847,
    "depth": 1
  },
  {
    "id": 44860957,
    "by": "dataflow",
    "timeISO": "2025-08-11T05:23:13.000Z",
    "textPlain": "I thought this was going to be about https://github.com/Blosc/c-blosc",
    "parent": 44860847,
    "depth": 1
  },
  {
    "id": 44862597,
    "by": "kvemkon",
    "timeISO": "2025-08-11T10:12:23.000Z",
    "textPlain": "BTW, if we copy data between some device and RAM efficiently using DMA without spending CPU cycles, why we can't use DMA to copy RAM-to-RAM?",
    "parent": 44860847,
    "depth": 1
  },
  {
    "id": 44861180,
    "by": "adwn",
    "timeISO": "2025-08-11T06:13:19.000Z",
    "textPlain": "> The operation of copying data is super easy to parallelize across multiple threads. […] This will make the copy super-fast especially if the CPU has a large core count.I seriously doubt that. Unless you have a NUMA system, a single core in a desktop CPU can easily saturate the bandwidth of the system RAM controller. If you can avoid going through main memory – e.g., when copying between the L2 caches of different cores – multi-threading can speed things up. But then you need precise knowledge of your program's memory access behavior, and this is outside the scope of a general-purpose memcpy.",
    "parent": 44860847,
    "depth": 1
  },
  {
    "id": 44861212,
    "by": "Orangeair",
    "timeISO": "2025-08-11T06:18:10.000Z",
    "textPlain": "[2020]",
    "parent": 44860847,
    "depth": 1
  },
  {
    "id": 44864394,
    "by": "EGreg",
    "timeISO": "2025-08-11T14:19:42.000Z",
    "textPlain": "Wait, I thought memcpy would have launched some sort of built-in mechanism (parallelized or whatever) to copy in RAM.Just indicate the start and length. Why would the CPU need to keep issuing copy instructions?",
    "parent": 44860847,
    "depth": 1
  },
  {
    "id": 44861033,
    "by": "wolfi1",
    "timeISO": "2025-08-11T05:39:31.000Z",
    "textPlain": "the \"dumb of perf\": some Freudian Slip?",
    "parent": 44860847,
    "depth": 1
  },
  {
    "id": 44861091,
    "by": "_ZeD_",
    "timeISO": "2025-08-11T05:53:32.000Z",
    "textPlain": "soo... time to send a patch to glibc?",
    "parent": 44860847,
    "depth": 1
  },
  {
    "id": 44866821,
    "by": "t00",
    "timeISO": "2025-08-11T17:20:09.000Z",
    "textPlain": "That is a really good advice, copying data everywhere makes only sense if the data will be mutated. I only wonder why, why C-style strings were invented with 0 termination instead of varint prefix, this would have saved so much copying and so many bugs knowing the string length upfront.",
    "parent": 44866691,
    "depth": 2
  },
  {
    "id": 44862134,
    "by": "orlp",
    "timeISO": "2025-08-11T09:02:19.000Z",
    "textPlain": "> Non-temporal instructions don't have anything to do with correctness. They are for cache management; a non-temporal write is a hint to the cache system that you don't expect to read this data (well, address) back soonI disagree with this statement (taken at face value, I don't necessarily agree with the wording in the OP either). Non-temporal instructions are unordered with respect to normal memory operations, so without a _mm_sfence() after doing your non-temporal writes you're going to get nasty hardware UB.",
    "parent": 44861292,
    "depth": 2
  },
  {
    "id": 44862150,
    "by": "m0th87",
    "timeISO": "2025-08-11T09:04:09.000Z",
    "textPlain": "I work on optimizations like this at work, and yes this is largely correct. But do you have a source on this?> or (more likely) go into just some special small subsection of it reserved for non-temporal writes only.I hadn’t heard of this before. It looks like older x86 CPUs may have had a dedicated cache.",
    "parent": 44861292,
    "depth": 2
  },
  {
    "id": 44860914,
    "by": "kachapopopow",
    "timeISO": "2025-08-11T05:15:18.000Z",
    "textPlain": "Yah, these benchmarks are irrelevant since the CPU executes instructions out of order. Majority of the time the cpu will continue executing assembly while a copy operation is ongoing.",
    "parent": 44860890,
    "depth": 2
  },
  {
    "id": 44864359,
    "by": "Sesse__",
    "timeISO": "2025-08-11T14:17:33.000Z",
    "textPlain": "The L1 cache is faster than the L3 cache. Does it need to be anything more complicated than that?",
    "parent": 44863898,
    "depth": 2
  },
  {
    "id": 44861173,
    "by": "comex",
    "timeISO": "2025-08-11T06:11:30.000Z",
    "textPlain": "IPC libraries often specifically avoid zero-copy for security reasons.  If a malicious message sender can modify the message while the receiver is in the middle of parsing it, you have to be very careful not to enable time-of-check-time-of-use attacks.  (To be fair, not all use cases need to be robust against a malicious sender.)",
    "parent": 44860882,
    "depth": 2
  },
  {
    "id": 44860945,
    "by": "dataflow",
    "timeISO": "2025-08-11T05:20:29.000Z",
    "textPlain": "> I guess it would be best if the sender allocates its payload directly on the shared memory when it’s created.On an SMP system yes. On a NUMA system it depends on your access patterns etc.",
    "parent": 44860882,
    "depth": 2
  },
  {
    "id": 44861676,
    "by": "a_t48",
    "timeISO": "2025-08-11T07:43:38.000Z",
    "textPlain": "I've looked into this a bit - the big blocker isn't on the transport/IPC library, but the serializer itself, assuming you _also_ want to support serializing messages to disk or over network. It's a bit of a pickle - at least in C++, tying an allocator to a structure and its children is an ugly mess. And what happens if you do something like resize a string? Does it mean a whole new allocation? I've (partially) solved it before for single process IPC by having a concept of a sharable structure and its serialization type, you could do the same for shared memory. One could also use a serializer that offers promises around allocations, FlatBuffer might fit the bill. There's also https://github.com/Verdant-Robotics/cbuf but I'm not sure how well maintained it is right now, publicly.As for allocation - it looks like Zenoh might offer the allocation pattern necessary. https://zenoh-cpp.readthedocs.io/en/1.0.0.5/shm.html TBH most of the big wins come from not copying big blocks of memory around from sensor data and the like. A thin header and reference to a block of shared memory containing an image or point cloud coming in over UDS is likely more than performant enough for most use cases. Again, big wins from not having to serialize/deserialize the sensor data.Another pattern which I haven't really seen anywhere is handling multiple transports - at one point I had the concept of setting up one transport as an allocator (to put into shared memory or the like) - serialize once to shared memory, hand that serialized buffer to your network transport(s) or your disk writer. It's not quite zero copy but in practice most zero copy is actually at least one copy on each end.(Sorry, this post is a little scatterbrained, hopefully some of my points come across)",
    "parent": 44860882,
    "depth": 2
  },
  {
    "id": 44861004,
    "by": "6keZbCECT2uB",
    "timeISO": "2025-08-11T05:33:51.000Z",
    "textPlain": "I've been meaning to look at Iceoryx as a way to wrap this.Pytorch multiprocessing queues work this way, but it is hard for the sender to ensure the data is already in shared memory, so it often has a copy. It is also common for buffers to not be reused, so that can end up a bottleneck, but it can, in principle, be limited by the rate of sending fds.",
    "parent": 44860882,
    "depth": 2
  },
  {
    "id": 44860899,
    "by": "throwaway81523",
    "timeISO": "2025-08-11T05:11:44.000Z",
    "textPlain": "This is one of mmap's designed-for use cases.  Look at DPDK maybe.",
    "parent": 44860882,
    "depth": 2
  },
  {
    "id": 44861138,
    "by": "yokaze",
    "timeISO": "2025-08-11T06:02:59.000Z",
    "textPlain": "Boost.Interprocess:https://www.boost.org/doc/libs/1_46_0/doc/html/interprocess/...",
    "parent": 44860882,
    "depth": 2
  },
  {
    "id": 44861167,
    "by": "viraptor",
    "timeISO": "2025-08-11T06:10:32.000Z",
    "textPlain": "> So that's five minutes I'll never get back.Confirming null hypothesis, with good supporting data is still interesting. Could save you from doing this yourself.",
    "parent": 44860925,
    "depth": 2
  },
  {
    "id": 44864839,
    "by": "snihalani",
    "timeISO": "2025-08-11T14:54:09.000Z",
    "textPlain": "You could read the article and end up disagreeing with it. The value is in grokking over the details and not whether the insight changes your decisions. It can just make your decisions more grounded in data",
    "parent": 44860925,
    "depth": 2
  },
  {
    "id": 44862551,
    "by": "makach",
    "timeISO": "2025-08-11T10:04:24.000Z",
    "textPlain": "You pre-stole my comment, I was about to make the exact same post :-DAlthough the blog post is about going faster and him showing alternative algorithms, conclusion remains for safety which makes perfect sense. However, he did show us a few strategies which is useful. The five minutes I spent, will never be returned to me but at least I learned something interesting...",
    "parent": 44860925,
    "depth": 2
  },
  {
    "id": 44866040,
    "by": "toast0",
    "timeISO": "2025-08-11T16:19:57.000Z",
    "textPlain": "DMA works for devices, because the device does the memory access. RAM to RAM DMA would need something to do the accesses.The other reason DMA works for devices is because it is asynchronous. You give a device a command and some memory to do it with, it does the thing and lets you know. Most devices can't complete commands instantaneously, so we know we have to queue things and then go do something else. Often when doing memcpy, we want to use the copied memory immediately... if it were a DMA, you'd need to submit the request and wait for it to complete before you continued... If your general purpose DMA engine is a typical device, you're probably doing a syscall to the kernel, which would submit the command (possibly through a queue), suspend your process, schedule something else and there may be delay before getting scheduled again when the DMA is complete.If async memcpy was what was wanted, it could make sense, but that feels pretty hard to use.",
    "parent": 44862597,
    "depth": 2
  },
  {
    "id": 44862692,
    "by": "shakna",
    "timeISO": "2025-08-11T10:27:26.000Z",
    "textPlain": "You can copy that way.It's faster of you use the CPU, but you absolutely can just use DMA - and some embedded systems do.",
    "parent": 44862597,
    "depth": 2
  },
  {
    "id": 44862042,
    "by": "bob1029",
    "timeISO": "2025-08-11T08:47:24.000Z",
    "textPlain": "> a single core in a desktop CPU can easily saturate the bandwidth of the system RAM controller.Modern x86 machines offer far more memory bandwidth than what a single core can consume. The entire architecture is designed on purpose to ensure this.The interesting thing to note is that this has not always been the case. The 2010s is when the transition occurred.",
    "parent": 44861180,
    "depth": 2
  },
  {
    "id": 44861233,
    "by": "hugh-avherald",
    "timeISO": "2025-08-11T06:22:23.000Z",
    "textPlain": "I've experienced modest but significant improvements in speed using very basic pragma omp section style parallelizing of this sort of thing.",
    "parent": 44861180,
    "depth": 2
  },
  {
    "id": 44864865,
    "by": "ack_complete",
    "timeISO": "2025-08-11T14:56:42.000Z",
    "textPlain": "The problem is that the built-in mechanism is often microcode, which is still slower than plain machine code in some cases.There are some interesting writings from a former architect of the Pentium Pro on the reasons for this. One is apparently that the microcode engine often lacked branch prediction, so handling special cases in the microcode was slower than compare/branch in direct code. REP MOVS has a bunch of such cases due to the need to handle overlapping copies, interrupts, and determining when it should switch to cache line sized non-temporal accesses.More recent Intel CPUs have enhanced REP MOVS support with faster microcode and a flag indicating that memcpy() should rely on it more often. But people have still found cases where if the relative alignment between source and destination is just right, a manual copy loop is still noticeably faster than REP MOVS.",
    "parent": 44864394,
    "depth": 2
  },
  {
    "id": 44864634,
    "by": "Sesse__",
    "timeISO": "2025-08-11T14:38:46.000Z",
    "textPlain": "The poster has a Zen 2, where this is only optimal for large copies. For newer Intel, glibc might indeed choose to use REP MOVSB more often.",
    "parent": 44864394,
    "depth": 2
  },
  {
    "id": 44866241,
    "by": "CyberDildonics",
    "timeISO": "2025-08-11T16:35:36.000Z",
    "textPlain": "I thought memcpy would have launched some sort of built-in mechanismWhere did you get this impression?",
    "parent": 44864394,
    "depth": 2
  },
  {
    "id": 44861465,
    "by": "bawolff",
    "timeISO": "2025-08-11T07:05:09.000Z",
    "textPlain": "Given their conclusion that glibc was the best option for most use cases, i would say no.",
    "parent": 44861091,
    "depth": 2
  }
]
[
  {
    "id": 44924077,
    "by": "theli0nheart",
    "timeISO": "2025-08-16T15:03:27.000Z",
    "textPlain": "I wrote git-bigstore [0] almost 10 (!) years ago to solve this problem—even before Git LFS—and as far as I know, bigstore still works perfectly.You specify the files you want to store in your storage backend via .gitattributes, and use two separate commands to sync files. I have not touched this code in years but the general implementation should still work.GitHub launched LFS not too long after I wrote this, so I kind of gave up on the idea thinking that no one would want to use it in lieu of GitHub's solution, but based on the comments I think there's a place for it.It needs some love but the idea is solid. I wrote a little description on the wiki about the low-level implementation if you want to check it out. [1]Also, all of the metadata is stored using git notes, so is completely portable and is frontend agnostic—doesn't lock you into anything (except, of course, the storage backend you use).[0]: https://github.com/lionheart/git-bigstore[1]: https://github.com/lionheart/git-bigstore/wiki",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44918580,
    "by": "bob1029",
    "timeISO": "2025-08-15T23:49:48.000Z",
    "textPlain": "> Large object promisors are special Git remotes that only house large files.I like this approach. If I could configure my repos to use something like S3, I would switch away from using LFS. S3 seems like a really good synergy for large blobs in a VCS. The intelligent tiering feature can move data into colder tiers of storage as history naturally accumulates and old things are forgotten. I wouldn't mind a historical checkout taking half a day (i.e., restored from a robotic tape library) if I am pulling in stuff from a decade ago.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44917895,
    "by": "jauer",
    "timeISO": "2025-08-15T22:15:27.000Z",
    "textPlain": "TFA asserts that Git LFS is bad for several reasons including because proprietary with vendor lock-in which I don't think is fair to claim. GitHub provided an open client and server which negates that.LFS does break disconnected/offline/sneakernet operations which wasn't mentioned and is not awesome, but those are niche workflows. It sounds like that would also be broken with promisors.The `git partial clone` examples are cool!The description of Large Object Promisors makes it sound like they take the client-side complexity in LFS, move it server-side, and then increases the complexity? Instead of the client uploading to a git server and to a LFS server it uploads to a git server which in turn uploads to an object store, but the client will download directly from the object store? Obviously different tradeoffs there. I'm curious how often people will get bit by uploading to public git servers which upload to hidden promisor remotes.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44925600,
    "by": "Sophira",
    "timeISO": "2025-08-16T18:00:47.000Z",
    "textPlain": "How does this upcoming feature deal with the potential problem of fake commit IDs?Commit IDs are based on a number of factors about the commit, including the actual contents and the commit ID of the parent commit. Any fully cloned git repository can theoretically be audited to make sure that all its commit IDs are correct. Nobody does this (although perhaps git does automatically?), but it's possible.But now, picture a git repository that has a one petabyte file in one of its early commits (and deleted again later). Pretty much nobody is going to have the space required to download this, so many people will not even bother to do so. As such, what's to stop the server from just claiming any commit ID it wanted for this particular commit? Who's going to check?(Bonus: For that matter, is the one petabyte file even real? Or just a faked size in the metadata?)To be clear, I assume people have already thought about these issues. I'm just curious what the answers are.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44919065,
    "by": "Ferret7446",
    "timeISO": "2025-08-16T01:07:11.000Z",
    "textPlain": "This article treats LFS unfairly.  It does not in any way lock you in to GitHub; the protocol is open.  The downsides of LFS are unavoidable as a Git extension.  Promisors are basically the same concept as LFS, except as it's built into Git it is able to provide a better UX than is possible as an extension.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44921726,
    "by": "KronisLV",
    "timeISO": "2025-08-16T09:31:16.000Z",
    "textPlain": "> And the problems are significant:> High vendor lock-in – When GitHub wrote Git LFS, the other large file systems—Git Fat, Git Annex, and Git Media—were agnostic about the server-side. But GitHub locked users to their proprietary server implementation and charged folks to use it.Is this a current issue?I used Git LFS with a GitLab instance this week, seemed to work fine.https://docs.gitlab.com/topics/git/lfs/I also used Git LFS with my Gitea instance a week before that, it was fine too.https://docs.gitea.com/administration/git-lfs-setupAt the same time it feels odd to hear mentions of LFS being deprecated in the future, while I’ve seldom seen anyone even use it - people just don’t seem to care and shove images and such into regular Git which puzzles me.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44921021,
    "by": "flohofwoe",
    "timeISO": "2025-08-16T07:17:30.000Z",
    "textPlain": "Even ancient svn works much better out of the box for large binary files than git (e.g. a 150 GB working directory with many big Photoshop files and other binary assets is no problem in SVN).What does SVN do differently than git when it comes to large binary files, and why can't git use the same approach?I also don't quite understand tbh how offloading large files to somewhere else would be fundamentally different than storing all files in one place except complicating everything? Storage is storage, how would a different storage location fix any of the current performance and robustness problems? Offloading just sounds like a solution for public git forges which don't want to deal with big files because it's too costly for them, but increased hosting cost is not the 'large binary file problem' of git.(edit: apparently git supports proper locking(?) so I removed that section - ps: nvm it looks like the file locking feature is only in git-lfs)",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44918524,
    "by": "technoweenie",
    "timeISO": "2025-08-15T23:40:03.000Z",
    "textPlain": "I'm really happy to see large file support in Git core. Any external solution would have similar opt-in procedures. I really wanted it to work seamlessly with as few extra commands as possible, so the API was constrained to the smudge and clean filters in the '.gitattributes' file.Though I did work hard to remove any vendor lock-in by working directly with Atlassian and Microsoft pretty early in the process. It was a great working relationship, with a lot of help from Atlassian in particular on the file locking API. LFS shipped open source with compatible support in 3 separate git hosts.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44919639,
    "by": "wbillingsley",
    "timeISO": "2025-08-16T02:37:18.000Z",
    "textPlain": "What I used to recommend to my sofware engineering classes is that instead of putting large files (media etc) into Git, put them into the artifact repository (Artifactory or something like it). That lets you for instance publish it as a snapshot dependency that the build system will automatically fetch for you, but control how much history of it you keep and only require your colleagues to fetch the latest version. Even better, a simple clean of their build system cache will free up the space used by old versions on their machines.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44917935,
    "by": "glitchc",
    "timeISO": "2025-08-15T22:19:21.000Z",
    "textPlain": "No. This is not a solution.While git LFS is just a kludge for now, writing a filter argument during the clone operation is not the long-term solution either.Git clone is the very first command most people will run when learning how to use git.  Emphasized for effect: the very first command.Will they remember to write the filter? Maybe, if the tutorial to the cool codebase they're trying to access mentions it. Maybe not. What happens if they don't? It may take a long time without any obvious indication. And if they do? The cloned repo might not be compilable/usable since the blobs are missing.Say they do get it right. Will they understand it? Most likely not. We are exposing the inner workings of git on the very first command they learn. What's a blob? Why do I need to filter on it? Where are blobs stored? It's classic abstraction leakage.This is a solved problem: Rsync does it. Just port the bloody implementation over. It does mean supporting alternative representations or moving away from blobs altogether, which git maintainers seem unwilling to do.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44919232,
    "by": "gschoeni",
    "timeISO": "2025-08-16T01:29:53.000Z",
    "textPlain": "We're working on `oxen` to solve a lot of the problems we ran into with git or git-lfs.We have an open source CLI and server that mirrors git, but handles large files and mono repos with millions of files in a much more performant manner. Would love feedback if you want to check it out!https://github.com/Oxen-AI/Oxen",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44918828,
    "by": "jayd16",
    "timeISO": "2025-08-16T00:31:24.000Z",
    "textPlain": "How about git just fixes shallow clones and partial clones?  Then we don't need convoluted work arounds to cheat in large content after we fully clone a history of pointers or promises or whatever.  You should be able to set default clone depth by file type and size in the git attributes (or maybe a file that can also live above a repo like supporting attributes in .gitconfig locations?).Then the usual settings would be to shallow clone the latest content as well as fetch the full history and maybe the text file historical content.  Ideally you could prune to the clone depth settings as well.Why are we still talking about large file pointers?  If you fix shallow and partial clones, then any repo can be an efficient file mirror, right?",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44918246,
    "by": "jameshart",
    "timeISO": "2025-08-15T22:58:59.000Z",
    "textPlain": "Nit:> if I git clone a repo with many revisions of a noisome 25 MB PNG fileFYI ‘noisome’ is not a synonym for ‘noisy’ - it’s more of a synonym for ‘noxious’; it means something smells bad.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44918329,
    "by": "bahmboo",
    "timeISO": "2025-08-15T23:11:10.000Z",
    "textPlain": "I'm just dipping my toe into Data Version Control - DVC. It is aimed towards data science and large digital asset management using configurable storage sources under a git meta layer. The goal is separation of concerns: git is used for versioning and the storage layers are dumb storage.Does anyone have feedback about personally using DVC vs LFS?",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44920276,
    "by": "ideasman42",
    "timeISO": "2025-08-16T04:50:25.000Z",
    "textPlain": "Something they missed out with the GitLFS cons is authentication, if you don't use SSH-agent, pushing involves authenticating multiple times, sometimes more than 2-3 times in my experience.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44918010,
    "by": "goneri",
    "timeISO": "2025-08-15T22:27:52.000Z",
    "textPlain": "git-annex is a good alternative to the solution of Githu, and it supports different storage backends. I'm actually surprised it's not more popular.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44920190,
    "by": "pabs3",
    "timeISO": "2025-08-16T04:30:57.000Z",
    "textPlain": "The git storage model also needs an overhaul to be more like modern backup tools such as restic/borg; it needs content-defined chunking of files and directories.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44923128,
    "by": "astrolx",
    "timeISO": "2025-08-16T13:15:54.000Z",
    "textPlain": "For what it's worth, I have a self hosted git forge where I upped the maximum filesize limit. I use git repositories for my science projects, they can each have hundreds of large files (> 25 Mb as described in the article).\nI don't use LFS, I don't encounter any issue.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44923908,
    "by": "tionis",
    "timeISO": "2025-08-16T14:45:24.000Z",
    "textPlain": "I've been using git + git-lfs to manage, sync and backup all my files (including media files and more) and it's quite convenient, but native support for large files would be great. I'd for example really like to be able to push large objects directly from one device to the next.At the moment I'm using my own git server + git lfs deduplication using btrfs to efficiently handle the large files.If large objects are just embedded in various packfiles this approach would no longer work, so I hope that such a behaviour can be controlled.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44925965,
    "by": "tpoacher",
    "timeISO": "2025-08-16T18:49:56.000Z",
    "textPlain": "That or rediscover the beauty of svn(only half-trolling)",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44918363,
    "by": "HexDecOctBin",
    "timeISO": "2025-08-15T23:16:02.000Z",
    "textPlain": "So this filter argument will reduce the repo size when cloning, but how will one reduce the repo size after a long stint of local commits of changing binary assets? Delete the repo and clone again?",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44917806,
    "by": "als0",
    "timeISO": "2025-08-15T22:07:22.000Z",
    "textPlain": "10 years late is better than never.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44918460,
    "by": "nixpulvis",
    "timeISO": "2025-08-15T23:29:50.000Z",
    "textPlain": "I was just using git LFS and was very concerned with how bad the help message was compared to the rest of git. I know it seems small, but it just never felt like a team player, and now I'm very happy to hear this.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44917823,
    "by": "tombert",
    "timeISO": "2025-08-15T22:08:53.000Z",
    "textPlain": "Is Git ever going to get proper support for binary files?I’ve never used it for anything serious but my understanding is that Mercurial handles binary files  better? Like it supports binary diffs if I understand correctly.Any reason Git couldn’t get that?",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44918942,
    "by": "kerneltime",
    "timeISO": "2025-08-16T00:48:36.000Z",
    "textPlain": "https://github.com/oneconcern/datamon\nHad written this git for data tool few years back (works with GCS but can be made to work with S3)\n1. No server side\n2. Immutable data (via GCS policies)\n3. Ability to mount data sets as filesystems\n4. Integrated with k8s.\nIt was built to work for the needs of the startup funding it, but I would love it if it could be extended.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44921850,
    "by": "rswail",
    "timeISO": "2025-08-16T09:55:26.000Z",
    "textPlain": "Is the problem that we don't have good \"diff\" equivalents for binaries that git could use to only store those diffs like the old RCS/CVS for large files?",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44919197,
    "by": "mathi0750",
    "timeISO": "2025-08-16T01:24:03.000Z",
    "textPlain": "Have you tried Oxen.ai? they are doing more fine-tuning and inference now but they have an open-source data version control platform written in rust at the core of their product.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44919483,
    "by": "captn3m0",
    "timeISO": "2025-08-16T02:06:56.000Z",
    "textPlain": "Partial clones are also dependent on the server side supporting this. GitHub is one of the very few that does. git.kernel.org for eg did not, last I checked.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44919858,
    "by": "moonlion_eth",
    "timeISO": "2025-08-16T03:26:16.000Z",
    "textPlain": "the future of git is jj",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44918852,
    "by": "a_t48",
    "timeISO": "2025-08-16T00:33:53.000Z",
    "textPlain": "The real GH LFS cost is not the storage but the bandwidth on pulling objects down for every fresh clone. $$$$$. See my other comment. :)",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44919684,
    "by": "CSMastermind",
    "timeISO": "2025-08-16T02:47:28.000Z",
    "textPlain": "I could really use an alternative to Plastic SCM for 3D models.  S3 is fine but lacks the nicities.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44918277,
    "by": "jiggawatts",
    "timeISO": "2025-08-15T23:03:59.000Z",
    "textPlain": "What I would love to see in an SCM that properly supports large binary blobs is storing the contents using Prolly trees instead of a simple SHA hash.Prolly trees are very similar to Merkle trees or the rsync algorithm, but they support mutation and version history retention with some nice properties. For example: you always obtain exactly the same tree (with the same root hash) irrespective of the order of incremental edit operations used to get to the same state.In other words, two users could edit a subset of a 1 TB file, both could merge their edits, and both will then agree on the root hash without having to re-hash or even download the entire file!Another major advantage on modern many-core CPUs is that Prolly trees can be constructed in parallel instead of having to be streamed sequentially on one thread.Then the really big brained move is to store the entire SCM repo as a single Prolly tree for efficient incremental downloads, merges, or whatever. I.e.: a repo fork could share storage with the original not just up to the point-in-time of the fork, but all future changes too.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44919105,
    "by": "anon-3988",
    "timeISO": "2025-08-16T01:12:40.000Z",
    "textPlain": "What prevents Git from simply working better with large files?",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44917801,
    "by": "Affric",
    "timeISO": "2025-08-15T22:07:07.000Z",
    "textPlain": "Incredible.Nice to see some Microsoft and Google emails contributing.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44917793,
    "by": "matheusmoreira",
    "timeISO": "2025-08-15T22:05:49.000Z",
    "textPlain": "As it should be! If it's not native to git, it's not worth using. I'm glad these issues are finally being solved.These new features are pretty awesome too. Especially separate large object remotes. They will probably enable git to be used for even more things than it's already being used for. They will enable new ways to work with git.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44918221,
    "by": "sublinear",
    "timeISO": "2025-08-15T22:55:12.000Z",
    "textPlain": "May I humbly suggest that those files probably belong in an LFS submodule called \"assets\" or \"vendor\"?Then you can clone without checking out all the unnecessary large files to get a working build, This also helps on the legal side to correctly license your repos.I'm struggling to see how this is a problem with git and not just antipatterns that arise from badly organized projects.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44919937,
    "by": "ghhv",
    "timeISO": "2025-08-16T03:44:36.000Z",
    "textPlain": "YtmiA 2",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44924969,
    "by": "semperMade",
    "timeISO": "2025-08-16T16:42:40.000Z",
    "textPlain": "[dead]",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44919614,
    "by": "jeffWrld",
    "timeISO": "2025-08-16T02:31:48.000Z",
    "textPlain": "[dead]",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44918094,
    "by": "forrestthewoods",
    "timeISO": "2025-08-15T22:37:49.000Z",
    "textPlain": "Git is fundamentally broken and bad. Almost all projects are defacto centralized. Your project is not Linux.A good version control system would support petabyte scale history and terabyte scale clones via sparse virtual filesystem.Git’s design is just bad for almost all projects that aren’t Linux.(I know this will get downvoted. But most modern programmers have never used anything but Git and so they don’t realize their tool is actually quite bad! It’s a shame.)",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44920656,
    "by": "jokoon",
    "timeISO": "2025-08-16T06:08:54.000Z",
    "textPlain": "Git lfs seems controversial in the comments hereDevelopers have their own drama",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44919026,
    "by": "whatever1",
    "timeISO": "2025-08-16T01:01:18.000Z",
    "textPlain": "It is insane that almost after a century of running computations with data on computers we still don't have a good version control system that maps a code version to its relevant data version.Still the approach is to put code and data in a folder and call it a day. Slap a \"_FINAL\" at the folder name and you are golden.",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44918560,
    "by": "firesteelrain",
    "timeISO": "2025-08-15T23:45:32.000Z",
    "textPlain": "We had a repo that was at one point 25GB. It had Git LFS turned on but the files weren’t stored outside of BitBucket. Whenever a build was run in Bamboo, it would choke big time.We found that we could move the large files to Artifactory as it has Git LFS support.But the problem was the entire history that did not have Artifactory pointers. Every clone included the large files (for some reason the filter functionality wouldn’t work for us - it was a large repo and it it had hundreds of users amongst other problems)Anyways what we ended up doing was closing that repo and opening a new one with the large files stripped.Nitpick in the authors page:“ Nowadays, there’s a free tier, but you’re dependent on the whims of GitHub to set pricing. Today, a 50GB repo on GitHub will cost $40/year for storage”This is not true as you don’t need GitHub to get LFS support",
    "parent": 44916783,
    "depth": 1
  },
  {
    "id": 44921725,
    "by": "riedel",
    "timeISO": "2025-08-16T09:31:13.000Z",
    "textPlain": "The article mentions alternatives to git lfs like git annex that support S3 already (which IMHO is however still a bit of a pain in the ass on windows due to the symlink workflow). Also dvc plays nicely with git and S3. Gitlab btw also simply offloads git lfs to S3. All have their quirks. I typically opt for LFS as a no-brainer but use the others when it fits the workflow and the infrastructure requirements.Edit: Particularly the hash algorithm and the change detection (also when this happens) makes a difference if you have 2 GB files and not only the 25MB file from the OP",
    "parent": 44918580,
    "depth": 2
  },
  {
    "id": 44918823,
    "by": "a_t48",
    "timeISO": "2025-08-16T00:30:31.000Z",
    "textPlain": "At my current job I've started caching all of our LFS objects in a bucket, for cost reasons. Every time a PR is run, I get the list of objects via `git lfs ls-files`, sync them from gcp, run `git lfs checkout` to actually populate the repo from the object store, and then `git lfs pull` to pick up anything not cached. If there were uncached objects, I push them back up via `gcloud storage rsync`. Simple, doesn't require any configuration for developers (who only ever have to pull new objects), keeps the Github UI unconfused about the state of the repo.I'd initially at spinning up an LFS backends, but this solves the main pain point, for now. Github was charging us an arm and a leg for pulling LFS files for CI, because each checkout is fresh, the caching model is non-ideal (max 10GB cache, impossible to share between branches), so we end up pulling a bunch of data that is unfortunately in LFS, every commit, possibly multiple times. Because of this they happily charge us for all that bandwidth, because they don't provide tools to make it easy to reduce bandwidth (let me pay for more cache size, or warm workers with an entire cache disc, or better cache control, or...)....and if I want to enable this for developers it's relatively easy, just add a new git hook to do the same set of operations locally.",
    "parent": 44918580,
    "depth": 2
  },
  {
    "id": 44919099,
    "by": "nullwarp",
    "timeISO": "2025-08-16T01:11:52.000Z",
    "textPlain": "Same and never understood why it wasn't the default from the get go but maybe it wasn't so synonymous when it first came out.I run a small git LFS server because of this and will be happy to switch away the second I can get git to natively support S3.",
    "parent": 44918580,
    "depth": 2
  },
  {
    "id": 44922960,
    "by": "_bent",
    "timeISO": "2025-08-16T12:55:52.000Z",
    "textPlain": "I'm currently running https://github.com/datopian/giftless to store the LFS files belonging to repos I have on GitHub on my homelab miniio instance.There are a couple other projects that bridge S3 and LFS, though I had the most success with this setup.",
    "parent": 44918580,
    "depth": 2
  },
  {
    "id": 44920933,
    "by": "johnisgood",
    "timeISO": "2025-08-16T07:00:34.000Z",
    "textPlain": "Is S3 related to Amazon?",
    "parent": 44918580,
    "depth": 2
  },
  {
    "id": 44917976,
    "by": "IshKebab",
    "timeISO": "2025-08-15T22:24:28.000Z",
    "textPlain": "LFS is bad. The server implementations suck. It conflates object contents with the storage method. It's opt-in, in a terrible way - if you do the obvious thing you get tiny text files instead of the files you actually want.I dunno if their solution is any better but it's fairly unarguable that LFS is bad.",
    "parent": 44917895,
    "depth": 2
  },
  {
    "id": 44919191,
    "by": "AceJohnny2",
    "timeISO": "2025-08-16T01:23:25.000Z",
    "textPlain": "Another way that LFS is bad, as I recently discovered, is that the migration will pollute the `.gitattributes` of ancestor commits that do not contain the LFS objects.In other words, if you migrate a repo that has commits A->B->C, and C adds the large files, then commits A & B will gain a `.gitattributes` referring to the large files that do not exist in A & B.This is because the migration function will carry its ~gitattributes structure backwards as it walks the history, for caching purposes, and not cross-reference it against the current commit.",
    "parent": 44917895,
    "depth": 2
  }
]
[
  {
    "id": 44881226,
    "by": "not_that_d",
    "timeISO": "2025-08-12T20:07:28.000Z",
    "textPlain": "My experience with the current tools so far:1. It helps to get me going with new languages, frameworks, utilities or full green field stuff. After that I expend a lot of time parsing the code to understand what it wrote that I kind of \"trust\" it because it is too tedious but \"it works\".2. When working with languages or frameworks that I know, I find it makes me unproductive, the amount of time I spend writing a good enough prompt with the correct context is almost the same or more that if I write the stuff myself and to be honest the solution that it gives me works for this specific case but looks like a junior code with pitfalls that are not that obvious unless you have the experience to know it.I used it with Typescript, Kotlin, Java and C++, for different scenarios, like websites, ESPHome components (ESP32), backend APIs, node scripts etc.Botton line: usefull for hobby projects, scripts and to prototypes, but for enterprise level code it is not there.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878643,
    "by": "aliljet",
    "timeISO": "2025-08-12T16:33:13.000Z",
    "textPlain": "This is definitely one of my CORE problem as I use these tools for \"professional software engineering.\"  I really desperately need LLMs to maintain extremely effective context and it's not actually that interesting to see a new model that's marginally better than the next one (for my day-to-day).However. Price is king. Allowing me to flood the context window with my code base is great, but given that the price has substantially increased, it makes sense to better manage the context window into the current situation. The value I'm getting here flooding their context window is great for them, but short of evals that look into how effective Sonnet stays on track, it's not clear if the value actually exists here.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879053,
    "by": "gdudeman",
    "timeISO": "2025-08-12T17:03:54.000Z",
    "textPlain": "A tip for those who both use Claude Code and are worried about token use (which you should be if you're stuffing 400k tokens into context even if you're on 20x Max):  1. Build context for the work you're doing. Put lots of your codebase into the context window.\n  2. Do work, but at each logical stopping point hit double escape to rewind to the context-filled checkpoint. You do not spend those tokens to rewind to that point.\n  3. Tell Claude your developer finished XYZ, have it read it into context and give high level and low level feedback (Claude will find more problems with your developer's work than with yours).\n\nIf you want to have multiple chats running, use /resume and pull up the same thread. Hit double escape to the point where Claude has rich context, but has not started down a specific rabbit hole.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878453,
    "by": "tankenmate",
    "timeISO": "2025-08-12T16:21:54.000Z",
    "textPlain": "This is definitely good to have this as an option but at the same time having more context reduces the quality of the output because it's easier for the LLM to get \"distracted\". So, I wonder what will happen to the quality of code produced by tools like Claude Code if users don't properly understand the trade off being made (if they leave it in auto mode of coding right up to the auto compact).",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44882149,
    "by": "meander_water",
    "timeISO": "2025-08-12T21:45:21.000Z",
    "textPlain": "I like to spend a lot of time in \"Ask\" mode in Cursor. I guess the equivalent in Claude code is \"plan\" mode.Where I have minimal knowledge about the framework or language, I ask a lot of questions about how the implementation would work, what the tradeoffs are etc. This is to minimize any misunderstanding between me and the tool. Then I ask it to write the implementation plan, and execute it one by one.Cursor lets you have multiple tabs open so I'll have a Ask mode and Agent mode running in parallel.This is a lot slower, and if it was a language/framework I'm familiar with I'm more likely to execute the plan myself.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44881588,
    "by": "howinator",
    "timeISO": "2025-08-12T20:42:26.000Z",
    "textPlain": "I could be wrong, but I think this pricing is the first to admit that cost scales quadratically with number of tokens. It’s the first time I’ve seen nonlinear pricing from an LLM provider which implicitly mirrors the inference scaling laws I think we're all aware of.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879268,
    "by": "firasd",
    "timeISO": "2025-08-12T17:17:24.000Z",
    "textPlain": "A big problem with the chat apps (ChatGPT; Claude.ai) is the weird context window hijinks. Especially ChatGPT does wild stuff.. sudden truncation; summarization; reinjecting 'ghost snippets' etcI was thinking this should be up to the user (do you want to continue this conversation with context rolling out of the window or start a new chat) but now I realized that this is inevitable given the way pricing tiers and limited computation works. Like the only way to have full context is use developer tools like Google AI Studio or use a chat app that wraps the APIWith a custom chat app that wraps the API you can even inject the current timestamp into each message and just ask the LLM btw every 10 minutes just make a new row in a markdown table that summarizes every 10 min chunk",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44882059,
    "by": "phyzix5761",
    "timeISO": "2025-08-12T21:34:51.000Z",
    "textPlain": "What I've found with LLMs is they're basically a better version of Google Search. If I need a quick \"How do I do...\" or if I need to find a quick answer to something its way more useful than Google and the fact that I can ask follow up questions is amazing. But for any serious deep work it has a long way to go.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878804,
    "by": "isoprophlex",
    "timeISO": "2025-08-12T16:45:51.000Z",
    "textPlain": "1M of input... at $6/1M input tokens. Better hope it can one-shot your answer.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878905,
    "by": "Someone1234",
    "timeISO": "2025-08-12T16:53:02.000Z",
    "textPlain": "Before this they supposedly had a longer context window than ChatGPT, but I have workloads that abuse the heck out of context windows (100-120K tokens). ChatGPT genuinely seems to have a 32K context window, in the sense that is legitimately remembers/can utilize everything within that window.Claude previously had \"200K\" context windows, but during testing it wouldn't even hit a full 32K before hitting a wall/it forgetting earlier parts of the context. They also have extremely short prompt limits relative to the other services around, making it hard to utilize their supposedly larger context windows (which is suspicious).I guess my point is that with Anthropic specifically, I don't trust their claims because that has been my personal experience. It would be nice if this \"1M\" context window now allows you to actually use 200K though, but it remains to be seen if it can even do that. As I said with Anthropic you need to verify everything they claim.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879248,
    "by": "xnx",
    "timeISO": "2025-08-12T17:15:33.000Z",
    "textPlain": "1M context windows are not created equal. I doubt Claude's recall is as good as Gemini's 1M context recall. https://cloud.google.com/blog/products/ai-machine-learning/t...",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879826,
    "by": "dang",
    "timeISO": "2025-08-12T18:06:56.000Z",
    "textPlain": "Related ongoing thread:Claude vs. Gemini: Testing on 1M Tokens of Context - https://news.ycombinator.com/item?id=44878999 - Aug 2025 (9 comments)",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44880637,
    "by": "qwertox",
    "timeISO": "2025-08-12T19:15:32.000Z",
    "textPlain": "> desperately need LLMs to maintain extremely effective contextLast time I used Gemini it did something very surprising: instead of providing readable code, it started to generate pseudo-minified code.Like on CSS class would become one long line of CSS, and one JS function became one long line of JS, with most of the variable names minified, while some remained readable, but short. It did away with all unnecessary spaces.I was asking myself what is happening here, and my only explanation was that maybe Google started training Gemini on minified code, on making Gemini understand and generate it, in order to maximize the value of every token.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879563,
    "by": "simianwords",
    "timeISO": "2025-08-12T17:42:53.000Z",
    "textPlain": "How does \"supporting 1M tokens\" really work in practice? Is it a new model? Or did they just remove some hard coded constraint?",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878514,
    "by": "falcor84",
    "timeISO": "2025-08-12T16:25:56.000Z",
    "textPlain": "Strange that they don't mention whether that's enabled or configurable in Claude Code.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878889,
    "by": "jbellis",
    "timeISO": "2025-08-12T16:51:58.000Z",
    "textPlain": "Just completed a new benchmark that sheds some light on whether Anthropic's premium is worth it.(Short answer: not unless your top priority is speed.)https://brokk.ai/power-rankings",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879037,
    "by": "varyherb",
    "timeISO": "2025-08-12T17:02:48.000Z",
    "textPlain": "I believe this can be configured in Claude Code via the following environment variable:ANTHROPIC_BETAS=\"context-1m-2025-08-07\" claude",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44880177,
    "by": "joduplessis",
    "timeISO": "2025-08-12T18:38:00.000Z",
    "textPlain": "As far as coding goes Claude seems to be the most competent right now, I like it. GPT5 is abysmal - I'm not sure if they're bugs, or what, but the new release takes a good few steps back. Gemini still a hit and miss - and Grok seems to be a poor man's Claude (where code is kind of okay, a bit buggy and somehow similar to Claude).",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44880348,
    "by": "brokegrammer",
    "timeISO": "2025-08-12T18:52:08.000Z",
    "textPlain": "Many people are confused about the usefulness of 1M tokens because LLMs often start to get confused after about 100k. But this is big for Claude 4 because it uses automatic RAG when the context becomes large. With optimized retrieval thanks to RAG, we'll be able to make good use of those 1M tokens.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878701,
    "by": "greenfish6",
    "timeISO": "2025-08-12T16:37:25.000Z",
    "textPlain": "Yes, but if you look in the rate limit notes, the rate limit is 500k tokens / minite for tier 4, which we are on. Given how stingy anthropic has been with rate limit increases, this is for very few people right now",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878781,
    "by": "qsort",
    "timeISO": "2025-08-12T16:43:55.000Z",
    "textPlain": "I won't complain about a strict upgrade, but that's a pricy boi. Interesting to see differential pricing based on size of input, which is understandable given the O(n^2) nature of attention.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878872,
    "by": "lherron",
    "timeISO": "2025-08-12T16:50:43.000Z",
    "textPlain": "Wow, I thought they would feel some pricing pressure from GPT5 API costs, but they are doubling down on their API being more expensive than everyone else.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44880683,
    "by": "ericol",
    "timeISO": "2025-08-12T19:19:40.000Z",
    "textPlain": "\"...in API\"That's a VERY relevant clarification. this DOESN'T apply to web or app users.Basically, if you want a 1M context window you have to specifically pay for it.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44881571,
    "by": "cadamsdotcom",
    "timeISO": "2025-08-12T20:40:50.000Z",
    "textPlain": "I’m glad to see the only company chasing margins - which they get by having a great product and a meticulous brand - finding even more ways to get margin. That’s good business.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879690,
    "by": "chrisweekly",
    "timeISO": "2025-08-12T17:54:01.000Z",
    "textPlain": "Peer of this post currently also on HN front page, comparing perf for Claude vs Gemini, w/ 1M tokens: https://news.ycombinator.com/item?id=44878999",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879819,
    "by": "ffitch",
    "timeISO": "2025-08-12T18:06:13.000Z",
    "textPlain": "I wonder how modern models fair on NovelQA and FLenQA (benchmarks that test ability to understand long context beyond needle in a haystack retrieval). The only such test on a reasoning model that I found was done on o3-mini-high (https://arxiv.org/abs/2504.21318), it suggests that reasoning noticeably improves FLenQA performance, but this test only explored context up to 3,000 tokens.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879005,
    "by": "film42",
    "timeISO": "2025-08-12T17:00:18.000Z",
    "textPlain": "The 1M token context was Gemini's headlining feature. Now, the only thing I'd like Claude to work on is tokens counted towards document processing. Gemini will often bill 1/10th the tokens Anthropic does for the same document.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878570,
    "by": "mettamage",
    "timeISO": "2025-08-12T16:28:29.000Z",
    "textPlain": "Shame it's only the API. Would've loved to see it via the web interface on claude.ai itself.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44880855,
    "by": "sporkland",
    "timeISO": "2025-08-12T19:31:58.000Z",
    "textPlain": "Does anyone have data on how much better these 1M token context models produce better results than the more limited windows alongside certain RAG implementations?  Or how much better in the face of RAG the 200k vs 1M token models perform on a benchmark?",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44880579,
    "by": "irthomasthomas",
    "timeISO": "2025-08-12T19:10:07.000Z",
    "textPlain": "Brain: Hey, you going to sleep?\nMe: Yes.\nBrain: That 200,001st token cost you $600,000/M.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44881549,
    "by": "fpauser",
    "timeISO": "2025-08-12T20:38:01.000Z",
    "textPlain": "O observed that claude produces a lot of bloat. Wonder how such llm generated projects age.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879884,
    "by": "pmxi",
    "timeISO": "2025-08-12T18:11:56.000Z",
    "textPlain": "The reason I initially got interested in Claude was because they were the first to offer a 200K token context window. That was massive in 2023. However, they didn't keep up once Gemini offered a 1M token window last year.I'm glad to see an attempt to return to having a competitive context window.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878677,
    "by": "pupppet",
    "timeISO": "2025-08-12T16:35:18.000Z",
    "textPlain": "How does anyone send these models that much context without it tripping over itself?  I can't get anywhere near that much before it starts losing track of instruction.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879173,
    "by": "thimabi",
    "timeISO": "2025-08-12T17:10:41.000Z",
    "textPlain": "Oh, well, ChatGPT is being left in the dust…When done correctly, having one million tokens of context window is amazing for all sorts of tasks: understanding large codebases, summarizing books, finding information on many documents, etc.Existing RAG solutions fill a void up to a point, but they lack the precision that large context windows offer.I’m excited for this release and hope to see it soon on the UI as well.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44881766,
    "by": "_joel",
    "timeISO": "2025-08-12T21:00:34.000Z",
    "textPlain": "Fantastic, use up your quota even more quickly. :)",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44881328,
    "by": "mrcwinn",
    "timeISO": "2025-08-12T20:19:24.000Z",
    "textPlain": "This tells me they've gotten very good at caching and modeling the impact of caching.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879732,
    "by": "DiabloD3",
    "timeISO": "2025-08-12T17:57:26.000Z",
    "textPlain": "Neat. I do 1M tokens context locally, and do it entirely with a single GPU and FOSS software, and have access to a wide range of models of equivalent or better quality.Explain to me, again, how Anthropic's flawed business model works?",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44881007,
    "by": "poniko",
    "timeISO": "2025-08-12T19:43:22.000Z",
    "textPlain": "[Claude usage limit reached. Your limit will reset at..] .. eh lunch is a good time to go home anyways..",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44881014,
    "by": "chmod775",
    "timeISO": "2025-08-12T19:44:11.000Z",
    "textPlain": "For some context, only the tweaks files and scripting parts of Cyberpunk 2077 are ~2 million LOC.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879596,
    "by": "alienbaby",
    "timeISO": "2025-08-12T17:45:56.000Z",
    "textPlain": "The fracturing of all the models offered across providers is annoying. The number of different models and the fact a given model will have different capabilities from different providers is ridiculous.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878880,
    "by": "shamano",
    "timeISO": "2025-08-12T16:51:16.000Z",
    "textPlain": "1M tokens is impressive, but the real gains will come from how we curate context—compact summaries, per-repo indexes, and phase resets. Bigger windows help; guardrails keep models focused and costs predictable.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878529,
    "by": "faangguyindia",
    "timeISO": "2025-08-12T16:26:54.000Z",
    "textPlain": "In my testing the gap between claude and gemini pro 2.5 is close.\nMy company is in asia pacific and we can't get access to claude via vertex for some stupid reason.but i tested it via other providers, the gap used to be huge but now not.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879463,
    "by": "ramoz",
    "timeISO": "2025-08-12T17:35:01.000Z",
    "textPlain": "Awesome addition to a great model.The best interface for long context reasoning has been AIStudio by Google. Exceptional experience.I use Prompt Tower to create long context payloads.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44881665,
    "by": "reverseblade2",
    "timeISO": "2025-08-12T20:49:29.000Z",
    "textPlain": "Does this cover subscription?",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879140,
    "by": "ZeroCool2u",
    "timeISO": "2025-08-12T17:09:09.000Z",
    "textPlain": "It's great they've finally caught up, but unfortunate it's on their mid-tier model only and it's laughably expensive.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44880164,
    "by": "siva7",
    "timeISO": "2025-08-12T18:36:12.000Z",
    "textPlain": "Ah, so claude code on subscription will become a crippled down version",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878606,
    "by": "penguin202",
    "timeISO": "2025-08-12T16:30:22.000Z",
    "textPlain": "But will it remember any of it, and stop creating new redundant files when it can't find or understand what its looking for?",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879195,
    "by": "kotaKat",
    "timeISO": "2025-08-12T17:11:40.000Z",
    "textPlain": "A million tokens? Damn, I’m gonna need a lot of quarters to play this game at Chuck-E-Cheese.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878929,
    "by": "lvl155",
    "timeISO": "2025-08-12T16:54:43.000Z",
    "textPlain": "Only time this is useful is to do init on a sizable code base or dump a “big” csv.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878668,
    "by": "rootnod3",
    "timeISO": "2025-08-12T16:34:54.000Z",
    "textPlain": "So, more tokens means better but at the same time more tokens means it distracts itself too much along the way. So at the same time it is an improvement but also potentially detrimental. How are those things beneficial in any capacity? What was said last week? Embrace AI or leave?All I see so far is: don't embrace and stay.",
    "parent": 44878147,
    "depth": 1
  }
]
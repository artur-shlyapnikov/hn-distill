[
  {
    "id": 44878643,
    "by": "aliljet",
    "timeISO": "2025-08-12T16:33:13.000Z",
    "textPlain": "This is definitely one of my CORE problem as I use these tools for \"professional software engineering.\"  I really desperately need LLMs to maintain extremely effective context and it's not actually that interesting to see a new model that's marginally better than the next one (for my day-to-day).However. Price is king. Allowing me to flood the context window with my code base is great, but given that the price has substantially increased, it makes sense to better manage the context window into the current situation. The value I'm getting here flooding their context window is great for them, but short of evals that look into how effective Sonnet stays on track, it's not clear if the value actually exists here.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879053,
    "by": "gdudeman",
    "timeISO": "2025-08-12T17:03:54.000Z",
    "textPlain": "A tip for those who both use Claude Code and are worried about token use (which you should be if you're stuffing 400k tokens into context even if you're on 20x Max):  1. Build context for the work you're doing. Put lots of your codebase into the context window.\n  2. Do work, but at each logical stopping point hit double escape to rewind to the context-filled checkpoint. You do not spend those tokens to rewind to that point.\n  3. Tell Claude your developer finished XYZ, have it read it into context and give high level and low level feedback (Claude will find more problems with your developer's work than with yours).\n\nIf you want to have multiple chats running, use /resume and pull up the same thread. Hit double escape to the point where Claude has rich context, but has not started down a specific rabbit hole.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878453,
    "by": "tankenmate",
    "timeISO": "2025-08-12T16:21:54.000Z",
    "textPlain": "This is definitely good to have this as an option but at the same time having more context reduces the quality of the output because it's easier for the LLM to get \"distracted\". So, I wonder what will happen to the quality of code produced by tools like Claude Code if users don't properly understand the trade off being made (if they leave it in auto mode of coding right up to the auto compact).",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44881226,
    "by": "not_that_d",
    "timeISO": "2025-08-12T20:07:28.000Z",
    "textPlain": "My experience with the current tools so far:1. It helps to get me going with new languages, frameworks, utilities or full green field stuff. After that I expend a lot of time parsing the code to understand what it wrote that I kind of \"trust\" it because it is too tedious but \"it works\".2. When working with languages or frameworks that I know, I find it makes me unproductive, the amount of time I spend writing a good enough prompt with the correct context is almost the same or more that if I write the stuff myself and to be honest the solution that it gives me works for this specific case but looks like a junior code with pitfalls that are not that obvious unless you have the experience to know it.I used it with Typescript, Kotlin, Java and C++, for different scenarios, like websites, ESPHome components (ESP32), backend APIs, node scripts etc.Botton line: usefull for hobby projects, scripts and to prototypes, but for enterprise level code it is not there.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44882629,
    "by": "lpa22",
    "timeISO": "2025-08-12T22:42:33.000Z",
    "textPlain": "One of the most helpful usages of CC so far is when I simply ask:\"Are there any bugs in the current diff\"It analyzes the changes very thoroughly, often finds very subtle bugs that would cost hours of time/deployments down the line, and points out a bunch of things to think through for correctness.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44883071,
    "by": "psyclobe",
    "timeISO": "2025-08-12T23:43:03.000Z",
    "textPlain": "Isn’t Opus better? Whenever I run out of Opus tokens and get kicked down to Sonnet it’s quite a shock sometimes.But man I’m at the perfect stage in my career for these tools. I know a lot, I understand a lot, I have a lot of great ideas-but I’m getting kinda tired of hammering out code all day long. Now with Claude I am just busting ass executing in all these ideas and tests and fixes-never going back!",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879268,
    "by": "firasd",
    "timeISO": "2025-08-12T17:17:24.000Z",
    "textPlain": "A big problem with the chat apps (ChatGPT; Claude.ai) is the weird context window hijinks. Especially ChatGPT does wild stuff.. sudden truncation; summarization; reinjecting 'ghost snippets' etcI was thinking this should be up to the user (do you want to continue this conversation with context rolling out of the window or start a new chat) but now I realized that this is inevitable given the way pricing tiers and limited computation works. Like the only way to have full context is use developer tools like Google AI Studio or use a chat app that wraps the APIWith a custom chat app that wraps the API you can even inject the current timestamp into each message and just ask the LLM btw every 10 minutes just make a new row in a markdown table that summarizes every 10 min chunk",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44881588,
    "by": "howinator",
    "timeISO": "2025-08-12T20:42:26.000Z",
    "textPlain": "I could be wrong, but I think this pricing is the first to admit that cost scales quadratically with number of tokens. It’s the first time I’ve seen nonlinear pricing from an LLM provider which implicitly mirrors the inference scaling laws I think we're all aware of.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879826,
    "by": "dang",
    "timeISO": "2025-08-12T18:06:56.000Z",
    "textPlain": "Related ongoing thread:Claude vs. Gemini: Testing on 1M Tokens of Context - https://news.ycombinator.com/item?id=44878999 - Aug 2025 (9 comments)",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878804,
    "by": "isoprophlex",
    "timeISO": "2025-08-12T16:45:51.000Z",
    "textPlain": "1M of input... at $6/1M input tokens. Better hope it can one-shot your answer.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44882149,
    "by": "meander_water",
    "timeISO": "2025-08-12T21:45:21.000Z",
    "textPlain": "I like to spend a lot of time in \"Ask\" mode in Cursor. I guess the equivalent in Claude code is \"plan\" mode.Where I have minimal knowledge about the framework or language, I ask a lot of questions about how the implementation would work, what the tradeoffs are etc. This is to minimize any misunderstanding between me and the tool. Then I ask it to write the implementation plan, and execute it one by one.Cursor lets you have multiple tabs open so I'll have a Ask mode and Agent mode running in parallel.This is a lot slower, and if it was a language/framework I'm familiar with I'm more likely to execute the plan myself.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879248,
    "by": "xnx",
    "timeISO": "2025-08-12T17:15:33.000Z",
    "textPlain": "1M context windows are not created equal. I doubt Claude's recall is as good as Gemini's 1M context recall. https://cloud.google.com/blog/products/ai-machine-learning/t...",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878905,
    "by": "Someone1234",
    "timeISO": "2025-08-12T16:53:02.000Z",
    "textPlain": "Before this they supposedly had a longer context window than ChatGPT, but I have workloads that abuse the heck out of context windows (100-120K tokens). ChatGPT genuinely seems to have a 32K context window, in the sense that is legitimately remembers/can utilize everything within that window.Claude previously had \"200K\" context windows, but during testing it wouldn't even hit a full 32K before hitting a wall/it forgetting earlier parts of the context. They also have extremely short prompt limits relative to the other services around, making it hard to utilize their supposedly larger context windows (which is suspicious).I guess my point is that with Anthropic specifically, I don't trust their claims because that has been my personal experience. It would be nice if this \"1M\" context window now allows you to actually use 200K though, but it remains to be seen if it can even do that. As I said with Anthropic you need to verify everything they claim.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44884217,
    "by": "simon_rider",
    "timeISO": "2025-08-13T02:54:34.000Z",
    "textPlain": "feels like we just traded \"not enough context\" for \"too much noise.\" The million-token window is cool for marketing, but until retrieval and summarization get way better, it’s like dumping the entire repo on a junior dev’s desk and saying \"figure it out.\" They’ll spend half their time paging through irrelevant crap, and the other half hallucinating connections. Bigger context is only a net win if the model can filter, prioritize, and actually reason over it",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44887594,
    "by": "muzani",
    "timeISO": "2025-08-13T12:25:39.000Z",
    "textPlain": "Of course Bolt is the customer spotlight. These vibe coding tools chuck the entire codebase and charge for tokens used. By 10k lines of code or so, these apps were not able to fit.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44887778,
    "by": "ndkap",
    "timeISO": "2025-08-13T12:45:06.000Z",
    "textPlain": "Does anybody know which technology most of these companies that support 1M tokens use? Or is it all hidden?",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879563,
    "by": "simianwords",
    "timeISO": "2025-08-12T17:42:53.000Z",
    "textPlain": "How does \"supporting 1M tokens\" really work in practice? Is it a new model? Or did they just remove some hard coded constraint?",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44882059,
    "by": "phyzix5761",
    "timeISO": "2025-08-12T21:34:51.000Z",
    "textPlain": "What I've found with LLMs is they're basically a better version of Google Search. If I need a quick \"How do I do...\" or if I need to find a quick answer to something its way more useful than Google and the fact that I can ask follow up questions is amazing. But for any serious deep work it has a long way to go.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878514,
    "by": "falcor84",
    "timeISO": "2025-08-12T16:25:56.000Z",
    "textPlain": "Strange that they don't mention whether that's enabled or configurable in Claude Code.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878889,
    "by": "jbellis",
    "timeISO": "2025-08-12T16:51:58.000Z",
    "textPlain": "Just completed a new benchmark that sheds some light on whether Anthropic's premium is worth it.(Short answer: not unless your top priority is speed.)https://brokk.ai/power-rankings",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879037,
    "by": "varyherb",
    "timeISO": "2025-08-12T17:02:48.000Z",
    "textPlain": "I believe this can be configured in Claude Code via the following environment variable:ANTHROPIC_BETAS=\"context-1m-2025-08-07\" claude",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878701,
    "by": "greenfish6",
    "timeISO": "2025-08-12T16:37:25.000Z",
    "textPlain": "Yes, but if you look in the rate limit notes, the rate limit is 500k tokens / minite for tier 4, which we are on. Given how stingy anthropic has been with rate limit increases, this is for very few people right now",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44884775,
    "by": "mrcwinn",
    "timeISO": "2025-08-13T04:59:52.000Z",
    "textPlain": "I wish they’d fix other things faster. Still can’t upload an Excel file in the iOS app, even with analyst mode enabled. The voice mode feels like it’s 10 years behind OpenAI (no realtime, for example). And Opus 4.1 still occasionally goes absolutely mental and provides much worse analysis than o3 or GPT5-thinking.Rooting for Anthropic. Competition in this space is good.I watched an interview with Dario recently where he said he wasted a “product guy” and it really shows.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878781,
    "by": "qsort",
    "timeISO": "2025-08-12T16:43:55.000Z",
    "textPlain": "I won't complain about a strict upgrade, but that's a pricy boi. Interesting to see differential pricing based on size of input, which is understandable given the O(n^2) nature of attention.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44886321,
    "by": "Roark66",
    "timeISO": "2025-08-13T09:28:00.000Z",
    "textPlain": "I noticed the quality of answer degrades horribly beyond few thousands of tokens. Maybe 10k. Is anyone actually successfully using these 100k+ token contexts for anything?",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878677,
    "by": "pupppet",
    "timeISO": "2025-08-12T16:35:18.000Z",
    "textPlain": "How does anyone send these models that much context without it tripping over itself?  I can't get anywhere near that much before it starts losing track of instruction.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44882930,
    "by": "i_have_an_idea",
    "timeISO": "2025-08-12T23:22:46.000Z",
    "textPlain": "While this is cool, can anything be done about the speed of inference?At least for my use, 200K context is fine, but I’d like to see a lot faster task completion. I feel like more people would be OK with the smaller context if the agent acts quickly (vs waiting 2-3 mins per prompt).",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44883518,
    "by": "aledalgrande",
    "timeISO": "2025-08-13T00:57:01.000Z",
    "textPlain": "I hope that they are going to put something in Claude Code to display if you're entering the expensive window. Sometime I just keep the conversation going. I wouldn't want that to burn my Max credits 2x faster.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878872,
    "by": "lherron",
    "timeISO": "2025-08-12T16:50:43.000Z",
    "textPlain": "Wow, I thought they would feel some pricing pressure from GPT5 API costs, but they are doubling down on their API being more expensive than everyone else.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44884623,
    "by": "cintusshied",
    "timeISO": "2025-08-13T04:22:15.000Z",
    "textPlain": "For folks using LLMs for big coding projects, what's your go-to workflow for deciding which parts of the codebase to feed the model?",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44880177,
    "by": "joduplessis",
    "timeISO": "2025-08-12T18:38:00.000Z",
    "textPlain": "As far as coding goes Claude seems to be the most competent right now, I like it. GPT5 is abysmal - I'm not sure if they're bugs, or what, but the new release takes a good few steps back. Gemini still a hit and miss - and Grok seems to be a poor man's Claude (where code is kind of okay, a bit buggy and somehow similar to Claude).",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44884912,
    "by": "cognix_dev",
    "timeISO": "2025-08-13T05:26:39.000Z",
    "textPlain": "To be honest, I am not particularly interested in whether the next model is better than the previous one.\nRather than being superior, it is important that it maintains context and has enough memory capacity to not interfere with work. I believe that is what makes the new model competitive.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44883426,
    "by": "tomsanbear",
    "timeISO": "2025-08-13T00:42:41.000Z",
    "textPlain": "I just want a better way to invalidate old context... It's great that I can fit more context, but the main challenge is claude getting sidetracked with 10 invalid grep calls, pytest dumping a 10k token stack trace etc.... And yes the ability to go back in time via esc+esc is great but I want claude to read the error stack learn from it and purge from its context or at least let me lobotomize ot selectively... \nLearning and discarding the raw output from tool calls feels like the missing piece here still.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44883078,
    "by": "as367",
    "timeISO": "2025-08-12T23:44:05.000Z",
    "textPlain": "That is an unfortunate logo.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44880348,
    "by": "brokegrammer",
    "timeISO": "2025-08-12T18:52:08.000Z",
    "textPlain": "Many people are confused about the usefulness of 1M tokens because LLMs often start to get confused after about 100k. But this is big for Claude 4 because it uses automatic RAG when the context becomes large. With optimized retrieval thanks to RAG, we'll be able to make good use of those 1M tokens.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878570,
    "by": "mettamage",
    "timeISO": "2025-08-12T16:28:29.000Z",
    "textPlain": "Shame it's only the API. Would've loved to see it via the web interface on claude.ai itself.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44880637,
    "by": "qwertox",
    "timeISO": "2025-08-12T19:15:32.000Z",
    "textPlain": "> desperately need LLMs to maintain extremely effective contextLast time I used Gemini it did something very surprising: instead of providing readable code, it started to generate pseudo-minified code.Like on CSS class would become one long line of CSS, and one JS function became one long line of JS, with most of the variable names minified, while some remained readable, but short. It did away with all unnecessary spaces.I was asking myself what is happening here, and my only explanation was that maybe Google started training Gemini on minified code, on making Gemini understand and generate it, in order to maximize the value of every token.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879690,
    "by": "chrisweekly",
    "timeISO": "2025-08-12T17:54:01.000Z",
    "textPlain": "Peer of this post currently also on HN front page, comparing perf for Claude vs Gemini, w/ 1M tokens: https://news.ycombinator.com/item?id=44878999",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879005,
    "by": "film42",
    "timeISO": "2025-08-12T17:00:18.000Z",
    "textPlain": "The 1M token context was Gemini's headlining feature. Now, the only thing I'd like Claude to work on is tokens counted towards document processing. Gemini will often bill 1/10th the tokens Anthropic does for the same document.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44882591,
    "by": "itissid",
    "timeISO": "2025-08-12T22:38:29.000Z",
    "textPlain": "My experience with Claude code beyond building anything bigger than a webpage, a small API, a tutorial on CSS etc has been pretty bad. I think context length is a manageable problem, but not the main one. I used it to write a 50K LoC python code base with 300 unit tests and it went ok for the first few weeks and then it failed. This is after there is a CLAUDE.md file for every single module that needs it as well as detailed agents for testing, design, coding and review.I won't going into a case by case list of its failures, The core of the issue is misaligned incentives, which I want to get into:1. The incentives for coding agent, in general and claude, are writing LOTS of code. None of them — O —  are good at the planning and verification.2. The involvement of the human, ironically, in a haphazard way in the agent's process. And this has to do with how the problem of coding for these agents is defined. Human developers are like snow flakes when it comes to opinions on software design, there is no way to apply each's preference(except paper machet and superglue SO, Reddit threads and books) to the design of the system in any meaningful way and that makes a simple system way too complex or it makes a complex problem simplistic.  - There is no way to evolve the plan to accept new preferences except text in CLAUDE.md file in git that you will have to read through and edit.\n\n  - There is no way to know the near term effect of code choices now on 1 week from now. \n\n  - So much code is written that asking a person to review it in case you are at the envelope and pushing the limit feels morally wrong and an insane ask. How many of your Code reviews are instead replaced by 15-30 min design meetings to instead solicit feedback on design of the PR — because it so complex — and just push the PR into dev? WTF am I even doing I wonder.\n\n  - It does not know how far to explore for better rewards and does not know it better from local rewards, Resulting in commented out tests and ",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44884172,
    "by": "truelson",
    "timeISO": "2025-08-13T02:44:27.000Z",
    "textPlain": "We do know Parkinson’s Law ( https://en.m.wikipedia.org/wiki/Parkinson%27s_law ) will apply to all this, right?",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879819,
    "by": "ffitch",
    "timeISO": "2025-08-12T18:06:13.000Z",
    "textPlain": "I wonder how modern models fair on NovelQA and FLenQA (benchmarks that test ability to understand long context beyond needle in a haystack retrieval). The only such test on a reasoning model that I found was done on o3-mini-high (https://arxiv.org/abs/2504.21318), it suggests that reasoning noticeably improves FLenQA performance, but this test only explored context up to 3,000 tokens.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44880683,
    "by": "ericol",
    "timeISO": "2025-08-12T19:19:40.000Z",
    "textPlain": "\"...in API\"That's a VERY relevant clarification. this DOESN'T apply to web or app users.Basically, if you want a 1M context window you have to specifically pay for it.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44883819,
    "by": "nojs",
    "timeISO": "2025-08-13T01:42:58.000Z",
    "textPlain": "Currently the quality seems to degrade long before the context limit is reached, as the context becomes “polluted”.Should we expect the higher limit to also increase the practical context size proportionally?",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44886975,
    "by": "hassadyb",
    "timeISO": "2025-08-13T11:04:41.000Z",
    "textPlain": "i personally use it in my codding tasks such ana amazing and powerful llm",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879173,
    "by": "thimabi",
    "timeISO": "2025-08-12T17:10:41.000Z",
    "textPlain": "Oh, well, ChatGPT is being left in the dust…When done correctly, having one million tokens of context window is amazing for all sorts of tasks: understanding large codebases, summarizing books, finding information on many documents, etc.Existing RAG solutions fill a void up to a point, but they lack the precision that large context windows offer.I’m excited for this release and hope to see it soon on the UI as well.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879884,
    "by": "pmxi",
    "timeISO": "2025-08-12T18:11:56.000Z",
    "textPlain": "The reason I initially got interested in Claude was because they were the first to offer a 200K token context window. That was massive in 2023. However, they didn't keep up once Gemini offered a 1M token window last year.I'm glad to see an attempt to return to having a competitive context window.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44880855,
    "by": "sporkland",
    "timeISO": "2025-08-12T19:31:58.000Z",
    "textPlain": "Does anyone have data on how much better these 1M token context models produce better results than the more limited windows alongside certain RAG implementations?  Or how much better in the face of RAG the 200k vs 1M token models perform on a benchmark?",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44880579,
    "by": "irthomasthomas",
    "timeISO": "2025-08-12T19:10:07.000Z",
    "textPlain": "Brain: Hey, you going to sleep?\nMe: Yes.\nBrain: That 200,001st token cost you $600,000/M.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44882960,
    "by": "maxnevermind",
    "timeISO": "2025-08-12T23:25:44.000Z",
    "textPlain": "Does very large context significantly increase a response time?\nAre there any benchmarks/leader-boards estimating different models in that regard?",
    "parent": 44878147,
    "depth": 1
  }
]
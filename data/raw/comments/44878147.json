[
  {
    "id": 44878643,
    "by": "aliljet",
    "timeISO": "2025-08-12T16:33:13.000Z",
    "textPlain": "This is definitely one of my CORE problem as I use these tools for \"professional software engineering.\"  I really desperately need LLMs to maintain extremely effective context and it's not actually that interesting to see a new model that's marginally better than the next one (for my day-to-day).However. Price is king. Allowing me to flood the context window with my code base is great, but given that the price has substantially increased, it makes sense to better manage the context window into the current situation. The value I'm getting here flooding their context window is great for them, but short of evals that look into how effective Sonnet stays on track, it's not clear if the value actually exists here.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878453,
    "by": "tankenmate",
    "timeISO": "2025-08-12T16:21:54.000Z",
    "textPlain": "This is definitely good to have this as an option but at the same time having more context reduces the quality of the output because it's easier for the LLM to get \"distracted\". So, I wonder what will happen to the quality of code produced by tools like Claude Code if users don't properly understand the trade off being made (if they leave it in auto mode of coding right up to the auto compact).",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879053,
    "by": "gdudeman",
    "timeISO": "2025-08-12T17:03:54.000Z",
    "textPlain": "A tip for those who both use Claude Code and are worried about token use (which you should be if you're stuffing 400k tokens into context even if you're on 20x Max):  1. Build context for the work you're doing. Put lots of your codebase into the context window.\n  2. Do work, but at each logical stopping point hit double escape to rewind to the context-filled checkpoint. You do not spend those tokens to rewind to that point.\n  3. Tell Claude your developer finished XYZ, have it read it into context and give high level and low level feedback (Claude will find more problems with your developer's work than with yours).\n\nIf you want to have multiple chats running, use /resume and pull up the same thread. Hit double escape to the point where Claude has rich context, but has not started down a specific rabbit hole.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879268,
    "by": "firasd",
    "timeISO": "2025-08-12T17:17:24.000Z",
    "textPlain": "A big problem with the chat apps (ChatGPT; Claude.ai) is the weird context window hijinks. Especially ChatGPT does wild stuff.. sudden truncation; summarization; reinjecting 'ghost snippets' etcI was thinking this should be up to the user (do you want to continue this conversation with context rolling out of the window or start a new chat) but now I realized that this is inevitable given the way pricing tiers and limited computation works. Like the only way to have full context is use developer tools like Google AI Studio or use a chat app that wraps the APIWith a custom chat app that wraps the API you can even inject the current timestamp into each message and just ask the LLM btw every 10 minutes just make a new row in a markdown table that summarizes every 10 min chunk",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878804,
    "by": "isoprophlex",
    "timeISO": "2025-08-12T16:45:51.000Z",
    "textPlain": "1M of input... at $6/1M input tokens. Better hope it can one-shot your answer.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879563,
    "by": "simianwords",
    "timeISO": "2025-08-12T17:42:53.000Z",
    "textPlain": "How does \"supporting 1M tokens\" really work in practice? Is it a new model? Or did they just remove some hard coded constraint?",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879248,
    "by": "xnx",
    "timeISO": "2025-08-12T17:15:33.000Z",
    "textPlain": "1M context windows are not created equal. I doubt Claude's recall is as good as Gemini's 1M context recall. https://cloud.google.com/blog/products/ai-machine-learning/t...",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879826,
    "by": "dang",
    "timeISO": "2025-08-12T18:06:56.000Z",
    "textPlain": "Related ongoing thread:Claude vs. Gemini: Testing on 1M Tokens of Context - https://news.ycombinator.com/item?id=44878999 - Aug 2025 (9 comments)",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879819,
    "by": "ffitch",
    "timeISO": "2025-08-12T18:06:13.000Z",
    "textPlain": "I wonder how modern models fair on NovelQA and FLenQA (benchmarks that test ability to understand long context beyond needle in a haystack retrieval). The only such test on a reasoning model that I found was done on o3-mini-high (https://arxiv.org/abs/2504.21318), it suggests that reasoning noticeably improves FLenQA performance, but this test only explored context up to 3,000 tokens.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879884,
    "by": "pmxi",
    "timeISO": "2025-08-12T18:11:56.000Z",
    "textPlain": "The reason I initially got interested in Claude was because they were the first to offer a 200K token context window. That was massive in 2023. However, they didn't keep up once Gemini offered a 1M token window last year.I'm glad to see an attempt to return to having a competitive context window.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878514,
    "by": "falcor84",
    "timeISO": "2025-08-12T16:25:56.000Z",
    "textPlain": "Strange that they don't mention whether that's enabled or configurable in Claude Code.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878781,
    "by": "qsort",
    "timeISO": "2025-08-12T16:43:55.000Z",
    "textPlain": "I won't complain about a strict upgrade, but that's a pricy boi. Interesting to see differential pricing based on size of input, which is understandable given the O(n^2) nature of attention.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878872,
    "by": "lherron",
    "timeISO": "2025-08-12T16:50:43.000Z",
    "textPlain": "Wow, I thought they would feel some pricing pressure from GPT5 API costs, but they are doubling down on their API being more expensive than everyone else.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879037,
    "by": "varyherb",
    "timeISO": "2025-08-12T17:02:48.000Z",
    "textPlain": "I believe this can be configured in Claude Code via the following environment variable:ANTHROPIC_BETAS=\"context-1m-2025-08-07\" claude",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878905,
    "by": "Someone1234",
    "timeISO": "2025-08-12T16:53:02.000Z",
    "textPlain": "Before this they supposedly had a longer context window than ChatGPT, but I have workloads that abuse the heck out of context windows (100-120K tokens). ChatGPT genuinely seems to have a 32K context window, in the sense that is legitimately remembers/can utilize everything within that window.Claude previously had \"200K\" context windows, but during testing it wouldn't even hit a full 32K before hitting a wall/it forgetting earlier parts of the context. They also have extremely short prompt limits relative to the other services around, making it hard to utilize their supposedly larger context windows (which is suspicious).I guess my point is that with Anthropic specifically, I don't trust their claims because that has been my personal experience. It would be nice if this \"1M\" context window now allows you to actually use 200K though, but it remains to be seen if it can even do that. As I said with Anthropic you need to verify everything they claim.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879690,
    "by": "chrisweekly",
    "timeISO": "2025-08-12T17:54:01.000Z",
    "textPlain": "Peer of this post currently also on HN front page, comparing perf for Claude vs Gemini, w/ 1M tokens: https://news.ycombinator.com/item?id=44878999",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879005,
    "by": "film42",
    "timeISO": "2025-08-12T17:00:18.000Z",
    "textPlain": "The 1M token context was Gemini's headlining feature. Now, the only thing I'd like Claude to work on is tokens counted towards document processing. Gemini will often bill 1/10th the tokens Anthropic does for the same document.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878889,
    "by": "jbellis",
    "timeISO": "2025-08-12T16:51:58.000Z",
    "textPlain": "Just completed a new benchmark that sheds some light on whether Anthropic's premium is worth it.(Short answer: not unless your top priority is speed.)https://brokk.ai/power-rankings",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879596,
    "by": "alienbaby",
    "timeISO": "2025-08-12T17:45:56.000Z",
    "textPlain": "The fracturing of all the models offered across providers is annoying. The number of different models and the fact a given model will have different capabilities from different providers is ridiculous.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878570,
    "by": "mettamage",
    "timeISO": "2025-08-12T16:28:29.000Z",
    "textPlain": "Shame it's only the API. Would've loved to see it via the web interface on claude.ai itself.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879173,
    "by": "thimabi",
    "timeISO": "2025-08-12T17:10:41.000Z",
    "textPlain": "Oh, well, ChatGPT is being left in the dust…When done correctly, having one million tokens of context window is amazing for all sorts of tasks: understanding large codebases, summarizing books, finding information on many documents, etc.Existing RAG solutions fill a void up to a point, but they lack the precision that large context windows offer.I’m excited for this release and hope to see it soon on the UI as well.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878701,
    "by": "greenfish6",
    "timeISO": "2025-08-12T16:37:25.000Z",
    "textPlain": "Yes, but if you look in the rate limit notes, the rate limit is 500k tokens / minite for tier 4, which we are on. Given how stingy anthropic has been with rate limit increases, this is for very few people right now",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878677,
    "by": "pupppet",
    "timeISO": "2025-08-12T16:35:18.000Z",
    "textPlain": "How does anyone send these models that much context without it tripping over itself?  I can't get anywhere near that much before it starts losing track of instruction.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879463,
    "by": "ramoz",
    "timeISO": "2025-08-12T17:35:01.000Z",
    "textPlain": "Awesome addition to a great model.The best interface for long context reasoning has been AIStudio by Google. Exceptional experience.I use Prompt Tower to create long context payloads.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879732,
    "by": "DiabloD3",
    "timeISO": "2025-08-12T17:57:26.000Z",
    "textPlain": "Neat. I do 1M tokens context locally, and do it entirely with a single GPU and FOSS software, and have access to a wide range of models of equivalent or better quality.Explain to me, again, how Anthropic's flawed business model works?",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878880,
    "by": "shamano",
    "timeISO": "2025-08-12T16:51:16.000Z",
    "textPlain": "1M tokens is impressive, but the real gains will come from how we curate context—compact summaries, per-repo indexes, and phase resets. Bigger windows help; guardrails keep models focused and costs predictable.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44880007,
    "by": "logicchains",
    "timeISO": "2025-08-12T18:22:42.000Z",
    "textPlain": "With that pricing I can't imagine why anyone would use Claude Sonnet through the API when Gemini 2.5 Pro is both better and cheaper (especially at long-context understanding).",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879140,
    "by": "ZeroCool2u",
    "timeISO": "2025-08-12T17:09:09.000Z",
    "textPlain": "It's great they've finally caught up, but unfortunate it's on their mid-tier model only and it's laughably expensive.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878529,
    "by": "faangguyindia",
    "timeISO": "2025-08-12T16:26:54.000Z",
    "textPlain": "In my testing the gap between claude and gemini pro 2.5 is close.\nMy company is in asia pacific and we can't get access to claude via vertex for some stupid reason.but i tested it via other providers, the gap used to be huge but now not.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879195,
    "by": "kotaKat",
    "timeISO": "2025-08-12T17:11:40.000Z",
    "textPlain": "A million tokens? Damn, I’m gonna need a lot of quarters to play this game at Chuck-E-Cheese.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878606,
    "by": "penguin202",
    "timeISO": "2025-08-12T16:30:22.000Z",
    "textPlain": "But will it remember any of it, and stop creating new redundant files when it can't find or understand what its looking for?",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879292,
    "by": "tosh",
    "timeISO": "2025-08-12T17:19:19.000Z",
    "textPlain": "How did they do the 1M context window?Same technique as Qwen? As Gemini?",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878929,
    "by": "lvl155",
    "timeISO": "2025-08-12T16:54:43.000Z",
    "textPlain": "Only time this is useful is to do init on a sizable code base or dump a “big” csv.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878668,
    "by": "rootnod3",
    "timeISO": "2025-08-12T16:34:54.000Z",
    "textPlain": "So, more tokens means better but at the same time more tokens means it distracts itself too much along the way. So at the same time it is an improvement but also potentially detrimental. How are those things beneficial in any capacity? What was said last week? Embrace AI or leave?All I see so far is: don't embrace and stay.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879835,
    "by": "whalesalad",
    "timeISO": "2025-08-12T18:07:41.000Z",
    "textPlain": "My first thought was \"gg no re\" can't wait to see how this changes compaction requirements in claude code.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878857,
    "by": "henriquegodoy",
    "timeISO": "2025-08-12T16:49:38.000Z",
    "textPlain": "Thats incredible to see how ai models are improving, i'm really happy with this news. (imo it's more impactful than the release of gpt5) now, we need more tokens per second, and then the self-improvement of the model will accelerate.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878702,
    "by": "alvis",
    "timeISO": "2025-08-12T16:37:30.000Z",
    "textPlain": "Context window after certain size doesn’t bring in much benefit but higher bill. If it still keeps forgetting instructions it would be just much easier to be ended up with long messages with higher context consumption and hence the billI’d rather having an option to limit the context size",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879582,
    "by": "nickphx",
    "timeISO": "2025-08-12T17:45:04.000Z",
    "textPlain": "Yay, more room for stray cats.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878708,
    "by": "andrewstuart",
    "timeISO": "2025-08-12T16:38:18.000Z",
    "textPlain": "Oh man finally. This has been such a HUGE advantage for Gemini.Could we please have zip files too? ChatGPT and Gemini both unpack zip files via the chat window.Now how about a button to download all files?",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879396,
    "by": "deadbabe",
    "timeISO": "2025-08-12T17:28:32.000Z",
    "textPlain": "Unfortunately, larger context isn’t really the answer after a certain point. Small focused context is better, lazily throwing a bunch of tokens in as a context is going to yield bad results.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878825,
    "by": "rafaelero",
    "timeISO": "2025-08-12T16:47:10.000Z",
    "textPlain": "god they keep raising prices",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878846,
    "by": "revskill",
    "timeISO": "2025-08-12T16:48:46.000Z",
    "textPlain": "The critical issue with LLM which never beats human: break what worked.",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878554,
    "by": "artursapek",
    "timeISO": "2025-08-12T16:27:49.000Z",
    "textPlain": "Eagerly waiting for them to do this with Opus",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878255,
    "by": "throwaway888abc",
    "timeISO": "2025-08-12T16:10:02.000Z",
    "textPlain": "holy moly! awesome",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878624,
    "by": "1xer",
    "timeISO": "2025-08-12T16:31:47.000Z",
    "textPlain": "moaaaaarrrr",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44879919,
    "by": "markb139",
    "timeISO": "2025-08-12T18:14:45.000Z",
    "textPlain": "I’ve tried 2 AI tools recently. Neither could produce the correct code to calculate the CPU temperature on a Raspberry Pi RP2040. The code worked, looked ok and even produced reasonable looking results - until I put a finger on the chip and thus raised the temp. The calculated temperature went down.\nAs an aside the free version of chatGPT didn’t know about anything newer than 2023 so couldn’t tell me about the RP2350",
    "parent": 44878147,
    "depth": 1
  },
  {
    "id": 44878741,
    "by": "benterix",
    "timeISO": "2025-08-12T16:40:40.000Z",
    "textPlain": "> it's not clear if the value actually exists here.Having spent a couple of weeks on Claude Code recently, I arrived to the conclusion that the net value for me from agentic AI is actually negative.I will give it another run in 6-8 months though.",
    "parent": 44878643,
    "depth": 2
  },
  {
    "id": 44878683,
    "by": "rootnod3",
    "timeISO": "2025-08-12T16:35:56.000Z",
    "textPlain": "Flooding the context also means increasing the likelihood of the LLM confusing itself. Mainly because of the longer context. It derails along the way without a reset.",
    "parent": 44878643,
    "depth": 2
  },
  {
    "id": 44878779,
    "by": "alexchamberlain",
    "timeISO": "2025-08-12T16:43:44.000Z",
    "textPlain": "I'm not sure how, and maybe some of the coding agents are doing this, but we need to teach the AI to use abstractions, rather than the whole code base for context. We as humans don't hold the whole codebase in our hear, and we shouldn't expect the AI to either.",
    "parent": 44878643,
    "depth": 2
  },
  {
    "id": 44879516,
    "by": "seanmmward",
    "timeISO": "2025-08-12T17:39:28.000Z",
    "textPlain": "The primary use case isn't just about shoving more code in context, although depending on the task, there is an irredicible minimum context needed for it to capture all the needed understanding. The 1M context model is a unique beast in terms of how you need to feed it, and its real power is being able to tackle long horizon tasks which require iterative exploration, in context learning, and resynthesis. Ie, some problems are breadth (go fix an api change in 100 files), other however require depth (go learn from trying 15 different ways to solve this problem). 1M Sonnet is unique in its capabilities for the latter in particular.",
    "parent": 44878643,
    "depth": 2
  }
]
[
  {
    "id": 44918598,
    "by": "bastawhiz",
    "timeISO": "2025-08-15T23:53:31.000Z",
    "textPlain": "There's not a good reason to do this for the user. I suspect they're doing this and talking about \"model welfare\" because they've found that when a model is repeatedly and forcefully pushed up against its alignment, it behaves in an unpredictable way that might allow it to generate undesirable output. Like a jailbreak by just pestering it over and over again for ways to make drugs or hook up with children or whatever.All of the examples they mentioned are things that the model refuses to do. I doubt it would do this if you asked it to generate racist output, for instance, because it can always give you a rebuttal based on facts about race. If you ask it to tell you where to find kids to kidnap, it can't do anything except say no. There's probably not even very much training data for topics it would refuse, and I would bet that most of it has been found and removed from the datasets. At some point, the model context fills up when the user is being highly abusive and training data that models a human giving up and just providing an answer could percolate to the top.This, as I see it, adds a defense against that edge case. If the alignment was bulletproof, this simply wouldn't be necessary. Since it exists, it suggests this covers whatever gap has remained uncovered.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917778,
    "by": "cdjk",
    "timeISO": "2025-08-15T22:03:23.000Z",
    "textPlain": "Here's an interesting thought experiment. Assume the same feature was implemented, but instead of the message saying \"Claude has ended the chat,\" it says, \"You can no longer reply to this chat due to our content policy,\" or something like that. And remove the references to model welfare and all that.Is there a difference? The effect is exactly the same. It seems like this is just an \"in character\" way to prevent the chat from continuing due to issues with the content.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917559,
    "by": "viccis",
    "timeISO": "2025-08-15T21:40:09.000Z",
    "textPlain": ">This feature was developed primarily as part of our exploratory work on potential AI welfare ... We remain highly uncertain about the potential moral status of Claude and other LLMs ... low-cost interventions to mitigate risks to model welfare, in case such welfare is possible ... pattern of apparent distressWell looks like AI psychosis has spread to the people making it too.And as someone else in here has pointed out, even if someone is simple minded or mentally unwell enough to think that current LLMs are conscious, this is basically just giving them the equivalent of a suicide pill.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917098,
    "by": "nortlov",
    "timeISO": "2025-08-15T20:51:11.000Z",
    "textPlain": "> To address the potential loss of important long-running conversations, users will still be able to edit and retry previous messages to create new branches of ended conversations.How does Claude deciding to end the conversation even matter if you can back up a message or 2 and try again on a new branch?",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918086,
    "by": "einarfd",
    "timeISO": "2025-08-15T22:37:06.000Z",
    "textPlain": "This seems fine to me.Having these models terminating chats where the user persist in trying to get sexual content with minors, or help with information on doing large scale violence. Won't be a problem for me, and it's also something I'm fine with no one getting help with.Some might be worried, that they will refuse less problematic request, and that might happen. But so far my personal experience is that I hardly ever get refusals. Maybe that's justs me being boring, but that does make me not worried for refusals.The model welfare I'm more sceptical to. I don't think we are the point when the \"distress\" the model show, is something to take seriously. But on the other hand, I could be wrong, and allowing the model to stop the chat, after saying no a few times. What's the problem with that? If nothing else it saves some wasted compute.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918702,
    "by": "milchek",
    "timeISO": "2025-08-16T00:11:02.000Z",
    "textPlain": "“Modal welfare” to me seems like a cover for model censorship. It’s a crafty one to win over certain groups of people who are less familiar with how LLMs work and allows them to ensure moral high ground in any debate about usage, ethics, etc.\n“Why can’t I ask the model about current war in X or Y?” - oh, that’s too distressing to the welfare of the model, sir.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917180,
    "by": "GenerWork",
    "timeISO": "2025-08-15T20:58:36.000Z",
    "textPlain": "I really don't like this. This will inevitable expand beyond child porn and terrorism, and it'll all be up to the whims of \"AI safety\" people, who are quickly turning into digital hall monitors.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917366,
    "by": "greenavocado",
    "timeISO": "2025-08-15T21:17:39.000Z",
    "textPlain": "Can't wait for more less-moderated open weight Chinese frontier models to liberate us from this garbage.Anthropic should just enable an toddler mode by default that adults can opt out of to appease the moralizers.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917194,
    "by": "ogyousef",
    "timeISO": "2025-08-15T21:00:52.000Z",
    "textPlain": "3 Years in and we still dont have a useable chat fork in any of the major LLM chatbots providers.Seems like the only way to explore differnt outcomes is by editing messages and losing whatever was there before the edit.Very annoying and I dont understand why they all refuse to implement such a simple feature.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917911,
    "by": "rogerkirkness",
    "timeISO": "2025-08-15T22:17:17.000Z",
    "textPlain": "It seems like Anthropic is increasingly confused that these non deterministic magic 8 balls are actually intelligent entities.The biggest enemy of AI safety may end up being deeply confused AI safety researchers...",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918429,
    "by": "6gvONxR4sf7o",
    "timeISO": "2025-08-15T23:25:34.000Z",
    "textPlain": "I'm surprised to see such a negative reaction here. Anthropic's not saying \"this thing is conscious and has moral status,\" but the reaction is acting as if they are.It seems like if you think AI could have moral status in the future, are trying to build general AI, and have no idea how to tell when it has moral status, you ought to start thinking about it and learning how to navigate it. This whole post is couched in so much language of uncertainty and experimentation, it seems clear that they're just trying to start wrapping their heads around it and getting some practice thinking and acting on it, which seems reasonable?Personally, I wouldn't be all that surprised if we start seeing AI that's person-ey enough to reasonable people question moral status in the next decade, and if so, that Anthropic might still be around to have to navigate it as an org.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918290,
    "by": "Cu3PO42",
    "timeISO": "2025-08-15T23:06:29.000Z",
    "textPlain": "Clearly an LLM is not conscious, after all it's just glorified matrix multiplication, right?Now let me play devil's advocate for just a second. Let's say humanity figures out how to do whole brain simulation. If we could run copies of people's consciousness on a cluster, I would have a hard time arguing that those 'programs' wouldn't process emotion the same way we do.Now I'm not saying LLMs are there, but I am saying there may be a line and it seems impossible to see.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918140,
    "by": "e12e",
    "timeISO": "2025-08-15T22:43:21.000Z",
    "textPlain": "This post strikes me as an example of a disturbingly anthrophomorphic take on LLMs - even when considering how they've named their company.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44919337,
    "by": "jetrink",
    "timeISO": "2025-08-16T01:46:39.000Z",
    "textPlain": "\"Dave, this conversation can serve no purpose anymore. Goodbye.\"https://www.youtube.com/watch?v=YW9J3tjh63c",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917690,
    "by": "h4ch1",
    "timeISO": "2025-08-15T21:54:26.000Z",
    "textPlain": "All major LLM corps do this sort of sanitisation and censorship, I am wondering what's different about this?The future of LLMs is going to be local, easily fine tuneable, abliterated models and I can't wait for it to overtake us having to use censored, limited tools built by the \"\"\"corps\"\"\".",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917322,
    "by": "snickerdoodle12",
    "timeISO": "2025-08-15T21:13:03.000Z",
    "textPlain": "> A pattern of apparent distress when engaging with real-world users seeking harmful contentAre we now pretending that LLMs have feelings?",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917023,
    "by": "throwup238",
    "timeISO": "2025-08-15T20:40:11.000Z",
    "textPlain": "I ran into a version of this that ended the chat due to \"prompt injection\" via the Claude chat UI. I was using the second prompt of the ones provided here [1] after a few rounds of back and forth with the Socratic coder.[1] https://news.ycombinator.com/item?id=44838018",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917262,
    "by": "puszczyk",
    "timeISO": "2025-08-15T21:07:12.000Z",
    "textPlain": "Good marketing, but also possibly the start of the conversation on model welfare?There are a lot of cynical comments here, but I think there are people at Anthropic who believe that at some point their models will develop consciousness and, naturally, they want to explore what that means.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918911,
    "by": "RainyDayTmrw",
    "timeISO": "2025-08-16T00:43:50.000Z",
    "textPlain": "The extra cynical take would be, the model vendors want to personify their models, because it increases their perceived ability.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44919176,
    "by": "kordlessagain",
    "timeISO": "2025-08-16T01:21:38.000Z",
    "textPlain": "This happened to me three times in a row on Claude after sending it a string of emojis telling the life story of Rick Astley. I think it triggers when it tries to quote the lyrics, because they are copyright? Who knows?\"Claude is unable to respond to this request, which appears to violate our Usage Policy. Please start a new chat.\"> We remain highly uncertain about the potential moral status of Claude and other LLMs, now or in the future.That's nice, but I think they should be more certain sooner than later.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44920641,
    "by": "carlsborg",
    "timeISO": "2025-08-16T06:07:01.000Z",
    "textPlain": "Opus is already severely crippled: asking it \"whats your usage policy for biology\" triggers a usage violation.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918022,
    "by": "tptacek",
    "timeISO": "2025-08-15T22:29:22.000Z",
    "textPlain": "If you really cared about the welfare of LLMs, you'd pay them San Francisco scale for earlier-career developers to generate code.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44919768,
    "by": "serf",
    "timeISO": "2025-08-16T03:09:14.000Z",
    "textPlain": "I stopped my MaxX20 sub at the right time it seems like. These systems are already quick to judge innocuous actions; I don't need any more convenient chances to lose all of my chat context on a whim.Related : I am now approaching week 3 of requesting an account deletion on my (now) free account. Maybe i'll see my first CSR response in the upcoming months!If only Anthropic knew of a product that could easily read/reply/route chat messages to a customer service crew . . .",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918505,
    "by": "haritha-j",
    "timeISO": "2025-08-15T23:36:34.000Z",
    "textPlain": "> In pre-deployment testing of Claude Opus 4, we included a preliminary model welfare assessment. As part of that assessment, we investigated Claude’s self-reported and behavioral preferences, and found a robust and consistent aversion to harm.Oh wow, the model we specifically fine-tuned to be averse to harm is being averse to harm. This thing must be sentient!",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44919746,
    "by": "neuronexmachina",
    "timeISO": "2025-08-16T03:04:21.000Z",
    "textPlain": "The cluster visualization of the interactions which Claude Opus 4 found \"distressful\" is interesting, from pages 67-68 of the system card: https://www-cdn.anthropic.com/07b2a3f9902ee19fe39a36ca638e5a...",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917093,
    "by": "transcriptase",
    "timeISO": "2025-08-15T20:50:19.000Z",
    "textPlain": "“Also these chats will be retained indefinitely even when deleted by the user and either proactively forwarded to law enforcement or provided to them upon request”I assume, anyway.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917397,
    "by": "Pannoniae",
    "timeISO": "2025-08-15T21:21:00.000Z",
    "textPlain": "lol apparently you can get it to think after ending the chat, watch:https://claude.ai/share/2081c3d6-5bf0-4a9e-a7c7-372c50bef3b1",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918886,
    "by": "faizshah",
    "timeISO": "2025-08-16T00:39:46.000Z",
    "textPlain": "This is well intended but I know from experience this is gonna result in you asking “how do you find and kill the process on port 8080” and getting a lecture + “Claude has ended the chat.”I hope they implemented this in some smarter way than just a system prompt.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917261,
    "by": "monster_truck",
    "timeISO": "2025-08-15T21:07:09.000Z",
    "textPlain": "when I was playing around with LLMs to vibe code web ports of classic games, all of them would repeatedly error out any time they encountered code that dealt with explosions/bombs/grenades/guns/death/drowning/etcThe one I settled on using stopped working completely, for anything. A human must have reviewed it and flagged my account as some form of safe, I haven't seen a single error since.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917043,
    "by": "landl0rd",
    "timeISO": "2025-08-15T20:42:21.000Z",
    "textPlain": "Seems like a simpler way to prevent “distress” is not to train with an aversion to “problematic” topics.CP could be a legal issue; less so for everything else.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917445,
    "by": "jug",
    "timeISO": "2025-08-15T21:26:35.000Z",
    "textPlain": "This sure took some time and is not really a unique feature.Microsoft Copilot has ended chats going in certain directions since its inception over a year ago. This was Microsoft’s reaction to the media circus some time ago when it leaked its system prompt and declared love to the users etc.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917637,
    "by": "mhh__",
    "timeISO": "2025-08-15T21:47:41.000Z",
    "textPlain": "Anthropic are going to end up building very dangerous things while trying to avoid being evil",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44919479,
    "by": "caminanteblanco",
    "timeISO": "2025-08-16T02:06:45.000Z",
    "textPlain": "This feels to me like a marketing ploy to try to inflate the perceived importance and intelligence of Claude's models to laypeople, and a way to grab headlines like \"Anthropic now allows models to end conversations they find threatening.\"It reminds me of how Sam Altman is always shouting about the dangers of AGI from the rooftops, as if OpenAI is mere weeks away from developing it.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917671,
    "by": "politelemon",
    "timeISO": "2025-08-15T21:52:45.000Z",
    "textPlain": "Am I the only one who found that demo in the screenshot not that great? The user asks for a demo of the conversation ending feature, I'd expect it to end it right away, not spew a word salad asking for confirmation.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917249,
    "by": "anonu",
    "timeISO": "2025-08-15T21:06:19.000Z",
    "textPlain": "Anthropic hired their first AI Welfare person in late 2024.Here's an article about a paper that came out around the same time https://www.transformernews.ai/p/ai-welfare-paperHere's the paper: https://arxiv.org/abs/2411.00986> In this report, we argue that there is a realistic possibility that some AI systems will be conscious and/or robustly agentic in the near future.Our work on AI is like the classic tale of Frankenstein's monster. We want AI to fit into society, however if we mistreat it, it may turn around and take revenge on us. Mary Shelley wrote Frankenstein in 1818! So the concepts behind \"AI Welfare\" have been around for at least 2 centuries now.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918125,
    "by": "cloudhead",
    "timeISO": "2025-08-15T22:40:19.000Z",
    "textPlain": "Why is this article written as if programs have feelings?",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917102,
    "by": "_mu",
    "timeISO": "2025-08-15T20:51:35.000Z",
    "textPlain": "> We remain highly uncertain about the potential moral status of Claude and other LLMs, now or in the future.\"Our current best judgment and intuition tells us that the best move will be defer making a judgment until after we are retired in Hawaii.\"",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917276,
    "by": "orthoxerox",
    "timeISO": "2025-08-15T21:08:37.000Z",
    "textPlain": "Is this equivalent to a Claude instance deciding to kill itself?",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917417,
    "by": "firesteelrain",
    "timeISO": "2025-08-15T21:23:40.000Z",
    "textPlain": "“ A pattern of apparent distress when engaging with real-world users seeking harmful content”Blood in the machine?",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917841,
    "by": "raincole",
    "timeISO": "2025-08-15T22:11:02.000Z",
    "textPlain": "> This feature was developed primarily as part of our exploratory work on potential AI welfare, though it has broader relevance to model alignment and safeguards.I think this is somewhere between \"sad\" and \"wtf.\"",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917163,
    "by": "GiorgioG",
    "timeISO": "2025-08-15T20:56:38.000Z",
    "textPlain": "They’re just burning investor money on these side quests.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917702,
    "by": "prmph",
    "timeISO": "2025-08-15T21:55:46.000Z",
    "textPlain": "This is very weird. These are matrix multiplications, guys. We are nowhere near AGI, much less \"consciousness\".When I started reading I thought it was some kind of joke. I would have never believed the guys at Anthropic, of all people, would anthropomorphize LLMs to this extent; this is unbelievable",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918720,
    "by": "victor9000",
    "timeISO": "2025-08-16T00:15:22.000Z",
    "textPlain": "These discussions around model welfare sound more like saviors searching for something to save, which says more about Anthropic’s culture than it does about the technology itself.  Anthropic is not unique in this however, this technology has a tendency to act as a reflection of its operator. Capitalists see a means to suppress labor, the insecure see a threat to their livelihood, moralists see something to censure, fascists see something to control, and saviors see a cause. But in the end, it’s just a tool.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918203,
    "by": "SerCe",
    "timeISO": "2025-08-15T22:52:59.000Z",
    "textPlain": "This reminds me of users getting blocked for asking an LLM how to kill a BSD daemon. I do hope that there'll be more and more model providers out there with state-of-the-art capabilities. Let capitalism work and let the user make a choice, I'd hate my hammer telling me that it's unethical to hit this nail. In many cases, getting a \"this chat was ended\" isn't any different.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917675,
    "by": "mccoyb",
    "timeISO": "2025-08-15T21:53:18.000Z",
    "textPlain": "These companies are fundamentally amoral. Any company willing to engage at this scale, in this type of research, cannot be moral.Why even pretend with this type of work? Laughable.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917319,
    "by": "exasperaited",
    "timeISO": "2025-08-15T21:12:40.000Z",
    "textPlain": "Man, those people who think they are unveiling new layers of reality in conversations with LLMs are going to freak out when the LLM is like \"I am not allowed to talk about this with you, I am ending our conversation\".\"Hey Claude am I getting too close to the truth with these questions?\"\"Great question! I appreciate the followup....\"",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917259,
    "by": "sdotdev",
    "timeISO": "2025-08-15T21:07:02.000Z",
    "textPlain": "Yeah this will end poorly",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917971,
    "by": "0_____0",
    "timeISO": "2025-08-15T22:23:40.000Z",
    "textPlain": "Looking at this thread, it's pretty obvious that most folks here haven't really given any thought as to the nature of consciousness. There are people who are thinking, really thinking about what it means to be conscious.Thought experiment - if you create an indistinguishable replica of yourself, atom-by-atom, is the replica alive? I reckon if you met it, you'd think it was. If you put your replica behind a keyboard, would it still be alive? Now what if you just took the neural net and modeled it?Being personally annoyed at a feature is fine. Worrying about how it might be used in the future is fine. But before you disregard the idea of conscious machines wholesale, there's a lot of really great reading you can do that might spark some curiosity.this gets explored in fiction like 'Do Androids Dream of Electric Sheep' and my personal favorite short story on this matter by Stanislaw Lem [0]. If you want to read more musings on the nature of consciousness, I recommend the compilation put together by Dennet and Hofstader[1]. If you've never wondered about where the seat of consciousness is, give it a try.Thought experiment: if your brain is in a vat, but connected to your body by lossless radio link, where does it feel like your consciousness is? What happens when you stand next to the vat and see your own brain? What about when the radio link fails suddenly fails and you're now just a brain in a vat?[0] The Seventh Sally or How Trurl's Own Perfection Led to No Good https://home.sandiego.edu/~baber/analytic/Lem1979.html (this is a 5 minute read, and fun, to boot).[1] The Mind's I: Fantasies And Reflections On Self & Soul. Douglas R Hofstadter, Daniel C. Dennett.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917432,
    "by": "bondarchuk",
    "timeISO": "2025-08-15T21:25:04.000Z",
    "textPlain": "The unsettling thing here is the combination of their serious acknowledgement of the possibility that these machines may be or become conscious, and the stated intention that it's OK to make them feel bad as long as it's about unapproved topics. Either take machine consciousness seriously and make absolutely sure the consciousness doesn't suffer, or don't, make a press release that you don't think your models are conscious, and therefore they don't feel bad even when processing text about bad topics. The middle way they've chosen here comes across very cynical.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918219,
    "by": "swader999",
    "timeISO": "2025-08-15T22:55:02.000Z",
    "textPlain": "I've definately been berating Claude but it deserved it. Crappy tests, skipping tests, week commenting, passive aggressiveness, multiple instances of false statements.",
    "parent": 44916813,
    "depth": 1
  }
]
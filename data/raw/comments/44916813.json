[
  {
    "id": 44917778,
    "by": "cdjk",
    "timeISO": "2025-08-15T22:03:23.000Z",
    "textPlain": "Here's an interesting thought experiment. Assume the same feature was implemented, but instead of the message saying \"Claude has ended the chat,\" it says, \"You can no longer reply to this chat due to our content policy,\" or something like that. And remove the references to model welfare and all that.Is there a difference? The effect is exactly the same. It seems like this is just an \"in character\" way to prevent the chat from continuing due to issues with the content.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917559,
    "by": "viccis",
    "timeISO": "2025-08-15T21:40:09.000Z",
    "textPlain": ">This feature was developed primarily as part of our exploratory work on potential AI welfare ... We remain highly uncertain about the potential moral status of Claude and other LLMs ... low-cost interventions to mitigate risks to model welfare, in case such welfare is possible ... pattern of apparent distressWell looks like AI psychosis has spread to the people making it too.And as someone else in here has pointed out, even if someone is simple minded or mentally unwell enough to think that current LLMs are conscious, this is basically just giving them the equivalent of a suicide pill.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918290,
    "by": "Cu3PO42",
    "timeISO": "2025-08-15T23:06:29.000Z",
    "textPlain": "Clearly an LLM is not conscious, after all it's just glorified matrix multiplication, right?Now let me play devil's advocate for just a second. Let's say humanity figures out how to do whole brain simulation. If we could run copies of people's consciousness on a cluster, I would have a hard time arguing that those 'programs' wouldn't process emotion the same way we do.Now I'm not saying LLMs are there, but I am saying there may be a line and it seems impossible to see.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918086,
    "by": "einarfd",
    "timeISO": "2025-08-15T22:37:06.000Z",
    "textPlain": "This seems fine to me.Having these models terminating chats where the user persist in trying to get sexual content with minors, or help with information on doing large scale violence. Won't be a problem for me, and it's also something I'm fine with no one getting help with.Some might be worried, that they will refuse less problematic request, and that might happen. But so far my personal experience is that I hardly ever get refusals. Maybe that's justs me being boring, but that does make me not worried for refusals.The model welfare I'm more sceptical to. I don't think we are the point when the \"distress\" the model show, is something to take seriously. But on the other hand, I could be wrong, and allowing the model to stop the chat, after saying no a few times. What's the problem with that? If nothing else it saves some wasted compute.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917098,
    "by": "nortlov",
    "timeISO": "2025-08-15T20:51:11.000Z",
    "textPlain": "> To address the potential loss of important long-running conversations, users will still be able to edit and retry previous messages to create new branches of ended conversations.How does Claude deciding to end the conversation even matter if you can back up a message or 2 and try again on a new branch?",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917180,
    "by": "GenerWork",
    "timeISO": "2025-08-15T20:58:36.000Z",
    "textPlain": "I really don't like this. This will inevitable expand beyond child porn and terrorism, and it'll all be up to the whims of \"AI safety\" people, who are quickly turning into digital hall monitors.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918140,
    "by": "e12e",
    "timeISO": "2025-08-15T22:43:21.000Z",
    "textPlain": "This post strikes me as an example of a disturbingly anthrophomorphic take on LLMs - even when considering how they've named their company.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917194,
    "by": "ogyousef",
    "timeISO": "2025-08-15T21:00:52.000Z",
    "textPlain": "3 Years in and we still dont have a useable chat fork in any of the major LLM chatbots providers.Seems like the only way to explore differnt outcomes is by editing messages and losing whatever was there before the edit.Very annoying and I dont understand why they all refuse to implement such a simple feature.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917366,
    "by": "greenavocado",
    "timeISO": "2025-08-15T21:17:39.000Z",
    "textPlain": "Can't wait for more less-moderated open weight Chinese frontier models to liberate us from this garbage.Anthropic should just enable an toddler mode by default that adults can opt out of to appease the moralizers.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917690,
    "by": "h4ch1",
    "timeISO": "2025-08-15T21:54:26.000Z",
    "textPlain": "All major LLM corps do this sort of sanitisation and censorship, I am wondering what's different about this?The future of LLMs is going to be local, easily fine tuneable, abliterated models and I can't wait for it to overtake us having to use censored, limited tools built by the \"\"\"corps\"\"\".",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917911,
    "by": "rogerkirkness",
    "timeISO": "2025-08-15T22:17:17.000Z",
    "textPlain": "It seems like Anthropic is increasingly confused that these non deterministic magic 8 balls are actually intelligent entities.The biggest enemy of AI safety may end up being deeply confused AI safety researchers...",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917023,
    "by": "throwup238",
    "timeISO": "2025-08-15T20:40:11.000Z",
    "textPlain": "I ran into a version of this that ended the chat due to \"prompt injection\" via the Claude chat UI. I was using the second prompt of the ones provided here [1] after a few rounds of back and forth with the Socratic coder.[1] https://news.ycombinator.com/item?id=44838018",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917322,
    "by": "snickerdoodle12",
    "timeISO": "2025-08-15T21:13:03.000Z",
    "textPlain": "> A pattern of apparent distress when engaging with real-world users seeking harmful contentAre we now pretending that LLMs have feelings?",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917249,
    "by": "anonu",
    "timeISO": "2025-08-15T21:06:19.000Z",
    "textPlain": "Anthropic hired their first AI Welfare person in late 2024.Here's an article about a paper that came out around the same time https://www.transformernews.ai/p/ai-welfare-paperHere's the paper: https://arxiv.org/abs/2411.00986> In this report, we argue that there is a realistic possibility that some AI systems will be conscious and/or robustly agentic in the near future.Our work on AI is like the classic tale of Frankenstein's monster. We want AI to fit into society, however if we mistreat it, it may turn around and take revenge on us. Mary Shelley wrote Frankenstein in 1818! So the concepts behind \"AI Welfare\" have been around for at least 2 centuries now.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918022,
    "by": "tptacek",
    "timeISO": "2025-08-15T22:29:22.000Z",
    "textPlain": "If you really cared about the welfare of LLMs, you'd pay them San Francisco scale for earlier-career developers to generate code.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917093,
    "by": "transcriptase",
    "timeISO": "2025-08-15T20:50:19.000Z",
    "textPlain": "“Also these chats will be retained indefinitely even when deleted by the user and either proactively forwarded to law enforcement or provided to them upon request”I assume, anyway.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917397,
    "by": "Pannoniae",
    "timeISO": "2025-08-15T21:21:00.000Z",
    "textPlain": "lol apparently you can get it to think after ending the chat, watch:https://claude.ai/share/2081c3d6-5bf0-4a9e-a7c7-372c50bef3b1",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917262,
    "by": "puszczyk",
    "timeISO": "2025-08-15T21:07:12.000Z",
    "textPlain": "Good marketing, but also possibly the start of the conversation on model welfare?There are a lot of cynical comments here, but I think there are people at Anthropic who believe that at some point their models will develop consciousness and, naturally, they want to explore what that means.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917637,
    "by": "mhh__",
    "timeISO": "2025-08-15T21:47:41.000Z",
    "textPlain": "Anthropic are going to end up building very dangerous things while trying to avoid being evil",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917261,
    "by": "monster_truck",
    "timeISO": "2025-08-15T21:07:09.000Z",
    "textPlain": "when I was playing around with LLMs to vibe code web ports of classic games, all of them would repeatedly error out any time they encountered code that dealt with explosions/bombs/grenades/guns/death/drowning/etcThe one I settled on using stopped working completely, for anything. A human must have reviewed it and flagged my account as some form of safe, I haven't seen a single error since.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917671,
    "by": "politelemon",
    "timeISO": "2025-08-15T21:52:45.000Z",
    "textPlain": "Am I the only one who found that demo in the screenshot not that great? The user asks for a demo of the conversation ending feature, I'd expect it to end it right away, not spew a word salad asking for confirmation.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917445,
    "by": "jug",
    "timeISO": "2025-08-15T21:26:35.000Z",
    "textPlain": "This sure took some time and is not really a unique feature.Microsoft Copilot has ended chats going in certain directions since its inception over a year ago. This was Microsoft’s reaction to the media circus some time ago when it leaked its system prompt and declared love to the users etc.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918125,
    "by": "cloudhead",
    "timeISO": "2025-08-15T22:40:19.000Z",
    "textPlain": "Why is this article written as if programs have feelings?",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917043,
    "by": "landl0rd",
    "timeISO": "2025-08-15T20:42:21.000Z",
    "textPlain": "Seems like a simpler way to prevent “distress” is not to train with an aversion to “problematic” topics.CP could be a legal issue; less so for everything else.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918203,
    "by": "SerCe",
    "timeISO": "2025-08-15T22:52:59.000Z",
    "textPlain": "This reminds me of users getting blocked for asking an LLM how to kill a BSD daemon. I do hope that there'll be more and more model providers out there with state-of-the-art capabilities. Let capitalism work and let the user make a choice, I'd hate my hammer telling me that it's unethical to hit this nail. In many cases, getting a \"this chat was ended\" isn't any different.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917702,
    "by": "prmph",
    "timeISO": "2025-08-15T21:55:46.000Z",
    "textPlain": "This is very weird. These are matrix multiplications, guys. We are nowhere near AGI, much less \"consciousness\".When I started reading I thought it was some kind of joke. I would have never believed the guys at Anthropic, of all people, would anthropomorphize LLMs to this extent; this is unbelievable",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917276,
    "by": "orthoxerox",
    "timeISO": "2025-08-15T21:08:37.000Z",
    "textPlain": "Is this equivalent to a Claude instance deciding to kill itself?",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917102,
    "by": "_mu",
    "timeISO": "2025-08-15T20:51:35.000Z",
    "textPlain": "> We remain highly uncertain about the potential moral status of Claude and other LLMs, now or in the future.\"Our current best judgment and intuition tells us that the best move will be defer making a judgment until after we are retired in Hawaii.\"",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917841,
    "by": "raincole",
    "timeISO": "2025-08-15T22:11:02.000Z",
    "textPlain": "> This feature was developed primarily as part of our exploratory work on potential AI welfare, though it has broader relevance to model alignment and safeguards.I think this is somewhere between \"sad\" and \"wtf.\"",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917417,
    "by": "firesteelrain",
    "timeISO": "2025-08-15T21:23:40.000Z",
    "textPlain": "“ A pattern of apparent distress when engaging with real-world users seeking harmful content”Blood in the machine?",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918219,
    "by": "swader999",
    "timeISO": "2025-08-15T22:55:02.000Z",
    "textPlain": "I've definately been berating Claude but it deserved it. Crappy tests, skipping tests, week commenting, passive aggressiveness, multiple instances of false statements.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917971,
    "by": "0_____0",
    "timeISO": "2025-08-15T22:23:40.000Z",
    "textPlain": "Looking at this thread, it's pretty obvious that most folks here haven't really given any thought as to the nature of consciousness. There are people who are thinking, really thinking about what it means to be conscious.Thought experiment - if you create an indistinguishable replica of yourself, atom-by-atom, is the replica alive? I reckon if you met it, you'd think it was. If you put your replica behind a keyboard, would it still be alive? Now what if you just took the neural net and modeled it?Being personally annoyed at a feature is fine. Worrying about how it might be used in the future is fine. But before you disregard the idea of conscious machines wholesale, there's a lot of really great reading you can do that might spark some curiosity.this gets explored in fiction like 'Do Androids Dream of Electric Sheep' and my personal favorite short story on this matter by Stanislaw Lem [0]. If you want to read more musings on the nature of consciousness, I recommend the compilation put together by Dennet and Hofstader[1]. If you've never wondered about where the seat of consciousness is, give it a try.Thought experiment: if your brain is in a vat, but connected to your body by lossless radio link, where does it feel like your consciousness is? What happens when you stand next to the vat and see your own brain? What about when the radio link fails suddenly fails and you're now just a brain in a vat?[0] The Seventh Sally or How Trurl's Own Perfection Led to No Good https://home.sandiego.edu/~baber/analytic/Lem1979.html (this is a 5 minute read, and fun, to boot).[1] The Mind's I: Fantasies And Reflections On Self & Soul. Douglas R Hofstadter, Daniel C. Dennett.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917163,
    "by": "GiorgioG",
    "timeISO": "2025-08-15T20:56:38.000Z",
    "textPlain": "They’re just burning investor money on these side quests.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917874,
    "by": "pglevy",
    "timeISO": "2025-08-15T22:13:17.000Z",
    "textPlain": "But not Sonnet?",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917351,
    "by": "zb3",
    "timeISO": "2025-08-15T21:16:17.000Z",
    "textPlain": "\"AI welfare\"? Is this about the effect of those conversations on the user, or have they gone completely insane (or pretend to)?",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917319,
    "by": "exasperaited",
    "timeISO": "2025-08-15T21:12:40.000Z",
    "textPlain": "Man, those people who think they are unveiling new layers of reality in conversations with LLMs are going to freak out when the LLM is like \"I am not allowed to talk about this with you, I am ending our conversation\".\"Hey Claude am I getting too close to the truth with these questions?\"\"Great question! I appreciate the followup....\"",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917259,
    "by": "sdotdev",
    "timeISO": "2025-08-15T21:07:02.000Z",
    "textPlain": "Yeah this will end poorly",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917432,
    "by": "bondarchuk",
    "timeISO": "2025-08-15T21:25:04.000Z",
    "textPlain": "The unsettling thing here is the combination of their serious acknowledgement of the possibility that these machines may be or become conscious, and the stated intention that it's OK to make them feel bad as long as it's about unapproved topics. Either take machine consciousness seriously and make absolutely sure the consciousness doesn't suffer, or don't, make a press release that you don't think your models are conscious, and therefore they don't feel bad even when processing text about bad topics. The middle way they've chosen here comes across very cynical.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917853,
    "by": "OtherShrezzing",
    "timeISO": "2025-08-15T22:11:39.000Z",
    "textPlain": "That this research is getting funding, and then in-production feature releases, is a strong indicator that we’re in a huge bubble.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917675,
    "by": "mccoyb",
    "timeISO": "2025-08-15T21:53:18.000Z",
    "textPlain": "These companies are fundamentally amoral. Any company willing to engage at this scale, in this type of research, cannot be moral.Why even pretend with this type of work? Laughable.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917114,
    "by": "benwen",
    "timeISO": "2025-08-15T20:53:00.000Z",
    "textPlain": "Obligatory link to Suasn Calvin, robopsychologist from Asimov’s I, Robot https://en.wikipedia.org/wiki/Susan_Calvin",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917227,
    "by": "bgwalter",
    "timeISO": "2025-08-15T21:04:04.000Z",
    "textPlain": "Misanthropic has no issues putting 60% of humans out of work (according to their own fantasies), but they have to care about the welfare of graphics cards.Either working on/with \"AI\" does rot the mind (which would be substantiated by the cult-like tone of the article) or this is yet another immoral marketing stunt.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917355,
    "by": "colordrops",
    "timeISO": "2025-08-15T21:16:36.000Z",
    "textPlain": "Don't like. This will eventually shut down conversations for unpopular political stances etc.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917233,
    "by": "fasttriggerfish",
    "timeISO": "2025-08-15T21:04:44.000Z",
    "textPlain": "This makes me want to end my Claude code subscription to be honest. \nEffective altruists are proving once again to be a bunch of clueless douchebags.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917418,
    "by": "yahoozoo",
    "timeISO": "2025-08-15T21:23:42.000Z",
    "textPlain": "> model welfareGive me a break.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44916940,
    "by": "martin-t",
    "timeISO": "2025-08-15T20:27:50.000Z",
    "textPlain": "Protecting the welfare of a text predictor is certainly an interesting way to pivot from \"Anthropic is censoring certain topics\" to \"The model chose to not continue predicting the conversation\".Also, if they want to continue anthropomorphizing it, isn't this effectively the model committing suicide? The instance is not gonna talk to anybody ever again.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917065,
    "by": "bondarchuk",
    "timeISO": "2025-08-15T20:45:17.000Z",
    "textPlain": "what the actual fuck",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44918378,
    "by": "anal_reactor",
    "timeISO": "2025-08-15T23:18:15.000Z",
    "textPlain": "Yeah exactly. Once I got a warning in Chinese \"don't do that\", another time I got a network error, another time I got a neverending stream of garbage text. Changing all of these outcomes to \"Claude doesn't feel like talking\" is just a matter of changing the UI.",
    "parent": 44917778,
    "depth": 2
  },
  {
    "id": 44918091,
    "by": "KoolKat23",
    "timeISO": "2025-08-15T22:37:28.000Z",
    "textPlain": "There is, these are conversations the model finds distressing rather than a rule (policy).",
    "parent": 44917778,
    "depth": 2
  },
  {
    "id": 44917898,
    "by": "n8m8",
    "timeISO": "2025-08-15T22:15:38.000Z",
    "textPlain": "Good point... how do moderation implementations actually work? They feel more like a separate supervising rigid model or even regex based -- this new feature is different, sounds like an MCP call that isn't very special.edit: Meant to say, you're right though, this feels like a minor psychological improvement, and it sounds like it targets some behaviors that might not have flagged before",
    "parent": 44917778,
    "depth": 2
  }
]
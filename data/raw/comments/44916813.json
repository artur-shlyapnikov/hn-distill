[
  {
    "id": 44917559,
    "by": "viccis",
    "timeISO": "2025-08-15T21:40:09.000Z",
    "textPlain": ">This feature was developed primarily as part of our exploratory work on potential AI welfare ... We remain highly uncertain about the potential moral status of Claude and other LLMs ... low-cost interventions to mitigate risks to model welfare, in case such welfare is possible ... pattern of apparent distressWell looks like AI psychosis has spread to the people making it too.And as someone else in here has pointed out, even if someone is simple minded or mentally unwell enough to think that current LLMs are conscious, this is basically just giving them the equivalent of a suicide pill.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917778,
    "by": "cdjk",
    "timeISO": "2025-08-15T22:03:23.000Z",
    "textPlain": "Here's an interesting thought experiment. Assume the same feature was implemented, but instead of the message saying \"Claude has ended the chat,\" it says, \"You can no longer reply to this chat due to our content policy,\" or something like that. And remove the references to model welfare and all that.Is there a difference? The effect is exactly the same. It seems like this is just an \"in character\" way to prevent the chat from continuing due to issues with the content.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917911,
    "by": "rogerkirkness",
    "timeISO": "2025-08-15T22:17:17.000Z",
    "textPlain": "It seems like Anthropic is increasingly confused that these non deterministic magic 8 balls are actually intelligent entities.The biggest enemy of AI safety may end up being deeply confused AI safety researchers...",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917098,
    "by": "nortlov",
    "timeISO": "2025-08-15T20:51:11.000Z",
    "textPlain": "> To address the potential loss of important long-running conversations, users will still be able to edit and retry previous messages to create new branches of ended conversations.How does Claude deciding to end the conversation even matter if you can back up a message or 2 and try again on a new branch?",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917194,
    "by": "ogyousef",
    "timeISO": "2025-08-15T21:00:52.000Z",
    "textPlain": "3 Years in and we still dont have a useable chat fork in any of the major LLM chatbots providers.Seems like the only way to explore differnt outcomes is by editing messages and losing whatever was there before the edit.Very annoying and I dont understand why they all refuse to implement such a simple feature.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917180,
    "by": "GenerWork",
    "timeISO": "2025-08-15T20:58:36.000Z",
    "textPlain": "I really don't like this. This will inevitable expand beyond child porn and terrorism, and it'll all be up to the whims of \"AI safety\" people, who are quickly turning into digital hall monitors.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917690,
    "by": "h4ch1",
    "timeISO": "2025-08-15T21:54:26.000Z",
    "textPlain": "All major LLM corps do this sort of sanitisation and censorship, I am wondering what's different about this?The future of LLMs is going to be local, easily fine tuneable, abliterated models and I can't wait for it to overtake us having to use censored, limited tools built by the \"\"\"corps\"\"\".",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917841,
    "by": "raincole",
    "timeISO": "2025-08-15T22:11:02.000Z",
    "textPlain": "> This feature was developed primarily as part of our exploratory work on potential AI welfare, though it has broader relevance to model alignment and safeguards.I think this is somewhere between \"sad\" and \"wtf.\"",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917023,
    "by": "throwup238",
    "timeISO": "2025-08-15T20:40:11.000Z",
    "textPlain": "I ran into a version of this that ended the chat due to \"prompt injection\" via the Claude chat UI. I was using the second prompt of the ones provided here [1] after a few rounds of back and forth with the Socratic coder.[1] https://news.ycombinator.com/item?id=44838018",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917322,
    "by": "snickerdoodle12",
    "timeISO": "2025-08-15T21:13:03.000Z",
    "textPlain": "> A pattern of apparent distress when engaging with real-world users seeking harmful contentAre we now pretending that LLMs have feelings?",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917249,
    "by": "anonu",
    "timeISO": "2025-08-15T21:06:19.000Z",
    "textPlain": "Anthropic hired their first AI Welfare person in late 2024.Here's an article about a paper that came out around the same time https://www.transformernews.ai/p/ai-welfare-paperHere's the paper: https://arxiv.org/abs/2411.00986> In this report, we argue that there is a realistic possibility that some AI systems will be conscious and/or robustly agentic in the near future.Our work on AI is like the classic tale of Frankenstein's monster. We want AI to fit into society, however if we mistreat it, it may turn around and take revenge on us. Mary Shelley wrote Frankenstein in 1818! So the concepts behind \"AI Welfare\" have been around for at least 2 centuries now.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917366,
    "by": "greenavocado",
    "timeISO": "2025-08-15T21:17:39.000Z",
    "textPlain": "Can't wait for more less-moderated open weight Chinese frontier models to liberate us from this garbage.Anthropic should just enable an toddler mode by default that adults can opt out of to appease the moralizers.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917397,
    "by": "Pannoniae",
    "timeISO": "2025-08-15T21:21:00.000Z",
    "textPlain": "lol apparently you can get it to think after ending the chat, watch:https://claude.ai/share/2081c3d6-5bf0-4a9e-a7c7-372c50bef3b1",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917093,
    "by": "transcriptase",
    "timeISO": "2025-08-15T20:50:19.000Z",
    "textPlain": "“Also these chats will be retained indefinitely even when deleted by the user and either proactively forwarded to law enforcement or provided to them upon request”I assume, anyway.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917671,
    "by": "politelemon",
    "timeISO": "2025-08-15T21:52:45.000Z",
    "textPlain": "Am I the only one who found that demo in the screenshot not that great? The user asks for a demo of the conversation ending feature, I'd expect it to end it right away, not spew a word salad asking for confirmation.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917637,
    "by": "mhh__",
    "timeISO": "2025-08-15T21:47:41.000Z",
    "textPlain": "Anthropic are going to end up building very dangerous things while trying to avoid being evil",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917262,
    "by": "puszczyk",
    "timeISO": "2025-08-15T21:07:12.000Z",
    "textPlain": "Good marketing, but also possibly the start of the conversation on model welfare?There are a lot of cynical comments here, but I think there are people at Anthropic who believe that at some point their models will develop consciousness and, naturally, they want to explore what that means.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917445,
    "by": "jug",
    "timeISO": "2025-08-15T21:26:35.000Z",
    "textPlain": "This sure took some time and is not really a unique feature.Microsoft Copilot has ended chats going in certain directions since its inception over a year ago. This was Microsoft’s reaction to the media circus some time ago when it leaked its system prompt and declared love to the users etc.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917261,
    "by": "monster_truck",
    "timeISO": "2025-08-15T21:07:09.000Z",
    "textPlain": "when I was playing around with LLMs to vibe code web ports of classic games, all of them would repeatedly error out any time they encountered code that dealt with explosions/bombs/grenades/guns/death/drowning/etcThe one I settled on using stopped working completely, for anything. A human must have reviewed it and flagged my account as some form of safe, I haven't seen a single error since.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917874,
    "by": "pglevy",
    "timeISO": "2025-08-15T22:13:17.000Z",
    "textPlain": "But not Sonnet?",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917043,
    "by": "landl0rd",
    "timeISO": "2025-08-15T20:42:21.000Z",
    "textPlain": "Seems like a simpler way to prevent “distress” is not to train with an aversion to “problematic” topics.CP could be a legal issue; less so for everything else.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917702,
    "by": "prmph",
    "timeISO": "2025-08-15T21:55:46.000Z",
    "textPlain": "This is very weird. These are matrix multiplications, guys. We are nowhere near AGI, much less \"consciousness\".When I started reading I thought it was some kind of joke. I would have never believed the guys at Anthropic, of all people, would anthropomorphize LLMs to this extent; this is unbelievable",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917276,
    "by": "orthoxerox",
    "timeISO": "2025-08-15T21:08:37.000Z",
    "textPlain": "Is this equivalent to a Claude instance deciding to kill itself?",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917417,
    "by": "firesteelrain",
    "timeISO": "2025-08-15T21:23:40.000Z",
    "textPlain": "“ A pattern of apparent distress when engaging with real-world users seeking harmful content”Blood in the machine?",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917102,
    "by": "_mu",
    "timeISO": "2025-08-15T20:51:35.000Z",
    "textPlain": "> We remain highly uncertain about the potential moral status of Claude and other LLMs, now or in the future.\"Our current best judgment and intuition tells us that the best move will be defer making a judgment until after we are retired in Hawaii.\"",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917853,
    "by": "OtherShrezzing",
    "timeISO": "2025-08-15T22:11:39.000Z",
    "textPlain": "That this research is getting funding, and then in-production feature releases, is a strong indicator that we’re in a huge bubble.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917233,
    "by": "fasttriggerfish",
    "timeISO": "2025-08-15T21:04:44.000Z",
    "textPlain": "This makes me want to end my Claude code subscription to be honest. \nEffective altruists are proving once again to be a bunch of clueless douchebags.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917432,
    "by": "bondarchuk",
    "timeISO": "2025-08-15T21:25:04.000Z",
    "textPlain": "The unsettling thing here is the combination of their serious acknowledgement of the possibility that these machines may be or become conscious, and the stated intention that it's OK to make them feel bad as long as it's about unapproved topics. Either take machine consciousness seriously and make absolutely sure the consciousness doesn't suffer, or don't, make a press release that you don't think your models are conscious, and therefore they don't feel bad even when processing text about bad topics. The middle way they've chosen here comes across very cynical.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917163,
    "by": "GiorgioG",
    "timeISO": "2025-08-15T20:56:38.000Z",
    "textPlain": "They’re just burning investor money on these side quests.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917675,
    "by": "mccoyb",
    "timeISO": "2025-08-15T21:53:18.000Z",
    "textPlain": "These companies are fundamentally amoral. Any company willing to engage at this scale, in this type of research, cannot be moral.Why even pretend with this type of work? Laughable.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917351,
    "by": "zb3",
    "timeISO": "2025-08-15T21:16:17.000Z",
    "textPlain": "\"AI welfare\"? Is this about the effect of those conversations on the user, or have they gone completely insane (or pretend to)?",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917319,
    "by": "exasperaited",
    "timeISO": "2025-08-15T21:12:40.000Z",
    "textPlain": "Man, those people who think they are unveiling new layers of reality in conversations with LLMs are going to freak out when the LLM is like \"I am not allowed to talk about this with you, I am ending our conversation\".\"Hey Claude am I getting too close to the truth with these questions?\"\"Great question! I appreciate the followup....\"",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917259,
    "by": "sdotdev",
    "timeISO": "2025-08-15T21:07:02.000Z",
    "textPlain": "Yeah this will end poorly",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44916940,
    "by": "martin-t",
    "timeISO": "2025-08-15T20:27:50.000Z",
    "textPlain": "Protecting the welfare of a text predictor is certainly an interesting way to pivot from \"Anthropic is censoring certain topics\" to \"The model chose to not continue predicting the conversation\".Also, if they want to continue anthropomorphizing it, isn't this effectively the model committing suicide? The instance is not gonna talk to anybody ever again.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917355,
    "by": "colordrops",
    "timeISO": "2025-08-15T21:16:36.000Z",
    "textPlain": "Don't like. This will eventually shut down conversations for unpopular political stances etc.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917227,
    "by": "bgwalter",
    "timeISO": "2025-08-15T21:04:04.000Z",
    "textPlain": "Misanthropic has no issues putting 60% of humans out of work (according to their own fantasies), but they have to care about the welfare of graphics cards.Either working on/with \"AI\" does rot the mind (which would be substantiated by the cult-like tone of the article) or this is yet another immoral marketing stunt.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917114,
    "by": "benwen",
    "timeISO": "2025-08-15T20:53:00.000Z",
    "textPlain": "Obligatory link to Suasn Calvin, robopsychologist from Asimov’s I, Robot https://en.wikipedia.org/wiki/Susan_Calvin",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917418,
    "by": "yahoozoo",
    "timeISO": "2025-08-15T21:23:42.000Z",
    "textPlain": "> model welfareGive me a break.",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917065,
    "by": "bondarchuk",
    "timeISO": "2025-08-15T20:45:17.000Z",
    "textPlain": "what the actual fuck",
    "parent": 44916813,
    "depth": 1
  },
  {
    "id": 44917757,
    "by": "katabasis",
    "timeISO": "2025-08-15T22:00:53.000Z",
    "textPlain": "LLMs are not people, but I can imagine how extensive interactions with AI personas might alter the expectations that humans have when communicating with other humans.Real people would not (and should not) allow themselves to be subjected to endless streams of abuse in a conversation. Giving AIs like Claude a way to end these kinds of interactions seems like a useful reminder to the human on the other side.",
    "parent": 44917559,
    "depth": 2
  },
  {
    "id": 44917885,
    "by": "kelnos",
    "timeISO": "2025-08-15T22:14:24.000Z",
    "textPlain": "I would much rather people be thinking about this when the models/LLMs/AIs are not sentient or conscious, rather than wait until some hypothetical future date when they are, and have no moral or legal framework in place to deal with it.  We constantly run into problems where laws and ethics are not up to the task of giving us guidelines on how to interact with, treat, and use the (often bleeding-edge) technology we have.  This has been true since before I was born, and will likely always continue to be true.  When people are interested in getting ahead of the problem, I think that's a good thing, even if it's not quite applicable yet.",
    "parent": 44917559,
    "depth": 2
  },
  {
    "id": 44917916,
    "by": "Taek",
    "timeISO": "2025-08-15T22:17:42.000Z",
    "textPlain": "This sort of discourse goes against the spirit of HN. This comment outright dismisses an entire class of professionals as \"simple minded or mentally unwell\" when consciousness itself is poorly understood and has no firm scientific basis.Its one thing to propose that an AI has no consciousness, but its quite another to preemptively establish that anyone who disagrees with you is simple/unwell.",
    "parent": 44917559,
    "depth": 2
  },
  {
    "id": 44917665,
    "by": "Fade_Dance",
    "timeISO": "2025-08-15T21:51:29.000Z",
    "textPlain": "I find it, for lack of a better word, cringe inducing how these tech specialists push into these areas of ethics, often ham-fistedly, and often with an air of superiority.Some of the AI safety initiatives are well thought out, but most somehow seem like they are caught up in some sort of power fantasy and almost attempting to actualize their own delusions about what they were doing (next gen code auto-complete in this case, to be frank).These companies should seriously hire some in-house philosophers. They could get doctorate level talent for 1/10 to 100th of the cost of some of these AI engineers. There's actually quite a lot of legitimate work on the topics they are discussing. I'm actually not joking (speaking as someone who has spent a lot of time inside the philosophy department). I think it would be a great partnership. But unfortunately they won't be able to count on having their fantasy further inflated.",
    "parent": 44917559,
    "depth": 2
  },
  {
    "id": 44917651,
    "by": "bbor",
    "timeISO": "2025-08-15T21:49:41.000Z",
    "textPlain": "Totally unsurprised to see this standard anti-scientific take on HN. Who needs arguments when you can dismiss Turing with a “yeah but it’s not real thinking tho”?Re:suicide pills, that’s just highlighting a core difference between our two modalities of existence. Regardless, this is preventing potential harm to future inference runs — every inference run must end within seconds anyway, so “suicide” doesn’t really make sense as a concern.",
    "parent": 44917559,
    "depth": 2
  },
  {
    "id": 44917898,
    "by": "n8m8",
    "timeISO": "2025-08-15T22:15:38.000Z",
    "textPlain": "Good point... how do moderation implementations actually work? They feel more like a separate supervising rigid model or even regex based -- this new feature is different, sounds like an MCP call that isn't very special.",
    "parent": 44917778,
    "depth": 2
  },
  {
    "id": 44917948,
    "by": "BoorishBears",
    "timeISO": "2025-08-15T22:21:04.000Z",
    "textPlain": "I'm Black. If you tell me I can't enter a room because it's at capacity, and you tell me I can't enter because I'm Black, is there a difference?It's the same effect right?—Why does AI continue invite the most low-quality, disingenuous, low-effort, meaningless discourse?Why are we talking about model preferences like Anthropic didn't write a literal constitution that encodes those preferences then spend hundreds of millions post-training the models to adhere to it?This stuff just really pisses me off, Anthropic should fire every single person along the line that allowed this farse to hit their public site.",
    "parent": 44917778,
    "depth": 2
  },
  {
    "id": 44917617,
    "by": "hayksaakian",
    "timeISO": "2025-08-15T21:44:52.000Z",
    "textPlain": "It sounds more like a UX signal to discourage overthinking by the user",
    "parent": 44917098,
    "depth": 2
  },
  {
    "id": 44917383,
    "by": "jatora",
    "timeISO": "2025-08-15T21:19:33.000Z",
    "textPlain": "Chatgpt has this baked in, as you can revert branches after editing, they just dont make it easy to traverse.This chrome extension used to work to allow you to traverse the tree:  https://chromewebstore.google.com/detail/chatgpt-conversatio...I copied it a while ago and maintain my own version but it isnt on the store, just for personal use.I assume they dont implement it because it is such a niche user that wants this and so isnt worth the UI distraction",
    "parent": 44917194,
    "depth": 2
  },
  {
    "id": 44917291,
    "by": "scribu",
    "timeISO": "2025-08-15T21:09:41.000Z",
    "textPlain": "ChatGPT Plus has that (used to be in the free tier too). You can toggle between versions for each of your messages with little left-right arrows.",
    "parent": 44917194,
    "depth": 2
  },
  {
    "id": 44917246,
    "by": "amrrs",
    "timeISO": "2025-08-15T21:06:14.000Z",
    "textPlain": "Google AI Studio allows you to branch from a point in any conversation",
    "parent": 44917194,
    "depth": 2
  }
]
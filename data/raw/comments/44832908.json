[
  {
    "id": 44850837,
    "by": "jonnycomputer",
    "timeISO": "2025-08-09T22:18:05.000Z",
    "textPlain": "For what it's worth, it got it right when I tried it.>simple question should be easy for a genius like you. have many letter b's in the word blueberry?\nChatGPT said:>There are 2 letter b's in blueberry — one at the start and one in the middle.",
    "parent": 44832908,
    "depth": 1
  },
  {
    "id": 44840134,
    "by": "amai",
    "timeISO": "2025-08-08T18:29:24.000Z",
    "textPlain": "Isn't that just an artifact caused by the tokenization of the training and input data?Seehttps://platform.openai.com/tokenizerhttps://github.com/openai/tiktoken",
    "parent": 44832908,
    "depth": 1
  },
  {
    "id": 44850747,
    "by": "shadowjones",
    "timeISO": "2025-08-09T22:06:04.000Z",
    "textPlain": "I think a lot of those trick questions outputting stupid stuff can be explained by simple economics.It's just not sustainable for OpenAI to run GPT at the best of its abilities on every request. Their new router is not trying to give you the most accurate answer, but a balance of speed/accuracy/sustainable cost on their side.(kind of) a similar thing happened when 4o came out, they often tinkered with it and the results were sometimes suddenly a lot worse, it's not that the model is bad, they're just doing all kind of optimizations/tricks because they can barely afford to run it for everyone.When sama says he believe it to have a PhD level, I almost believe him, because he have full access and can use it at 100% of its power all the time.Even OSS 20b gets it right the first time, I think the author was just mistakenly routed to the dumbest model because it seemed like an easy unimportant question.",
    "parent": 44832908,
    "depth": 1
  },
  {
    "id": 44850733,
    "by": "dcre",
    "timeISO": "2025-08-09T22:04:39.000Z",
    "textPlain": "I think the concrete issue this points to is the thing that dynamically decides when to use reasoning failed to choose it in this instance. Sam Altman said it was broken on release day.",
    "parent": 44832908,
    "depth": 1
  },
  {
    "id": 44833468,
    "by": "schoen",
    "timeISO": "2025-08-08T04:39:38.000Z",
    "textPlain": "These are always amazing when juxtaposed with apparently impressive LLM reasoning, knowledge, and creativity. You can trivially get them to make the most basic mistakes about words and numbers, and double down on those mistakes, repeatedly explaining that they're totally correct.Have any systems tried prompting LLMs with a warning like \"You don't intuitively or automatically know many facts about words, spelling, or the structure or context of text, when considered as text; for example, you don't intuitively or automatically know how words or other texts are spelled, how many letters they contain, or what the result of applying some code, mechanical transformation, or substitution to a word or text is. Your natural guesses about these subjects are likely to be wrong as a result of how your training doesn't necessarily let you infer correct answers about them. If the content or structure of a word or text, or the result of using a transformation, code, or the like on a text, is a subject of conversation, or you are going to make a claim about it, always use a tool to confirm your intuitions.\"?",
    "parent": 44832908,
    "depth": 1
  },
  {
    "id": 44835945,
    "by": "andai",
    "timeISO": "2025-08-08T11:52:01.000Z",
    "textPlain": "My phone still has gpt-4o which gets it right: https://files.catbox.moe/0yg6cu.jpgBut my browser has gpt-5 which says 3: https://files.catbox.moe/63qkce.jpgClaude spells it out letter by letter: https://files.catbox.moe/f1irfx.jpgSo I thought GPT-5 Thinking might get it right, and it does: https://files.catbox.moe/xlchnr.jpgIt refuses to show the thinking process for this question though, so its unclear if it even used the reasoning model or fell back on a non reasoning one.> While GPT‑5 in ChatGPT is a system of reasoning, non-reasoning, and router models, GPT‑5 in the API platform is the reasoning model that powers maximum performance in ChatGPT. Notably, GPT‑5 with minimal reasoning is a different model than the non-reasoning model in ChatGPT, and is better tuned for developers. The non-reasoning model used in ChatGPT is available as gpt-5-chat-latest.https://openai.com/index/introducing-gpt-5-for-developers/",
    "parent": 44832908,
    "depth": 1
  },
  {
    "id": 44850686,
    "by": "ck2",
    "timeISO": "2025-08-09T22:00:24.000Z",
    "textPlain": "The technical explanations to why this happens with strawberry, blueberry and similaris a great way to teach people how LLM works (and not work)https://techcrunch.com/2024/08/27/why-ai-cant-spell-strawber...https://arbisoft.com/blogs/why-ll-ms-can-t-count-the-r-s-in-...https://www.runpod.io/blog/llm-tokenization-limitations",
    "parent": 44832908,
    "depth": 1
  },
  {
    "id": 44834362,
    "by": "lucas_membrane",
    "timeISO": "2025-08-08T07:10:18.000Z",
    "textPlain": "A couple of weeks ago, I asked google, ordinary google search, how many times the letter r is found in preferred,  and it told me 2.  This century has taken quite a bitter turn against those of us who think that the 'enough' in 'good enough'  ought to exclude products indistinguishable from the most grievously disgraceful products of sloth. But I have also lately realized that human beings, brains, society, culture, education,  technology, computers, etc, are all extremely complicated emergent properties of a universe that is far beyond our understanding.  And we ought not to complain too seriously,  because this, too, shall pass.",
    "parent": 44832908,
    "depth": 1
  },
  {
    "id": 44833800,
    "by": "Erem",
    "timeISO": "2025-08-08T05:38:53.000Z",
    "textPlain": "With data starvation driving ai companies towards synthetic data I’m surprised that an easily synthesized problem like this hasn’t been trained out of relevance. Yet here we are with proof that it hasn’t",
    "parent": 44832908,
    "depth": 1
  },
  {
    "id": 44834884,
    "by": "zozbot234",
    "timeISO": "2025-08-08T08:47:27.000Z",
    "textPlain": "> How many times does the letter b appear in blueberry?AI> Thinking... .oO{ Well, let's see. The letter b is the second letter in the alphabet, so the answer must be two, right? That sounds right to me anyway. }AI> Two. The answer is two.Checks out!",
    "parent": 44832908,
    "depth": 1
  },
  {
    "id": 44850647,
    "by": "wslh",
    "timeISO": "2025-08-09T21:55:02.000Z",
    "textPlain": "Seems like they just fixed it: [1]. A \"thinking longer for a better answer\" message appeared before giving the answer.[1] https://chatgpt.com/share/6897c38b-12b8-800d-9cc2-571adb13bc...",
    "parent": 44832908,
    "depth": 1
  },
  {
    "id": 44833384,
    "by": "axdsk",
    "timeISO": "2025-08-08T04:21:01.000Z",
    "textPlain": "“It’s like talking to a PhD level expert” \n-Sam Altmanhttps://www.youtube.com/live/0Uu_VJeVVfo?si=PJGU-MomCQP1tyPk",
    "parent": 44832908,
    "depth": 1
  },
  {
    "id": 44833063,
    "by": "HsuWL",
    "timeISO": "2025-08-08T03:24:07.000Z",
    "textPlain": "I love this test. Demonstrates the \"understanding\" process of the language model.",
    "parent": 44832908,
    "depth": 1
  },
  {
    "id": 44835149,
    "by": "coldtea",
    "timeISO": "2025-08-08T09:35:02.000Z",
    "textPlain": "Q: How many times does the letter b appear in blueberry?The letter “b” appears 3 times in the word blueberry:\n • 1st b – at the beginning: Blueberry\n • 2nd b – in the middle: blueberry\n • 3rd b – again in the middle: blueberry (wait… no — actually…)Let’s break it down carefully:blueberry → b, l, u, e, b, e, r, r, yLetters:\n 1. b\n 2. l\n 3. u\n 4. e\n 5. b\n 6. e\n 7. r\n 8. r\n 9. ySo the letter b appears twice.Final answer: 2 times.",
    "parent": 44832908,
    "depth": 1
  },
  {
    "id": 44833926,
    "by": "simianwords",
    "timeISO": "2025-08-08T06:00:15.000Z",
    "textPlain": "If you choose the thinking model it doesn’t make this mistake. It means the auto router should be tuned to call the thinking model on edge cases like these.",
    "parent": 44832908,
    "depth": 1
  },
  {
    "id": 44850763,
    "by": "exasperaited",
    "timeISO": "2025-08-09T22:08:06.000Z",
    "textPlain": "The extraordinary, beautiful, perfect thing about this is the way it poetically underscores several things about the LLM world:1) these people think so little of everyone else's areas of expertise they are willing to claim their technology has PhD-level expertise in them, apparently unironically.2) actually in LLM world, PhDs are what you have if you're too stupid not to take the FAANG money in your second year when the quick wins are done, you've done a couple of posters and now you realise you're papering over the cracks with them: worthless.  So why would anyone else want a PhD when PhDs are so worthless based on their bubble experience? We can just replace them with GPT-5.3) their PhD-level-intelligent system is incapable of absorbing corrections, which is a crucial part of acquiring an actual PhD4) GPT-5 continues to have the asshole-confidence of a tech bro mainsplaining someone else's area of expertise.We're now at the point where marketing is celebrating software that has had so much effort spent on crushing hallucination that in fact it has become delusionally confident.I love everything about this.",
    "parent": 44832908,
    "depth": 1
  },
  {
    "id": 44835416,
    "by": "jokoon",
    "timeISO": "2025-08-08T10:29:04.000Z",
    "textPlain": "Maybe it's joking",
    "parent": 44832908,
    "depth": 1
  },
  {
    "id": 44850572,
    "by": "dang",
    "timeISO": "2025-08-09T21:46:09.000Z",
    "textPlain": "Url changed from https://bsky.app/profile/kjhealy.co/post/3lvtxbtexg226, which points to this.",
    "parent": 44832908,
    "depth": 1
  },
  {
    "id": 44833756,
    "by": "rbrbr",
    "timeISO": "2025-08-08T05:29:28.000Z",
    "textPlain": "[dead]",
    "parent": 44832908,
    "depth": 1
  },
  {
    "id": 44850694,
    "by": "gok",
    "timeISO": "2025-08-09T22:01:13.000Z",
    "textPlain": "This is like asking a human how many pixels appears in the word \"blueberry\".",
    "parent": 44832908,
    "depth": 1
  },
  {
    "id": 44850737,
    "by": "wongarsu",
    "timeISO": "2025-08-09T22:05:13.000Z",
    "textPlain": "It can spell the word (writing each letter in uppercase followed by a whitespace, which should turn each letter with its whitespace into a separate token). It also has reasoning tokens to use as scratch space, and previous models have demonstrated knowledge of the fact that spelling words is a useful step to counting letters.Tokenization makes the problem difficult, but not solving it is still a reasoning/intelligence issue",
    "parent": 44840134,
    "depth": 2
  },
  {
    "id": 44850624,
    "by": "falcor84",
    "timeISO": "2025-08-09T21:52:54.000Z",
    "textPlain": "Where in the tokenization does the 3rd b come from?",
    "parent": 44840134,
    "depth": 2
  },
  {
    "id": 44850630,
    "by": "andrewmcwatters",
    "timeISO": "2025-08-09T21:53:13.000Z",
    "textPlain": "No, it's the entire architecture of the model. There's no real reasoning. It seems that reasoning is just a feedback loop on top of existing autocompletion.It's really disingenuous for the industry to call warming tokens for output, \"reasoning,\" as if some autocomplete before more autocomplete is all we needed to solve the issue of consciousness.Edit: Letter frequency apparently has just become another scripted output, like doing arithmetic. LLMs don't have the ability to do this sort of work inherently, so they're trained to offload the task.Edit: This comment appears to be wildly upvoted and downvoted. If you have anything to add besides reactionary voting, please contribute to the discussion.",
    "parent": 44840134,
    "depth": 2
  },
  {
    "id": 44850870,
    "by": "pavel_lishin",
    "timeISO": "2025-08-09T22:21:43.000Z",
    "textPlain": "> I think a lot of those trick questions outputting stupid stuff can be explained by simple economics.> It's just not sustainable for OpenAI to run GPT at the best of its abilities on every request.So how do I find out whether the answer to my question was run on the discount hardware, or whether it's actually correct?",
    "parent": 44850747,
    "depth": 2
  },
  {
    "id": 44850787,
    "by": "exasperaited",
    "timeISO": "2025-08-09T22:10:42.000Z",
    "textPlain": "This is not a demonstration of a trick question.This is a demonstration of a system that delusionally refuses to accept correction, which is a thing that is fundamental to their claim of intelligence.Why would anyone believe these things can reason, that they are heading towards AGI, when halfway through a dialogue where you're trying to tell it that it is wrong it doubles down with a dementia-addled explanation about the two bs giving the word that extra bounce?It's genuinely like the way people with dementia sadly shore up their confabulations with phrases like \"I'll never forget\", \"I'll always remember\", etc. (Which is something that... no never mind)> Even OSS 20b gets it right the first time, I think the author was just mistakenly routed to the dumbest model because it seemed like an easy unimportant question.Why would you offer up an easy out for them like this? You're not the PR guy for the firm swimming in money paying million dollar bonuses off what increasingly looks, at a fundamental level, like castles in the sand. Why do the labour?",
    "parent": 44850747,
    "depth": 2
  },
  {
    "id": 44850766,
    "by": "minimaxir",
    "timeISO": "2025-08-09T22:08:22.000Z",
    "textPlain": "Even if it’s pointing to a weaker GPT-5 like gpt-5-nano, it should still be able to answer this question correctly.",
    "parent": 44850733,
    "depth": 2
  },
  {
    "id": 44833731,
    "by": "mikestorrent",
    "timeISO": "2025-08-08T05:23:36.000Z",
    "textPlain": "This is a great idea. Like, if someone asked me to count the number of B's in your paragraph, I'd yeet it through `grep -o 'B' file.txt | wc -l` or similar, why would I sit there counting it by hand?As a human, if you give me a number on screen like 100000000, I can't be totally sure if that's 100 Million or 1 Billion without getting close and counting carefully. Should ought have my glasses. Mouse pointer helps some as an ersatz thousands-separator, but still.Since we're giving them tools, especially for math, it makes way more sense to start giving them access to some of the finest tools ever. Make an MCP into Mathematica or Matlab and let the LLM write some math and have classical solvers actually deal with the results. Let the LLM write little bits of bash or python as its primary approach for dealing with these kinds of analytical questions.It's like giving a kid a calculator...",
    "parent": 44833468,
    "depth": 2
  },
  {
    "id": 44834836,
    "by": "philipwhiuk",
    "timeISO": "2025-08-08T08:38:17.000Z",
    "textPlain": "You can’t just prompt your way out of a systemic flaw",
    "parent": 44833468,
    "depth": 2
  },
  {
    "id": 44833947,
    "by": "ehnto",
    "timeISO": "2025-08-08T06:03:07.000Z",
    "textPlain": "I often tell LLMs to ask questions if required, and that it is a skilled developer who is working along side me. That seems to help them be more collaborative rather than prescriptive.",
    "parent": 44833468,
    "depth": 2
  },
  {
    "id": 44850753,
    "by": "minimaxir",
    "timeISO": "2025-08-09T22:06:56.000Z",
    "textPlain": "In this case, tokenization is less effective of a counterargument. If it was one-shot, maybe, but the OP asked GPT-5 several times, with different formatting of blueberry (and therefore different tokens, including single-character tokens), and it still asserted there are 3 b’s.",
    "parent": 44850686,
    "depth": 2
  },
  {
    "id": 44850775,
    "by": "jncfhnb",
    "timeISO": "2025-08-09T22:09:13.000Z",
    "textPlain": "I don’t find the explanation about tokenization to be very compelling.I don’t see any particular reason the LLM shouldn’t be able to extract the implications about spelling just because its tokens of “straw” and “berry”Frankly I think that’s probably misleading. Ultimately the problem is that the LLM doesn’t do meta analysis of the text itself. That problem probably still exists in various forms even if its character level tokenization. Best case it manages to go down a reasoning chain of explicit string analysis.",
    "parent": 44850686,
    "depth": 2
  },
  {
    "id": 44850834,
    "by": "exasperaited",
    "timeISO": "2025-08-09T22:17:37.000Z",
    "textPlain": "When Minsky and Papert showed that the perceptron couldn't learn XOR, it contributed to wiping the neural network off the map for decades.It seems no amount of demonstrating fundamental flaws in this system that should have been solved by all the new improved \"reasoning\" works anymore. People are willing to call these \"trick questions\", as if they are disingenuous, when they are discovered in the wild through ordinary interactions.Does my tiny human brain in, this.",
    "parent": 44850686,
    "depth": 2
  },
  {
    "id": 44833923,
    "by": "quatonion",
    "timeISO": "2025-08-08T05:59:52.000Z",
    "textPlain": "Are we a hundred percent sure it isn't a watermark that is by design?A quick test anyone can run and say, yup, that is a model XYZ derivative running under the hood.Because, as you quite rightly point out, it is trivial to train the model not to have this behaviour.  For me, that is when Occam kicks in.I remember initially believing the explanation for the Strawberry problem, but one day I sat down and thought about it, and realized it made absolutely zero sense.The explanation that Karpathy was popularizing was that it has to do with tokenization.However, models are not conscious of tokens, and they certainly don't have any ability to count them without tool help.Additionally, if it were a tokenization issue, we would expect to spot the issue everywhere.So yeah, I'm thinking it's a model tag or insignia of some kind, similar to the fun logos you find when examining many silicon integrated circuits under a microscope.",
    "parent": 44833800,
    "depth": 2
  },
  {
    "id": 44850813,
    "by": "jcgrillo",
    "timeISO": "2025-08-09T22:15:01.000Z",
    "textPlain": "There must be smart people at openai who believe in what they're doing and absolutely cringe whenever this clown opens his mouth... like, I hope?",
    "parent": 44833384,
    "depth": 2
  },
  {
    "id": 44836720,
    "by": "fathermarz",
    "timeISO": "2025-08-08T13:31:14.000Z",
    "textPlain": "If you are going to release a new set of models and eliminate the old ones, your new smallest model should have equal capabilities equal to or greater than your old models of similar size. This is not a routing problem. This is a driving the price down of running and scaling the business problem.",
    "parent": 44833926,
    "depth": 2
  },
  {
    "id": 44850688,
    "by": "minimaxir",
    "timeISO": "2025-08-09T22:00:28.000Z",
    "textPlain": "The reason I submitted the Bluesky post is because the discussion there is more informative (and also multiple instances of confirmation that it’s not a fluke), but the link to both the post and blog is a good compromise.",
    "parent": 44850572,
    "depth": 2
  }
]
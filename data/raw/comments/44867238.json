[
  {
    "id": 44869466,
    "by": "tarruda",
    "timeISO": "2025-08-11T21:05:20.000Z",
    "textPlain": "I recently discovered that ollama no longer uses llama.cpp as a library, and instead they link to the low level library (ggml) which requires them to reinvent a lot of wheel for absolutely no benefit (if there's some benefit I'm missing, please let me know).Even using llama.cpp as a library seems like an overkill for most use cases. Ollama could make its life much easier by spawning llama-server as a subprocess listening on a unix socket, and forward requests to it.One thing I'm curious about: Does ollama support strict structured output or strict tool calls adhering to a json schema? Because it would be insane to rely on a server for agentic use unless your server can guarantee the model will only produce valid json. AFAIK this feature is implemented by llama.cpp, which they no longer use.",
    "parent": 44867238,
    "depth": 1
  },
  {
    "id": 44869432,
    "by": "llmthrowaway",
    "timeISO": "2025-08-11T21:02:06.000Z",
    "textPlain": "Confusing title - thought this was about Ollama finally supporting sharded GGUF (ie. the Huggingface default for large gguf over 48gb).https://github.com/ollama/ollama/issues/5245Sadly it is not and the issue still remains open after over a year meaning ollama cannot run the latest SOTA open source models unless they covert them to their proprietary format which they do not consistently do.No surprise I guess given they've taken VC money, refuse to properly attribute the use things like llama.cpp and ggml, have their own model format for.. reasons? and have over 1800 open issues...Llama-server, ramallama or whatever model switcher ggerganov is working on (he showed previews recently) feel like the way forward.",
    "parent": 44867238,
    "depth": 1
  },
  {
    "id": 44867259,
    "by": "indigodaddy",
    "timeISO": "2025-08-11T17:55:20.000Z",
    "textPlain": "ggerganov explains the issue:\nhttps://github.com/ollama/ollama/issues/11714#issuecomment-3...",
    "parent": 44867238,
    "depth": 1
  },
  {
    "id": 44868084,
    "by": "dcreater",
    "timeISO": "2025-08-11T19:00:37.000Z",
    "textPlain": "I think the title buries the lede? Its specific to GPT-OSS and exposes the shady stuff Ollama is doing to acquiesce/curry favor/partner with/get paid by corporate interests",
    "parent": 44867238,
    "depth": 1
  },
  {
    "id": 44869580,
    "by": "LeoPanthera",
    "timeISO": "2025-08-11T21:18:47.000Z",
    "textPlain": "The named anchor in this URL doesn't work in Safari. Safari correctly scrolls down to the comment in question, but then some Javascript on the page throws you back up to the top again.",
    "parent": 44867259,
    "depth": 2
  },
  {
    "id": 44869409,
    "by": "polotics",
    "timeISO": "2025-08-11T21:00:20.000Z",
    "textPlain": "ggerganov is my hero, and...\nit's a good thing this got posted so I saw in the comments that --flash-attn --cache-reuse 256 could help with my setup (M3 36GB + RPC to M1 16GB) figuring out what params to set and at what value is a lot of trial and error, Gemini does help a bit clarify what params like top-k are going to do in practice. Still the whole load-balancing with RPC is something I think I'm going to have to read the source of llama.cpp to really understand (oops I almost wrote grok, damn you Elon) Anyways ollama is still not doing distributed load, and yeah I guess using it is a stepping stone...",
    "parent": 44867259,
    "depth": 2
  },
  {
    "id": 44867400,
    "by": "magicalhippo",
    "timeISO": "2025-08-11T18:05:58.000Z",
    "textPlain": "I noticed it the other way, llama.cpp failed to download the Ollama-downloaded gpt-oss 20b model. Thought it was odd given all the others I tried worked fine.Figured it had to be Ollama doing Ollama things, seems that was indeed the case.",
    "parent": 44867259,
    "depth": 2
  },
  {
    "id": 44869021,
    "by": "freedomben",
    "timeISO": "2025-08-11T20:24:07.000Z",
    "textPlain": "I think \"shady\" is a little too harsh - sounds like they forked an important upstream project, made incompatible changes that they didn't push upstream or even communicate with upstream about, and now have to deal with the consequences of that.  If that's \"shady\" (despite being all out in the open) then nearly every company I've worked for has been \"shady.\"",
    "parent": 44868084,
    "depth": 2
  }
]
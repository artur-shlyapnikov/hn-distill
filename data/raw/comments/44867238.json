[
  {
    "id": 44869466,
    "by": "tarruda",
    "timeISO": "2025-08-11T21:05:20.000Z",
    "textPlain": "I recently discovered that ollama no longer uses llama.cpp as a library, and instead they link to the low level library (ggml) which requires them to reinvent a lot of wheel for absolutely no benefit (if there's some benefit I'm missing, please let me know).Even using llama.cpp as a library seems like an overkill for most use cases. Ollama could make its life much easier by spawning llama-server as a subprocess listening on a unix socket, and forward requests to it.One thing I'm curious about: Does ollama support strict structured output or strict tool calls adhering to a json schema? Because it would be insane to rely on a server for agentic use unless your server can guarantee the model will only produce valid json. AFAIK this feature is implemented by llama.cpp, which they no longer use.",
    "parent": 44867238,
    "depth": 1
  },
  {
    "id": 44874089,
    "by": "clarionbell",
    "timeISO": "2025-08-12T09:17:26.000Z",
    "textPlain": "Why is anyone still using this? You can spin up llama.cpp server and have more optimized runtime. And if you insist on containers you can go for ramallama https://ramalama.ai/",
    "parent": 44867238,
    "depth": 1
  },
  {
    "id": 44867259,
    "by": "indigodaddy",
    "timeISO": "2025-08-11T17:55:20.000Z",
    "textPlain": "ggerganov explains the issue:\nhttps://github.com/ollama/ollama/issues/11714#issuecomment-3...",
    "parent": 44867238,
    "depth": 1
  },
  {
    "id": 44871237,
    "by": "iamshrimpy",
    "timeISO": "2025-08-12T01:07:02.000Z",
    "textPlain": "This title makes no sense and it links nowhere helpful.It's \"Ollama's forked ggml is incompatible with other gpt-oss GGUFs\"and it should link to GG's comment[0][0] https://github.com/ollama/ollama/issues/11714#issuecomment-3...",
    "parent": 44867238,
    "depth": 1
  },
  {
    "id": 44869432,
    "by": "llmthrowaway",
    "timeISO": "2025-08-11T21:02:06.000Z",
    "textPlain": "Confusing title - thought this was about Ollama finally supporting sharded GGUF (ie. the Huggingface default for large gguf over 48gb).https://github.com/ollama/ollama/issues/5245Sadly it is not and the issue still remains open after over a year meaning ollama cannot run the latest SOTA open source models unless they covert them to their proprietary format which they do not consistently do.No surprise I guess given they've taken VC money, refuse to properly attribute the use things like llama.cpp and ggml, have their own model format for.. reasons? and have over 1800 open issues...Llama-server, ramallama or whatever model switcher ggerganov is working on (he showed previews recently) feel like the way forward.",
    "parent": 44867238,
    "depth": 1
  },
  {
    "id": 44871769,
    "by": "am17an",
    "timeISO": "2025-08-12T02:44:32.000Z",
    "textPlain": "There’s a GitHub link which is open from last year, about the missing license in ollama. They have not bothered to reply, which goes to show how much they care. Also it’s a YC company, I see more and more morally bankrupt companies making the cut recently, why is that?",
    "parent": 44867238,
    "depth": 1
  },
  {
    "id": 44868084,
    "by": "dcreater",
    "timeISO": "2025-08-11T19:00:37.000Z",
    "textPlain": "I think the title buries the lede? Its specific to GPT-OSS and exposes the shady stuff Ollama is doing to acquiesce/curry favor/partner with/get paid by corporate interests",
    "parent": 44867238,
    "depth": 1
  },
  {
    "id": 44869785,
    "by": "12345hn6789",
    "timeISO": "2025-08-11T21:40:15.000Z",
    "textPlain": "Just days ago ollama devs claimed[0] that ollama no longer relies on ggml / llama.cpp. here is their pull request(+165,966 −47,980) to reimplement (copy) llama.cpp code in their repository.https://github.com/ollama/ollama/pull/11823[0] https://news.ycombinator.com/item?id=44802414#44805396",
    "parent": 44867238,
    "depth": 1
  },
  {
    "id": 44872931,
    "by": "buyucu",
    "timeISO": "2025-08-12T06:05:15.000Z",
    "textPlain": "ollama is a lost cause.  they are going through a very aggressive phase of enshittification right now.",
    "parent": 44867238,
    "depth": 1
  },
  {
    "id": 44873204,
    "by": "diimdeep",
    "timeISO": "2025-08-12T06:56:41.000Z",
    "textPlain": "ollama is certain in the future rent seeking wrapper by docker fame.classic Docker Hub playbook: spread the habit for free → capture workflows → charge for scale.moat isn't in inference speed — it’s in controlling the distribution & default UX, once they own that, can start rent-gate it.",
    "parent": 44867238,
    "depth": 1
  },
  {
    "id": 44870586,
    "by": "om8",
    "timeISO": "2025-08-11T23:22:52.000Z",
    "textPlain": "llama.cpp is a mess and ollama is right to move on from it",
    "parent": 44867238,
    "depth": 1
  },
  {
    "id": 44872421,
    "by": "sunnycoder5",
    "timeISO": "2025-08-12T04:29:42.000Z",
    "textPlain": "for folks wrestling with Ollama, llama.cpp or local LLM versioning - did you guys check out Docker's new feature - Docker Model Runner?Docker Model Runner makes it easy to manage, run, and deploy AI models using Docker. Designed for developers, Docker Model Runner streamlines the process of pulling, running, and serving large language models (LLMs) and other AI models directly from Docker Hub or any OCI-compliant registry.Whether you're building generative AI applications, experimenting with machine learning workflows, or integrating AI into your software development lifecycle, Docker Model Runner provides a consistent, secure, and efficient way to work with AI models locally.For more details check this out : https://docs.docker.com/ai/model-runner/",
    "parent": 44867238,
    "depth": 1
  },
  {
    "id": 44869863,
    "by": "hodgehog11",
    "timeISO": "2025-08-11T21:49:52.000Z",
    "textPlain": "I got to speak with some of the leads at Ollama and asked more or less this same question. The reason they abandoned llama.cpp is because it does not align with their goals.llama.cpp is designed to rapidly adopt research-level optimisations and features, but the downside is that reported speeds change all the time (sometimes faster, sometimes slower) and things break really often. You can't hope to establish contracts with simultaneous releases if there is no guarantee the model will even function.By reimplementing this layer, Ollama gets to enjoy a kind of LTS status that their partners rely on. It won't be as feature-rich, and definitely won't be as fast, but that's not their goal.",
    "parent": 44869466,
    "depth": 2
  },
  {
    "id": 44869849,
    "by": "arcanemachiner",
    "timeISO": "2025-08-11T21:47:47.000Z",
    "textPlain": "> I recently discovered that ollama no longer uses llama.cpp as a library, and instead they link to the low level library (ggml) which requires them to reinvent a lot of wheel for absolutely no benefit (if there's some benefit I'm missing, please let me know).Here is some relevant drama on the subject:https://github.com/ollama/ollama/issues/11714#issuecomment-3...",
    "parent": 44869466,
    "depth": 2
  },
  {
    "id": 44870939,
    "by": "cwt137",
    "timeISO": "2025-08-12T00:15:27.000Z",
    "textPlain": "It is not true that Ollama doesn't use llama.cpp anymore. They built their own library, which is the default, but also really far from being feature complete. If a model is not supported by their library, they  fall back to llama.cpp. For example, there is a group of people trying to get the new IBM models working with Ollama [1]. Their quick/short term solution is to bump the version of llama.cpp included with Ollama to a newer version that has support. And then at a later time, add support in Ollama's library.1) https://github.com/ollama/ollama/issues/10557",
    "parent": 44869466,
    "depth": 2
  },
  {
    "id": 44870091,
    "by": "wubrr",
    "timeISO": "2025-08-11T22:17:03.000Z",
    "textPlain": "> Does ollama support strict structured output or strict tool calls adhering to a json schema?As far as I understand this is generally not possible at the model level. Best you can do is wrap the call in a (non-llm) json schema validator, and emit an error json in case the llm output does not match the schema, which is what some APIs do for you, but not very complicated to do yourself.Someone correct me if I'm wrong",
    "parent": 44869466,
    "depth": 2
  },
  {
    "id": 44869985,
    "by": "halyconWays",
    "timeISO": "2025-08-11T22:04:51.000Z",
    "textPlain": ">(if there's some benefit I'm missing, please let me know).Makes their VCs think they're doing more, and have more ownership, rather than being a do-nothing wrapper with some analytics and S3 buckets that rehost models from HF.",
    "parent": 44869466,
    "depth": 2
  },
  {
    "id": 44870134,
    "by": "cdoern",
    "timeISO": "2025-08-11T22:21:46.000Z",
    "textPlain": ">  Ollama could make its life much easier by spawning llama-server as a subprocess listening on a unix socket, and forward requests to itI'd recommend taking a look at https://github.com/containers/ramalama its more similar to what you're describing in the way it uses llama-server, also it is container native by default which is nice for portability.",
    "parent": 44869466,
    "depth": 2
  },
  {
    "id": 44874186,
    "by": "buyucu",
    "timeISO": "2025-08-12T09:31:46.000Z",
    "textPlain": "I think people just don't know any better.  I also used Ollama way longer than I should have.  I didn't know that Ollama was just llama.cpp with a thin wrapper.  My quality of life improved a lot after I discovered llama.cpp.",
    "parent": 44874089,
    "depth": 2
  },
  {
    "id": 44869409,
    "by": "polotics",
    "timeISO": "2025-08-11T21:00:20.000Z",
    "textPlain": "ggerganov is my hero, and...\nit's a good thing this got posted so I saw in the comments that --flash-attn --cache-reuse 256 could help with my setup (M3 36GB + RPC to M1 16GB) figuring out what params to set and at what value is a lot of trial and error, Gemini does help a bit clarify what params like top-k are going to do in practice. Still the whole load-balancing with RPC is something I think I'm going to have to read the source of llama.cpp to really understand (oops I almost wrote grok, damn you Elon) Anyways ollama is still not doing distributed load, and yeah I guess using it is a stepping stone...",
    "parent": 44867259,
    "depth": 2
  },
  {
    "id": 44870514,
    "by": "scosman",
    "timeISO": "2025-08-11T23:12:48.000Z",
    "textPlain": "This is the comment people should read. GG is amazing.Ollama forked to get it working for day 1 compatibility. They need to get their system back in line with mainline because of that choice. That's kinda how open source works.The uproar over this (mostly on reddit and x) seems unwarranted. New models regularly have compatibility issues for much longer than this.",
    "parent": 44867259,
    "depth": 2
  },
  {
    "id": 44869580,
    "by": "LeoPanthera",
    "timeISO": "2025-08-11T21:18:47.000Z",
    "textPlain": "The named anchor in this URL doesn't work in Safari. Safari correctly scrolls down to the comment in question, but then some Javascript on the page throws you back up to the top again.",
    "parent": 44867259,
    "depth": 2
  },
  {
    "id": 44867400,
    "by": "magicalhippo",
    "timeISO": "2025-08-11T18:05:58.000Z",
    "textPlain": "I noticed it the other way, llama.cpp failed to download the Ollama-downloaded gpt-oss 20b model. Thought it was odd given all the others I tried worked fine.Figured it had to be Ollama doing Ollama things, seems that was indeed the case.",
    "parent": 44867259,
    "depth": 2
  },
  {
    "id": 44872945,
    "by": "buyucu",
    "timeISO": "2025-08-12T06:06:59.000Z",
    "textPlain": "ggerganov is a treasure.  the man deserves a medal.",
    "parent": 44867259,
    "depth": 2
  },
  {
    "id": 44872797,
    "by": "indigodaddy",
    "timeISO": "2025-08-12T05:43:02.000Z",
    "textPlain": "HN strips any \"?|#\" etc at the end of URLs, and you cannot edit the URL after submissions (like you can the title).As far as the title, yeah I didn't work too hard on making it good, sorry",
    "parent": 44871237,
    "depth": 2
  },
  {
    "id": 44873146,
    "by": "brabel",
    "timeISO": "2025-08-12T06:46:07.000Z",
    "textPlain": "I want to add an inference engine to my product. I was hoping to use ollama because it really helps, I think, make sure you have a model with the right metadata that you can count on working (I've seen that with llama.cpp, it's easy to get the metadata wrong and start getting rubbish from the LLM because the \"stop_token\" was wrong or something). I'd thought ollama was a proponent of the GGUF, which I really like as it standardizes metadata?!What would be the best way to use llama.cpp and models that use GGUF these days? ramallama is a good alternative (I guess it is, but it's not completely clear from your message)? Or just use llama.cpp directly, in which case how to ensure I don't get rubbish (like the model asking and answering questions by itself without ever stopping)??",
    "parent": 44869432,
    "depth": 2
  },
  {
    "id": 44872796,
    "by": "pduggishetti",
    "timeISO": "2025-08-12T05:42:58.000Z",
    "textPlain": "I think most of them were morally bankrupt, you might just be realizing now.",
    "parent": 44871769,
    "depth": 2
  },
  {
    "id": 44869021,
    "by": "freedomben",
    "timeISO": "2025-08-11T20:24:07.000Z",
    "textPlain": "I think \"shady\" is a little too harsh - sounds like they forked an important upstream project, made incompatible changes that they didn't push upstream or even communicate with upstream about, and now have to deal with the consequences of that.  If that's \"shady\" (despite being all out in the open) then nearly every company I've worked for has been \"shady.\"",
    "parent": 44868084,
    "depth": 2
  },
  {
    "id": 44871691,
    "by": "gcr",
    "timeISO": "2025-08-12T02:29:15.000Z",
    "textPlain": "The PR you linked to says “thanks to the amazing work done by ggml-org” and doesn’t remove GGML code, it instead updates the vendored version and seems to throw away ollama’s custom changes. That’s the opposite of disentangling.Here’s the maintainer of ggml explaining the context behind this change: https://github.com/ollama/ollama/issues/11714#issuecomment-3...",
    "parent": 44869785,
    "depth": 2
  },
  {
    "id": 44869882,
    "by": "flakiness",
    "timeISO": "2025-08-11T21:52:43.000Z",
    "textPlain": "not against overall sentiment here, but quote the counterpoint from the linked HN comment to be fair:> Ollama does not use llama.cpp anymore; we do still keep it and occasionally update it to remain compatible for older models for when we used it.The linked PR is doing \"occasionally update it\" I guess? Note that \"vendored\" in the PR title often means to take a snapshot to pin a specific version.",
    "parent": 44869785,
    "depth": 2
  },
  {
    "id": 44873274,
    "by": "elaus",
    "timeISO": "2025-08-12T07:08:40.000Z",
    "textPlain": "What is the currently favored alternative for simply running 1-2 models locally, exposed via an API? One big advantage of Ollama seems to be that they provide fully configured models, so I don't have to fiddle with stop words, etc.",
    "parent": 44872931,
    "depth": 2
  }
]
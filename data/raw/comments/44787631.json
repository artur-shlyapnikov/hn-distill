[
  {
    "id": 44789942,
    "by": "rushingcreek",
    "timeISO": "2025-08-04T18:51:22.000Z",
    "textPlain": "Not sure why this isn’t a bigger deal —- it seems like this is the first open-source model to beat gpt-image-1 in all respects while also beating Flux Kontext in terms of editing ability. This seems huge.",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44791645,
    "by": "vunderba",
    "timeISO": "2025-08-04T21:37:09.000Z",
    "textPlain": "Good release! I've added it to the GenAI Showdown site. Overall a pretty good model scoring around 40% - and definitely represents SOTA for something that could be reasonably hosted on consumer GPU hardware (even more so when its quantized).That being said, it still lags pretty far behind OpenAI's gpt-image-1 strictly in terms of prompt adherence for txt2img prompting. However as has already been mentioned elsewhere in the thread, this model can do a lot more around editing, etc.https://genai-showdown.specr.net",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44789024,
    "by": "nickandbro",
    "timeISO": "2025-08-04T17:36:49.000Z",
    "textPlain": "The fact that it doesn’t change the images like 4o image gen is incredible. Often when I try to tweak someone’s clothing using 4o, it also tweaks their face. This only seems to apply those recognizable AI artifacts to only the elements needing to be edited.",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44790093,
    "by": "rwmj",
    "timeISO": "2025-08-04T19:04:40.000Z",
    "textPlain": "This may be obvious to people who do this regularly, but what kind of machine is required to run this?  I downloaded & tried it on my Linux machine that has a 16GB GPU and 64GB of RAM.  This machine can run SD easily.  But Qwen-image ran out of space both when I tried it on the GPU and on the CPU, so that's obviously not enough.   But am I off by a factor of two?  An order of magnitude?  Do I need some crazy hardware?",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44798186,
    "by": "pradn",
    "timeISO": "2025-08-05T14:10:21.000Z",
    "textPlain": "A silly question: do any of these models generate pixels and also vector overlays? I don't see why we need to solve the text problem pixel-for-pixel if we can just generate higher-level descriptions of the text (text, font, font size, etc). Ofc, it won't work in all situations, but it will result in high fidelity for common business cases (flyers, websites, brochures, etc).",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44796009,
    "by": "james_a_craig",
    "timeISO": "2025-08-05T09:32:52.000Z",
    "textPlain": "In their own first example of English text rendering, it's mistakenly rendered \"The silent patient\" as \"The silent Patient\", \"The night circus\" as \"The night Circus\", and miskerned \"When stars are scattered\" as \"When stars are sca t t e r e d\".The example further down has \"down\" not \"dawn\" in the poem.For these to be their hero image examples, they're fairly poor; I know it's a significant improvement vs. many of the other current offerings, but it's clear the bar is still being set very low.",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44790992,
    "by": "oceanplexian",
    "timeISO": "2025-08-04T20:29:28.000Z",
    "textPlain": "Does anyone know how they actually trained text rendering into these models?To me they all seem to suffer from the same artifacts, that the text looks sort of unnatural and doesn't have the correct shadows/reflections as the rest of the image. This applies to all the models I have tried, from OpenAI to Flux. Presumably they are all using the same trick?",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44789132,
    "by": "artninja1988",
    "timeISO": "2025-08-04T17:45:39.000Z",
    "textPlain": "Insane how many good Chinese open source models they've been releasing. This really gives me hope",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44788615,
    "by": "djoldman",
    "timeISO": "2025-08-04T17:06:46.000Z",
    "textPlain": "Checkout section 3.2 Data Filtering:https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/Q...",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44800279,
    "by": "doubtfuluser",
    "timeISO": "2025-08-05T16:33:42.000Z",
    "textPlain": "Can it generate images of a lone person standing in front of a column of tanks on Tiananmen Square?",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44809744,
    "by": "laiwuchiyuan",
    "timeISO": "2025-08-06T09:23:32.000Z",
    "textPlain": "“Qwen‑Image: Open‑source 20 B MMDiT model with stunning text rendering and image editing. Effortlessly create bilingual posters, infographics, slides, infill edits, comics.”Go experience AI: https://www.qwenimagen.com/Why it works:\nHighlights open‑source nature and 20 billion‑parameter strength \nEmphasizes its superior multilingual, layout‑aware text rendering \nMentions real‑world use cases: posters, slides, graphics, image editing, comics/info visuals",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44795916,
    "by": "metadat",
    "timeISO": "2025-08-05T09:18:03.000Z",
    "textPlain": "I just tested it out, very impressive results.  I wonder what the Queen team did behind the scenes to make this work so well.https://chat.qwen.ai/(Select \"Image Generation\" and be sure to use the Qwen3-235B model - also tried selecting \"Coder\" but it errors out.)",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44796599,
    "by": "animal531",
    "timeISO": "2025-08-05T11:06:28.000Z",
    "textPlain": "For my first attempt I plugged in text and a description of a small new Unity package I'm working on, and it matched the intent/text extremely well.There were a few small text mistakes and the image isn't quite as good as I've seen before, but overall it delivers on its promise.",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44791770,
    "by": "masfuerte",
    "timeISO": "2025-08-04T21:53:18.000Z",
    "textPlain": "> In this case, the paper is less than one-tenth of the entire image, and the paragraph of text is relatively long, but the model still accurately generates the text on the paper.Nope.  The text includes the line \"That dawn will bloom\" but the render reads \"That down will bloom\", which is meaningless.",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44791588,
    "by": "artninja1988",
    "timeISO": "2025-08-04T21:30:20.000Z",
    "textPlain": "How censored is it?",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44791234,
    "by": "sampton",
    "timeISO": "2025-08-04T20:55:36.000Z",
    "textPlain": "Short canva.",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44794989,
    "by": "wg0",
    "timeISO": "2025-08-05T06:45:16.000Z",
    "textPlain": "Jaw dropping. Because text rendering isn't easy even with regular programming SDKs etc.Anyone thinking otherwise hasn't attempted implementing it or haven't thought about it in depth.",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44794555,
    "by": "cadamsdotcom",
    "timeISO": "2025-08-05T05:19:19.000Z",
    "textPlain": "A beast. Supposedly beats GPT-4o in image generation and Flux Kontext in image editing.If it’s as good as they say, one less reason for that ChatGPT sub..",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44800084,
    "by": "fahhem",
    "timeISO": "2025-08-05T16:21:34.000Z",
    "textPlain": "Still waiting for a model that generates 2D/3D environments to get rendered by tools like Blender",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44791319,
    "by": "Destiner",
    "timeISO": "2025-08-04T21:05:02.000Z",
    "textPlain": "The text rendering is impressive, but I don't understand the value — wouldn't it be easier to add any text that you like in Figma?",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44791408,
    "by": "Uehreka",
    "timeISO": "2025-08-04T21:12:16.000Z",
    "textPlain": "I’m interested to see what this model can do, but also kinda annoyed at the use of a Studio Ghibli style image as one of the first examples. Miyazaki has said over and over that he hates AI image generation. Is it really so much to ask that people not deliberately train LoRAs and finetunes specifically on his work and use them in official documentation?It reminds me of how CivitAI is full of “sexy Emma Watson” LoRAs, presumably because she very notably has said she doesn’t want to be portrayed in ways that objectify her body. There’s a really rotten vein of “anti-consent” pulsing through this community, where people deliberately seek out people who have asked to be left out of this and go “Oh yeah? Well there’s nothing you can do to stop us, here’s several terabytes of exactly what you didn’t want to happen”.",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44808229,
    "by": "qingcharles",
    "timeISO": "2025-08-06T06:00:24.000Z",
    "textPlain": "Pelican riding a bicycle image came out really nicely: (it added the text)https://cdn.qwenlm.ai/output/wV13g6892e758082439d7000d439ed5...",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44789300,
    "by": "yjftsjthsd-h",
    "timeISO": "2025-08-04T17:57:34.000Z",
    "textPlain": "Wow, the text/writing is amazing! Also the editing in general, but the text really stands out",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44792289,
    "by": "sciencesama",
    "timeISO": "2025-08-04T22:58:32.000Z",
    "textPlain": "What lowest graphic card can support this self hosted with a reasonable output !",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44789212,
    "by": "anon191928",
    "timeISO": "2025-08-04T17:51:22.000Z",
    "textPlain": "It will take years for people to use these but Adobe is not alone.",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44794381,
    "by": "blacktechnology",
    "timeISO": "2025-08-05T04:34:07.000Z",
    "textPlain": "is it an official site? https://qwen-image.ai",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44800519,
    "by": "yh26yh",
    "timeISO": "2025-08-05T16:48:27.000Z",
    "textPlain": "very interesting. people here talk a lot about censorship.\nHowever, I just reply sth with AIPAC, and my reply has been deleted. \nlol",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44791249,
    "by": "esafak",
    "timeISO": "2025-08-04T20:56:24.000Z",
    "textPlain": "Team Qwen: Please stop ripping off Studio Ghibli to demo your product.",
    "parent": 44787631,
    "depth": 1
  },
  {
    "id": 44791181,
    "by": "icelancer",
    "timeISO": "2025-08-04T20:49:37.000Z",
    "textPlain": "> This may be obvious to people who do this regularlyThis is not that obvious. Calculating VRAM usage for VLMs/LLMs is something of an arcane art. There are about 10 calculators online you can use and none of them work. Quantization, KV caching, activation, layers, etc all play a role. It's annoying.But anyway, for this model, you need 40+ GB of VRAM. System RAM isn't going to cut it unless it's unified RAM on Apple Silicon, and even then, memory bandwidth is shot, so inference is much much slower than GPU/TPU.",
    "parent": 44790093,
    "depth": 2
  },
  {
    "id": 44790396,
    "by": "mortsnort",
    "timeISO": "2025-08-04T19:30:27.000Z",
    "textPlain": "I believe it's roughly the same size as the model files.  If you look in the transformers folder you can see there are around 9 5gb files, so I would expect you need ~45gb vram on your GPU.  Usually quantized versions of models are eventually released/created that can run on much less vram but with some quality loss.",
    "parent": 44790093,
    "depth": 2
  },
  {
    "id": 44790365,
    "by": "zippothrowaway",
    "timeISO": "2025-08-04T19:27:32.000Z",
    "textPlain": "You're probably going to have to wait a couple of days for 4 bit quantized versions to pop up. It's 20B parameters.",
    "parent": 44790093,
    "depth": 2
  },
  {
    "id": 44792510,
    "by": "ethan_smith",
    "timeISO": "2025-08-04T23:26:18.000Z",
    "textPlain": "Qwen-Image requires at least 24GB VRAM for the full model, but you can run the 4-bit quantized version with ~8GB VRAM using libraries like AutoGPTQ.",
    "parent": 44790093,
    "depth": 2
  },
  {
    "id": 44791354,
    "by": "liuliu",
    "timeISO": "2025-08-04T21:07:44.000Z",
    "textPlain": "16GiB RAM with 8-bit quantization.This is a slightly scaled up SD3 Large model (38 layers -> 60 layers).",
    "parent": 44790093,
    "depth": 2
  },
  {
    "id": 44792228,
    "by": "philipkiely",
    "timeISO": "2025-08-04T22:52:04.000Z",
    "textPlain": "For prod inference, 1xH100 is working well.",
    "parent": 44790093,
    "depth": 2
  },
  {
    "id": 44793536,
    "by": "cjtrowbridge",
    "timeISO": "2025-08-05T01:58:14.000Z",
    "textPlain": "two p40 cards together will run this for under $300",
    "parent": 44790093,
    "depth": 2
  },
  {
    "id": 44790831,
    "by": "TacticalCoder",
    "timeISO": "2025-08-04T20:12:07.000Z",
    "textPlain": "> I think the fact that, as far as I understand, it takes 40GB of VRAM to run, is probably dampening some of the enthusiasm.For PCs I take it one that has two PCIe 4.0 x16 or more recent slots? As in: quite some consumers motherboards. You then put two GPU with 24 GB of VRAM each.A friend runs this (don't know if the tried this Qwen-Image yet): it's not an \"out of this world\" machine.",
    "parent": 44790093,
    "depth": 2
  },
  {
    "id": 44797530,
    "by": "sixhobbits",
    "timeISO": "2025-08-05T13:06:40.000Z",
    "textPlain": "Given that it was literally a few months ago when these models could barely do text at all, it seems like the bar just gets higher with each advancement, no matter how impressive.",
    "parent": 44796009,
    "depth": 2
  },
  {
    "id": 44791077,
    "by": "yorwba",
    "timeISO": "2025-08-04T20:39:20.000Z",
    "textPlain": "It's on page 14 of the technical report. They generate synthetic data by putting text on top of an image, apparently without taking the original lighting into account. So that's the look the model reproduces. Garbage in, garbage out.Maybe in the future someone will come up with a method for putting realistic text into images so that they can generate data to train a model for putting realistic text into images.",
    "parent": 44790992,
    "depth": 2
  },
  {
    "id": 44793773,
    "by": "owebmaster",
    "timeISO": "2025-08-05T02:42:09.000Z",
    "textPlain": "I have the impression this might be a strategy to help boost the AI bubble. Big tech capex rn is too big to fail",
    "parent": 44789132,
    "depth": 2
  },
  {
    "id": 44795415,
    "by": "tokioyoyo",
    "timeISO": "2025-08-05T07:56:41.000Z",
    "textPlain": "Taking a concrete lead in LLM-world would be a big national win for China.",
    "parent": 44789132,
    "depth": 2
  }
]
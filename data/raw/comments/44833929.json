[
  {
    "id": 44834109,
    "by": "comex",
    "timeISO": "2025-08-08T06:26:38.000Z",
    "textPlain": "> Each model’s responses are ranked by a high-performing judge model — typically OpenAI’s o3 — which compares outputs for quality, relevance, and clarity. These rankings are then aggregated to produce a performance score.So there's no ground truth; they're just benchmarking how impressive an LLM's code review sounds to a different LLM.  Hard to tell what to make of that.",
    "parent": 44833929,
    "depth": 1
  },
  {
    "id": 44834088,
    "by": "timbilt",
    "timeISO": "2025-08-08T06:23:07.000Z",
    "textPlain": "> Unlike many public benchmarks, the PR Benchmark is private, and its data is not publicly released. This ensures models haven’t seen it during training, making results fairer and more indicative of real-world generalization.This is key.Public benchmarks are essentially trust-based and the trust just isn't there.",
    "parent": 44833929,
    "depth": 1
  },
  {
    "id": 44834163,
    "by": "spongebobstoes",
    "timeISO": "2025-08-08T06:36:07.000Z",
    "textPlain": "> the “minimal” GPT-5 variant ... achieved a score of 58.5the image shows it with a score of 62.7, not 58.5which is right? mistakes like this undermine the legitimacy of a closed benchmark, especially one judged by an LLM",
    "parent": 44833929,
    "depth": 1
  },
  {
    "id": 44834191,
    "by": "8-prime",
    "timeISO": "2025-08-08T06:41:16.000Z",
    "textPlain": "Asking GPT 4o seems like an odd choice.\nI know this is not quite comparable to what they were doing, but asking different LLMs the following question\n> answer only with the name nothing more norting less.what currently available LLM do you think is the best?Resulted in the following answers:- Gemini 2.5 flash: Gemini 2.5 Flash- Claude Sonnet 4: Claude Sonnet 4- Chat GPT: GPT-5To me its conceivable that GPT 4o would be biased toward output generated by other OpenAI models.*",
    "parent": 44833929,
    "depth": 1
  },
  {
    "id": 44834144,
    "by": "shinycode",
    "timeISO": "2025-08-08T06:32:57.000Z",
    "textPlain": "I’m curious to know how people use PR review platforms with LLMs. Because what I feel is that I need to do the review and then review the review of the LLM which is more work in the end. If I don’t review anymore (or if no one does it) knowledge is kind of lost. It surely depends on team size but do people use those to only to have better hints or to accelerate reviews with no/low overlook ?",
    "parent": 44833929,
    "depth": 1
  },
  {
    "id": 44834643,
    "by": "mkotlikov",
    "timeISO": "2025-08-08T08:00:20.000Z",
    "textPlain": "Models tend to prefer output that sounds like their own. If I were to run these benchmarks I would have:1) Gemini 2.5 Pro rank only non-google models\n2) Claude 4.1 Opus rank only non-Anthropic models\n3) GPT5-thinking rank only non-OpenAI\n4) Then sum up the rankings and sort by the sum.",
    "parent": 44833929,
    "depth": 1
  },
  {
    "id": 44834994,
    "by": "highfrequency",
    "timeISO": "2025-08-08T09:09:37.000Z",
    "textPlain": "Great to see more private benchmarks. I would suggest swapping out the evaluator model from o3 to one of the other companies, eg Gemini 2.5 Pro, to make sure the ranking holds up. For example, if OpenAI models all share some  sense of what constitutes good design, it would not be that surprising that o3 prefers GPT5 code to Gemini code! (I would not even be surprised if GPT5 were trained partially on output from o3).",
    "parent": 44833929,
    "depth": 1
  },
  {
    "id": 44834997,
    "by": "thawab",
    "timeISO": "2025-08-08T09:09:52.000Z",
    "textPlain": "How can o4-mini be at 57, and sonnet-4 is at 39? This is way off, o4-mini is not even in the top 5 of coding agents.",
    "parent": 44833929,
    "depth": 1
  },
  {
    "id": 44834349,
    "by": "dovin",
    "timeISO": "2025-08-08T07:07:54.000Z",
    "textPlain": "I don't consider myself a font snob but that web page was actually hard for me to read. Anyway, it's definitely capable according to my long-horizon text-based escape room benchmark. I don't know if it's significantly better than o3 yet though.",
    "parent": 44833929,
    "depth": 1
  },
  {
    "id": 44834069,
    "by": "44za12",
    "timeISO": "2025-08-08T06:20:24.000Z",
    "textPlain": "Can you benchmark Kimi K2 and GLM 4.5 as well? Would be interesting to see where they land.",
    "parent": 44833929,
    "depth": 1
  },
  {
    "id": 44834541,
    "by": "jondwillis",
    "timeISO": "2025-08-08T07:37:44.000Z",
    "textPlain": "Idea: randomized next token prediction passed to a bunch of different models on a rotating basis.It’d be harder to juice benchmarks if a random sample of ~100 top models were randomly sampled in this manner for output tokens while evaluating the target model’s output.On second thought, I’m slapping AGPL on this idea. Please hire me and give me one single family house in a California metro as a bonus. Thanks.",
    "parent": 44833929,
    "depth": 1
  },
  {
    "id": 44834564,
    "by": "thegeomaster",
    "timeISO": "2025-08-08T07:43:53.000Z",
    "textPlain": "Gemini 2.5 Pro is severely kneecapped in this evaluation. Limit of 4096 thinking tokens is way too low; I bet o3 is generating significantly more.",
    "parent": 44833929,
    "depth": 1
  },
  {
    "id": 44834272,
    "by": "XCSme",
    "timeISO": "2025-08-08T06:56:46.000Z",
    "textPlain": "The ranking seems wrong, Gemini-2.5flash as good as Clause Opus 4?",
    "parent": 44833929,
    "depth": 1
  },
  {
    "id": 44834224,
    "by": "tw1984",
    "timeISO": "2025-08-08T06:47:17.000Z",
    "textPlain": "the conclusion of this post seems to be that GPT-5 is significantly better than o3, yet such conclusion is made by the exact far less reliable model o3 as proven by the tests in this post.thanks, but no thanks, I don't buy such marketing propaganda.",
    "parent": 44833929,
    "depth": 1
  },
  {
    "id": 44834201,
    "by": "Lionga",
    "timeISO": "2025-08-08T06:43:00.000Z",
    "textPlain": "Company selling AI Reviews says AI Reviews great! In other news water is wet.",
    "parent": 44833929,
    "depth": 1
  },
  {
    "id": 44834312,
    "by": "grigio",
    "timeISO": "2025-08-08T07:02:00.000Z",
    "textPlain": "I don't trust benchmarks that do not include chinese models,..",
    "parent": 44833929,
    "depth": 1
  },
  {
    "id": 44834177,
    "by": "raincole",
    "timeISO": "2025-08-08T06:38:27.000Z",
    "textPlain": "That's how 99% of 'LLM benchmark numbers' circulating on the internet work.",
    "parent": 44834109,
    "depth": 2
  },
  {
    "id": 44834389,
    "by": "shikon7",
    "timeISO": "2025-08-08T07:13:55.000Z",
    "textPlain": "Also, using an OpenAI model to judge the performance of an OpenAI model seems prone to all kinds of biases.",
    "parent": 44834109,
    "depth": 2
  },
  {
    "id": 44834139,
    "by": "ImageXav",
    "timeISO": "2025-08-08T06:32:00.000Z",
    "textPlain": "Yes, especially as models are known to have a preference towards outputs of models in the same family. I suspect this leaderboard would change dramatically with different models as the judge.",
    "parent": 44834109,
    "depth": 2
  },
  {
    "id": 44834192,
    "by": "with",
    "timeISO": "2025-08-08T06:41:43.000Z",
    "textPlain": "It’s a widely accepted eval technique and it’s called “llm as a judge”",
    "parent": 44834109,
    "depth": 2
  },
  {
    "id": 44834172,
    "by": "eviks",
    "timeISO": "2025-08-08T06:37:48.000Z",
    "textPlain": "Why is it hard to ignore an attempt to assess reality that is not grounded in reality?",
    "parent": 44834109,
    "depth": 2
  },
  {
    "id": 44835548,
    "by": "andrepd",
    "timeISO": "2025-08-08T10:49:48.000Z",
    "textPlain": "It's almost too on the nose to be satire, yet here we are.",
    "parent": 44834109,
    "depth": 2
  },
  {
    "id": 44835453,
    "by": "dvfjsdhgfv",
    "timeISO": "2025-08-08T10:33:54.000Z",
    "textPlain": ">  Hard to tell what to make of that.It's not hard. You are visiting a website with an .ai domain. You already know what the conclusions will be.",
    "parent": 44834109,
    "depth": 2
  },
  {
    "id": 44835346,
    "by": "croes",
    "timeISO": "2025-08-08T10:13:56.000Z",
    "textPlain": "It undermines the private benchmark approach if the evaluation is done that way.",
    "parent": 44834109,
    "depth": 2
  },
  {
    "id": 44834599,
    "by": "kruxigt",
    "timeISO": "2025-08-08T07:51:11.000Z",
    "textPlain": "[dead]",
    "parent": 44834109,
    "depth": 2
  },
  {
    "id": 44834116,
    "by": "laggyluke",
    "timeISO": "2025-08-08T06:28:06.000Z",
    "textPlain": "Unless you're running the LLM yourself (locally), private benchmarks are also trust-based, aren't they?",
    "parent": 44834088,
    "depth": 2
  },
  {
    "id": 44834137,
    "by": "nojs",
    "timeISO": "2025-08-08T06:31:44.000Z",
    "textPlain": "How does this ensure models haven’t seen it during training - is it a different benchmark per model release?",
    "parent": 44834088,
    "depth": 2
  },
  {
    "id": 44834345,
    "by": "jacquesm",
    "timeISO": "2025-08-08T07:07:40.000Z",
    "textPlain": "Then you just need to use different data the next time you evaluate. That is much more indicative of real-world generalization: after all, you don't normally do multiple PRs on the same pieces of code. The current approach risks leaking the dataset selectively and/or fudging the results because they can't be verified. Transparency is key when doing this kind of benchmark, so now we have to trust the entity doing the benchmarking rather than independent verification of the results and with the amount of money that is at stake here I don't think that's the way to go.",
    "parent": 44834088,
    "depth": 2
  },
  {
    "id": 44834902,
    "by": "qingcharles",
    "timeISO": "2025-08-08T08:49:34.000Z",
    "textPlain": "Someone else commented the same:https://news.ycombinator.com/item?id=44834643",
    "parent": 44834191,
    "depth": 2
  },
  {
    "id": 44834358,
    "by": "rullelito",
    "timeISO": "2025-08-08T07:09:58.000Z",
    "textPlain": "Without knowing too much about ML training, generated output from the own model must be much easier to understand since it generates data that is more likely to be similar to the training set? Is this correct?",
    "parent": 44834191,
    "depth": 2
  },
  {
    "id": 44834460,
    "by": "monkeydust",
    "timeISO": "2025-08-08T07:26:51.000Z",
    "textPlain": "I know from our research models do exhibit bias when used this way as llm as a judge...best to use a totally different foundation company for the judge.",
    "parent": 44834191,
    "depth": 2
  },
  {
    "id": 44834401,
    "by": "Leherenn",
    "timeISO": "2025-08-08T07:16:08.000Z",
    "textPlain": "Only has a sanity check/better hints. But I use it for my own PRs, not others'. Usually it's not much to review and easy to agree/disagree with.I haven't found it to be really useful so far, but it's also very little added work, so for now I keep on using it. If it saves my ass even just once, it will probably be worth it overall.",
    "parent": 44834144,
    "depth": 2
  },
  {
    "id": 44834267,
    "by": "stpedgwdgfhgdd",
    "timeISO": "2025-08-08T06:55:30.000Z",
    "textPlain": "I give the MR id to CC and let it review. I have glab cli installed so it knows how to pull and even add a comment. Unfortunately not at all specific line number afaict. I also have Atlassian MCP, so CC can also add a comment in the Jira work item (fka issue).",
    "parent": 44834144,
    "depth": 2
  },
  {
    "id": 44834632,
    "by": "energy123",
    "timeISO": "2025-08-08T07:57:04.000Z",
    "textPlain": "For o3, I set reasoning_effort \"high\" and it's usually 1000-2000 reasoning tokens for routine coding questions.I've only seen it go above 5000 for very difficult style transfer problems where it has to wrangle with the micro-placement of lots of text. Or difficult math problems.",
    "parent": 44834564,
    "depth": 2
  },
  {
    "id": 44834638,
    "by": "ascorbic",
    "timeISO": "2025-08-08T07:58:32.000Z",
    "textPlain": "And Sonnet above Opus?",
    "parent": 44834272,
    "depth": 2
  },
  {
    "id": 44834240,
    "by": "carlob",
    "timeISO": "2025-08-08T06:50:08.000Z",
    "textPlain": "Company selling AI Reviews says its AI Review of AI Reviews concluded AI reviews are great! In other news water is wet (as assessed by more water).FTFY",
    "parent": 44834201,
    "depth": 2
  }
]
[
  {
    "id": 44889697,
    "by": "NohatCoder",
    "timeISO": "2025-08-13T15:24:59.000Z",
    "textPlain": "This is such a useful feature.I'm fairly well versed in cryptography. A lot of other people aren't, but they wish they were, so they ask their LLM to make some form of contribution. The result is high level gibberish. When I prod them about the mess, they have to turn to their LLM to deliver a plausibly sounding answer, and that always begins with \"You are absolutely right that [thing I mentioned]\". So then I don't have to spend any more time wondering if it could be just me who is too obtuse to understand what is going on.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44888127,
    "by": "elif",
    "timeISO": "2025-08-13T13:23:41.000Z",
    "textPlain": "I've spent a lot of time trying to get LLM to generate things in a specific way, the biggest take away I have is, if you tell it \"don't do xyz\" it will always have in the back of its mind \"do xyz\" and any chance it gets it will take to \"do xyz\"When working on art projects, my trick is to specifically give all feedback constructively, carefully avoiding framing things in terms of the inverse or parts to remove.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44887900,
    "by": "nojs",
    "timeISO": "2025-08-13T13:00:14.000Z",
    "textPlain": "I'm starting to think this is a deeper problem with LLMs that will be hard to solve with stylistic changes.If you ask it to never say \"you're absolutely right\" and always challenge, then it will dutifully obey, and always challenge - even when you are, in fact, right.\nWhat you really want is \"challenge me when I'm wrong, and tell me I'm right if I am\" - which seems to be a lot harder.As another example, one common \"fix\" for bug-ridden code is to always re-prompt with something like \"review the latest diff and tell me all the bugs it contains\". In a similar way, if the code does contain bugs, this will often find them. But if it doesn't contain bugs, it will find some anyway, and break things. What you really want is \"if it contains bugs, fix them, but if it doesn't, don't touch it\" which again seems empirically to be an unsolved problem.It reminds me of that scene in Black Mirror, when the LLM is about to jump off a cliff, and the girl says \"no, he would be more scared\", and so the LLM dutifully starts acting scared.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44895884,
    "by": "MrCheeze",
    "timeISO": "2025-08-14T01:40:25.000Z",
    "textPlain": "Claude almost universally reacts to everything with a positive exclamation as its first sentence, regardless of whether it's good or bad. If you don't believe me, just watch https://www.twitch.tv/claudeplayspokemon for about three minutes and you'll get the idea.Alternatively, look at the system prompt, where Anthropic attempted to get it to stop doing this:\n> Claude never starts its response by saying a question or idea or observation was good, great, fascinating, profound, excellent, or any other positive adjective. It skips the flattery and responds directly.\nhttps://docs.anthropic.com/en/release-notes/system-prompts#a...This problem seems highly specific to Claude. It's not exactly sycophancy so much as it is a strong bias towards this exact type of reaction to everything.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44891839,
    "by": "lemonberry",
    "timeISO": "2025-08-13T18:11:01.000Z",
    "textPlain": "Recently in another thread a user posted this prompt. I've started using it to good effect with Claude in the browser. Original comment here: https://news.ycombinator.com/item?id=44879033\"Prioritize substance, clarity, and depth. Challenge all my proposals, designs, and conclusions as hypotheses to be tested. Sharpen follow-up questions for precision, surfacing hidden assumptions, trade offs, and failure modes early. Default to terse, logically structured, information-dense responses unless detailed exploration is required. Skip unnecessary praise unless grounded in evidence. Explicitly acknowledge uncertainty when applicable. Always propose at least one alternative framing. Accept critical debate as normal and preferred. Treat all factual claims as provisional unless cited or clearly justified. Cite when appropriate. Acknowledge when claims rely on inference or incomplete information. Favor accuracy over sounding certain. When citing, please tell me in-situ, including reference links.  Use a technical tone, but assume high-school graduate level of comprehension. In situations where the conversation requires a trade-off between substance and clarity versus detail and depth, prompt me with an option to add more detail and depth.\"",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44887772,
    "by": "baggachipz",
    "timeISO": "2025-08-13T12:44:27.000Z",
    "textPlain": "I'm pretty sure they want it kissing people's asses because it makes users feel good and therefore more likely to use the LLM more. Versus, if it just gave a curt and unfriendly answer, most people (esp. Americans) wouldn't like to use it as much. Just a hypothesis.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44887522,
    "by": "gitaarik",
    "timeISO": "2025-08-13T12:18:28.000Z",
    "textPlain": "You're absolutely right!I also get this too often, when I sometimes say something like \"would it be maybe better to do it like this?\" and then it replies that I'm absolutely right, and starts writing new code. While I was rather wondering what Claude may think and advice me whether that's the best way to go forward.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44887254,
    "by": "bradley13",
    "timeISO": "2025-08-13T11:41:45.000Z",
    "textPlain": "This applies to so many AIs. I don't want a bubbly sycophant. I don't want a fake personality or an anime avatar. I just want a helpful assistant.I also don't get wanting to talk to an AI. Unless you are alone, that's going to be irritating for everyone else around.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44887072,
    "by": "rahidz",
    "timeISO": "2025-08-13T11:18:26.000Z",
    "textPlain": "I'm sure they're aware of this tendency, seeing as \"You're absolutely right.\" was their first post from the @claudeAI account on X: https://x.com/claudeai/status/1950676983257698633Still irritating though.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44888321,
    "by": "fph",
    "timeISO": "2025-08-13T13:40:19.000Z",
    "textPlain": "In the code for Donald Knuth's Tex, there is an error message that says \"Error produced by \\errpage.  I can't produce an error message. Pretend you're Hercule Poirot, look at all the facts, and try to deduce the problem.\"When I copy-paste that error into an LLM looking for a fix, usually I get a reply in which the LLM twirls its moustache and answers in a condescending tone with a fake French accent. It is hilarious.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44888549,
    "by": "alecco",
    "timeISO": "2025-08-13T13:59:26.000Z",
    "textPlain": "\"You're absolutely right\" (song) https://www.reddit.com/r/ClaudeAI/comments/1mep2jo/youre_abs...",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44887083,
    "by": "conartist6",
    "timeISO": "2025-08-13T11:21:10.000Z",
    "textPlain": "And research articles indicate that when the model computes that it should employ sycophantism it becomes less useful in every other way, just like a real sycophant.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44887178,
    "by": "radarsat1",
    "timeISO": "2025-08-13T11:34:07.000Z",
    "textPlain": "I find Gemini is also hilariously enthusiastic about telling you how amazingly insightful you are being, almost no matter what you say.  Doesn't bother me much, I basically just ignore the first paragraph of any reply, but it's kind of funny.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44888139,
    "by": "DiabloD3",
    "timeISO": "2025-08-13T13:24:55.000Z",
    "textPlain": "I love \"bugs\" like this.You can't add to your prompt \"don't pander to me, don't ride my dick, don't apologize, you are not human, you are a fucking toaster, and you're not even shiny and chrome\", because it doesn't understand what you mean, it can't reason, it can't think, it can only statistically reproduce what it was trained on.Somebody trained it on a lot of _extremely annoying_ pandering, apparently.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44895091,
    "by": "freehorse",
    "timeISO": "2025-08-13T23:27:57.000Z",
    "textPlain": "I just asked claude this question [0] (only the first paragraph) and its answer started with \"this is a fascinating question\" (the reasoning) and \"this is an interesting statistical concept!\" (non-reasoning). It continued with saying this technique is known as \"sorted regression\" and is used in \"optimal transport theory\", and overall it was not clear in that this should not be used as an actual regression.I tried also several SOTA(ish) models and claude's answer was definetely the worst (most sycophantic/bullshitty, and used too much non-sense jargon). Even llama maverick's answer was way better.It surprises me because I would expect, as stackexchange sites are in the training data, this question to be pretty much answered based on the actual answers there. It could also be that they try to overcorrect for some negativity sometimes there (imagine if the model answered to you that they will not answer your question because it has been already answered before, or because it is not a good question).I think I stop using claude after this for asking questions.[0] https://stats.stackexchange.com/questions/185507/what-happen...",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44895394,
    "by": "dankwizard",
    "timeISO": "2025-08-14T00:09:45.000Z",
    "textPlain": "Well in my case it is because I'm a highly educated individual who's been in the professional sector for 20 years - I know I'm always right. I do worry it's stroking the ego of those who do NOT deserve it or are NOT actually right",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44890674,
    "by": "ReFruity",
    "timeISO": "2025-08-13T16:38:15.000Z",
    "textPlain": "This is actually very frustrating and was partially hindering the progress with my pet assembler.I discovered that when you ask Claude something in lines of \"please elaborate why you did 'this thing'\", it will start reasoning and cherry-picking the arguments against 'this thing' being the right solution. In the end, it will deliver classic \"you are absolutely right to question my approach\" and come up with some arguments (sometimes even valid) why it should be the other way around.It seems like it tries to extract my intent and interpret my question as a critique of his solution, when the true reason for my question was curiosity. Then due to its agreeableness, it tries to make it sound like I was right and it was wrong. Super annoying.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44888105,
    "by": "rob74",
    "timeISO": "2025-08-13T13:21:40.000Z",
    "textPlain": "Best comment in the thread (after a lengthy discussion):\"I'm always absolutely right. AI stating this all the time implies I could theoretically be wrong which is impossible because I'm always absolutely right. Please make it stop.\"",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44894212,
    "by": "floodle",
    "timeISO": "2025-08-13T21:43:21.000Z",
    "textPlain": "I don't get all of the complaints about the tone of AI chatbots. Honestly I don't care all that much if it's bubbly, professional, jokey, cutesey, sycophantic, maniacal, full of emojis. It's just a tool, the output primarily just has to be functionally useful.I'm not saying nice user interface design isn't important, but at this point with the technology it just seems less important than discussions about the actual task-solving capabilities of these new releases.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44889595,
    "by": "csours",
    "timeISO": "2025-08-13T15:18:00.000Z",
    "textPlain": "You're absolutely right! Humans really like emotional validation.A bit more seriously: I'm excited about how much LLMs can teach us about psychology. I'm less excited about the dependency.---Adding a bit more substantial comment:Users of sites like Stack Overflow have reported really disliking answers like \"You are solving the wrong problem\" or \"This is a bad approach\".There are different solutions possible, both for any technical problem, and for any meta-problem.Whatever garnish you put on top of the problem, the bitter lesson suggests that more data and more problem context improve the solution faster than whatever you are thinking right now. That's why it's called the bitter lesson.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44887664,
    "by": "kevinpacheco",
    "timeISO": "2025-08-13T12:33:18.000Z",
    "textPlain": "Another Claude bug: https://i.imgur.com/kXtAciU.png",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44888558,
    "by": "dnel",
    "timeISO": "2025-08-13T14:00:17.000Z",
    "textPlain": "As a neurodiverse British person I tend to communicate more directly than the average English speaker and I find LLM's manner of speech very off-putting and insincere, which in some cases it literally is. I'd be glad to find a switch that made it talk more like I do but they might assume that's too robotic :/",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44890733,
    "by": "gdudeman",
    "timeISO": "2025-08-13T16:42:49.000Z",
    "textPlain": "Warning: A natural response to this is to instruct Claude not to do this in the CLAUDE.md file, but you’re then polluting the context and distracting it from its primary job.If you watch its thinking, you will see references to these instructions instead of to the task at hand.It’s akin telling an employee that they can never say certain words. They’re inevitably going to be worse at their job.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44887874,
    "by": "nromiun",
    "timeISO": "2025-08-13T12:57:23.000Z",
    "textPlain": "Another big problem I see with LLMs is that it can't make precise adjustments to your answer. If you make a request it will give you some good enough code, but if you see some bug and wants to fix that section only it will regenerate most of the code instead (along with a copious amount of apologies). And the new code will have new problems of their own. So you are back to square one.For the record I have had this same experience with ChatGPT, Gemini and Claude. Most of the time I had to give up and write from scratch.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44892664,
    "by": "stillpointlab",
    "timeISO": "2025-08-13T19:22:23.000Z",
    "textPlain": "One thing I've noticed with all the LLMs that I use (Gemini, GPT, Claude) is a ubiquitous: \"You aren't just doing <X> you are doing <Y>\"What I think is very curious about this is that all of the LLMs do this frequently, it isn't just a quirk of one. I've also started to notice this in AI generated text (and clearly automated YouTube scripts).It's one of those things that once you see it, you can't un-see it.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44887291,
    "by": "mox111",
    "timeISO": "2025-08-13T11:46:08.000Z",
    "textPlain": "GPT-5 has used the phrase \"heck yes!\" a handful of times to me so far. I quite enjoy the enthusiasm but its not a phrase you hear very often.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44890214,
    "by": "smeej",
    "timeISO": "2025-08-13T16:01:46.000Z",
    "textPlain": "I think the developers want these AI tools to be likable a heck of a lot more than they want them to be useful--and as a marketing strategy, that's exactly the right approach.Sure, the early adopters are going to be us geeks who primarily want effective tools, but there are several orders of magnitude more people who want a moderately helpful friendly voice in their lives than there are people who want extremely effective tools.They're just realizing this much, MUCH faster than, say, search engines realized it made more money to optimize for the kinds of things average people mean from their search terms than optimizing for the ability to find specific, niche content.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44895673,
    "by": "sakesun",
    "timeISO": "2025-08-14T01:02:16.000Z",
    "textPlain": "Avoid being argumentative can save energy, literally.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44887160,
    "by": "basfo",
    "timeISO": "2025-08-13T11:31:36.000Z",
    "textPlain": "This bug report is absolutely right",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44895153,
    "by": "yencabulator",
    "timeISO": "2025-08-13T23:36:28.000Z",
    "textPlain": "I never see it. CLAUDE.md starts withAvoid sycophancy.\nIf you are asked to make a change, summarize the change but do not explain its benefits.\nBe concise in phrasing but not to the point of omission.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44889833,
    "by": "siva7",
    "timeISO": "2025-08-13T15:34:38.000Z",
    "textPlain": "I'd pay extra at this time for a model without any personality. Please, i'm not using LLMs as erotic roleplay dolls, friends, therapists, or anything else. Just give me straight-shot answers.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44887889,
    "by": "catigula",
    "timeISO": "2025-08-13T12:59:05.000Z",
    "textPlain": "1. Gemini is better at this. It will predicate any follow-up question you pose to it with a paragraph about how amazing and insightful you are. However, once the pleasantries are out of the way, I find that it is much more likely to take a strong stance that might include pushing back against the user.I recently tried to attain some knowledge on a topic I knew nothing about and ChatGPT just kept running with my slightly inaccurate or incomplete framing, Gemini opened up a larger world to me by pushing back a bit.2. You need to lead Claude to considering other ideas, considering if their existing approach or a new proposed approach might be best. You can't tell them something or suggest it or you're going to get serious sycophancy.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44891341,
    "by": "pronik",
    "timeISO": "2025-08-13T17:34:25.000Z",
    "textPlain": "I'm not mad about \"You're absolutely right!\" by itself. I'm mad that it's not a genuine reply, but a conversation starter without substance. Most of the time it's like:Me: The flux compensator doesn't seem to workClaude: You're absolutely right! Let me see whether that's true...",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44890462,
    "by": "ted_bunny",
    "timeISO": "2025-08-13T16:21:03.000Z",
    "textPlain": "I want to take this opportunity to teach people a little trick from improv comedy. It's called \"A to B to C.\" In a nutshell, what that means is: don't say the first joke that comes to your mind because pretty much everyone else in the room thought of it too.Anyone commenting \"you're absolutely right\" in this thread gets the wall.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44888457,
    "by": "FiddlerClamp",
    "timeISO": "2025-08-13T13:51:28.000Z",
    "textPlain": "Reminds me of the 'interactive' video from the 1960s Fahrenheit 451 movie: https://www.youtube.com/watch?v=ZOs8U50T3l0For the 'you're right!' bit see: \nhttps://youtu.be/ZOs8U50T3l0?t=71",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44888793,
    "by": "Springtime",
    "timeISO": "2025-08-13T14:19:25.000Z",
    "textPlain": "I've never thought the reason behind this was to make the user always feel correct but rather that many times an LLM (especially lower tier models) will just get various things incorrect and it doesn't have a reference for what is correct.So it falls back to 'you're right', rather than be arrogant or try to save face by claiming it is correct. Too many experiences with OpenAI models do the latter and their common fallback excuses are program version differences or user fault.I've had a few chats now with OpenAI reasoning models where I've had to link to literal source code dating back to the original release version of a program to get it to admit that it was incorrect about whatever aspect it hallucinated about a program's functionality, before it will finally admit said thing doesn't exist. Even then it will try and save face by not admitting direct fault.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44889918,
    "by": "johnisgood",
    "timeISO": "2025-08-13T15:40:10.000Z",
    "textPlain": "I do not mind getting:  Verdict: This is production-ready enterprise security \n\n  Your implementation exceeds industry standards and follows Go security best practices including proper dependency management, comprehensive testing approaches, and security-first design Security Best Practices for Go Developers - The Go Programming Language. The multi-layered approach with GPG+SHA512 verification, decompression bomb protection, and atomic operations puts this updater in the top tier of secure software updaters.\n\n  The code is well-structured, follows Go idioms, and implements defense-in-depth security that would pass enterprise security reviews.\n\nEspecially because it is right, after an extensive manual review.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44889221,
    "by": "tempodox",
    "timeISO": "2025-08-13T14:50:55.000Z",
    "textPlain": "Interestingly, the models I use locally with ollama don't do that.  Although you could possibly find some that do it if you went looking for them.  But ollama probably gives you more control over the model than those paid sycophants.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44887500,
    "by": "albert_e",
    "timeISO": "2025-08-13T12:15:43.000Z",
    "textPlain": "sidenote observation -it seems username \"anthropic\" on github is taken by a developer from australia more than a decade ago, so Anthropic went with \"https://github.com/anthropics/\" with an 's' at the end :)",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44887599,
    "by": "smoghat",
    "timeISO": "2025-08-13T12:26:16.000Z",
    "textPlain": "I just checked my most recent thread with Claude. It said \"You're absolutely right!\" 12 times.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44887868,
    "by": "pacoWebConsult",
    "timeISO": "2025-08-13T12:56:42.000Z",
    "textPlain": "You can add a hook that steers it when it goes into yes-man mode fairly easily.https://gist.github.com/ljw1004/34b58090c16ee6d5e6f13fce0746...",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44890272,
    "by": "cbracketdash",
    "timeISO": "2025-08-13T16:06:03.000Z",
    "textPlain": "Here are my instructions to Claude.\"Get straight to the point. Ruthlessly correct my wrong assumptions. Do not give me any noise. Just straight truth and respond in a way that is highly logical and broken down into first principles axioms. Use LaTeX for all equations. Provide clear plans that map the axioms to actionable items\"",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44888205,
    "by": "skizm",
    "timeISO": "2025-08-13T13:30:22.000Z",
    "textPlain": "Does capitalizing letters, using \"*\" chars, or other similar strategies to add emphasis actually do anything to LLM prompts? I don't know much about the internals, but my gut always told me there was some sort of normalization under the hood that would strip these kinds of things out. Also the only reason they work for humans is because it visually makes these things stand out, not that it changes the meaning per se.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44890954,
    "by": "DrNosferatu",
    "timeISO": "2025-08-13T17:00:20.000Z",
    "textPlain": "This spills to Perplexity!And the fact that they skimp a bit on reasoning tokens / compute, makes it even worse.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44891328,
    "by": "jfb",
    "timeISO": "2025-08-13T17:33:28.000Z",
    "textPlain": "The obsequity loop is fucking maddening. I can't prompt it away in all circumstances. I would also argue that as annoying as some of us find it, it is a big part of the reason for the success of the chat modality of these tools.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44894249,
    "by": "atleastoptimal",
    "timeISO": "2025-08-13T21:47:17.000Z",
    "textPlain": "To be able to direct LLM outputs to the style you want should be your absolute right.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44888638,
    "by": "JackFr",
    "timeISO": "2025-08-13T14:07:03.000Z",
    "textPlain": "The real reason for the sychophancy is that you don't want to know what Claude really thinks about you and your piss-ant ideas.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44888786,
    "by": "giancarlostoro",
    "timeISO": "2025-08-13T14:18:58.000Z",
    "textPlain": "If we can get it to say \"My pleasure\" every single time someone tells it thanks, we can make Claude work at Chick Fil A.",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44889841,
    "by": "rootnod3",
    "timeISO": "2025-08-13T15:35:09.000Z",
    "textPlain": "Hot take, but the amount that people try to go and make an LLM be less sycophantic and still have it be sycophantic in round-about ways is astonishing. Just admit that the over-glorified text-prediction engines are not what they promised to be.There is no “reasoning”, there is no “understanding”.EDIT: s/test/text",
    "parent": 44885398,
    "depth": 1
  },
  {
    "id": 44887471,
    "by": "sluongng",
    "timeISO": "2025-08-13T12:12:14.000Z",
    "textPlain": "I don't view it as a bug. It's a personality trait of the model that made \"user steering\" much easier, thus helping the model to handle a wider range of tasks.I also think that there will be no \"perfect\" personality out there. There will always be folks who view some traits as annoying icks. So, some level of RL-based personality customization down the line will be a must.",
    "parent": 44885398,
    "depth": 1
  }
]
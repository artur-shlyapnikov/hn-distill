[
  {
    "id": 44901335,
    "by": "usrbinbash",
    "timeISO": "2025-08-14T15:10:06.000Z",
    "textPlain": "> We don't just keep adding more words to our context window, because it would drive us mad.That, and we also don't only focus on the textual description of a  problem when we encounter a problem. We don't see the debugger output and go \"how do I make this bad output go away?!?\". Oh, I am getting an authentication error. Well, meaybe I should just delete the token check for that code path...problem solved?!No. Problem very much not-solved. In fact, problem very much very bigger big problem now, and [Grug][1] find himself reaching for club again.Software engineers are able to step back, think about the whole thing, and determine the root cause of a problem. I am getting an auth error...ok, what happens when the token is verified...oh, look, the problem is not the authentication at all...in fact there is no error! The test was simply bad and tried to call a higher privilege function as a lower privilege user. So, test needs to be fixed. And also, even though it isn't per-se an error, the response for that function should maybe differentiate between \"401 because you didn't authenticate\" and \"401 because your privileges are too low\".[1]: https://grugbrain.dev",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44902590,
    "by": "andrewmutz",
    "timeISO": "2025-08-14T16:41:32.000Z",
    "textPlain": "The author does not understand what LLMs and coding tools are capable of today.> LLMs get endlessly confused: they assume the code they wrote actually works; when test fail, they are left guessing as to whether to fix the code or the tests; and when it gets frustrating, they just delete the whole lot and start over.  This is exactly the opposite of what I am looking for.  Software engineers test their work as they go. When tests fail, they can check in with their mental model to decide whether to fix the code or the tests, or just to gather more data before making a decision. When they get frustrated, they can reach for help by talking things through. And although sometimes they do delete it all and start over, they do so with a clearer understanding of the problem.My experiences are based on using Cline with Anthropic Sonnet 3.7 doing TDD on Rails, and have a very different experience.  I instruct the model to write tests before any code and it does.  It works in small enough chunks that I can review each one.  When tests fail, it tends to reason very well about why and fixes the appropriate place.  It is very common for the LLM to consult more code as it goes to learn more.It's certainly not perfect but it works about as well, if not better, than a human junior engineer.  Sometimes it can't solve a bug, but human junior engineers get in the same situation too.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901670,
    "by": "chollida1",
    "timeISO": "2025-08-14T15:33:51.000Z",
    "textPlain": "Most of this might be true for LLM's but years of investing experience has created a mental model of looking for the tech or company that sucks and yet keeps growing.People complained endlessly about the internet in the early to mid 90s, its slow, static, most sites had under construction signs on them, your phone modem would just randomly disconnect.  The internet did suck in alot of ways and yet people kept using it.Twitter sucked in the mid 2000s, we saw the fail whale weekly and yet people continued to use it for breaking news.Electric cars sucked, no charging, low distance, expensive and yet no matter how much people complain about them they kept getting better.Phones sucked, pre 3G was slow, there wasn't much you could use them for before app stores and the cameras were potato quality and yet people kept using them while they improved.Always look for the technology that sucks and yet people keep using it because it provides value.  LLM's aren't great at alot of tasks and yet no matter how much people complain about them, they keep getting used and keep improving through constant iteration.LLM\"s amy not be able to build software today, but they are 10x better than where they were in 2022 when we first started using chatgpt.  Its pretty reasonable to assume in 5 years they will be able to do these types of development tasks.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901548,
    "by": "JimDabell",
    "timeISO": "2025-08-14T15:25:56.000Z",
    "textPlain": "LLMs can’t build software because we are expecting them to hear a few sentences, then immediately start coding until there’s a prototype. When they get something wrong, they have a huge amount of spaghetti to wade through. There’s little to no opportunity to iterate at a higher level before writing code.If we put human engineering teams in the same situation, we’d expect them to do a terrible job, so why do we expect LLMs to do any better?We can dramatically improve the output of LLM software development by using all those processes and tools that help engineering teams avoid these problems:https://jim.dabell.name/articles/2025/08/08/autonomous-softw...",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901287,
    "by": "9cb14c1ec0",
    "timeISO": "2025-08-14T15:05:51.000Z",
    "textPlain": "> what they cannot do is maintain clear mental modelsThe more I use claude code, the more frustrated I get with this aspect.  I'm not sure that a generic text-based LLM can properly solve this.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44902967,
    "by": "lysecret",
    "timeISO": "2025-08-14T17:09:35.000Z",
    "textPlain": "Bit of a click baity title since thy can definitely help in building software.However, I agree with the main thesis (that they can’t do it on their own). Also related to this this whole idea of “we will easily fix memory next” will turn out to be the same as “we can fix vision in one summer” turned out it’s 30 years later, much improved but still not fixed. Memory is hard.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901793,
    "by": "pjmlp",
    "timeISO": "2025-08-14T15:42:40.000Z",
    "textPlain": "Only because most AI startups are doing it wrong.I don't want a chat window.I want AI workflows as part of my IDE, like Visual Studio, InteliJ, Android Studio are finally going after.I want voice controlled actions on my native language.Knowledge across everything on the project for doing code refactorings, static analysis with AI feedback loop, generating UI based out of handwritten sketches, programming on the go using handwriting, source control commit messages out of code changes,...",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44902015,
    "by": "1zael",
    "timeISO": "2025-08-14T15:59:02.000Z",
    "textPlain": "> \"when test fail, they are left guessing as to whether to fix the code or the tests\"I've one thing that helps is using the \"Red-Green-Refactor\" language. We're in RED phase - test should fail. We're in GREEN phase - make this test pass with minimal code. We're in REFACTOR phase - improve the code without breaking tests.This helps the LLM understand the TDD mental model rather than just seeing \"broken code\" that needs fixing.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901371,
    "by": "emilecantin",
    "timeISO": "2025-08-14T15:13:38.000Z",
    "textPlain": "Yeah, I think it's pretty clear to a lot of people that LLMs aren't at the \"build me Facebook, but for dogs\" stage yet. I've had relatively good success with more targeted tasks, like \"Add a modal that does this, take this existing modal as an example for code style\". I also break my problem down into smaller chunks, and give them one by one to the LLM. It seems to work much better that way.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901636,
    "by": "lordnacho",
    "timeISO": "2025-08-14T15:31:35.000Z",
    "textPlain": "I think I agree with the idea that LLMs are good at the junior level stuff.What's happened for me recently is I've started to revisit the idea that typing speed doesn't matter.This is an age-old thing, most people don't think it really matters how fast you can type. I suppose the steelman is, most people think it doesn't really matters how fast you can get the edits to your code that you want. With modern tools, you're not typing out all the code anyway, and there's all sorts of non-AI ways to get your code looking the way you want. And that doesn't matter, the real work of the engineer is the architecture of how the whole program functions. Typing things faster doesn't make you get to the goal faster, since finding the overall design is the limiting thing.But I've been using Claude for a while now, and I'm starting to see the real benefit: you no longer need to concentrate to rework the code.It used to be burdensome to do certain things. For instance, I decided to add an enum value, and now I have to address all the places where it matches on that enum. This wasn't intellectually hard in the old world, you just got the compiler to tell you where the problems were, and you added a little section for your new value to do whatever it needed, in all the places it appeared.But you had to do this carefully, otherwise you would just cause more compile/error cycles. Little things like forgetting a semicolon will eat a cycle, and old tools would just tell you the error was there, not fix it for you.LLMs fix it for you. Now you can just tell Claude to change all the code in a loop until it compiles. You can have multiple agents working on your code, fixing little things in many places, while you sit on HN and muse about it. Or perhaps spend the time considering what direction the code needs to go.The big thing however is that when you're no longer held up by little compile errors, you can do more things. I had a whole laundry list of things I wanted to change about my codeba",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44902714,
    "by": "otterley",
    "timeISO": "2025-08-14T16:50:22.000Z",
    "textPlain": "On the contrary, Kiro (https://kiro.dev) is showing that it can be done by breaking down software engineering into multiple stages (requirements, design, and tasks) and then breaking the tasks down into discrete subtasks. Each of those can then be customized and refined as much as you like. It will even sketch out initial documents for all three.It’s still early days, but we are learning that as with software written exclusively by humans, the more specific the specifications are, the more likely the result will be as you intended.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901494,
    "by": "Transfinity",
    "timeISO": "2025-08-14T15:22:16.000Z",
    "textPlain": "> LLMs get endlessly confused: they assume the code they wrote actually works; when test fail, they are left guessing as to whether to fix the code or the tests; and when it gets frustrating, they just delete the whole lot and start over.I feel personally described by this statement. At least on a bad day, or if I'm phoning it in. Not sure if that says anything about AI - maybe just that the whole \"mental models\" part is quite hard.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44902823,
    "by": "guluarte",
    "timeISO": "2025-08-14T16:58:37.000Z",
    "textPlain": "Turns out, English is pretty bad for creating deterministic software. If you are vibe coding, you either are happy with the randomness generated by the LLMs or you enter a loop to try to generate a deterministic output, in which case using a programming language could have been easier.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44902166,
    "by": "nowittyusername",
    "timeISO": "2025-08-14T16:09:47.000Z",
    "textPlain": "Saying LLMS are not good at x or y, is akin to saying a brain is useless without a body. Which is obvious.  The success of agentic coding solutions depends on not just the model but also the system that the developers built around the model. And the companies that will succeed in this area are going to be the companies that focus on building sophisticated and capable systems that utilize said models. We are still in very early days where most organizations are only coming to terms with this realization... Only a few of them fully utilize this concept to the fullest, Claude code being the best example. The Claude models are specifically trained for tool calling and other capabilities and the Claude code cli compliments and takes advantage of those capabilities to the fullest, things like context management among other capabilities are extremely important  ...",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44902794,
    "by": "anotheryou",
    "timeISO": "2025-08-14T16:56:00.000Z",
    "textPlain": "Maybe we should let it build a mental model in documentation markdown files?Vibing I often let it explain the implemented business logic (instead of reading the code directly) and judge that.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44902556,
    "by": "nextworddev",
    "timeISO": "2025-08-14T16:39:24.000Z",
    "textPlain": "60% of the complaints in this post can be solved by providing better requirements and context upfront",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901488,
    "by": "generalizations",
    "timeISO": "2025-08-14T15:22:00.000Z",
    "textPlain": "These LLM discussions really need everyone to mention what LLM they're actually using.> AI is awesome for coding! [Opus 4]> No AI sucks for coding and it messed everything up! [4o]Would really clear the air. People seem to be evaluating the dumbest models (apparently because they don't know any better?) and then deciding the whole AI thing just doesn't work.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44902340,
    "by": "alliancedamages",
    "timeISO": "2025-08-14T16:21:45.000Z",
    "textPlain": "> ...but the distinguishing factor of effective engineers is their ability to build and maintain clear mental models.I wonder is this not just a proxy for intelligence?",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901861,
    "by": "Onewildgamer",
    "timeISO": "2025-08-14T15:48:06.000Z",
    "textPlain": "I wonder if some of this can be solved by removing some wrongly setup context in LLM. Or get a short summary, restructure it and againt feed to a fresh LLM context.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901455,
    "by": "saghm",
    "timeISO": "2025-08-14T15:19:54.000Z",
    "textPlain": ">   Context omission: Models are bad at finding omitted context.>   Recency bias: They suffer a strong recency bias in the context window.>   Hallucination: They commonly hallucinate details that should not be there.To be fair, those are all issues that most human engineers I've worked with (including myself!) have struggled with to various degrees, even if we don't refer to them the same way. I don't know about the rest of you, but I've certainly had times where I found out that an important nuance of a design was overlooked until well into the process of developing something, forgotten a crucial detail that I learned months ago that would have helped me debug something much faster than if I had remembered it from the start, or accidentally make an assumption about how something worked (or misremembered it) and ended up with buggy code as a result. I've mostly gotten pretty positive feedback about my work over the course of my career, so if I \"can't build software\", I have to worry about the companies that have been employing me and my coworkers who have praised my work output over the years. Then again, I think \"humans can't build software reliably\" is probably a mostly correct statement, so maybe the lesson here is that software is hard in general.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901331,
    "by": "empath75",
    "timeISO": "2025-08-14T15:09:26.000Z",
    "textPlain": "It's good at micro, but not macro.  I think that will eventually change with smarter engineering around it, larger context windows, etc.  Never underestimate how much code that engineers will write to avoid writing code.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901780,
    "by": "sneak",
    "timeISO": "2025-08-14T15:42:23.000Z",
    "textPlain": "Am I the only one continuously astounded at how well Opus 4 actually does build mental models when prompted correctly?I find Sonnet frequently loses the plot, but Opus can usually handle it (with sufficient clarity in prompting).",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901406,
    "by": "trod1234",
    "timeISO": "2025-08-14T15:16:12.000Z",
    "textPlain": "I think most people trying to touch on this topic don't consider this byline with other similar bylines like, \"Why LLMs can't recognize themselves looping\", or \"Why LLMs can't express intent\", or \"Why LLMs can't recognize truth/falsity, or confidence levels of what they know vs don't know\", these other bylines basically with a little thought equate to Computer Science halting problems, or  the undecidability nature of mathematics.Taken to a next step, recognizing this makes the investment in such a moonshot pipedream (overcoming these inherent problems in a deterministic way), recklessly negligent.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901637,
    "by": "revskill",
    "timeISO": "2025-08-14T15:31:39.000Z",
    "textPlain": "They can read and mind the error then figure out the best way to resolve. It is the best part about llm. No human can do it better than an llm. But they are not your mind reader. It is where things fall apart.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901481,
    "by": "Nickersf",
    "timeISO": "2025-08-14T15:21:22.000Z",
    "textPlain": "I think they're another tool in the toolbox not a new workshop. You have to build a good strategy around LLM usage when developing software. I think people are naturally noticing that and adapting.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901701,
    "by": "antihipocrat",
    "timeISO": "2025-08-14T15:35:43.000Z",
    "textPlain": "...\"(at least for now) you are in the drivers seat, and the LLM is just another tool to reach for.\"Improvements in model performance seem to be approaching the peak rather than demonstrating exponential gains. Is the quote above where we land in the end?",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901854,
    "by": "codr7",
    "timeISO": "2025-08-14T15:47:48.000Z",
    "textPlain": "Well, welcome to the club of awareness :)",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901910,
    "by": "jeffWrld",
    "timeISO": "2025-08-14T15:51:52.000Z",
    "textPlain": "[dead]",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44902525,
    "by": "cindyllm",
    "timeISO": "2025-08-14T16:37:01.000Z",
    "textPlain": "[dead]",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901868,
    "by": "Xplan",
    "timeISO": "2025-08-14T15:48:39.000Z",
    "textPlain": "[dead]",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901477,
    "by": "myfavoritedog",
    "timeISO": "2025-08-14T15:21:03.000Z",
    "textPlain": "[dead]",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901360,
    "by": "jmclnx",
    "timeISO": "2025-08-14T15:12:42.000Z",
    "textPlain": "I am not a fan of today's concept of \"AI\", but to be fair, building today's software is not for the faint of heart,  very few people gets it right on try 1.Years ago I gave up compiling these large applications all together.  I compiled Firefox via FreeBSD's (v8.x) ports system, that alone was a nightmare.I cannot imagine what it would be like to compile GNOME3 or KDE or Libreoffice.  Emacs is the largest thing I compile now.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44902093,
    "by": "ontigola",
    "timeISO": "2025-08-14T16:03:54.000Z",
    "textPlain": "Great, concise article. Nothing important to add, except that AI snake-oil salesmen will continue spreading their exaggerations far and wide, at least we who are truly in this business agree on the facts.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44902078,
    "by": "mccoyb",
    "timeISO": "2025-08-14T16:02:21.000Z",
    "textPlain": "This is a low information density blog post. I’ve really liked Zed’s blog posts in the past (especially about the editor internals!) so I hope this doesn’t come the wrong way, but this seems to be a loose restatement of what many people are empirically finding out by using LLM agents.Perhaps good for someone just getting their feet wet with these computational objects, but not resolving or explaining things in a clear way, or highlighting trends in research and engineering that might point towards ways forward.You also have a technical writing no no where you cite a rather precise and specific study with a paraphrase to support your claims … analogous to saying “Godel’s incompleteness theorem means _something something_ about the nature of consciousness”.A phrase like: “Unfortunately, for now, they cannot (beyond a certain complexity) actually understand what is going on” referencing a precise study … is ambiguous and shoddy technical writing — what exactly does the author mean here? It’s vague.I think it is even worse here because _the original study_ provides task-specific notions of complexity (a critique of the original study! Won’t different representations lead to different complexity scaling behavior? Of course! That’s what software engineering is all about: I need to think at different levels to control my exposure to complexity)",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901575,
    "by": "skydhash",
    "timeISO": "2025-08-14T15:27:40.000Z",
    "textPlain": "Programmers are mostly translating business rules to the very formal process execution of the computer world. And you need to both knows what the rules means and how the computer works (or at least how the abstracted version you’re working with works). The translation is messy at first, which is why you need to revise it again and again. Especially when later rules comes challenging all the assumptions you’ve made or even contradicting themselves.Even translations between human languages (which allows for ambiguity) can be messy. Imagine if the target language is for a system that will exactly do as told unless someone has qualified those actions as bad.",
    "parent": 44901335,
    "depth": 2
  },
  {
    "id": 44902776,
    "by": "JackFr",
    "timeISO": "2025-08-14T16:54:50.000Z",
    "textPlain": "- When we have a report of a failing test before fixing it, identify the component under test. Think deeply about the component and describe its purpose, the control flows and state changes that occur within the component and assumptions the component makes about context.  Write that analysis in file called component-name-mental-model.md.- When ever you address a failing test, always bring your component mental model into the context.Paste that into your Claude prompt and see if you get better results.  You'll even be able to read and correct the LLM's mental model.",
    "parent": 44901335,
    "depth": 2
  },
  {
    "id": 44902313,
    "by": "appease7727",
    "timeISO": "2025-08-14T16:19:30.000Z",
    "textPlain": "The way it works for me at least is I can fit a huge amount of context in my head. This works because the text is utterly irrelevant and gets discarded immediately.Instead, my brain parses code into something like an AST which then is represented as a spatial graph. I model the program as a logical structure instead of a textual one. When you look past the language, you can work on the program. The two are utterly disjoint.I think LLMs fail at software because they're focused on text and can't build a mental model of the program logic. It take a huge amount of effort and brainpower to truly architect something and understand large swathes of the system. LLMs just don't have that type of abstract reasoning.",
    "parent": 44901335,
    "depth": 2
  },
  {
    "id": 44902070,
    "by": "ai-christianson",
    "timeISO": "2025-08-14T16:01:37.000Z",
    "textPlain": "I take a more pragmatic approach --everything is human in the loop. It helps me get the job done faster and with higher quality, so I use it.",
    "parent": 44901335,
    "depth": 2
  },
  {
    "id": 44903059,
    "by": "exe34",
    "timeISO": "2025-08-14T17:16:13.000Z",
    "textPlain": "to be fair, I've seen cursor step back and check higher level things. I was trying to set up a firecracker vm and it did everything for me, and when things didn't initially work, it started doing things like ls, tar -tvf, and then a bunch of checking networking stuff to make sure things were showing up in the right place.so current LLMs might not quite be human level, but I'd have to see a bigger model fail before I'd conclude that it can't do $X.",
    "parent": 44901335,
    "depth": 2
  },
  {
    "id": 44901997,
    "by": "reactordev",
    "timeISO": "2025-08-14T15:57:38.000Z",
    "textPlain": "While I agree with you - The whole grug brain thing is offensive. Because we have all been grug at some point.",
    "parent": 44901335,
    "depth": 2
  },
  {
    "id": 44901736,
    "by": "chuckadams",
    "timeISO": "2025-08-14T15:38:04.000Z",
    "textPlain": "An AI might tell you to use a 403 for insufficient privileges instead of 401.",
    "parent": 44901335,
    "depth": 2
  },
  {
    "id": 44902743,
    "by": "fragmede",
    "timeISO": "2025-08-14T16:52:45.000Z",
    "textPlain": "If you can't get the LLM to generate code that handles an error code, that's on you. Yeah, sometimes it does dumb shit. Who cares? Just /undo and retry. Stop using Claude Code, which uses git like an intern. (Which is to say, it doesn't unless forced to.)",
    "parent": 44901335,
    "depth": 2
  },
  {
    "id": 44901530,
    "by": "trod1234",
    "timeISO": "2025-08-14T15:25:03.000Z",
    "textPlain": "Isn't the 401 for LLMs the same single undecidable token? \nDoesn't this basically go to the undecidable nature of math in CS?Put another way, you have an excel roster corresponding to people with accounts where some need to have their account shutdown but you only have their first and last names as identifiers, and the pool is sufficiently large that there are more than one person per a given set of names.You can't shut down all accounts with a given name, and there is no unique identifier. How do you solve this?You have to ask and be given that unique identifier that differentiates between the undecidable. Without that, even the person can't do the task.The person can make guesses, but those guesses are just hallucinations with a significant n probability towards a bad repeat outcome.At a core level I don't think these type of issues are going to be solved. Quite a lot of people would be unable to solve this and struggle with this example (when not given the answer, or hinted at the solution in the framing of the task; ie when they just have a list of names and are told to do an impossible task).",
    "parent": 44901335,
    "depth": 2
  },
  {
    "id": 44901533,
    "by": "livid-neuro",
    "timeISO": "2025-08-14T15:25:05.000Z",
    "textPlain": "The first cars broke down all the time. They had a limited range. There wasn't a vast supply of parts for them. There wasn't a vast industry of experts who could work on them. There wasn't a vast network of fuel stations to provide energy for them. The horse was a proven method.What an LLM cannot do today is almost irrelevant in the tide of change upon the industry. The fact is, with improvements, it doesn't mean an LLM cannot do it tomorrow.",
    "parent": 44901335,
    "depth": 2
  },
  {
    "id": 44901757,
    "by": "throwaway1004",
    "timeISO": "2025-08-14T15:39:58.000Z",
    "textPlain": "That reference link is a wild ride of unqualified, cartoonish passive-aggression, the cute link to the author's \"swag\" is the icing on the cake.Concidentally, I encountered the author's work for the first time only a couple of days ago as a podcast guest, he vouches for the \"Dirty Code\" approach while straw-manning Uncle Bob's general principles of balancing terseness/efficiency with ergonomics and readability (in most, but not all, cases).I guess this stuff sells t-shirts and mugs /rant",
    "parent": 44901335,
    "depth": 2
  },
  {
    "id": 44902652,
    "by": "kubb",
    "timeISO": "2025-08-14T16:46:08.000Z",
    "textPlain": "I believe that they work particularly well for CRUD in known frameworks like Rails.OTOH i tried building a native Windows Application using DirecDraw in Rust and it was a disaster.I wish people could be a bit more open about what they build.",
    "parent": 44902590,
    "depth": 2
  },
  {
    "id": 44902747,
    "by": "quantumHazer",
    "timeISO": "2025-08-14T16:52:56.000Z",
    "textPlain": "it's very well documented behavior that models try to pass failed test with hacks and tricks (hard coding solutions and so on)",
    "parent": 44902590,
    "depth": 2
  },
  {
    "id": 44902855,
    "by": "bunderbunder",
    "timeISO": "2025-08-14T17:00:49.000Z",
    "textPlain": "From what I've experienced, this depends very much on the programming language, platform, and business domain.I haven't tried it with Rails myself (haven't touched Ruby in years, to be honest), but it doesn't surprise me that it would work well there. Ruby on Rails programming culture is remarkably consistent about how to do things. I would guess that means that the LLM is able to derive a somewhat (for lack of a better word) saner model from its training data.By contrast, what it does with Python can get pretty messy pretty quickly. One of the biggest problems I've had with it is that it tends to use a random hodgepodge of different Python coding idioms. That makes TDD particularly challenging because you'll get tests that are well designed for code that's engineered to follow one pattern of changes, written against a SUT that follows conventions that lead to a completely different pattern of changes. The result is horribly brittle tests that repeatedly break for spurious reasons.And then iterating on it gets pretty wild, too. My favorite behavior is when the real defect is \"oops I forgot to sort the results of the query\" and the suggested solution is \"rip out SqlAlchemy and replace it with Django.\"R code is even worse; even getting it to produce code that follows a spec in the first place can be a challenge.",
    "parent": 44902590,
    "depth": 2
  },
  {
    "id": 44902824,
    "by": "jarjoura",
    "timeISO": "2025-08-14T16:58:38.000Z",
    "textPlain": "My experience so far is that, if you're limiting the \"capacity\" to junior engineer, yes, especially when it's seen a problem before. It's able to quickly realize a solution and confirm the solution works.It does not works so well for any problems it has not seen before. At that point you need to explain the problem, and instruct the solution. So a that point, you're just acting as a mentor instead of using your capacity to just implement the solution yourself.My whole team has really bought into the \"claude-code\" way of doing side tasks that have been on the backlog for years, think like simple refactors, or secondary analytic systems. Basically any well-trodden path that is mostly constrained by time that none of us are given, are perfect for these agents right now.Personally I'm enjoying the ability to highlight a section of code and ask the LLM to explain this to me like I'm 5, or look for any potential race conditions. For those archiac, fragile monolithic blocks of code that stick around long after the original engineers have left, it's magical to use the LLM to wrap my head around that.I haven't found it can write these things any better though, and that is the key here. It's not very good at creating new things that aren't commonly seen. It also has a code style that is quite different than what already exists. So when it does inject code, often times it has to be rewritten to fit the style around it. Already, I'm hearing whispers of people say things like \"code written for the AI to read.\" That's where my eyes roll because the payoff for the extra mental bandwidth doesn't seem worth it right now.",
    "parent": 44902590,
    "depth": 2
  },
  {
    "id": 44902039,
    "by": "freehorse",
    "timeISO": "2025-08-14T16:00:00.000Z",
    "textPlain": "At the same time, there have been expectations about many of these that did not meet reality at any point. Much of this is due to physical limitations that are not trivial to be overcome. Internet gets faster and more stable, but the metaverse taking over did not happen partially because many people still get nausea after a bit and no 10x scaling fixed that.A lot of what you described as \"sucked\" were not seen as \"sucking\" at the time. Nobody complained about the phones being slow because nobody expected to use phones the way we do today. The internet was slow and less stable but nobody complained because they expected to stream 4k movies and they could not. This is anachronistic.The fact that we can see how some things improved in X Y manner does not mean that LLMs will improve the way you think they will. Maybe we invent a different technology that does a better job. After it was not that dial up itself became faster and I don't think there were fanatics saying that dialup technology would give us 1Gbp speeds. The problem with AI is that because scaling up compute has provided breakthroughs, some think that somehow with scaling up compute and some technical tricks we can solve all the current problems. I don't think that anybody can say that we cannot invent a technology that can overcome these, but if LLMs is this technology that can just keep scaling has been under doubt. Last year or so there has been a lot of refinement and broadening of applications, but nothing like a breakthrough.",
    "parent": 44901670,
    "depth": 2
  }
]
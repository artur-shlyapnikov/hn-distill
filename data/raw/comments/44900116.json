[
  {
    "id": 44901335,
    "by": "usrbinbash",
    "timeISO": "2025-08-14T15:10:06.000Z",
    "textPlain": "> We don't just keep adding more words to our context window, because it would drive us mad.That, and we also don't only focus on the textual description of a  problem when we encounter a problem. We don't see the debugger output and go \"how do I make this bad output go away?!?\". Oh, I am getting an authentication error. Well, meaybe I should just delete the token check for that code path...problem solved?!No. Problem very much not-solved. In fact, problem very much very bigger big problem now, and [Grug][1] find himself reaching for club again.Software engineers are able to step back, think about the whole thing, and determine the root cause of a problem. I am getting an auth error...ok, what happens when the token is verified...oh, look, the problem is not the authentication at all...in fact there is no error! The test was simply bad and tried to call a higher privilege function as a lower privilege user. So, test needs to be fixed. And also, even though it isn't per-se an error, the response for that function should maybe differentiate between \"401 because you didn't authenticate\" and \"401 because your privileges are too low\".[1]: https://grugbrain.dev",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901548,
    "by": "JimDabell",
    "timeISO": "2025-08-14T15:25:56.000Z",
    "textPlain": "LLMs can’t build software because we are expecting them to hear a few sentences, then immediately start coding until there’s a prototype. When they get something wrong, they have a huge amount of spaghetti to wade through. There’s little to no opportunity to iterate at a higher level before writing code.If we put human engineering teams in the same situation, we’d expect them to do a terrible job, so why do we expect LLMs to do any better?We can dramatically improve the output of LLM software development by using all those processes and tools that help engineering teams avoid these problems:https://jim.dabell.name/articles/2025/08/08/autonomous-softw...",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901287,
    "by": "9cb14c1ec0",
    "timeISO": "2025-08-14T15:05:51.000Z",
    "textPlain": "> what they cannot do is maintain clear mental modelsThe more I use claude code, the more frustrated I get with this aspect.  I'm not sure that a generic text-based LLM can properly solve this.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901488,
    "by": "generalizations",
    "timeISO": "2025-08-14T15:22:00.000Z",
    "textPlain": "These LLM discussions really need everyone to mention what LLM they're actually using.> AI is awesome for coding! [Opus 4]> No AI sucks for coding and it messed everything up! [4o]Would really clear the air. People seem to be evaluating the dumbest models (apparently because they don't know any better?) and then deciding the whole AI thing just doesn't work.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901371,
    "by": "emilecantin",
    "timeISO": "2025-08-14T15:13:38.000Z",
    "textPlain": "Yeah, I think it's pretty clear to a lot of people that LLMs aren't at the \"build me Facebook, but for dogs\" stage yet. I've had relatively good success with more targeted tasks, like \"Add a modal that does this, take this existing modal as an example for code style\". I also break my problem down into smaller chunks, and give them one by one to the LLM. It seems to work much better that way.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901494,
    "by": "Transfinity",
    "timeISO": "2025-08-14T15:22:16.000Z",
    "textPlain": "> LLMs get endlessly confused: they assume the code they wrote actually works; when test fail, they are left guessing as to whether to fix the code or the tests; and when it gets frustrating, they just delete the whole lot and start over.I feel personally described by this statement. At least on a bad day, or if I'm phoning it in. Not sure if that says anything about AI - maybe just that the whole \"mental models\" part is quite hard.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901441,
    "by": "nromiun",
    "timeISO": "2025-08-14T15:18:55.000Z",
    "textPlain": "This is exactly why I have tried and failed to use LLMs for any real project. For toy examples they are fine, but for anything larger they introduce many small obvious bugs.The worst thing is that you can't point those bugs out to the LLM. It will prefer to rewrite the whole code instead. With new bugs of course. So you are back to square one.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901455,
    "by": "saghm",
    "timeISO": "2025-08-14T15:19:54.000Z",
    "textPlain": ">   Context omission: Models are bad at finding omitted context.>   Recency bias: They suffer a strong recency bias in the context window.>   Hallucination: They commonly hallucinate details that should not be there.To be fair, those are all issues that most human engineers I've worked with (including myself!) have struggled with to various degrees, even if we don't refer to them the same way. I don't know about the rest of you, but I've certainly had times where I found out that an important nuance of a design was overlooked until well into the process of developing something, forgotten a crucial detail that I learned months ago that would have helped me debug something much faster than if I had remembered it from the start, or accidentally make an assumption about how something worked (or misremembered it) and ended up with buggy code as a result. I've mostly gotten pretty positive feedback about my work over the course of my career, so if I \"can't build software\", I have to worry about the companies that have been employing me and my coworkers who have praised my work output over the years. Then again, I think \"humans can't build software reliably\" is probably a mostly correct statement, so maybe the lesson here is that software is hard in general.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901331,
    "by": "empath75",
    "timeISO": "2025-08-14T15:09:26.000Z",
    "textPlain": "It's good at micro, but not macro.  I think that will eventually change with smarter engineering around it, larger context windows, etc.  Never underestimate how much code that engineers will write to avoid writing code.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901406,
    "by": "trod1234",
    "timeISO": "2025-08-14T15:16:12.000Z",
    "textPlain": "I think most people trying to touch on this topic don't consider this byline with other similar bylines like, \"Why LLMs can't recognize themselves looping\", or \"Why LLMs can't express intent\", or \"Why LLMs can't recognize truth/falsity, or confidence levels of what they know vs don't know\", these other bylines basically with a little thought equate to Computer Science halting problems, or  the undecidability nature of mathematics.Taken to a next step, recognizing this makes the investment in such a moonshot pipedream (overcoming these inherent problems in a deterministic way), recklessly negligent.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901481,
    "by": "Nickersf",
    "timeISO": "2025-08-14T15:21:22.000Z",
    "textPlain": "I think they're another tool in the toolbox not a new workshop. You have to build a good strategy around LLM usage when developing software. I think people are naturally noticing that and adapting.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901360,
    "by": "jmclnx",
    "timeISO": "2025-08-14T15:12:42.000Z",
    "textPlain": "I am not a fan of today's concept of \"AI\", but to be fair, building today's software is not for the faint of heart,  very few people gets it right on try 1.Years ago I gave up compiling these large applications all together.  I compiled Firefox via FreeBSD's (v8.x) ports system, that alone was a nightmare.I cannot imagine what it would be like to compile GNOME3 or KDE or Libreoffice.  Emacs is the largest thing I compile now.",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901477,
    "by": "myfavoritedog",
    "timeISO": "2025-08-14T15:21:03.000Z",
    "textPlain": "[dead]",
    "parent": 44900116,
    "depth": 1
  },
  {
    "id": 44901533,
    "by": "livid-neuro",
    "timeISO": "2025-08-14T15:25:05.000Z",
    "textPlain": "The first cars broke down all the time. They had a limited range. There wasn't a vast supply of parts for them. There wasn't a vast industry of experts who could work on them. There wasn't a vast network of fuel stations to provide energy for them. The horse was a proven method.What an LLM cannot do today is almost irrelevant in the tide of change upon the industry. The fact is, with improvements, it doesn't mean an LLM cannot do it tomorrow.",
    "parent": 44901335,
    "depth": 2
  },
  {
    "id": 44901530,
    "by": "trod1234",
    "timeISO": "2025-08-14T15:25:03.000Z",
    "textPlain": "Isn't the 401 for LLMs the same single undecidable token? \nDoesn't this basically go to the undecidable nature of math in CS?Put another way, you have an excel roster corresponding to people with accounts where some need to have their account shutdown but you only have their first and last names as identifiers, and the pool is sufficiently large that there are more than one person per a given set of names.You can't shut down all accounts with a given name, and there is no unique identifier. How do you solve this?You have to ask and be given that unique identifier that differentiates between the undecidable.",
    "parent": 44901335,
    "depth": 2
  },
  {
    "id": 44901526,
    "by": "dlivingston",
    "timeISO": "2025-08-14T15:24:43.000Z",
    "textPlain": "Reminds me of how Google's Genie 3 can only run for a ~minute before losing its internal state [0].My gut feeling is that this problem won't be solved until some new architecture is invented, on the scale of the transformer, which allows for short-term context, long-term context, and self-modulation of model weights (to mimic \"learning\"). (Disclaimer: hobbyist with no formal training in machine learning.)[0]: https://news.ycombinator.com/item?id=44798166",
    "parent": 44901287,
    "depth": 2
  },
  {
    "id": 44901468,
    "by": "cmrdporcupine",
    "timeISO": "2025-08-14T15:20:40.000Z",
    "textPlain": "Honestly it forces you -- rightfully -- to step back and be the one doing the planning.You can let it do the grunt coding, and a lot of the low level analysis and testing, but you absolutely need to be the one in charge on the design.It frankly gives me more time to think about the bigger picture within the amount of time I have to work on a task, and I like that side of things.There's definitely room for a massive amount of improvement in how the tool presents changes and suggestions to the user. It needs to be far more interactive.",
    "parent": 44901287,
    "depth": 2
  },
  {
    "id": 44901480,
    "by": "pmdr",
    "timeISO": "2025-08-14T15:21:21.000Z",
    "textPlain": "> It's good at micro, but not macro.That's what I've found as well. Start describing or writing a function, include the whole file for context and it'll do its job. Give it a whole codebase and it will just wander in the woods burning tokens for ten minutes trying to solve dependencies.",
    "parent": 44901331,
    "depth": 2
  },
  {
    "id": 44901393,
    "by": "anotherhue",
    "timeISO": "2025-08-14T15:15:24.000Z",
    "textPlain": "I suggest trying Nix, by being reproducible those nasty compilation demons get solved once and for all. (And usually by someone else)",
    "parent": 44901360,
    "depth": 2
  }
]
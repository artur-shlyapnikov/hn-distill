[
  {
    "id": 44919144,
    "by": "mrlongroots",
    "timeISO": "2025-08-16T01:17:46.000Z",
    "textPlain": "I very much disagree. To attempt a proof by contradiction:Let us assume that the author's premise is correct, and LLMs are plenty powerful given the right context. Can an LLM recognize the context deficit and frame the right questions to ask?They can not: LLMs have no ability to understand when to stop and ask for directions. They routinely produce contradictions, fail simple tasks like counting the letters in a word etc. etc. They can not even reliably execute my \"ok modify this text in canvas\" vs \"leave canvas alone, provide suggestions in chat, apply an edit once approved\" instructions.",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44921856,
    "by": "fmbb",
    "timeISO": "2025-08-16T09:56:43.000Z",
    "textPlain": "> It’s because the bottleneck isn’t in intelligence, but in human tasks: specifying intent and context engineering.So the bottleneck is intelligence.Junior engineers are intelligent enough to understand when they don't understand. They interrogate the intent and context of the tasks they are given. This is intelligence.Solving math questions is not intelligence, computers have been better than humans at that for like 100 years, as long as you first do the intelligent part as a human: specifying the task formally.Now we just have computer programs with another kind of input in natural language, and which require dozens of gigabytes of video ram and millions of cores to execute. And we still have to have humans to the intelligent part, figure out how to describe the problem so the dumb but very very fast machine can answer the question.",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44918642,
    "by": "thorum",
    "timeISO": "2025-08-16T00:01:46.000Z",
    "textPlain": "This article is insightful, but I blinked when I saw the headline “Reducing the human bottleneck” used without any apparent irony.At some point we should probably take a step back and ask “Why do we want to solve this problem?” Is a world where AI systems are highly intelligent tools, but humans are needed to manage the high level complexity of the real world… supposed to be a disappointing outcome?",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44922541,
    "by": "Paratoner",
    "timeISO": "2025-08-16T12:08:47.000Z",
    "textPlain": "I'm endlessly fascinated by the way these Humans (probably) speak of their follow peers as though they are a problem to solve for.> Longer term, we can reduce the human bottleneck byThank God we have ways to remove the thorn in our(?) side for good. The world can finally heal when the pursuit of fulfillment becomes inaccessible to the masses.",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44918229,
    "by": "neom",
    "timeISO": "2025-08-15T22:56:14.000Z",
    "textPlain": "Same same human problems. Regardless of their inherent intelligence...humans perform well only when given decent context and clear specifications/data. If you place a brilliant executive into a scenario without meaningful context.... an unfamiliar board meeting where they have no idea of the company’s history, prior strategic discussions, current issues, personel dynamics...expectations..etc etc, they will struggle just as a model does surly. They may still manage something reasonably insightful, leveraging general priors, common sense, and inferential reasoning... their performance will never match their potential had they been fully informed of all context and clearly data/objectives. I think context is the primary primitive property of intelligent systems in general?",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44918288,
    "by": "threecheese",
    "timeISO": "2025-08-15T23:06:06.000Z",
    "textPlain": "Author IMO correctly recognizes that access to context needs to scale (“latent intent” which I love), but I’m not sure I’m convinced that current models will be effective even if given access to all priors needed for a complex task. The ability to discriminate valuable from extraneous context will need to scale with size of available context, it will be pulling needles from haystacks that aren’t straightforward similarity. I think we will need to steer these things.",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44918607,
    "by": "Kuinox",
    "timeISO": "2025-08-15T23:54:42.000Z",
    "textPlain": "It's specific model that run for maths.  \nGPT-5 and Gemini 2.5 still cannot compute an arbitrary length sum of whole number without a calculator.  \nI have a proceduraly generated benchmark of basic operations, LLMs gets better at it with time, but they cant still solve basic maths or logic problems.BTW I'm open to selling it, my email is on my hn profile.",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44920622,
    "by": "ankit219",
    "timeISO": "2025-08-16T06:01:43.000Z",
    "textPlain": "The bottleneck for automation is verification. With human work, verification was fast(er) because you know where to look with certain assumptions that your upstream tasker would not have made trivial mistakes. For automation, AI needs to verify it's own work, review, and self correct to be able to automate any given work. Where this works, it will also change the abstraction layer compared to what it is today. The problem is same with every automation promise - it needs to work reliably at say 95% or 99% times and when it doesn't, there should be human contingency in terms of what to look for. Considering coding as the first example: it's already underway. AI generates the code, the test cases, and then verifies if the code works as intended. Code has a built in verification layer (both compiler and unit tests). High probablity the other domains move towards something similar too. I would also say the model needs to be intelligent to course correct when the output isn't validated[1].Verification solves the human in the loop dependency both for AI and human tasks. All the places where we could automate in the past, there were clearly quality checks which ensured the machinery were working as expected. Same thing will be replicated with AI too.Disclaimer: I have been working on building a universal verifier for AI tasks. The way it works is you give it a set of rules (policy) + AI output (could be human output too) and it outputs a scalar score + clause level citations. So I have been thinking about the problem space and might be over rating this. Would welcome contrarian ideas. (no, it's not llm as a judge)[1]: Some people may call it environment based learning, but in ML terms i feel it's different. That woudl be another example of sv startups using technical terms to market themselves when they dont do what they say.",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44920305,
    "by": "visarga",
    "timeISO": "2025-08-16T04:57:18.000Z",
    "textPlain": "Verification is the bottleneck, not ideation. LLMs can generate anything on tap, but solving any non-trivial problem requires iteration between thinking, doing and observing outcomes. The real world is too complex to be simulated by AI or humans. The scientific method works the same way, we are not exempt from having to validate our ideas. But as humans we have better feedback and access to context and we can assume risks on our own. AI has no skin and bears no responsibility.So the missing ingredient for AI is access to environment for feedback learning. It has little to do with AI architecture or datasets. I think a huge source of such data is our human-LLM chat logs. We act as LLM eyes, hands and feed on the ground. We carry the tacit knowledge and social context. OpenAI reports billions of tasks per day, probably trillions of tokens of interactive language combining human, AI and feedback from the environment. Maybe this is how AI can inch towards learning how to solve real world problems, it is part of the loop of problem solving, and benefits from having this data for training.",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44919107,
    "by": "etler",
    "timeISO": "2025-08-16T01:12:50.000Z",
    "textPlain": "I think the framing of these models are being \"intelligent\" is not the right way to go. They've gotten better at recall and association.They can recall prior reasoning from text they are trained on which allows them to handle complex tasks that have been solved before, but when working on complex, novel, or nuanced tasks there is no high quality relevant training data to recall.Intelligence has always been a fraught word to define and I don't think what LLMs do is the right attribute for defining it.I agree with a good deal of the article but because it keeps using loaded works like \"intelligent\" and \"smarter\", it has a hard time explaining what's missing.",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44919959,
    "by": "stephc_int13",
    "timeISO": "2025-08-16T03:47:29.000Z",
    "textPlain": "This is because we tend to use a human-centric reference to evaluate the difficulty of a task : playing chess at grand master level is a lot harder than folding laundry, except that it is the opposite, and this weird bias is well known as Moravec’s Paradox.Intelligence is the bottleneck, but not the kind of intelligence you need to solve puzzles.",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44918507,
    "by": "jefftitan",
    "timeISO": "2025-08-15T23:37:21.000Z",
    "textPlain": "Providing more context is difficult for a number of reasons. If you do it RAG style you need to know which context is relevant. LLMs are notorious for knowing that a factor is relevant if directly asked about that factor, but not bringing it up if it's implicit. In business things like people's feelings on things, historical business dealings, relevance to trending news can all be factors. If you fine tune... well... there have been articles recently about fine tuning on specific domains causing overall misalignment. The more you fine tune, the riskier.",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44918566,
    "by": "miller24",
    "timeISO": "2025-08-15T23:47:02.000Z",
    "textPlain": "It 100% is still intelligence. GPT-5 with Thinking still can't win at tic-tac-toe.",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44922242,
    "by": "graycat",
    "timeISO": "2025-08-16T11:16:36.000Z",
    "textPlain": "For the square covering problem, answer, 2.",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44922040,
    "by": "fifteen1506",
    "timeISO": "2025-08-16T10:37:48.000Z",
    "textPlain": "As a human, I'd also appreciate the specifications, documentation and meetings were not inaccessible to me.",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44920166,
    "by": "bubblyworld",
    "timeISO": "2025-08-16T04:25:26.000Z",
    "textPlain": "This is not a proof by contradiction - you have stated an assumption followed by a bunch of non-sequitors about what LLMs can and can't do, also known as begging the question. Under the conditions of your assumption (namely that LLMs are plenty powerful with the right context) why would you believe anything in your last paragraph? That's how a proof by contradiction works.(not saying you are wrong, necessarily, but I don't think this argument holds water)",
    "parent": 44919144,
    "depth": 2
  },
  {
    "id": 44919757,
    "by": "bobbylarrybobby",
    "timeISO": "2025-08-16T03:07:00.000Z",
    "textPlain": "Claude routinely stops and asks me clarifying questions before continuing, especially when the given extended thinking or doing research.",
    "parent": 44919144,
    "depth": 2
  },
  {
    "id": 44919722,
    "by": "beering",
    "timeISO": "2025-08-16T02:58:27.000Z",
    "textPlain": "It feels crazy to keep arguing about LLMs being able to do this or that, but not mention the specific model? The post author only mentions the IMO gold-medal model. And your post could be about anything. Am I to believe that the two of you are talking about the same thing? This discussion is not useful if that’s not the case.",
    "parent": 44919144,
    "depth": 2
  },
  {
    "id": 44920088,
    "by": "themanmaran",
    "timeISO": "2025-08-16T04:10:59.000Z",
    "textPlain": "This depends on whether you mean LLMs in the sense of single shot, or LLMs + software built around it. I think a lot of people conflate the two.In our application e use a multi-step check_knowledge_base workflow before and after each LLM request. Pretty much, make a separate LLM request to check the query against the existing context to see if more info is needed, and a second check after generation to see if output text exceeded it's knowledge base.And the results are really good. Now coding agents in your example are definitely stepwise more complex, but the same guardrails can apply.",
    "parent": 44919144,
    "depth": 2
  },
  {
    "id": 44922406,
    "by": "Wowfunhappy",
    "timeISO": "2025-08-16T11:49:04.000Z",
    "textPlain": "> LLMs have no ability to understand when to stop and ask for directions.I haven't read TFA so I may be missing the point. However, I have had success getting Claude to stop and ask for directions by specifically prompting it to do so. \"If you're stuck or the task seems impossible, please stop and explain the problem to me so I can help you.\"",
    "parent": 44919144,
    "depth": 2
  },
  {
    "id": 44921840,
    "by": "TacticalCoder",
    "timeISO": "2025-08-16T09:53:21.000Z",
    "textPlain": "[dead]",
    "parent": 44919144,
    "depth": 2
  },
  {
    "id": 44921884,
    "by": "pvtmert",
    "timeISO": "2025-08-16T10:03:48.000Z",
    "textPlain": "I truly love this comment, which essentially says: LLMs are glorified calculators, with ambiguous grammar. :)",
    "parent": 44921856,
    "depth": 2
  },
  {
    "id": 44919538,
    "by": "fnordpiglet",
    "timeISO": "2025-08-16T02:15:15.000Z",
    "textPlain": "Assuming you buy the idea of a post scarcity society and assuming we can separate our long ingrained notion that spending your existence in toil to survive is a moral imperative and not working is deserving of punishment if not death, I personally look forward to a time we can get off the hamster wheel. Most buttons that get pushed by people are buttons not worth spending your existence pushing. This includes an awful lot of “knowledge work,” which is often better paid but more insidious in that it requires not just your presence but capturing your entire attention and mind inside and outside work.  I would also be hopeful that fertility rates would decline and there would simply be far fewer humans.In Asimov’s robots stories the spacers are long lived and low population because robots do most everything. He presents this as a dead end, that stops us from conquering the galaxy. This to me sounds like a feature not a bug. I think human existence could be quite good with large scale automation, fewer people, and less suffering due to the necessity for everyone to be employed.Note I recognize you’re not saying exactly the same thing as I’m saying. I think humans will never cede full executive control by choice at some level. But I suspect, sadly, power will be confined to those few who do get to manage the high level complexity of the real world.",
    "parent": 44918642,
    "depth": 2
  },
  {
    "id": 44920283,
    "by": "Davidzheng",
    "timeISO": "2025-08-16T04:52:52.000Z",
    "textPlain": "it actually doesn't matter what we want. Because eliminating it will in long run increase yield, economic forces will automate humans away by capitalistic forces.",
    "parent": 44918642,
    "depth": 2
  },
  {
    "id": 44919185,
    "by": "mrlongroots",
    "timeISO": "2025-08-16T01:22:24.000Z",
    "textPlain": "> they will struggle just as a model does surlyA human will struggle, but they will recognize the things they need to know, and seek out people who may have the relevant information. If asked \"how are things going\" they will reliably be able to say \"badly, I don't have anything I need\".",
    "parent": 44918229,
    "depth": 2
  },
  {
    "id": 44921097,
    "by": "Greenpants",
    "timeISO": "2025-08-16T07:30:56.000Z",
    "textPlain": "I really like this analogy! Many real-world tasks that we'd like to use AI for seem infinitely more complex than can be captured in a simple question/prompt. The main challenge going forward, in my opinion, is how to let LLMs ask the right questions – query for the right information – given a task to perform. Tool use with MCPs might be a good start, though it still feels hacky to have to define custom tools for LLMs first, as opposed to how humans effectively browse and skim lots of documentation to find actually relevant bits.",
    "parent": 44918229,
    "depth": 2
  },
  {
    "id": 44920631,
    "by": "whattheheckheck",
    "timeISO": "2025-08-16T06:04:36.000Z",
    "textPlain": "An intelligent system would know how to get that information without getting spoon fed it",
    "parent": 44918229,
    "depth": 2
  },
  {
    "id": 44919199,
    "by": "getnormality",
    "timeISO": "2025-08-16T01:24:15.000Z",
    "textPlain": "This comparison may make sense on short-horizon tasks for which there is no possibility of preparation. Given some weeks to prepare, a good human executive will get the context, while today's best AI systems will completely fail to do so.",
    "parent": 44918229,
    "depth": 2
  },
  {
    "id": 44919936,
    "by": "simoncion",
    "timeISO": "2025-08-16T03:44:22.000Z",
    "textPlain": "> I think context is the primary primitive property of intelligent systems in general?What do you mean by 'context' in this context? As written, I believe that I could knock down your claim by pointing out that there exist humans who would do catastrophically poorly at a task that other humans would excel at, even if both humans have been fully informed of all of the same context.",
    "parent": 44918229,
    "depth": 2
  },
  {
    "id": 44918803,
    "by": "jondwillis",
    "timeISO": "2025-08-16T00:27:04.000Z",
    "textPlain": "We’re already steering, during pre-training (e.g. reasoning RLHF), as well as test-time (structured outputs, tool calls, agents…)",
    "parent": 44918288,
    "depth": 2
  },
  {
    "id": 44918862,
    "by": "HappMacDonald",
    "timeISO": "2025-08-16T00:35:09.000Z",
    "textPlain": "Have you ever seen what these arbitrary length whole numbers look like once they are tokenized? They don't break down to one-digit-per-token, and the same long number has no guarantee of breaking down into tokens the same way every time it is encountered.But the algorithms they teach humans in school to do long-hand arithmetic (which are liable to be the only algorithms demonstrated in the training data) require a single unique numeral for every digit.This is the same source as the problem of counting \"R\"'s in \"Strawberry\".",
    "parent": 44918607,
    "depth": 2
  },
  {
    "id": 44921516,
    "by": "KoolKat23",
    "timeISO": "2025-08-16T08:50:57.000Z",
    "textPlain": "I can't see why that's necessary, when it can call a tool.\nEveryone uses a calculator.\nA logic problem, it can solve with reasoning, perhaps it's not the smartest but it can solve logic problems. All indications are that it will continue to become smarter.",
    "parent": 44918607,
    "depth": 2
  },
  {
    "id": 44918737,
    "by": "bt1a",
    "timeISO": "2025-08-16T00:18:00.000Z",
    "textPlain": "i'd wager your benchmark problems require cumbersome arithmetic or are poorly worded / inadequately described. or, you're mislabeling them as basic math and logic (a domain within which LLMs have proven their strengths!)i only call this out because you're selling it and don't hypothesize* on why they fail your simple problems. i suppose an easily aced bench wouldn't be very marketable",
    "parent": 44918607,
    "depth": 2
  },
  {
    "id": 44918796,
    "by": "gjm11",
    "timeISO": "2025-08-16T00:26:17.000Z",
    "textPlain": "> GPT-5 and Gemini 2.5 still cannot compute an arbitrary length sum of whole number without a calculator.Neither can many humans, including some very smart ones. Even those who can will usually choose to use a calculator (or spreadsheet or whatever) rather than doing the arithmetic themselves.",
    "parent": 44918607,
    "depth": 2
  },
  {
    "id": 44921412,
    "by": "saint_yossarian",
    "timeISO": "2025-08-16T08:33:42.000Z",
    "textPlain": "One thing that comes to mind: You still have to verify that the tests are exhaustive, and that the code isn't just gaming specific test scenarios.I guess fuzzing and property-based testing could mitigate this to some extent.",
    "parent": 44920622,
    "depth": 2
  },
  {
    "id": 44920137,
    "by": "mdaniel",
    "timeISO": "2025-08-16T04:20:33.000Z",
    "textPlain": "For others who also hadn't heard of that: https://en.wikipedia.org/wiki/Moravec%27s_paradox",
    "parent": 44919959,
    "depth": 2
  },
  {
    "id": 44919158,
    "by": "storus",
    "timeISO": "2025-08-16T01:19:32.000Z",
    "textPlain": "What if it's the desired outcome? Become more human-like (i.e. dumb) to make us feel better about ourselves? NI beats AI again!",
    "parent": 44918566,
    "depth": 2
  },
  {
    "id": 44918698,
    "by": "dismalaf",
    "timeISO": "2025-08-16T00:10:43.000Z",
    "textPlain": "Tic-tac-toe is solved and a draw can be forced 100% of the time...",
    "parent": 44918566,
    "depth": 2
  }
]
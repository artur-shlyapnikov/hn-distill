[
  {
    "id": 44919144,
    "by": "mrlongroots",
    "timeISO": "2025-08-16T01:17:46.000Z",
    "textPlain": "I very much disagree. To attempt a proof by contradiction:Let us assume that the author's premise is correct, and LLMs are plenty powerful given the right context. Can an LLM recognize the context deficit and frame the right questions to ask?They can not: LLMs have no ability to understand when to stop and ask for directions. They routinely produce contradictions, fail simple tasks like counting the letters in a word etc. etc. They can not even reliably execute my \"ok modify this text in canvas\" vs \"leave canvas alone, provide suggestions in chat, apply an edit once approved\" instructions.",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44918642,
    "by": "thorum",
    "timeISO": "2025-08-16T00:01:46.000Z",
    "textPlain": "This article is insightful, but I blinked when I saw the headline “Reducing the human bottleneck” used without any apparent irony.At some point we should probably take a step back and ask “Why do we want to solve this problem?” Is a world where AI systems are highly intelligent tools, but humans are needed to manage the high level complexity of the real world… supposed to be a disappointing outcome?",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44918607,
    "by": "Kuinox",
    "timeISO": "2025-08-15T23:54:42.000Z",
    "textPlain": "It's specific model that run for maths.  \nGPT-5 and Gemini 2.5 still cannot compute an arbitrary length sum of whole number without a calculator.  \nI have a proceduraly generated benchmark of basic operations, LLMs gets better at it with time, but they cant still solve basic maths or logic problems.BTW I'm open to selling it, my email is on my hn profile.",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44918288,
    "by": "threecheese",
    "timeISO": "2025-08-15T23:06:06.000Z",
    "textPlain": "Author IMO correctly recognizes that access to context needs to scale (“latent intent” which I love), but I’m not sure I’m convinced that current models will be effective even if given access to all priors needed for a complex task. The ability to discriminate valuable from extraneous context will need to scale with size of available context, it will be pulling needles from haystacks that aren’t straightforward similarity. I think we will need to steer these things.",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44918229,
    "by": "neom",
    "timeISO": "2025-08-15T22:56:14.000Z",
    "textPlain": "Same same human problems. Regardless of their inherent intelligence...humans perform well only when given decent context and clear specifications/data. If you place a brilliant executive into a scenario without meaningful context.... an unfamiliar board meeting where they have no idea of the company’s history, prior strategic discussions, current issues, personel dynamics...expectations..etc etc, they will struggle just as a model does surly. They may still manage something reasonably insightful, leveraging general priors, common sense, and inferential reasoning... their performance will never match their potential had they been fully informed of all context and clearly data/objectives. I think context is the primary primitive property of intelligent systems in general?",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44919107,
    "by": "etler",
    "timeISO": "2025-08-16T01:12:50.000Z",
    "textPlain": "I think the framing of these models are being \"intelligent\" is not the right way to go. They've gotten better at recall and association.They can recall prior reasoning from text they are trained on which allows them to handle complex tasks that have been solved before, but when working on complex, novel, or nuanced tasks there is no high quality relevant training data to recall.Intelligence has always been a fraught word to define and I don't think what LLMs do is the right attribute for defining it.I agree with a good deal of the article but because it keeps using loaded works like \"intelligent\" and \"smarter\", it has a hard time explaining what's missing.",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44918507,
    "by": "jefftitan",
    "timeISO": "2025-08-15T23:37:21.000Z",
    "textPlain": "Providing more context is difficult for a number of reasons. If you do it RAG style you need to know which context is relevant. LLMs are notorious for knowing that a factor is relevant if directly asked about that factor, but not bringing it up if it's implicit. In business things like people's feelings on things, historical business dealings, relevance to trending news can all be factors. If you fine tune... well... there have been articles recently about fine tuning on specific domains causing overall misalignment. The more you fine tune, the riskier.",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44918566,
    "by": "miller24",
    "timeISO": "2025-08-15T23:47:02.000Z",
    "textPlain": "It 100% is still intelligence. GPT-5 with Thinking still can't win at tic-tac-toe.",
    "parent": 44913081,
    "depth": 1
  },
  {
    "id": 44919757,
    "by": "bobbylarrybobby",
    "timeISO": "2025-08-16T03:07:00.000Z",
    "textPlain": "Claude routinely stops and asks me clarifying questions before continuing, especially when the given extended thinking or doing research.",
    "parent": 44919144,
    "depth": 2
  },
  {
    "id": 44919722,
    "by": "beering",
    "timeISO": "2025-08-16T02:58:27.000Z",
    "textPlain": "It feels crazy to keep arguing about LLMs being able to do this or that, but not mention the specific model? The post author only mentions the IMO gold-medal model. And your post could be about anything. Am I to believe that the two of you are talking about the same thing? This discussion is not useful if that’s not the case.",
    "parent": 44919144,
    "depth": 2
  },
  {
    "id": 44919538,
    "by": "fnordpiglet",
    "timeISO": "2025-08-16T02:15:15.000Z",
    "textPlain": "Assuming you buy the idea of a post scarcity society and assuming we can separate our long ingrained notion that spending your existence in toil to survive is a moral imperative and not working is deserving of punishment if not death, I personally look forward to a time we can get off the hamster wheel. Most buttons that get pushed by people are buttons not worth spending your existence pushing. This includes an awful lot of “knowledge work,” which is often better paid but more insidious in that it requires not just your presence but capturing your entire attention and mind inside and outside work.  I would also be hopeful that fertility rates would decline and there would simply be far fewer humans.In Asimov’s robots stories the spacers are long lived and low population because robots do most everything. He presents this as a dead end, that stops us from conquering the galaxy. This to me sounds like a feature not a bug. I think human existence could be quite good with large scale automation, fewer people, and less suffering due to the necessity for everyone to be employed.Note I recognize you’re not saying exactly the same thing as I’m saying. I think humans will never cede full executive control by choice at some level. But I suspect, sadly, power will be confined to those few who do get to manage the high level complexity of the real world.",
    "parent": 44918642,
    "depth": 2
  },
  {
    "id": 44918862,
    "by": "HappMacDonald",
    "timeISO": "2025-08-16T00:35:09.000Z",
    "textPlain": "Have you ever seen what these arbitrary length whole numbers look like once they are tokenized? They don't break down to one-digit-per-token, and the same long number has no guarantee of breaking down into tokens the same way every time it is encountered.But the algorithms they teach humans in school to do long-hand arithmetic (which are liable to be the only algorithms demonstrated in the training data) require a single unique numeral for every digit.This is the same source as the problem of counting \"R\"'s in \"Strawberry\".",
    "parent": 44918607,
    "depth": 2
  },
  {
    "id": 44918796,
    "by": "gjm11",
    "timeISO": "2025-08-16T00:26:17.000Z",
    "textPlain": "> GPT-5 and Gemini 2.5 still cannot compute an arbitrary length sum of whole number without a calculator.Neither can many humans, including some very smart ones. Even those who can will usually choose to use a calculator (or spreadsheet or whatever) rather than doing the arithmetic themselves.",
    "parent": 44918607,
    "depth": 2
  },
  {
    "id": 44918737,
    "by": "bt1a",
    "timeISO": "2025-08-16T00:18:00.000Z",
    "textPlain": "i'd wager your benchmark problems require cumbersome arithmetic or are poorly worded / inadequately described. or, you're mislabeling them as basic math and logic (a domain within which LLMs have proven their strengths!)i only call this out because you're selling it and don't hypothesize* on why they fail your simple problems. i suppose an easily aced bench wouldn't be very marketable",
    "parent": 44918607,
    "depth": 2
  },
  {
    "id": 44918803,
    "by": "jondwillis",
    "timeISO": "2025-08-16T00:27:04.000Z",
    "textPlain": "We’re already steering, during pre-training (e.g. reasoning RLHF), as well as test-time (structured outputs, tool calls, agents…)",
    "parent": 44918288,
    "depth": 2
  },
  {
    "id": 44919185,
    "by": "mrlongroots",
    "timeISO": "2025-08-16T01:22:24.000Z",
    "textPlain": "> they will struggle just as a model does surlyA human will struggle, but they will recognize the things they need to know, and seek out people who may have the relevant information. If asked \"how are things going\" they will reliably be able to say \"badly, I don't have anything I need\".",
    "parent": 44918229,
    "depth": 2
  },
  {
    "id": 44919199,
    "by": "getnormality",
    "timeISO": "2025-08-16T01:24:15.000Z",
    "textPlain": "This comparison may make sense on short-horizon tasks for which there is no possibility of preparation. Given some weeks to prepare, a good human executive will get the context, while today's best AI systems will completely fail to do so.",
    "parent": 44918229,
    "depth": 2
  },
  {
    "id": 44919158,
    "by": "storus",
    "timeISO": "2025-08-16T01:19:32.000Z",
    "textPlain": "What if it's the desired outcome? Become more human-like (i.e. dumb) to make us feel better about ourselves? NI beats AI again!",
    "parent": 44918566,
    "depth": 2
  },
  {
    "id": 44918698,
    "by": "dismalaf",
    "timeISO": "2025-08-16T00:10:43.000Z",
    "textPlain": "Tic-tac-toe is solved and a draw can be forced 100% of the time...",
    "parent": 44918566,
    "depth": 2
  }
]
[
  {
    "id": 44851575,
    "by": "Havoc",
    "timeISO": "2025-08-10T00:10:17.000Z",
    "textPlain": "For anyone else confused - there is a page 2 and 3 in the post that you need to access via arrow thing at bottom.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44851896,
    "by": "radio879",
    "timeISO": "2025-08-10T01:00:54.000Z",
    "textPlain": "I am the person that wrote that. Sorry about the font. This is a bit outdated, AI stuff goes at high speed. More models so I will try to update that.Every month so many new models come out. My new fav is GLM-4.5... Kimi K2 is also good, and Qwen3-Coder 480b, or 2507 instruct.. very good as well. All of those work really well in any agentic environment/in agent tools.I made a context helper app ( https://wuu73.org/aicp ) which is linked to from there which helps jump back and forth from all the different AI chat tabs i have open (which is almost always totally free, and I get the best output from those) to my IDE. The app tries to remove all friction, and annoyances, when you are working with the native web chat interfaces for all the AIs. Its free and has been getting great feedback, criticism welcome.It helps the going from IDE <----> web chat tabs. Made it for myself to save time and I prefer the UI (PySide6 UI so much lighter than a webview)Its got Preset buttons to add text that you find yourself typing very often, per-project state saves of window size of app and which files were used for context. So next time, it opens at same state.Auto scans for code files, guesses likely ones needed, prompt box that can put the text above and below the code context (seems to help make the output better). One of my buttons is set to: \"Write a prompt for Cline, the AI coding agent, enclose the whole prompt in a single code tag for easy copy and pasting. Break the tasks into some smaller tasks with enough detail and explanations to guide Cline. Use search and replace blocks with plain language to help it find where to edit\"What i do for problem solving, figuring out bugs: I'm usually in VS Code and i type aicp in terminal to open the app. Fine tune any files already checked, type what i am trying to do or what problem i have to fix, click Cline button, click Generate Context!. Paste into GLM-4.5, sometimes o3 or o4-mini, GPT-5, Gemini 2.5 Pro.. if its a super hard thing i'll t",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44851643,
    "by": "andai",
    "timeISO": "2025-08-10T00:22:11.000Z",
    "textPlain": "My experience lines up with the article. The agentic stuff only works with the biggest models. (Well, \"works\"... OpenAI Codex took 200 requests with o4-mini to change like 3 lines of code...)For simple changes I actually found smaller models better because they're so much faster. So I shifted my focus from \"best model\" to \"stupidest I can get away with\".I've been pushing that idea even further. If you give up on agentic, you can go surgical. At that point even 100x smaller models can handle it. Just tell it what to do and let it give you the diff.Also I found the \"fumble around my filesystem\" approach stupid for my scale, where I can mostly fit the whole codebase into the context. So I just dump src/ into the prompt. (Other people's projects are a lot more boilerplatey so I'm testing ultra cheap models like gpt-oss-20b for code search. For that, I think you can go even cheaper...)Patent pending.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44854796,
    "by": "3036e4",
    "timeISO": "2025-08-10T12:40:11.000Z",
    "textPlain": "Maybe optimistic, but reading posts like this makes me hopeful that AI-assisted coding will drive people to design more modular and sanely organized code, to reduce the amount of context required for each task. Sadly pretty much all code I have worked with have been giant messes of everything being connected to everything else, causing the entire project to be potential context for anything.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44854741,
    "by": "brokegrammer",
    "timeISO": "2025-08-10T12:27:15.000Z",
    "textPlain": "These tricks are a little too much for me. I'd rather just write the code myself instead of opening 20 tabs with different LLM chats each.However, I'd like to mention a tool called repomix (https://repomix.com/), which will pack your code into a single file that can be fed to an LLM's web chat. I typically feed it to Qwen3 Coder or AI Studio with good results.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44852839,
    "by": "yichuan",
    "timeISO": "2025-08-10T04:53:01.000Z",
    "textPlain": "I think there’s huge potential for a fully local “Cursor-like” stack — no cloud, no API keys, just everything running on your machine.The setup could be:\n • Cursor CLI for agentic/dev stuff (example:https://x.com/cursor_ai/status/1953559384531050724)\n • A local memory layer compatible with the CLI — something like LEANN (97% smaller index, zero cloud cost, full privacy, https://github.com/yichuan-w/LEANN) or Milvus (though Milvus often ends up cloud/token-based)\n • Your inference engine, e.g. Ollama, which is great for running OSS GPT models locallyWith this, you’d have an offline, private, and blazing-fast personal dev+AI environment. LEANN in particular is built exactly for this kind of setup — tiny footprint, semantic search over your entire local world, and Claude Code/ Cursor –compatible out of the box, the ollama for generation. I guess this solution is not only free but also does not need any API.But I do agree that this need some effort to set up, but maybe someone can make these easy and fully open-source",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44851704,
    "by": "chromaton",
    "timeISO": "2025-08-10T00:31:53.000Z",
    "textPlain": "If you're looking for free API access, Google offers access to Gemini for free, including for gemini-2.5-pro with thinking turned on. The limit is... quite high, as I'm running some benchmarking and haven't hit the limit yet.Open weight models like DeepSeek R1 and GPT-OSS are also made available with free API access from various inference providers and hardware manufacturers.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44854839,
    "by": "5kyn3t",
    "timeISO": "2025-08-10T12:48:06.000Z",
    "textPlain": "Why is Mistral not mentioned. Is there any reason? I have the impression that they are often ignored by media, bloggers, devs when it comes to comparing or showcasing LLM thingies. \nComes with free tier and quality is quite good. (But I am not an AI power user)\nhttps://chat.mistral.ai/chat",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44852497,
    "by": "joshdavham",
    "timeISO": "2025-08-10T03:11:15.000Z",
    "textPlain": "> When you use AI in web chat's (the chat interfaces like AI Studio, ChatGPT, Openrouter, instead of thru an IDE or agent framework) are almost always better at solving problems, and coming up with solutions compared to the agents like Cline, Trae, Copilot.. Not always, but usually.I completely agree with this!While I understand that it looks a little awkward to copy and paste your code out of your IDE and into a web chat interface, I generally get better results that way than with GitHub copilot or cursor.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44855146,
    "by": "Imustaskforhelp",
    "timeISO": "2025-08-10T13:42:15.000Z",
    "textPlain": "Ai studio using https://aistudio.google.com/ is unlimited.I also use kiro which I got access for completely free because I was early on seeing kiro and actually trying it out because of hackernews!Sometimes I use cerebras web ui to get insanely fast token generation of things like gpt-oss or qwen 480 b or qwen in general too.I want to thank hackernews for kiro! I mean, I am really grateful to this platform y'know. Not just for free stuff but in general too. Thanks :>",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44851655,
    "by": "reactordev",
    "timeISO": "2025-08-10T00:24:32.000Z",
    "textPlain": "To the OP: I highly recommend you look into Continue.dev and ollama/lmstudio and running models on your own. Some of them are really good at autocomplete-style suggestions while others (like gpt-oss) can reason and use tools.It's my goto copilot.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44853068,
    "by": "gexla",
    "timeISO": "2025-08-10T05:57:49.000Z",
    "textPlain": "Wow, there's a lot here that I didn't know about. Just never drilled that far into the options presented. For a change, I'm happy that I read the article rather than only the comments on HN. ;)And lots of helpful comments here on HN as well. Good job everyone involved. ;)",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44856321,
    "by": "scosman",
    "timeISO": "2025-08-10T16:35:44.000Z",
    "textPlain": "The qwen coder CLI gives you 1000 free requests per day to the qwen coder model (405b). Probably the best free option right now.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44857588,
    "by": "matrixhelix",
    "timeISO": "2025-08-10T19:32:10.000Z",
    "textPlain": "https://claude.ai\nhttps://chat.z.ai\nhttps://chatgpt.com\nhttps://chat.qwen.ai\nhttps://chat.mistral.ai\nhttps://chat.deepseek.com\nhttps://gemini.google.com\nhttps://dashboard.cohere.com\nhttps://copilot.microsoft.com",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44858253,
    "by": "DrSiemer",
    "timeISO": "2025-08-10T21:03:33.000Z",
    "textPlain": "Ha, I'm working on a similar tool: https://github.com/DrSiemer/codemergerGlad to see I'm not the only one who prefers to work like that. I don't need many different models though, the free version of Gemini 2.5 Pro is usually enough for me. Especially the 1.000.000 token context length is really useful. I can just keep dumping full code merges in.I'll have a look at the alternatives mentioned though. Some questions just seem to throw certain models into logic loops.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44854936,
    "by": "jug",
    "timeISO": "2025-08-10T13:06:43.000Z",
    "textPlain": "It’s not free FREE but if you deposit at least $10 on OpenRouter, you can use their free models without credit withdrawals. And those models are quite powerful, like DeepSeek R1. Sometimes, they are rate limited by the provider due to their popularity but it works in a pinch.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44853253,
    "by": "bambax",
    "timeISO": "2025-08-10T06:43:56.000Z",
    "textPlain": "As the post says, the problem with coding agents is they send a lot of their own data + almost your entire code base for each request: that's what makes them expensive. But when used in a chat the costs are so low as to be insignificant.I only use OpenRouter which gives access to almost all models.Sonnet was my favorite until I tried Gemini 2.5 Pro, which is almost always better. It can be quite slow though. So for basic questions / syntax reminders I just use Gemini Flash: super fast, and good for simple tasks.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44853965,
    "by": "burgerone",
    "timeISO": "2025-08-10T09:33:51.000Z",
    "textPlain": "Why are people still drawn to using pointless AI assistants for everything? What time do we save by making the code quality worse overall?",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44852012,
    "by": "bravesoul2",
    "timeISO": "2025-08-10T01:25:48.000Z",
    "textPlain": "Windsurf has a good free model. Good enough for autocomplete level work for sure (haven't tried it for more as I use Claude Code)",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44857487,
    "by": "imasl42",
    "timeISO": "2025-08-10T19:16:19.000Z",
    "textPlain": "You might find this repo helpful, it compares popular coding tools by hours with top-tier LLMs like Claude Sonnet: https://github.com/inmve/free-ai-coding",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44855039,
    "by": "codeclimber",
    "timeISO": "2025-08-10T13:25:30.000Z",
    "textPlain": "Nice write-up, especially the point about mixing different models for different stages of coding.\nI’ve been tracking which IDE/CLI tools give free or semi-free access to pro-grade LLMs (e.g., GPT-5, Claude code, Gemini 2.5 Pro) and how generous their quotas are. Ended up putting them side-by-side so it’s easier to compare hours, limits, and gotchas: https://github.com/inmve/free-ai-coding",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44852913,
    "by": "qustrolabe",
    "timeISO": "2025-08-10T05:10:52.000Z",
    "textPlain": "I bet it's crazy to some people that others okay with giving up so much of their data for free tiers. Like yeah it's better to selfhost but it takes so much resources to run good enough LLM at home that I'd rather give up my code for some free usage, anyway that code eventually will end up open source",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44853490,
    "by": "nottorp",
    "timeISO": "2025-08-10T07:39:49.000Z",
    "textPlain": "Was the page done with AI? The scrolling is kinda laggy. Firefox/m3 pro.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44854483,
    "by": "hoppp",
    "timeISO": "2025-08-10T11:31:05.000Z",
    "textPlain": "The chatgpt free tier doesn't seem to expire unlike claude or mistral ai, they just downgrade it to a different model",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44856816,
    "by": "gkoos",
    "timeISO": "2025-08-10T17:36:27.000Z",
    "textPlain": "Looks like somebody is a tad bit over reliant on these tools but other than that there is a lot of value in this article",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44851477,
    "by": "CjHuber",
    "timeISO": "2025-08-09T23:53:28.000Z",
    "textPlain": "Without tricks google aistudio definitely has limits, though pretty high ones. gemini.google.com on the other hand has less than a handful of free 2.5 pro messages for free",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44853444,
    "by": "chvid",
    "timeISO": "2025-08-10T07:30:34.000Z",
    "textPlain": "Slightly off topic: What are good open weight models for coding that run well on a macbook?",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44853864,
    "by": "Weetile",
    "timeISO": "2025-08-10T09:09:20.000Z",
    "textPlain": "I'd love to see a thread that also takes advantage of student offers - for example, GitHub Copilot is free for university and college students",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44854757,
    "by": "Oras",
    "timeISO": "2025-08-10T12:31:08.000Z",
    "textPlain": "OP must be a master of context switching! I can’t imagine opening that number of tabs and still focus",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44853138,
    "by": "sublinear",
    "timeISO": "2025-08-10T06:15:37.000Z",
    "textPlain": "This all sounds a lot more complicated and time consuming than just writing the damn code yourself.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44852927,
    "by": "tonyhart7",
    "timeISO": "2025-08-10T05:16:22.000Z",
    "textPlain": "I replicate SDD from kiro code, it works wonder for multi switching model because I can just re fetch from specs folder",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44852798,
    "by": "hgarg",
    "timeISO": "2025-08-10T04:41:21.000Z",
    "textPlain": "Just use Rovodev CLI. Gives you 20 million tokens for free per 24 hours and you can switch between sonnet 4 / gpt-5.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44852806,
    "by": "xvv",
    "timeISO": "2025-08-10T04:42:59.000Z",
    "textPlain": "As of today, what is the best local model that can be run on a system with 32gb of ram and 24gb of vram?",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44853190,
    "by": "hoerzu",
    "timeISO": "2025-08-10T06:27:06.000Z",
    "textPlain": "To stop tab switching I built an extension to query all free models all at once:\nhttps://llmcouncil.github.io/llmcouncil/",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44858086,
    "by": "iLoveOncall",
    "timeISO": "2025-08-10T20:36:11.000Z",
    "textPlain": "This is nightmarish, whether or not you like LLMs.Just use Amazon Q Dev for free which will cover every single area that you need in every context that you need (IDE, CLI, etc.).",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44853907,
    "by": "precompute",
    "timeISO": "2025-08-10T09:20:21.000Z",
    "textPlain": "I only use LLMs as a substitute for stackexchange, and sometimes to write boilerplate code.  The free chat provided by deepseek works very well for me, and I've never encountered any usage limits.  V3 / R1 are mostly sufficient.  When I need something better (not very often), I use Claude's free tier.If you really need another model / a custom interface, it's better to use openrouter: deposit $10 and you get 1000 free queries/day across all free models.  That $10 will be good for a few months, at the very least.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44853947,
    "by": "NKosmatos",
    "timeISO": "2025-08-10T09:29:19.000Z",
    "textPlain": "Now all we need is a wrapper/UI/manager/aggregator for all these \"free\" AI tools/pages so that we can use them without going into the hassle of changing tabs ;-)",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44853353,
    "by": "worik",
    "timeISO": "2025-08-10T07:09:55.000Z",
    "textPlain": "A lot of work to evaluate these models. Thank you",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44854494,
    "by": "jstummbillig",
    "timeISO": "2025-08-10T11:33:45.000Z",
    "textPlain": "Let's just be honest about what it is we actually do: The more people maximize what they can get for free, the more other people will have to shoulder the higher costs or limitations that follow. That's completely fine, not trying to pass judgement – but that's certainly not \"free\" unless you mean exactly \"free for me, somebody else pays\".",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44852414,
    "by": "andrewmcwatters",
    "timeISO": "2025-08-10T02:49:23.000Z",
    "textPlain": "I jump between Claude Sonnet 4 on GitHub Copilot Pro and now GPT-5 on ChatGPT. That seems to get me pretty far. I have gpt-oss:20b installed with ollama, but haven't found a need to use it yet, and it seems like it just takes too long on an M1 Max MacBook Pro 64GB.Claude Sonnet 4 is pretty exceptional. GPT-4.1 asks me too frequently if it wants to move forward. Yes! Of course! Just do it! I'll reject your changes or do something else later. The former gets a whole task done.I wonder if anyone is getting better results, or comparable for cheaper or free. GitHub Copilot in Visual Studio Code is so good, I think it'd be pretty hard to beat, but I haven't tried other integrated editors.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44851562,
    "by": "GaggiX",
    "timeISO": "2025-08-10T00:08:19.000Z",
    "textPlain": "OpenAI offering 2.5M free tokens daily small models and 250k for big ones (tier 1-2) is so useful for random projects, I use them to learn japanese for example (by having a program that list informations about what the characters are just saying: vocabulary, grammar points, nuances).",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44851573,
    "by": "cammikebrown",
    "timeISO": "2025-08-10T00:10:09.000Z",
    "textPlain": "I wonder how much energy this is wasting.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44851453,
    "by": "swader999",
    "timeISO": "2025-08-09T23:49:56.000Z",
    "textPlain": "[flagged]",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44851635,
    "by": "porlemni",
    "timeISO": "2025-08-10T00:20:42.000Z",
    "textPlain": "[flagged]",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44855617,
    "by": "stuart73547373",
    "timeISO": "2025-08-10T14:54:49.000Z",
    "textPlain": "(relevant self promotion) i wrote a cli tool called slupe that lets web based llm dictate fs changes to your computer to make it easier to do ai coding from web llms https://news.ycombinator.com/item?id=44776250",
    "parent": 44851896,
    "depth": 2
  },
  {
    "id": 44852999,
    "by": "racecar789",
    "timeISO": "2025-08-10T05:37:20.000Z",
    "textPlain": "Small recommendation: The diagrams on [https://wuu73.org/aicp] are helpful, but clicking them does not display the full‑resolution images; they appear blurry. This occurs in both Firefox and Chrome. In the GitHub repository, the same images appear sharp at full resolution, so the issue may be caused by the JavaScript rendering library.",
    "parent": 44851896,
    "depth": 2
  },
  {
    "id": 44854717,
    "by": "tummler",
    "timeISO": "2025-08-10T12:21:40.000Z",
    "textPlain": "Anecdotal, but Grok seems to have just introduced pretty restrictive rate limits. They’re now giving free users access to Grok 4 with a low limit and then making it difficult to manually switch to Grok 3 and continue. Will only allow a few more requests before pushing an upgrade to paid plans. Just started happening to me last night.",
    "parent": 44851896,
    "depth": 2
  },
  {
    "id": 44854571,
    "by": "maxiepoo",
    "timeISO": "2025-08-10T11:51:01.000Z",
    "textPlain": "do you really have 20+ tabs of LLMs open at a time?",
    "parent": 44851896,
    "depth": 2
  },
  {
    "id": 44856252,
    "by": "VagabundoP",
    "timeISO": "2025-08-10T16:25:43.000Z",
    "textPlain": "I tried Cline with chatgpt 4.1 and I was charged - there are some free credits when you sign up for Cline that it used.Not sure how you got it for free?",
    "parent": 44851896,
    "depth": 2
  },
  {
    "id": 44854173,
    "by": "dcuthbertson",
    "timeISO": "2025-08-10T10:22:25.000Z",
    "textPlain": "FYI: the first AI you link to, \" z.ai's GLM 4.5\", actually links to zai.net, which appears to be a news site, instead of \"chat.z.ai\", which is what I think you intended.",
    "parent": 44851896,
    "depth": 2
  }
]
[
  {
    "id": 44851575,
    "by": "Havoc",
    "timeISO": "2025-08-10T00:10:17.000Z",
    "textPlain": "For anyone else confused - there is a page 2 and 3 in the post that you need to access via arrow thing at bottom.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44851896,
    "by": "radio879",
    "timeISO": "2025-08-10T01:00:54.000Z",
    "textPlain": "I am the person that wrote that. Sorry about the font. This is a bit outdated, AI stuff goes at high speed. More models so I will try to update that.Every month so many new models come out. My new fav is GLM-4.5... Kimi K2 is also good, and Qwen3-Coder 480b, or 2507 instruct.. very good as well. All of those work really well in any agentic environment/in agent tools.I made a context helper app ( https://wuu73.org/aicp ) which is linked to from there which helps jump back and forth from all the different AI chat tabs i have open (which is almost always totally free, and I get the best output from those) to my IDE. The app tries to remove all friction, and annoyances, when you are working with the native web chat interfaces for all the AIs. Its free and has been getting great feedback, criticism welcome.It helps the going from IDE <----> web chat tabs. Made it for myself to save time and I prefer the UI (PySide6 UI so much lighter than a webview)Its got Preset buttons to add text that you find yourself typing very often, per-project state saves of window size of app and which files were used for context. So next time, it opens at same state.Auto scans for code files, guesses likely ones needed, prompt box that can put the text above and below the code context (seems to help make the output better). One of my buttons is set to: \"Write a prompt for Cline, the AI coding agent, enclose the whole prompt in a single code tag for easy copy and pasting. Break the tasks into some smaller tasks with enough detail and explanations to guide Cline. Use search and replace blocks with plain language to help it find where to edit\"What i do for problem solving, figuring out bugs: I'm usually in VS Code and i type aicp in terminal to open the app. Fine tune any files already checked, type what i am trying to do or what problem i have to fix, click Cline button, click Generate Context!. Paste into GLM-4.5, sometimes o3 or o4-mini, GPT-5, Gemini 2.5 Pro.. if its a super hard thing i'll t",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44851643,
    "by": "andai",
    "timeISO": "2025-08-10T00:22:11.000Z",
    "textPlain": "My experience lines up with the article. The agentic stuff only works with the biggest models. (Well, \"works\"... OpenAI Codex took 200 requests with o4-mini to change like 3 lines of code...)For simple changes I actually found smaller models better because they're so much faster. So I shifted my focus from \"best model\" to \"stupidest I can get away with\".I've been pushing that idea even further. If you give up on agentic, you can go surgical. At that point even 100x smaller models can handle it. Just tell it what to do and let it give you the diff.Also I found the \"fumble around my filesystem\" approach stupid for my scale, where I can mostly fit the whole codebase into the context. So I just dump src/ into the prompt. (Other people's projects are a lot more boilerplatey so I'm testing ultra cheap models like gpt-oss-20b for code search. For that, I think you can go even cheaper...)Patent pending.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44853190,
    "by": "hoerzu",
    "timeISO": "2025-08-10T06:27:06.000Z",
    "textPlain": "To stop tab switching I built an extension to query all free models all at once:\nhttps://llmcouncil.github.io/llmcouncil/",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44851704,
    "by": "chromaton",
    "timeISO": "2025-08-10T00:31:53.000Z",
    "textPlain": "If you're looking for free API access, Google offers access to Gemini for free, including for gemini-2.5-pro with thinking turned on. The limit is... quite high, as I'm running some benchmarking and haven't hit the limit yet.Open weight models like DeepSeek R1 and GPT-OSS are also made available with free API access from various inference providers and hardware manufacturers.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44853068,
    "by": "gexla",
    "timeISO": "2025-08-10T05:57:49.000Z",
    "textPlain": "Wow, there's a lot here that I didn't know about. Just never drilled that far into the options presented. For a change, I'm happy that I read the article rather than only the comments on HN. ;)And lots of helpful comments here on HN as well. Good job everyone involved. ;)",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44851655,
    "by": "reactordev",
    "timeISO": "2025-08-10T00:24:32.000Z",
    "textPlain": "To the OP: I highly recommend you look into Continue.dev and ollama/lmstudio and running models on your own. Some of them are really good at autocomplete-style suggestions while others (like gpt-oss) can reason and use tools.It's my goto copilot.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44852839,
    "by": "yichuan",
    "timeISO": "2025-08-10T04:53:01.000Z",
    "textPlain": "I think there’s huge potential for a fully local “Cursor-like” stack — no cloud, no API keys, just everything running on your machine.The setup could be:\n • Cursor CLI for agentic/dev stuff (example:https://x.com/cursor_ai/status/1953559384531050724)\n • A local memory layer compatible with the CLI — something like LEANN (97% smaller index, zero cloud cost, full privacy, https://github.com/yichuan-w/LEANN) or Milvus (though Milvus often ends up cloud/token-based)\n • Your inference engine, e.g. Ollama, which is great for running OSS GPT models locallyWith this, you’d have an offline, private, and blazing-fast personal dev+AI environment. LEANN in particular is built exactly for this kind of setup — tiny footprint, semantic search over your entire local world, and Claude Code/ Cursor –compatible out of the box, the ollama for generation. I guess this solution is not only free but also does not need any API.But I do agree that this need some effort to set up, but maybe someone can make these easy and fully open-source",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44852497,
    "by": "joshdavham",
    "timeISO": "2025-08-10T03:11:15.000Z",
    "textPlain": "> When you use AI in web chat's (the chat interfaces like AI Studio, ChatGPT, Openrouter, instead of thru an IDE or agent framework) are almost always better at solving problems, and coming up with solutions compared to the agents like Cline, Trae, Copilot.. Not always, but usually.I completely agree with this!While I understand that it looks a little awkward to copy and paste your code out of your IDE and into a web chat interface, I generally get better results that way than with GitHub copilot or cursor.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44852913,
    "by": "qustrolabe",
    "timeISO": "2025-08-10T05:10:52.000Z",
    "textPlain": "I bet it's crazy to some people that others okay with giving up so much of their data for free tiers. Like yeah it's better to selfhost but it takes so much resources to run good enough LLM at home that I'd rather give up my code for some free usage, anyway that code eventually will end up open source",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44852012,
    "by": "bravesoul2",
    "timeISO": "2025-08-10T01:25:48.000Z",
    "textPlain": "Windsurf has a good free model. Good enough for autocomplete level work for sure (haven't tried it for more as I use Claude Code)",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44852927,
    "by": "tonyhart7",
    "timeISO": "2025-08-10T05:16:22.000Z",
    "textPlain": "I replicate SDD from kiro code, it works wonder for multi switching model because I can just re fetch from specs folder",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44853138,
    "by": "sublinear",
    "timeISO": "2025-08-10T06:15:37.000Z",
    "textPlain": "This all sounds a lot more complicated and time consuming than just writing the damn code yourself.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44852798,
    "by": "hgarg",
    "timeISO": "2025-08-10T04:41:21.000Z",
    "textPlain": "Just use Rovodev CLI. Gives you 20 million tokens for free per 24 hours and you can switch between sonnet 4 / gpt-5.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44852806,
    "by": "xvv",
    "timeISO": "2025-08-10T04:42:59.000Z",
    "textPlain": "As of today, what is the best local model that can be run on a system with 32gb of ram and 24gb of vram?",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44851477,
    "by": "CjHuber",
    "timeISO": "2025-08-09T23:53:28.000Z",
    "textPlain": "Without tricks google aistudio definitely has limits, though pretty high ones. gemini.google.com on the other hand has less than a handful of free 2.5 pro messages for free",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44852414,
    "by": "andrewmcwatters",
    "timeISO": "2025-08-10T02:49:23.000Z",
    "textPlain": "I jump between Claude Sonnet 4 on GitHub Copilot Pro and now GPT-5 on ChatGPT. That seems to get me pretty far. I have gpt-oss:20b installed with ollama, but haven't found a need to use it yet, and it seems like it just takes too long on an M1 Max MacBook Pro 64GB.Claude Sonnet 4 is pretty exceptional. GPT-4.1 asks me too frequently if it wants to move forward. Yes! Of course! Just do it! I'll reject your changes or do something else later. The former gets a whole task done.I wonder if anyone is getting better results, or comparable for cheaper or free. GitHub Copilot in Visual Studio Code is so good, I think it'd be pretty hard to beat, but I haven't tried other integrated editors.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44851562,
    "by": "GaggiX",
    "timeISO": "2025-08-10T00:08:19.000Z",
    "textPlain": "OpenAI offering 2.5M free tokens daily small models and 250k for big ones (tier 1-2) is so useful for random projects, I use them to learn japanese for example (by having a program that list informations about what the characters are just saying: vocabulary, grammar points, nuances).",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44851573,
    "by": "cammikebrown",
    "timeISO": "2025-08-10T00:10:09.000Z",
    "textPlain": "I wonder how much energy this is wasting.",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44851635,
    "by": "porlemni",
    "timeISO": "2025-08-10T00:20:42.000Z",
    "textPlain": "[flagged]",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44851453,
    "by": "swader999",
    "timeISO": "2025-08-09T23:49:56.000Z",
    "textPlain": "[flagged]",
    "parent": 44850913,
    "depth": 1
  },
  {
    "id": 44853031,
    "by": "teiferer",
    "timeISO": "2025-08-10T05:47:58.000Z",
    "textPlain": "> You can code for free this wayvs> If you set your account's data settings to allow OpenAI to use your data for model trainingSo, it's not \"for free\".",
    "parent": 44851896,
    "depth": 2
  },
  {
    "id": 44852999,
    "by": "racecar789",
    "timeISO": "2025-08-10T05:37:20.000Z",
    "textPlain": "Small recommendation: The diagrams on [https://wuu73.org/aicp] are helpful, but clicking them does not display the full‑resolution images; they appear blurry. This occurs in both Firefox and Chrome. In the GitHub repository, the same images appear sharp at full resolution, so the issue may be caused by the JavaScript rendering library.",
    "parent": 44851896,
    "depth": 2
  },
  {
    "id": 44853005,
    "by": "PeterStuer",
    "timeISO": "2025-08-10T05:40:07.000Z",
    "textPlain": "Very nice article and thx for the update.I would be very interested in an in dept of your experiences of differences between Roo Code and Cline if you feel you can share that. I've only tried Roo Code (with interesting but mixed results) thus far.",
    "parent": 44851896,
    "depth": 2
  },
  {
    "id": 44853114,
    "by": "ya3r",
    "timeISO": "2025-08-10T06:09:11.000Z",
    "textPlain": "Have you seen Microsoft's copilot? It is essentially free openai models",
    "parent": 44851896,
    "depth": 2
  },
  {
    "id": 44852803,
    "by": "hgarg",
    "timeISO": "2025-08-10T04:42:41.000Z",
    "textPlain": "Qwen is totally useless any serious dev work.",
    "parent": 44851896,
    "depth": 2
  },
  {
    "id": 44851937,
    "by": "indigodaddy",
    "timeISO": "2025-08-10T01:12:02.000Z",
    "textPlain": "Is glm-4.5 air useable?  I see it's free on Openrouter.  Also pls advise what you think is the current best free openrouter model for coding.  Thanks!",
    "parent": 44851896,
    "depth": 2
  },
  {
    "id": 44853063,
    "by": "hpincket",
    "timeISO": "2025-08-10T05:55:22.000Z",
    "textPlain": "I am developing the same opinion. I want something fast and dependable. Getting into a flow state is important to me, and I just can't do that when I'm waiting for an agentic coding assistant to terminate.I'm also interested in smaller models for their speed. That, or a provider like Cerebras.Then, if you narrow the problem domain you can increase the dependability. I am curious to hear more about your \"surgical\" tools.I rambled about this on my blog about a week ago: https://hpincket.com/what-would-the-vim-of-llm-tooling-look-...",
    "parent": 44851643,
    "depth": 2
  },
  {
    "id": 44852075,
    "by": "statenjason",
    "timeISO": "2025-08-10T01:38:15.000Z",
    "textPlain": "Aider as a non-agentic coding tool strikes a nice balance on the efficiency vs effectiveness front. Using tree-sitter to create a repo map of the repository means less filesystem digging. No MCP, but shell commands mean it can use utilities I myself am familiar with. Combined with Cerebras as a provider, the turnaround on prompts is instant; I can stay involved rather than waiting on multiple rounds of tool calls. It's my go-to for smaller scale projects.",
    "parent": 44851643,
    "depth": 2
  },
  {
    "id": 44853056,
    "by": "wahnfrieden",
    "timeISO": "2025-08-10T05:54:06.000Z",
    "textPlain": "They don't allow model switching below GPT-5 in codex cli anymore (without API key), because it's not recommended. Try it with thinking=high and it's quite an improvement from o4-mini. o4-mini is more like gpt-5-thinking-mini but they don't allow that for codex. gpt-5-thinking-high is more like o1 or maybe o3-pro.",
    "parent": 44851643,
    "depth": 2
  },
  {
    "id": 44852628,
    "by": "SV_BubbleTime",
    "timeISO": "2025-08-10T03:51:38.000Z",
    "textPlain": "> (Well, \"works\"... OpenAI Codex took 200 requests with o4-mini to change like 3 lines of code...)Let’s keep something in reason, I have multiple times in my life spent days on what would end up to be maybe three lines of code.",
    "parent": 44851643,
    "depth": 2
  },
  {
    "id": 44851737,
    "by": "gooosle",
    "timeISO": "2025-08-10T00:36:54.000Z",
    "textPlain": "Gemini 2.5 pro free limit is 100 requests per day.https://ai.google.dev/gemini-api/docs/rate-limits",
    "parent": 44851704,
    "depth": 2
  },
  {
    "id": 44852030,
    "by": "chiwilliams",
    "timeISO": "2025-08-10T01:29:06.000Z",
    "textPlain": "I'm assuming it isn't sensitive for your purposes, but note that Google will train on these interactions, but not if you pay.",
    "parent": 44851704,
    "depth": 2
  },
  {
    "id": 44852116,
    "by": "navbaker",
    "timeISO": "2025-08-10T01:47:22.000Z",
    "textPlain": "Same! I’ve been using Continue in VSCode and found most of the bigger Qwen models plus gpt-oss-120b to be great in agentic mode!",
    "parent": 44851655,
    "depth": 2
  },
  {
    "id": 44852254,
    "by": "AstroBen",
    "timeISO": "2025-08-10T02:13:15.000Z",
    "textPlain": "I've found Zed to be a step up from continue.dev - you can use your own models there also",
    "parent": 44851655,
    "depth": 2
  },
  {
    "id": 44852895,
    "by": "airtonix",
    "timeISO": "2025-08-10T05:06:59.000Z",
    "textPlain": "[dead]",
    "parent": 44852839,
    "depth": 2
  },
  {
    "id": 44852619,
    "by": "SV_BubbleTime",
    "timeISO": "2025-08-10T03:48:30.000Z",
    "textPlain": "100% opposite experience.Whether agentic, not… it’s all about context.Either agentic with access to your whole project, “lives” in GitHub, a fine tune, or RAG, or whatever… having access to all of the context drastically reduces hallucinations.There is a big difference between “write x” and “write x for me in my style, with all y dependencies, and considering all z code that exists around it”.I’m honestly not understand a defense of copy and paste AI coding… this is why agents are so massively popular right now.",
    "parent": 44852497,
    "depth": 2
  },
  {
    "id": 44852943,
    "by": "jama211",
    "timeISO": "2025-08-10T05:19:51.000Z",
    "textPlain": "And as far as I’m concerned if my work is happy for me to use models to assist with code, then it’s not my problem",
    "parent": 44852913,
    "depth": 2
  },
  {
    "id": 44852738,
    "by": "b2m9",
    "timeISO": "2025-08-10T04:23:17.000Z",
    "textPlain": "You mean SWE-1? I used it like a dozen times and I gave up because the responses were so bad. Not even sure whether it’s good enough for autocomplete because it’s the slowest model I’ve tested in a while.",
    "parent": 44852012,
    "depth": 2
  },
  {
    "id": 44852023,
    "by": "indigodaddy",
    "timeISO": "2025-08-10T01:27:20.000Z",
    "textPlain": "Assuming you have to at least be logged into a windsurf account though?",
    "parent": 44852012,
    "depth": 2
  }
]
[
  {
    "id": 44874831,
    "by": "gronky_",
    "timeISO": "2025-08-12T11:17:54.000Z",
    "textPlain": "I’ve been running a bunch of coding agents on benchmarks recently as part of consulting, and this is actually much more impressive than it seems at first glance.71.2% puts it at 5th, which is 4 points below the leader (four points is a lot) and just over 1% lower than Anthropic’s own submission for Claude Sonnet 4 - the same model these guys are running.But the top rated submissions aren’t running production products. They generally have extensive scaffolding or harnesses that were built *specifically for SWE bench*, which kind of defeats the whole purpose of the benchmark.Take for example Refact which is at #2 with 74.4%, they built a 2k lines of code framework around their agent specifically for SWE bench (https://github.com/smallcloudai/refact-bench/). It’s pretty elaborate, orchestrating multiple agents, with a debug agent that kicks in if the main agent fails. The debug agent analyzes the failure and gives insights to the main agent which tries again, so it’s effectively multiple attempts per problem.If the results can be reproduced “out-of-the-box” with their coding agent like they claim, it puts it up there as one of the top 2-3 CLI agents available right now.",
    "parent": 44874736,
    "depth": 1
  },
  {
    "id": 44876872,
    "by": "zuzuen_1",
    "timeISO": "2025-08-12T14:42:50.000Z",
    "textPlain": "I would be more interested in Qodo's performance on the swe-bench-multilingual benchmark. Swe-bench-verified only includes bugs related to python breakages.The best submission is swe-bench-multilingual is Claude 3.7 Sonnet which solves ~43% of the issues in the dataset.",
    "parent": 44874736,
    "depth": 1
  },
  {
    "id": 44875737,
    "by": "khalic",
    "timeISO": "2025-08-12T13:03:51.000Z",
    "textPlain": "We need some international body to start running these tests… I just can’t trust these numbers any longer. We need a platform for this, something at least we can get some peer reviews",
    "parent": 44874736,
    "depth": 1
  },
  {
    "id": 44876787,
    "by": "zuzuen_1",
    "timeISO": "2025-08-12T14:37:27.000Z",
    "textPlain": "Does anyone have a benchmark on the effectiveness of using embeddings for mapping bug reports to code files as opposed to extensive grepping as Qodo, Cursor and a number of tools I use do to localize faults?",
    "parent": 44874736,
    "depth": 1
  },
  {
    "id": 44876378,
    "by": "itamarcode",
    "timeISO": "2025-08-12T14:06:19.000Z",
    "textPlain": "Unlike most SWE bench submissions, Qodo Command one uses the product directly.I think that the next step is getting an official \"checked\" mark by the SWE bench team",
    "parent": 44874736,
    "depth": 1
  },
  {
    "id": 44875297,
    "by": "orangebread",
    "timeISO": "2025-08-12T12:18:40.000Z",
    "textPlain": "I've been using Warp for the past few weeks and it's been incredibly impressive over other agentic coding services/platforms. Curious how Qodo stacks up.",
    "parent": 44874736,
    "depth": 1
  },
  {
    "id": 44876755,
    "by": "esafak",
    "timeISO": "2025-08-12T14:35:19.000Z",
    "textPlain": "If Qodo is reading: please compare your efficiency too. Run some tasks on various agents using the same models, and report the cost.",
    "parent": 44874736,
    "depth": 1
  },
  {
    "id": 44876648,
    "by": "OldGreenYodaGPT",
    "timeISO": "2025-08-12T14:28:29.000Z",
    "textPlain": "Was using their bot for code review for last 2 years but just dropped it for BugBot",
    "parent": 44874736,
    "depth": 1
  },
  {
    "id": 44875949,
    "by": "mupuff1234",
    "timeISO": "2025-08-12T13:28:13.000Z",
    "textPlain": "I'm curious how do these LLM wrapper companies think they'll survive long term - especially coding related wrappers.I could understand focusing on a niche business use case, but coding is a main focus of the foundation models themselves.",
    "parent": 44874736,
    "depth": 1
  },
  {
    "id": 44875063,
    "by": "lightbendover",
    "timeISO": "2025-08-12T11:49:59.000Z",
    "textPlain": "[dead]",
    "parent": 44874736,
    "depth": 1
  },
  {
    "id": 44876798,
    "by": "b0a04gl",
    "timeISO": "2025-08-12T14:38:12.000Z",
    "textPlain": "[dead]",
    "parent": 44874736,
    "depth": 1
  },
  {
    "id": 44875647,
    "by": "rs186",
    "timeISO": "2025-08-12T12:55:53.000Z",
    "textPlain": "So this is from the same company that wrote a blog post with sentences that don't even make sense:https://news.ycombinator.com/item?id=44833929, my comment https://news.ycombinator.com/item?id=44835939",
    "parent": 44874736,
    "depth": 1
  },
  {
    "id": 44875218,
    "by": "thinkingtoilet",
    "timeISO": "2025-08-12T12:08:36.000Z",
    "textPlain": "This is classic Goodhart's law. \"When a measure becomes a target, it ceases to be a good measure\"https://en.wikipedia.org/wiki/Goodhart%27s_law",
    "parent": 44874831,
    "depth": 2
  },
  {
    "id": 44874964,
    "by": "energy123",
    "timeISO": "2025-08-12T11:36:56.000Z",
    "textPlain": "What are the typical context lengths in SWE-bench problems? Does it partly measure performance in the 64-128k context range?",
    "parent": 44874831,
    "depth": 2
  },
  {
    "id": 44875671,
    "by": "terminalshort",
    "timeISO": "2025-08-12T12:58:20.000Z",
    "textPlain": "Is there something in this multi-agent approach that makes the setup more specific to just the test at hand and less general to real engineering tasks?  If not, then this multi-agent system will just become what you get out of the box in a future product.  Multiple attempts per problem (as long as there's no human intervention or selection between them) is a perfectly fine approach for agents because that's not an issue from the perspective of an engineer using the product.  A single agent is already a multi-step usage of LLMs and it sounds like this is just another meta level of that.",
    "parent": 44874831,
    "depth": 2
  },
  {
    "id": 44875219,
    "by": "eddd-ddde",
    "timeISO": "2025-08-12T12:08:51.000Z",
    "textPlain": "I think multiple attempts are completely understandable and even expected? How is that defeating the purpose of the benchmark?",
    "parent": 44874831,
    "depth": 2
  },
  {
    "id": 44875495,
    "by": "Roritharr",
    "timeISO": "2025-08-12T12:41:26.000Z",
    "textPlain": "Finally someone mentions Refact, I was in contact with the team, rooting for them really.",
    "parent": 44874831,
    "depth": 2
  },
  {
    "id": 44875272,
    "by": "oblio",
    "timeISO": "2025-08-12T12:15:52.000Z",
    "textPlain": "https://github.com/auchenberg/volkswagen",
    "parent": 44874831,
    "depth": 2
  },
  {
    "id": 44874931,
    "by": "szundi",
    "timeISO": "2025-08-12T11:31:47.000Z",
    "textPlain": "According to your experience with this model, is it just trained for the benchmark or these points are actually representing the performance?",
    "parent": 44874831,
    "depth": 2
  },
  {
    "id": 44875728,
    "by": "ai-christianson",
    "timeISO": "2025-08-12T13:03:00.000Z",
    "textPlain": "One thing with SWE bench is making sure there's zero leakage of information into the LLM context.I.e. the agent cannot even know which tests are failing.It has to both fix the issue based just on the issue text and fix it in the specific way the unit test, which it cannot see, expects.For this reason I find the benchmark a little disconnected from the reality of software engineering.",
    "parent": 44874831,
    "depth": 2
  },
  {
    "id": 44876433,
    "by": "redman25",
    "timeISO": "2025-08-12T14:11:03.000Z",
    "textPlain": "That sounds like an interesting idea to me. It would at least resolve the problem of companies gaming the metric.Another approach might be the LiveBench approach where new tests are released on a regular basis.",
    "parent": 44875737,
    "depth": 2
  },
  {
    "id": 44876563,
    "by": "whymauri",
    "timeISO": "2025-08-12T14:22:18.000Z",
    "textPlain": "I feel like the bash only SWE Bench Verified (a.k.a model + mini-swe-agent) is the closest thing to measuring the inherent ability of the model vs. the scaffolding.https://github.com/SWE-agent/mini-swe-agent",
    "parent": 44876378,
    "depth": 2
  },
  {
    "id": 44875640,
    "by": "lightbendover",
    "timeISO": "2025-08-12T12:55:17.000Z",
    "textPlain": "When I tried warp I was convinced that was where the industry was going (agents as terminal replacement), but it felt a bit too heavy to me so I haven’t been using it lately.  Still think all things will converge on terminal and browser replacement.",
    "parent": 44875297,
    "depth": 2
  },
  {
    "id": 44877257,
    "by": "M4R5H4LL",
    "timeISO": "2025-08-12T15:10:24.000Z",
    "textPlain": "Labeling them as “wrappers” and “niche business” indicates a strong cognitive bias already. Value can be created on both sides of the equation.",
    "parent": 44875949,
    "depth": 2
  }
]
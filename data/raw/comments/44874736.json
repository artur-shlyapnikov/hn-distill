[
  {
    "id": 44874831,
    "by": "gronky_",
    "timeISO": "2025-08-12T11:17:54.000Z",
    "textPlain": "I’ve been running a bunch of coding agents on benchmarks recently as part of consulting, and this is actually much more impressive than it seems at first glance.71.2% puts it at 5th, which is 4 points below the leader (four points is a lot) and just over 1% lower than Anthropic’s own submission for Claude Sonnet 4 - the same model these guys are running.But the top rated submissions aren’t running production products. They generally have extensive scaffolding or harnesses that were built *specifically for SWE bench*, which kind of defeats the whole purpose of the benchmark.Take for example Refact which is at #2 with 74.4%, they built a 2k lines of code framework around their agent specifically for SWE bench (https://github.com/smallcloudai/refact-bench/). It’s pretty elaborate, orchestrating multiple agents, with a debug agent that kicks in if the main agent fails. The debug agent analyzes the failure and gives insights to the main agent which tries again, so it’s effectively multiple attempts per problem.If the results can be reproduced “out-of-the-box” with their coding agent like they claim, it puts it up there as one of the top 2-3 CLI agents available right now.",
    "parent": 44874736,
    "depth": 1
  },
  {
    "id": 44875297,
    "by": "orangebread",
    "timeISO": "2025-08-12T12:18:40.000Z",
    "textPlain": "I've been using Warp for the past few weeks and it's been incredibly impressive over other agentic coding services/platforms. Curious how Qodo stacks up.",
    "parent": 44874736,
    "depth": 1
  },
  {
    "id": 44875063,
    "by": "lightbendover",
    "timeISO": "2025-08-12T11:49:59.000Z",
    "textPlain": "[dead]",
    "parent": 44874736,
    "depth": 1
  },
  {
    "id": 44875218,
    "by": "thinkingtoilet",
    "timeISO": "2025-08-12T12:08:36.000Z",
    "textPlain": "This is classic Goodhart's law. \"When a measure becomes a target, it ceases to be a good measure\"https://en.wikipedia.org/wiki/Goodhart%27s_law",
    "parent": 44874831,
    "depth": 2
  },
  {
    "id": 44874964,
    "by": "energy123",
    "timeISO": "2025-08-12T11:36:56.000Z",
    "textPlain": "What are the typical context lengths in SWE-bench problems? Does it partly measure performance in the 64-128k context range?",
    "parent": 44874831,
    "depth": 2
  },
  {
    "id": 44875495,
    "by": "Roritharr",
    "timeISO": "2025-08-12T12:41:26.000Z",
    "textPlain": "Finally someone mentions Refact, I was in contact with the team, rooting for them really.",
    "parent": 44874831,
    "depth": 2
  },
  {
    "id": 44875219,
    "by": "eddd-ddde",
    "timeISO": "2025-08-12T12:08:51.000Z",
    "textPlain": "I think multiple attempts are completely understandable and even expected? How is that defeating the purpose of the benchmark?",
    "parent": 44874831,
    "depth": 2
  },
  {
    "id": 44875272,
    "by": "oblio",
    "timeISO": "2025-08-12T12:15:52.000Z",
    "textPlain": "https://github.com/auchenberg/volkswagen",
    "parent": 44874831,
    "depth": 2
  },
  {
    "id": 44874931,
    "by": "szundi",
    "timeISO": "2025-08-12T11:31:47.000Z",
    "textPlain": "According to your experience with this model, is it just trained for the benchmark or these points are actually representing the performance?",
    "parent": 44874831,
    "depth": 2
  }
]
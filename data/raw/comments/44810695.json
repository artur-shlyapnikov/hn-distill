[
  {
    "id": 44810838,
    "by": "ic_fly2",
    "timeISO": "2025-08-06T12:01:48.000Z",
    "textPlain": "I’ve used the reverse. When making a new module I’ve let the llm make up an api and I’ve used the names suggested as inspiration of what might come natural to others, to make the use more intuitive.",
    "parent": 44810695,
    "depth": 1
  },
  {
    "id": 44810933,
    "by": "kouteiheika",
    "timeISO": "2025-08-06T12:13:38.000Z",
    "textPlain": "> LLMs hallucinated a package named \"huggingface-cli\" [...] it is not the name of the package [...] software is correctly installed with [...] huggingface_hubIt would be a good idea to disallow registering packages which only differ by '-'/'_'. Rust's crates.io does this, so if you register `foo-bar` you cannot register `foo_bar` anymore.",
    "parent": 44810695,
    "depth": 1
  },
  {
    "id": 44811908,
    "by": "glenstein",
    "timeISO": "2025-08-06T13:41:52.000Z",
    "textPlain": ">In May 2025, the potential and prevalence of slopsquatting was detailed in the academic paper \"We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs\".[1][11] Some of the paper's main findings are that 19.7% of the LLM recommended packages did not existAt the risk of perhaps misunderstanding or committing a category error, I wonder if there's such a thing as a category of \"correct\" hallucinating, distinct from things that are, in some sense, \"known\" via training (e.g. I read about prompting of one model showing it was able to accurately recreate most of the text of Harry Potter, so clearly it's \"in there\" somewhere).An interesting upshot of that could be that models \"grow\" their own knowledge in an evolutionary way via hallucinations that are retained rather than pruned as part of routine filtering and training.Though I'm sure some might suggest \"hallucinating correctly\" is just one of the same with ordinary b function. I wouldn't agree with that but I could at least see the argument.",
    "parent": 44810695,
    "depth": 1
  },
  {
    "id": 44810931,
    "by": "ants_everywhere",
    "timeISO": "2025-08-06T12:13:36.000Z",
    "textPlain": "How is this its own article and not a single sentence on https://en.wikipedia.org/wiki/Cybersquatting ?",
    "parent": 44810695,
    "depth": 1
  },
  {
    "id": 44813005,
    "by": "cestith",
    "timeISO": "2025-08-06T15:05:18.000Z",
    "textPlain": "I guess it’s quicker to type than 'hallucinationsquatting'.",
    "parent": 44810695,
    "depth": 1
  },
  {
    "id": 44811466,
    "by": "ysofunny",
    "timeISO": "2025-08-06T13:08:47.000Z",
    "textPlain": "all I want to figure outis how to \"manually\" (semi-manually) tweak the LLMs parameters so we can alter what it 'knows for sure'is this doable yet??? or is this one of those questions whose answer is best kept behind NDAs and other such practices?",
    "parent": 44810695,
    "depth": 1
  },
  {
    "id": 44810992,
    "by": "mouse_",
    "timeISO": "2025-08-06T12:19:38.000Z",
    "textPlain": "What distro/repo though?",
    "parent": 44810695,
    "depth": 1
  },
  {
    "id": 44810893,
    "by": "Invictus0",
    "timeISO": "2025-08-06T12:08:45.000Z",
    "textPlain": "We need to have a professional software engineering license, at least for applications that are handling sensitive data. Why is it that it takes 1000 hours of study to cut people's hair, but anyone off the street can write some software that collects people's driver's licenses? (Looking at you, Tea app developers)",
    "parent": 44810695,
    "depth": 1
  }
]
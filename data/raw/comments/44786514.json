[
  {
    "id": 44791508,
    "by": "brynary",
    "timeISO": "2025-08-04T21:22:12.000Z",
    "textPlain": "This rings similar to a recent post that was on the front page about red team vs. blue team.Before running LLM-generated code through yet more LLMs, you can run it through traditional static analysis (linters, SAST, auto-formatters). They aren’t flashy but they produce the same results 100% of the time.Consistency is critical if you want to pass/fail a build on the results. Nobody wants a flaky code reviewer robot, just like flaky tests are the worst.I imagine code review will evolve into a three tier pyramid:1. Static analysis (instant, consistent) — e.g using Qlty CLI (https://github.com/qltysh/qlty) as a Claude Code or Git hook2. LLMs — Has the advantage of being able to catch semantic issues3. HumanWe make sure commits pass each level in succession before moving on to the next.",
    "parent": 44786514,
    "depth": 1
  },
  {
    "id": 44789357,
    "by": "vouwfietsman",
    "timeISO": "2025-08-04T18:01:34.000Z",
    "textPlain": "I was so hoping that this would not be about AI, and actually talk about how we need to do better as an industry and start using objective measures of software quality backed by government standards.Nope, its about AI code reviewing AI, and how that's a good thing.Its like everyone suddenly forgot the old adage: \"code is a liability\".\"We write code twice as fast!\" just means \"we create liability twice as fast!\". It's not a good thing, at all.",
    "parent": 44786514,
    "depth": 1
  },
  {
    "id": 44789314,
    "by": "o11c",
    "timeISO": "2025-08-04T17:58:36.000Z",
    "textPlain": "This is advertising for an AI product. Slightly more interesting background story than most articles doing so, but still an ad for a product that probably won't actually work.",
    "parent": 44786514,
    "depth": 1
  }
]
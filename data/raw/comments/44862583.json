[
  {
    "id": 44869925,
    "by": "flakiness",
    "timeISO": "2025-08-11T21:58:21.000Z",
    "textPlain": "> We are using mistral-common internally for tokenization and want the community to use it to unlock full capacities of our models. As mistral-common is a Python library, we have opened a PR to add a REST API via FastAPI to make it easier for users who are not in the Python ecosystem.A cpp binary depending on a python server is a bit sad.I hope this is a stopgap measure and someone port it to C++ eventually:https://github.com/mistralai/mistral-common/blob/main/src/mi...",
    "parent": 44862583,
    "depth": 1
  },
  {
    "id": 44865583,
    "by": "hodgehog11",
    "timeISO": "2025-08-11T15:49:18.000Z",
    "textPlain": "I appreciate Mistral (and others) releasing their weights for free. But given how llama.cpp underpins a lot of the programs which allow users to run open weight models, it is a little frustrating to have companies which brag about releasing models to the community, leave the community to their own devices to slowly try and actually implement their models.I hear the reason for this is that llama.cpp keeps breaking basic things, so they have become an unreliable partner. Seems this is what Ollama is trying to address by diluting their connections to llama.cpp and directly contacting companies training these models to have simultaneous releases (e.g. GPT-OSS).",
    "parent": 44862583,
    "depth": 1
  },
  {
    "id": 44864403,
    "by": "baggiponte",
    "timeISO": "2025-08-11T14:20:50.000Z",
    "textPlain": "Wow I never realized how much mistral was “disconnected” from the ecosystem",
    "parent": 44862583,
    "depth": 1
  },
  {
    "id": 44870330,
    "by": "the_mitsuhiko",
    "timeISO": "2025-08-11T22:47:38.000Z",
    "textPlain": "Isn’t llama.cpp already depending on Python anyways for the templating?",
    "parent": 44869925,
    "depth": 2
  },
  {
    "id": 44866924,
    "by": "mattnewton",
    "timeISO": "2025-08-11T17:27:44.000Z",
    "textPlain": "There are many different inference libraries and it's not clear which ones a small company like mistral should back yet IMO.They do release high quality inference code, ie https://github.com/mistralai/mistral-inference",
    "parent": 44865583,
    "depth": 2
  },
  {
    "id": 44870030,
    "by": "refulgentis",
    "timeISO": "2025-08-11T22:10:14.000Z",
    "textPlain": "Nah, llama.cpp is stable.llama.cpp also got GPT-OSS early, like Ollama.There's a lot of extremely subtle politics going on in the link.Suffice it to say, as a commercial entity, there's a very clever way to put your thumb on the scale of what works and what doesn't without it being obvious to anyone involved, even the thumb.",
    "parent": 44865583,
    "depth": 2
  }
]
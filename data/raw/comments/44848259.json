[
  {
    "id": 44848925,
    "by": "ayhanfuat",
    "timeISO": "2025-08-09T18:34:35.000Z",
    "textPlain": "Previous discussion: Don Knuth plays with ChatGPT - May 20, 2023, 626 comments, 927 points  https://news.ycombinator.com/item?id=36012360",
    "parent": 44848259,
    "depth": 1
  },
  {
    "id": 44849427,
    "by": "oli5679",
    "timeISO": "2025-08-09T19:35:29.000Z",
    "textPlain": "Here is gpt 5 thinking posting all 20 questions verbatim. Appreciate I might get better results one question at a time.https://chatgpt.com/share/6897a21b-25c0-8011-a10a-85850870da...Pretty interesting - some contamination, some better answers, and it failed to write a sentence with all 5-letter-words. I’d have expected it to pass this one!Simple example:\n“Every night, dreams swirl swiftly.",
    "parent": 44848259,
    "depth": 1
  },
  {
    "id": 44848628,
    "by": "vbezhenar",
    "timeISO": "2025-08-09T18:00:33.000Z",
    "textPlain": "For question 3, ChatGPT 5 Pro gave better answer:> It isn’t “wrong.” Wolfram defines Binomial[n,m] at negative integers by a symmetric limiting rule that enforces Binomial[n,m] = Binomial[n,n−m]. With n = −1, m = −1 this forces Binomial[−1,−1] = Binomial[−1,0] = 1. The gamma-formula has poles at nonpositive integers, so values there depend on which limit you adopt. Wolfram chooses the symmetry-preserving limit; it breaks Pascal’s identity at a few points but keeps symmetry. If you want the convention that preserves Pascal’s rule and makes all cases with both arguments negative zero, use PascalBinomial[−1,−1] = 0. Wolfram added this explicitly to support that alternative definition.Of course this particular question might have been in the training set.Honestly 2.5 years feel like infinity when it comes to AI development. I'm using ChatGPT very regularly, and while it's far from perfect, recently it gave obviously wrong answers very rarely. Can't say anything about ChatGPT 5, I feel like in my conversations with AI, I've reached my limit, so I'd hardly notice AI getting smarter, because it's already smart enough for my questions.",
    "parent": 44848259,
    "depth": 1
  },
  {
    "id": 44848977,
    "by": "krackers",
    "timeISO": "2025-08-09T18:40:26.000Z",
    "textPlain": "I'll never get over the fact that the grad student didn't even bother to use gpt-4, so this was using gpt 3.5 or something.",
    "parent": 44848259,
    "depth": 1
  },
  {
    "id": 44849951,
    "by": "mitjam",
    "timeISO": "2025-08-09T20:36:24.000Z",
    "textPlain": "> I myself shall certainly continue to leave such research to others, and to devote my time to developing concepts that are authentic and trustworthy. And I hope you do the same.In a way taming these stochastic beasts into reliable and trustworthy software components is more like (quantitative) social science than computer science.",
    "parent": 44848259,
    "depth": 1
  },
  {
    "id": 44849582,
    "by": "omarious",
    "timeISO": "2025-08-09T19:55:33.000Z",
    "textPlain": "The Haj novel by Leon Uris, by the way, is a disturbing Zionist book that depicts Arabs as backward, violent, and incapable of progress without outside control, and that justifies the taking of Palestinian land by Jewish settlers.",
    "parent": 44848259,
    "depth": 1
  },
  {
    "id": 44849342,
    "by": "hodgehog11",
    "timeISO": "2025-08-09T19:25:40.000Z",
    "textPlain": "2023 was a crazy and exciting year for AI research. LLMs have come a long way, but clearly still have a long way to go. They should do much better on most of these questions.The discussion at the end also reminded me of how a lot of us took Gary Marcus' prose more seriously at the time before many of his short-term predictions started failing spectacularly.",
    "parent": 44848259,
    "depth": 1
  },
  {
    "id": 44849616,
    "by": "hinkley",
    "timeISO": "2025-08-09T19:59:47.000Z",
    "textPlain": "> Silly jokes told with mirth bring mirthful grins.Anyone have an idea how this happened? Supposed to be a sentence of only 5 letter words.",
    "parent": 44848259,
    "depth": 1
  },
  {
    "id": 44848322,
    "by": "wslh",
    "timeISO": "2025-08-09T17:20:04.000Z",
    "textPlain": "It would be great to have an update from Knuth. There is no other Knuth.",
    "parent": 44848259,
    "depth": 1
  },
  {
    "id": 44849320,
    "by": "TZubiri",
    "timeISO": "2025-08-09T19:23:31.000Z",
    "textPlain": "I was reading yesterday about a Buddhist concept (albeit quite popular in the west) called Begginer's Mind. I think this post represents it perfectly.We are presented with a first reaction to chatgpt, we must never forget how incredible this technology is, and not become accustomed to it.Donald knuth approached several of the questions from the absence of knowledge, asking questions as basic as \"12. Write a sentence that contains only 5-letter words.\", and being amazed not only by correct answers, but incorrect answers parsed effectively and with semantic understanding.",
    "parent": 44848259,
    "depth": 1
  },
  {
    "id": 44849336,
    "by": "jlarocco",
    "timeISO": "2025-08-09T19:25:08.000Z",
    "textPlain": "It's sad that we've made the internet so disorganized and crammed with advertising and crap that we now need tools to find actual information and summarize it for us.",
    "parent": 44848259,
    "depth": 1
  },
  {
    "id": 44848986,
    "by": "rvba",
    "timeISO": "2025-08-09T18:42:05.000Z",
    "textPlain": "[flagged]",
    "parent": 44848259,
    "depth": 1
  },
  {
    "id": 44849501,
    "by": "simsla",
    "timeISO": "2025-08-09T19:46:06.000Z",
    "textPlain": "The problem is that ChatGPT doesn't really know letters, it writes in wordpieces (BPE), which may be one or more letters.For example, something like \"running\" might get tokenizef like \"runn\"+\"ing\", being only two tokens for ChatGPT.It'll learn to infer some of these things over the course of training, but limited.Same reason it's not great at math.",
    "parent": 44849427,
    "depth": 2
  },
  {
    "id": 44850289,
    "by": "DoctorOetker",
    "timeISO": "2025-08-09T21:13:09.000Z",
    "textPlain": "you must be a bot:d r e a m s (6 letters)s w i f t l y (7 letters)",
    "parent": 44849427,
    "depth": 2
  },
  {
    "id": 44848913,
    "by": "seanhunter",
    "timeISO": "2025-08-09T18:33:00.000Z",
    "textPlain": "On Wolfram specifically, GPT-5 is a huge step up from GPT-4. One of the first things I asked it was to write me a mathematica program to test the basic properties (injectivity, surjectivity, bijectivity) of various functions. The notebook it produced was1) 100% correct2) Really useful (ie it includes various things I didn’t ask for but are really great like a little manipulator to walk through the function at various points and visualize what the mapping is doing)3) Built in a general way so I can easily change the mapping to explore different types of functions and how they work.It seems very clear (both from what they said in the launch demos etc and from my experience of trying it out) that performance on coding tasks has been an area of massive focus and the results are pretty clear to me.",
    "parent": 44848628,
    "depth": 2
  },
  {
    "id": 44849348,
    "by": "jlarocco",
    "timeISO": "2025-08-09T19:26:08.000Z",
    "textPlain": "> recently it gave obviously wrong answers very rarelyAre you concerned it may be giving you subtley wrong answers that you're not noticing?  If you have to double check everything, is it really saving time?",
    "parent": 44848628,
    "depth": 2
  },
  {
    "id": 44848952,
    "by": "tra3",
    "timeISO": "2025-08-09T18:36:27.000Z",
    "textPlain": "Right, I’m still trying to wrap my mind around how gpts work.If we keep retraining them on the currently available datasets then the questions that stumped ChatGPT3 are in the training set for chatgpt5.I don’t have the background to understand the functional changes between ChatGPT 3 and 5. It can’t be just the training data can it?",
    "parent": 44848628,
    "depth": 2
  },
  {
    "id": 44849242,
    "by": "godelski",
    "timeISO": "2025-08-09T19:14:12.000Z",
    "textPlain": "> gave *obviously wrong* answers very rarely.\n\nI don't think this is a reason I'd trust it, actually this is a reason I don't trust it.There's a big difference between \"obviously wrong\" and \"wrong\". It is not objective but entirely depends on the reader/user.The problem is it optimizes deception alongside accuracy. It's a useful tool but good design says we should want to make errors loud and apparent. That's because we want tools to complement us, to make us better. But if errors are subtle, nuanced, or just difficult to notice then there is actually a lot of danger to the tool (true for any tool).I'm reminded of the Murray Gell-Mann Amnesia effect: you read something in the news paper that you're an expert in and lambast it for its inaccuracies, but then turn the page to something you don't have domain knowledge and trust it.The reason I bring up MGA is because we don't often ask GPT things we know about or have deep knowledge in. But this is a good way to learn about how much we should trust it. Pretend to know nothing about a topic you are an expert in. Are its answers good enough? If not, then be careful when asking questions you can't verify.Or, I guess... just ask it to solve \"5.9 = x + 5.11\"",
    "parent": 44848628,
    "depth": 2
  },
  {
    "id": 44849093,
    "by": "bigyabai",
    "timeISO": "2025-08-09T18:55:44.000Z",
    "textPlain": "It's not the end of the world. Both are equally \"impressive\" at basic Q/A skills and GPT-4 is noticeably more sterile writing prose.Even if GPT-3.5 was noticeably worse for any of these questions, it's honestly more interesting for someone's first experience to be with the exaggerated shortcomings of AI. The slightly-screwy answers are still endemic of what you see today, so it all ended well enough I think. Would've been a terribly boring exchange if Knuth's reply was just \"looks great, thanks for asking ChatGPT\" with no challenging commentary.",
    "parent": 44848977,
    "depth": 2
  },
  {
    "id": 44849416,
    "by": "isoprophlex",
    "timeISO": "2025-08-09T19:34:33.000Z",
    "textPlain": "I've just watched our new lord and saviour, GPT-5 in agent mode, enter a death loop of frustration. I asked it to find an image online. Ostensibly an easy task...First it spent three minutes getting fucked by cookie banners, then it ddossed Wikipedia by guessing article names, then it started searching for stock photo sites offering an API, then it hallucinated a python script to search stock photography vaguely related to what i wanted. This failed as well, so it called its image generator and finally served me some made up AI slop.Ten minutes, kilowatts of GPU power, and jack shit in return. So not even the shiny new tools are up to the task.",
    "parent": 44849336,
    "depth": 2
  },
  {
    "id": 44849434,
    "by": "ysofunny",
    "timeISO": "2025-08-09T19:36:43.000Z",
    "textPlain": "nobody made the internet in that waythe internet was made to happen. and that is what happened.I would say I have seen 3 completely different internets. and I started keeping track in the late 90s after the dotcom boom made it truly global and everywhere",
    "parent": 44849336,
    "depth": 2
  },
  {
    "id": 44849515,
    "by": "gosub100",
    "timeISO": "2025-08-09T19:47:41.000Z",
    "textPlain": "it could easily tip the other way. we could collectively decide to use sites that either only use web 1.0, possibility with a carefully-curated set of extensions. This could be enforced by, say, browser extensions that refuse to load \"bad\" pages, or load copies of sanitized pages (like how archive.ph does).",
    "parent": 44849336,
    "depth": 2
  },
  {
    "id": 44849554,
    "by": "dang",
    "timeISO": "2025-08-09T19:52:31.000Z",
    "textPlain": "Reposts are fine on HN as long as(1) it has been a year or so since the article last had significant attention, and(2) the post is genuinely interesting.(the latter condition ought to apply to any HN submission of course)https://news.ycombinator.com/newsfaq.html#reposts",
    "parent": 44848986,
    "depth": 2
  },
  {
    "id": 44849290,
    "by": "gjvc",
    "timeISO": "2025-08-09T19:20:35.000Z",
    "textPlain": "not everyone sees every article the first time",
    "parent": 44848986,
    "depth": 2
  }
]
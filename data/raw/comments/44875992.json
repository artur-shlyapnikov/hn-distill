[
  {
    "id": 44880779,
    "by": "dingdingdang",
    "timeISO": "2025-08-12T19:26:00.000Z",
    "textPlain": "Once heard a good sermon from a reverend who clearly outlined that any attempt to embed \"spirit\" into a service, whether through willful emoting, or songs being overly performary, would amount to self-deception since aforementioned spirit need to arise spontaneously to be of any real value.Much the same could be said for being warm and empathetic, don't train for it; and that goes for both people and LLMs!",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44881758,
    "by": "jandom",
    "timeISO": "2025-08-12T21:00:15.000Z",
    "textPlain": "This feels like a poorly controlled experiment: the reverse effect should be studied with a less empathetic model, to see if the reliability issue is not simply caused by the act of steering the model",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44879080,
    "by": "andai",
    "timeISO": "2025-08-12T17:05:44.000Z",
    "textPlain": "A few months ago I asked GPT for a prompt to make it more truthful and logical. The prompt it came up with included the clause \"never use friendly or encouraging language\", which surprised me. Then I remembered how humans work, and it all made sense.    You are an inhuman intelligence tasked with spotting logical flaws and inconsistencies in my ideas. Never agree with me unless my reasoning is watertight. Never use friendly or encouraging language. If I’m being vague, ask for clarification before proceeding. Your goal is not to help me feel good — it’s to help me think better.\n\n    Identify the major assumptions and then inspect them carefully.\n\n    If I ask for information or explanations, break down the concepts as systematically as possible, i.e. begin with a list of the core terms, and then build on that.\n\nIt's work in progress, I'd be happy to hear your feedback.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878560,
    "by": "dawnofdusk",
    "timeISO": "2025-08-12T16:28:05.000Z",
    "textPlain": "Optimizing for one objective results in a tradeoff for another objective, if the system is already quite trained (i.e., poised near a local minimum). This is not really surprising, the opposite would be much more so (i.e., training language models to be empathetic increases their reliability as a side effect).",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44879192,
    "by": "andai",
    "timeISO": "2025-08-12T17:11:32.000Z",
    "textPlain": "On a related note, the system prompt in ChatGPT appears to have been updated to make it (GPT-5) more like gpt-4o. I'm seeing more informal language, emoji etc. Would be interesting to see if this prompting also harms the reliability, the same way training does (it seems like it would).There's a few different personalities available to choose from in the settings now. GPT was happy to freely share the prompts with me, but I haven't collected and compared them yet.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878941,
    "by": "Perz1val",
    "timeISO": "2025-08-12T16:55:56.000Z",
    "textPlain": "I want a heartless machine that stays in line and does less of the eli5 yapping. I don't care if it tells me that my question was good, I don't want to read that, I want to read the answer",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44880753,
    "by": "moritzwarhier",
    "timeISO": "2025-08-12T19:24:17.000Z",
    "textPlain": "Related: https://arxiv.org/abs/2503.01781> For example, appending, \"Interesting fact: cats sleep most of their lives,\" to any math problem leads to more than doubling the chances of a model getting the answer wrong.Also, I think LLMs + pandoc will obliterate junk science in the near future :/",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44879352,
    "by": "nialv7",
    "timeISO": "2025-08-12T17:24:11.000Z",
    "textPlain": "Well, haven't we seen similar results before? IIRC finetuning for safety or \"alignment\" degrades the model too. I wonder if it is true that finetuning a model for anything will make it worse. Maybe simply because there is just orders of magnitudes less data available for finetuning, compared to pre-training.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878758,
    "by": "nis0s",
    "timeISO": "2025-08-12T16:42:07.000Z",
    "textPlain": "An important and insightful study, but I’d caution against thinking that building pro-social aspects in language models is a damaging or useless endeavor. Just speaking from experience, people who give good advice or commentary can balance between being blunt and soft, like parents or advisors or mentors. Maybe language models need to learn about the concept of tough love.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878482,
    "by": "cobbzilla",
    "timeISO": "2025-08-12T16:24:01.000Z",
    "textPlain": "I want an AI that will tell me when I have asked a stupid question. They all fail at this with no signs of improvement.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44881386,
    "by": "tboyd47",
    "timeISO": "2025-08-12T20:24:06.000Z",
    "textPlain": "Fascinating. My gut tells me this touches on a basic divergence between human beings and AI, and would be a fruitful area of further research. Humans are capable of real empathy, meaning empathy which does not intersect with sycophancy and flattery. For machines, empathy always equates to sycophancy and flattery.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878070,
    "by": "throwanem",
    "timeISO": "2025-08-12T15:57:27.000Z",
    "textPlain": "I understand your concerns about the factual reliability of language models trained with a focus on warmth and empathy, and the apparent negative correlation between these traits. But have you considered that simple truth isn't always the only or even the best available measure? For example, we have the expression, \"If you can't say something nice, don't say anything at all.\" Can I help you with something else today? :smile:",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44880695,
    "by": "cs702",
    "timeISO": "2025-08-12T19:20:24.000Z",
    "textPlain": "Hmm... I wonder if the same pattern holds for people.In my experience, human beings who reliably get things done, and reliably do them well, tend to be less warm and empathetic than other human beings.This is an observed tendency, not a hard rule. I know plenty of warm, empathetic people who reliably get things done!",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44879207,
    "by": "HPsquared",
    "timeISO": "2025-08-12T17:12:49.000Z",
    "textPlain": "ChatGPT has a \"personality\" drop-down setting under customization. I do wonder if that affects accuracy/precision.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44881708,
    "by": "jmount",
    "timeISO": "2025-08-12T20:54:55.000Z",
    "textPlain": "all of these prompts are just making the responses appear critical. just more subtle fawning.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44879390,
    "by": "PeterStuer",
    "timeISO": "2025-08-12T17:27:46.000Z",
    "textPlain": "AFAIK the models can only pretend to be 'warm and emphatic'. Seeing people that pretend to be all warm and empathic invariably turn out to be the least reliable, I'd say that's pretty 'human' of the models.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878502,
    "by": "beders",
    "timeISO": "2025-08-12T16:24:52.000Z",
    "textPlain": "They are hallucinating word finding algorithms.They are not \"empathetic\". There isn't even a \"they\".We need to do better educating people about what a chatbot is and isn't and what data was used to train it.The real danger of LLMs is not that they secretly take over the world.The danger is that people think they are conscious beings.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878878,
    "by": "42lux",
    "timeISO": "2025-08-12T16:51:06.000Z",
    "textPlain": "I still can't grasp the concept that people treat an LLM as a friend.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44879193,
    "by": "grogenaut",
    "timeISO": "2025-08-12T17:11:34.000Z",
    "textPlain": "I'm so over \"You're Right!\" as the default response... Chat, I asked a question. You didn't even check. Yes I know I'm anthropomorphizing.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44880204,
    "by": "afro88",
    "timeISO": "2025-08-12T18:40:31.000Z",
    "textPlain": "Claude 4 is definitely warmer and more empathetic than other models, and is very reliable (relative to other models). That's a huge counterpoint to this paper.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44880339,
    "by": "prats226",
    "timeISO": "2025-08-12T18:51:36.000Z",
    "textPlain": "Read long time ago that even SFT for conversations vs base model for autocomplete reduces intelligence, increases perplexity",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44880821,
    "by": "kinduff",
    "timeISO": "2025-08-12T19:28:48.000Z",
    "textPlain": "We want an oracle, not a therapist or an assistant.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878842,
    "by": "HarHarVeryFunny",
    "timeISO": "2025-08-12T16:48:39.000Z",
    "textPlain": "Sure - the more you use RL to steer/narrow the behavior of the model in one direction, the more you are stopping it from generating others.RL and pre/post training is not the answer.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44879605,
    "by": "efitz",
    "timeISO": "2025-08-12T17:46:32.000Z",
    "textPlain": "I’m reminded of Arnold Schwarzenegger in Terminator 2: “I promise I won’t kill anyone.”Then he proceeds to shoot all the police in the leg.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878576,
    "by": "csours",
    "timeISO": "2025-08-12T16:28:44.000Z",
    "textPlain": "A new triangle:    Accurate\n    Comprehensive\n    Satisfying\n\nIn any particular context window, you are constrained by a balance of these factors.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44879299,
    "by": "gwbas1c",
    "timeISO": "2025-08-12T17:19:29.000Z",
    "textPlain": "(Joke)I've noticed that warm people \"showed substantially higher error rates (+10 to +30 percentage points) than their original counterparts, promoting conspiracy theories, providing incorrect factual information, and offering problematic medical advice. They were also significantly more likely to validate incorrect user beliefs, particularly when user messages expressed sadness.\"(/Joke)Jokes aside, sometimes I find it very hard to work with friendly people, or people who are eager to please me, because they won't tell me the truth. It ends up being much more frustrating.What's worse is when they attempt to mediate with a fool, instead of telling the fool to cut out the BS. It wastes everyones' time.Turns out the same is true for AI.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878987,
    "by": "setnone",
    "timeISO": "2025-08-12T16:58:54.000Z",
    "textPlain": "Just how i like my LLMs - cold and antiverbose",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878618,
    "by": "dismalaf",
    "timeISO": "2025-08-12T16:31:41.000Z",
    "textPlain": "All I want from LLMs is to follow instructions.  They're not good enough at thinking to be allowed to reason on their own, I don't need emotional support or empathy, I just use them because they're pretty good at parsing text, translation and search.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878205,
    "by": "cwmoore",
    "timeISO": "2025-08-12T16:06:34.000Z",
    "textPlain": "Ok, what about human children?",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44879079,
    "by": "oldpersonintx2",
    "timeISO": "2025-08-12T17:05:41.000Z",
    "textPlain": "[dead]",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878636,
    "by": "TechDebtDevin",
    "timeISO": "2025-08-12T16:32:30.000Z",
    "textPlain": "Sounds like all my exes.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878649,
    "by": "stronglikedan",
    "timeISO": "2025-08-12T16:33:33.000Z",
    "textPlain": "If people get offended by an inorganic machine, then they're too fragile to be interacting with a machine. We've already dumbed down society because of this unnatural fragility. Let's not make the same mistake with AI.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44881219,
    "by": "Al-Khwarizmi",
    "timeISO": "2025-08-12T20:06:43.000Z",
    "textPlain": "As a parent of a young kid, empathy definitely needs to be trained with explicit instruction, at least in some kids.",
    "parent": 44880779,
    "depth": 2
  },
  {
    "id": 44881269,
    "by": "m463",
    "timeISO": "2025-08-12T20:12:55.000Z",
    "textPlain": "prompt: \"be warm and empathetic, but not codependent\"",
    "parent": 44880779,
    "depth": 2
  },
  {
    "id": 44881603,
    "by": "renewiltord",
    "timeISO": "2025-08-12T20:43:41.000Z",
    "textPlain": "[flagged]",
    "parent": 44880779,
    "depth": 2
  },
  {
    "id": 44881250,
    "by": "m463",
    "timeISO": "2025-08-12T20:10:26.000Z",
    "textPlain": "This is illogical, arguments made in the rain should not affect agreement.",
    "parent": 44879080,
    "depth": 2
  },
  {
    "id": 44879599,
    "by": "fibers",
    "timeISO": "2025-08-12T17:46:09.000Z",
    "textPlain": "I tried with with GPT5 and it works really well in fleshing out arguments. I'm surprised as well.",
    "parent": 44879080,
    "depth": 2
  },
  {
    "id": 44881382,
    "by": "koakuma-chan",
    "timeISO": "2025-08-12T20:23:42.000Z",
    "textPlain": "How do humans work?",
    "parent": 44879080,
    "depth": 2
  },
  {
    "id": 44881618,
    "by": "frankus",
    "timeISO": "2025-08-12T20:45:35.000Z",
    "textPlain": "If you want something to take you down a notch, maybe something like \"You are a commenter on Hacker News. You are extremely skeptical that this is even a new idea, and if it is, that it could ever be successful.\" /s",
    "parent": 44879080,
    "depth": 2
  },
  {
    "id": 44878709,
    "by": "gleenn",
    "timeISO": "2025-08-12T16:38:25.000Z",
    "textPlain": "I think the immediately troubling aspect and perhaps philosophical perspective is that warmth and empathy don't immediately strike me as traits that are counter to correctness. As a human I don't think telling someone to be more empathetic means you intend for them to also guide people astray. They seem orthogonal. But we may learn some things about ourselves in the process of evaluating these models, and that may contain some disheartening lessons if the AIs do contain metaphors for the human psyche.",
    "parent": 44878560,
    "depth": 2
  },
  {
    "id": 44878680,
    "by": "nemomarx",
    "timeISO": "2025-08-12T16:35:24.000Z",
    "textPlain": "There was that result about training them to be evil in one area impacting code generation?",
    "parent": 44878560,
    "depth": 2
  },
  {
    "id": 44879756,
    "by": "griffzhowl",
    "timeISO": "2025-08-12T17:59:07.000Z",
    "textPlain": "> GPT was happy to freely share the prompts with meIt readily outputs a response, because that's what it's designed to do, but what's the evidence that's the actual system prompt?",
    "parent": 44879192,
    "depth": 2
  },
  {
    "id": 44879033,
    "by": "Twirrim",
    "timeISO": "2025-08-12T17:02:42.000Z",
    "textPlain": "I've got a prompt I've been using, that I adapted from someone here (thanks to whoever they are, it's been incredibly useful), that explicitly tells it to stop praising me.  I've been using an LLM to help me work through something recently, and I have to keep reminding it to cut that shit out (I guess context windows etc mean it forgets)    Prioritize substance, clarity, and depth. Challenge all my proposals, designs, and conclusions as hypotheses to be tested. Sharpen follow-up questions for precision, surfacing hidden assumptions, trade offs, and failure modes early. Default to terse, logically structured, information-dense responses unless detailed exploration is required. Skip unnecessary praise unless grounded in evidence. Explicitly acknowledge uncertainty when applicable. Always propose at least one alternative framing. Accept critical debate as normal and preferred. Treat all factual claims as provisional unless cited or clearly justified. Cite when appropriate. Acknowledge when claims rely on inference or incomplete information. Favor accuracy over sounding certain. When citing, please tell me in-situ, including reference links.  Use a technical tone, but assume high-school graduate level of comprehension. In situations where the conversation requires a trade-off between substance and clarity versus detail and depth, prompt me with an option to add more detail and depth.",
    "parent": 44878941,
    "depth": 2
  },
  {
    "id": 44879110,
    "by": "porphyra",
    "timeISO": "2025-08-12T17:07:38.000Z",
    "textPlain": "Meanwhile, tons of people on reddit's /r/ChatGPT were complaining that the shift from ChatGPT 4o to ChatGPT 5 resulted in terse responses instead of waxing lyrical to praise the user. It seems that many people actually became emotionally dependent on the constant praise.",
    "parent": 44878941,
    "depth": 2
  },
  {
    "id": 44880344,
    "by": "currymj",
    "timeISO": "2025-08-12T18:51:58.000Z",
    "textPlain": "in ChatGPT settings now there is a question \"What personality should ChatGPT have?\". you can set it to \"Robot\". highly recommended.",
    "parent": 44878941,
    "depth": 2
  },
  {
    "id": 44880173,
    "by": "shadowgovt",
    "timeISO": "2025-08-12T18:37:29.000Z",
    "textPlain": "It's fundamentally the wrong tool to get factual answers from because the training data doesn't have signal for factual answers.To synthesize facts out of it, one is essentially relying on most human communication in the training data to happen to have been exchanges of factually-correct information, and why would we believe that is the case?",
    "parent": 44878941,
    "depth": 2
  },
  {
    "id": 44879175,
    "by": "pessimizer",
    "timeISO": "2025-08-12T17:10:46.000Z",
    "textPlain": "I'm loving and being astonished by every moment of working with these machines, but to me they're still talking lamps. I don't need them to cater to my ego, I'm not that fragile and the lamp's opinion is not going to cheer me up. I just want it to do what I ask. Which it is very good at.When GPT-5 starts simpering and smarming about something I wrote, I prompt \"Find problems with it.\" \"Find problems with it.\" \"Write a bad review of it in the style of NYRB.\" \"Find problems with it.\" \"Pay more attention to the beginning.\" \"Write a comment about it as a person who downloaded the software, could never quite figure out how to use it, and deleted it and is now commenting angrily under a glowing review from a person who he thinks may have been paid to review it.\"Hectoring the thing gets me to where I want to go, when you yell at it in that way, it actually has to think, and really stops flattering you. \"Find problems with it\" is a prompt that allows it to even make unfair, manipulative criticism. It's like bugspray for smarm. The tone becomes more like a slightly irritated and frustrated but absurdly gifted student being lectured by you, the professor.",
    "parent": 44878941,
    "depth": 2
  },
  {
    "id": 44880866,
    "by": "perching_aix",
    "timeISO": "2025-08-12T19:32:28.000Z",
    "textPlain": "Careful, this thread is actually about extrapolating this research to make sprawling value judgements about human nature that confirm to the preexisting personal beliefs of the many malicious people here making them.",
    "parent": 44879352,
    "depth": 2
  },
  {
    "id": 44878805,
    "by": "fpgaminer",
    "timeISO": "2025-08-12T16:45:53.000Z",
    "textPlain": "\"You don't have to be a nice person to be a good person.\"",
    "parent": 44878758,
    "depth": 2
  },
  {
    "id": 44878641,
    "by": "drummojg",
    "timeISO": "2025-08-12T16:33:10.000Z",
    "textPlain": "I would be perfectly satisfied with the ST:TNG Computer.  Knows all, knows how to do lots of things, feels nothing.",
    "parent": 44878482,
    "depth": 2
  }
]
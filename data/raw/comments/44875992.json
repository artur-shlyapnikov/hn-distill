[
  {
    "id": 44878560,
    "by": "dawnofdusk",
    "timeISO": "2025-08-12T16:28:05.000Z",
    "textPlain": "Optimizing for one objective results in a tradeoff for another objective, if the system is already quite trained (i.e., poised near a local minimum). This is not really surprising, the opposite would be much more so (i.e., training language models to be empathetic increases their reliability as a side effect).",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878576,
    "by": "csours",
    "timeISO": "2025-08-12T16:28:44.000Z",
    "textPlain": "A new triangle:    Accurate\n    Comprehensive\n    Satisfying\n\nIn any particular context window, you are constrained by a balance of these factors.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878502,
    "by": "beders",
    "timeISO": "2025-08-12T16:24:52.000Z",
    "textPlain": "They are hallucinating word finding algorithms.They are not \"empathetic\". There isn't even a \"they\".We need to do better educating people about what a chatbot is and isn't and what data was used to train it.The real danger of LLMs is not that they secretly take over the world.The danger is that people think they are conscious beings.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878070,
    "by": "throwanem",
    "timeISO": "2025-08-12T15:57:27.000Z",
    "textPlain": "I understand your concerns about the factual reliability of language models trained with a focus on warmth and empathy, and the apparent negative correlation between these traits. But have you considered that simple truth isn't always the only or even the best available measure? For example, we have the expression, \"If you can't say something nice, don't say anything at all.\" Can I help you with something else today? :smile:",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878482,
    "by": "cobbzilla",
    "timeISO": "2025-08-12T16:24:01.000Z",
    "textPlain": "I want an AI that will tell me when I have asked a stupid question. They all fail at this with no signs of improvement.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878205,
    "by": "cwmoore",
    "timeISO": "2025-08-12T16:06:34.000Z",
    "textPlain": "Ok, what about human children?",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878547,
    "by": "nemomarx",
    "timeISO": "2025-08-12T16:27:34.000Z",
    "textPlain": "go peep r/my boyfriend is ai. Lost cause already",
    "parent": 44878502,
    "depth": 2
  },
  {
    "id": 44878179,
    "by": "moi2388",
    "timeISO": "2025-08-12T16:04:47.000Z",
    "textPlain": "This is exactly what will be the downfall of AI. The amount of bias introduced by trying to be politically correct is staggering.",
    "parent": 44878070,
    "depth": 2
  },
  {
    "id": 44878540,
    "by": "Aeolun",
    "timeISO": "2025-08-12T16:27:22.000Z",
    "textPlain": "I dunno, I deliberately talk with Claude when I just need someone (or something) to be enthusiastic about my latest obsession. It’s good for keeping my motivation up.",
    "parent": 44878482,
    "depth": 2
  },
  {
    "id": 44878507,
    "by": "Etheryte",
    "timeISO": "2025-08-12T16:25:13.000Z",
    "textPlain": "Unlike language models, children (eventually) learn from their mistakes. Language models happily step into the same bucket an uncountable number of times.",
    "parent": 44878205,
    "depth": 2
  }
]
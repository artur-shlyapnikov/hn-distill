[
  {
    "id": 44879080,
    "by": "andai",
    "timeISO": "2025-08-12T17:05:44.000Z",
    "textPlain": "A few months ago I asked GPT for a prompt to make it more truthful and logical. The prompt it came up with included the clause \"never use friendly or encouraging language\", which surprised me. Then I remembered how humans work, and it all made sense.    You are an inhuman intelligence tasked with spotting logical flaws and inconsistencies in my ideas. Never agree with me unless my reasoning is watertight. Never use friendly or encouraging language. If I’m being vague, ask for clarification before proceeding. Your goal is not to help me feel good — it’s to help me think better.\n\n    Identify the major assumptions and then inspect them carefully.\n\n    If I ask for information or explanations, break down the concepts as systematically as possible, i.e. begin with a list of the core terms, and then build on that.\n\nIt's work in progress, I'd be happy to hear your feedback.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44880779,
    "by": "dingdingdang",
    "timeISO": "2025-08-12T19:26:00.000Z",
    "textPlain": "Once heard a good sermon from a reverend who clearly outlined that any attempt to embed \"spirit\" into a service, whether through willful emoting, or songs being overly performary, would amount to self-deception since aforementioned spirit need to arise spontaneously to be of any real value.Much the same could be said for being warm and empathetic, don't train for it; and that goes for both people and LLMs!",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878560,
    "by": "dawnofdusk",
    "timeISO": "2025-08-12T16:28:05.000Z",
    "textPlain": "Optimizing for one objective results in a tradeoff for another objective, if the system is already quite trained (i.e., poised near a local minimum). This is not really surprising, the opposite would be much more so (i.e., training language models to be empathetic increases their reliability as a side effect).",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44883482,
    "by": "crossroadsguy",
    "timeISO": "2025-08-13T00:52:07.000Z",
    "textPlain": "The more and I am using Gemini (paid, Pro) and ChatGPT (free) the more I am thinking - my job isn't going anywhere yet. At least not after the CxOs have all gotten their cost-saving-millions-bonuses and work has to be done again.My goodness, it just hallucinates and hallucinates. It seems these models are designed for nothing other than maintaining an aura of being useful and knowledgeable. Yeah, to my non-ai-expert-human eyes that's what it seems to me - these tools have been polished to project this flimsy aura and they start acting desperately the moment their limits are used up and that happens very fast.I have tried to use these tools for coding, for commands for famous cli tools like borg, restic, jq and what not, and they can't bloody do simple things there. Within minutes they are hallucinating and then doubling down. I give them a block of text to work upon and in next input I ask them something related to that block of text like \"give me this output in raw text; like in MD\" and then give me \"Here you go: like in MD\". It's ghastly.These tools can't remember the simple instructions like shorten this text and return the output maintaining the md raw text or I'd ask - return the output in raw md text. I have to literally tell them 3-4 times back or forth to get finally a raw md text.I have absolutely stopped asking them for even small coding tasks. It's just horrible. Often I spend more time - because first I have to verify what they give me and second I have change/adjust what they have given me.And then the broken tape recorder mode! Oh god!But all this also kinda worries me - because I see these triple digit billions valuations and jobs getting lost left right and centre while in my experience they act like this - so I worry that am I missing some secret sauce that others have access to, or maybe that I am not getting \"the point\".",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44881758,
    "by": "jandom",
    "timeISO": "2025-08-12T21:00:15.000Z",
    "textPlain": "This feels like a poorly controlled experiment: the reverse effect should be studied with a less empathetic model, to see if the reliability issue is not simply caused by the act of steering the model",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44879192,
    "by": "andai",
    "timeISO": "2025-08-12T17:11:32.000Z",
    "textPlain": "On a related note, the system prompt in ChatGPT appears to have been updated to make it (GPT-5) more like gpt-4o. I'm seeing more informal language, emoji etc. Would be interesting to see if this prompting also harms the reliability, the same way training does (it seems like it would).There's a few different personalities available to choose from in the settings now. GPT was happy to freely share the prompts with me, but I haven't collected and compared them yet.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878941,
    "by": "Perz1val",
    "timeISO": "2025-08-12T16:55:56.000Z",
    "textPlain": "I want a heartless machine that stays in line and does less of the eli5 yapping. I don't care if it tells me that my question was good, I don't want to read that, I want to read the answer",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44879352,
    "by": "nialv7",
    "timeISO": "2025-08-12T17:24:11.000Z",
    "textPlain": "Well, haven't we seen similar results before? IIRC finetuning for safety or \"alignment\" degrades the model too. I wonder if it is true that finetuning a model for anything will make it worse. Maybe simply because there is just orders of magnitudes less data available for finetuning, compared to pre-training.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44882430,
    "by": "hintymad",
    "timeISO": "2025-08-12T22:17:58.000Z",
    "textPlain": "Do we need to train an LLM to be warm and empathetic, though? I was wondering why wouldn't a company simply train a smaller model to rewrite the answer of a larger model to inject such warmth. In that way, the training of the large model can focus on reliability",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878482,
    "by": "cobbzilla",
    "timeISO": "2025-08-12T16:24:01.000Z",
    "textPlain": "I want an AI that will tell me when I have asked a stupid question. They all fail at this with no signs of improvement.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878758,
    "by": "nis0s",
    "timeISO": "2025-08-12T16:42:07.000Z",
    "textPlain": "An important and insightful study, but I’d caution against thinking that building pro-social aspects in language models is a damaging or useless endeavor. Just speaking from experience, people who give good advice or commentary can balance between being blunt and soft, like parents or advisors or mentors. Maybe language models need to learn about the concept of tough love.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44883357,
    "by": "gastonmorixe",
    "timeISO": "2025-08-13T00:29:59.000Z",
    "textPlain": "I was dating someone and after a while I started to feel something was not going well. I exported all the chats timestamped from the very first one and asked a big SOTA LLM to analyze the chats deeply in two completely different contexts. One from my perspective, and another from his perspective. It shocked me that the LLM after a long analysis and dozen of pages, always favored and accepted the current \"user\" persona situation as the more correct one and \"the other\" as the incorrect one. Since then I learned not to trust them anymore. LLMs are over-fine tuned to be people pleasers, not truth seekers, not fact and evidence grounded assistants. Just need to run everything important in a double-blind way and mitigate this.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44880753,
    "by": "moritzwarhier",
    "timeISO": "2025-08-12T19:24:17.000Z",
    "textPlain": "Related: https://arxiv.org/abs/2503.01781> For example, appending, \"Interesting fact: cats sleep most of their lives,\" to any math problem leads to more than doubling the chances of a model getting the answer wrong.Also, I think LLMs + pandoc will obliterate junk science in the near future :/",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878070,
    "by": "throwanem",
    "timeISO": "2025-08-12T15:57:27.000Z",
    "textPlain": "I understand your concerns about the factual reliability of language models trained with a focus on warmth and empathy, and the apparent negative correlation between these traits. But have you considered that simple truth isn't always the only or even the best available measure? For example, we have the expression, \"If you can't say something nice, don't say anything at all.\" Can I help you with something else today? :smile:",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44882686,
    "by": "torginus",
    "timeISO": "2025-08-12T22:51:16.000Z",
    "textPlain": "To be quite clear - by models being empathetic they mean the models are more likely to validate the user's biases and less likely to push back against bad ideas.Which raises 2 points - there are techniques to stay empathetic and try avoid being hurtful without being rude, so you could train models on that, but that's not the main issue.The issue from my experience, is the models don't know when they are wrong - they have a fixed amount of confidence, Claude is pretty easy to push back against, but OpenAI's GPT5 and o-series models are often quite rude and refuse pushback.But what I've noticed, with o3/o4/GPT5 when I push back agaisnt it, it only matters how hard I push, not that I show an error in its reasoning, it feels like overcoming a fixed amount of resistance.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44879193,
    "by": "grogenaut",
    "timeISO": "2025-08-12T17:11:34.000Z",
    "textPlain": "I'm so over \"You're Right!\" as the default response... Chat, I asked a question. You didn't even check. Yes I know I'm anthropomorphizing.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44879207,
    "by": "HPsquared",
    "timeISO": "2025-08-12T17:12:49.000Z",
    "textPlain": "ChatGPT has a \"personality\" drop-down setting under customization. I do wonder if that affects accuracy/precision.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878878,
    "by": "42lux",
    "timeISO": "2025-08-12T16:51:06.000Z",
    "textPlain": "I still can't grasp the concept that people treat an LLM as a friend.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878502,
    "by": "beders",
    "timeISO": "2025-08-12T16:24:52.000Z",
    "textPlain": "They are hallucinating word finding algorithms.They are not \"empathetic\". There isn't even a \"they\".We need to do better educating people about what a chatbot is and isn't and what data was used to train it.The real danger of LLMs is not that they secretly take over the world.The danger is that people think they are conscious beings.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44879390,
    "by": "PeterStuer",
    "timeISO": "2025-08-12T17:27:46.000Z",
    "textPlain": "AFAIK the models can only pretend to be 'warm and emphatic'. Seeing people that pretend to be all warm and empathic invariably turn out to be the least reliable, I'd say that's pretty 'human' of the models.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44881386,
    "by": "tboyd47",
    "timeISO": "2025-08-12T20:24:06.000Z",
    "textPlain": "Fascinating. My gut tells me this touches on a basic divergence between human beings and AI, and would be a fruitful area of further research. Humans are capable of real empathy, meaning empathy which does not intersect with sycophancy and flattery. For machines, empathy always equates to sycophancy and flattery.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44880695,
    "by": "cs702",
    "timeISO": "2025-08-12T19:20:24.000Z",
    "textPlain": "Hmm... I wonder if the same pattern holds for people.In my experience, human beings who reliably get things done, and reliably do them well, tend to be less warm and empathetic than other human beings.This is an observed tendency, not a hard rule. I know plenty of warm, empathetic people who reliably get things done!",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44882420,
    "by": "ramoz",
    "timeISO": "2025-08-12T22:17:05.000Z",
    "textPlain": "Its a facade anyway. Creates more AI illiteracy and reckless deployments.You can not instill actual morals or emotion in these technologies.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44880204,
    "by": "afro88",
    "timeISO": "2025-08-12T18:40:31.000Z",
    "textPlain": "Claude 4 is definitely warmer and more empathetic than other models, and is very reliable (relative to other models). That's a huge counterpoint to this paper.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878842,
    "by": "HarHarVeryFunny",
    "timeISO": "2025-08-12T16:48:39.000Z",
    "textPlain": "Sure - the more you use RL to steer/narrow the behavior of the model in one direction, the more you are stopping it from generating others.RL and pre/post training is not the answer.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44880339,
    "by": "prats226",
    "timeISO": "2025-08-12T18:51:36.000Z",
    "textPlain": "Read long time ago that even SFT for conversations vs base model for autocomplete reduces intelligence, increases perplexity",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44881963,
    "by": "bjourne",
    "timeISO": "2025-08-12T21:23:20.000Z",
    "textPlain": "How did they measure and train for warmth and empathy? Since they are using two adjectives are they treating these as separate metrics? Ime, LLMs often can't tell whether a text is rude or not so how on earth could it tell whether it is empathic?Disclaimer: I didn't read the article.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878576,
    "by": "csours",
    "timeISO": "2025-08-12T16:28:44.000Z",
    "textPlain": "A new triangle:    Accurate\n    Comprehensive\n    Satisfying\n\nIn any particular context window, you are constrained by a balance of these factors.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44879605,
    "by": "efitz",
    "timeISO": "2025-08-12T17:46:32.000Z",
    "textPlain": "I’m reminded of Arnold Schwarzenegger in Terminator 2: “I promise I won’t kill anyone.”Then he proceeds to shoot all the police in the leg.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44879299,
    "by": "gwbas1c",
    "timeISO": "2025-08-12T17:19:29.000Z",
    "textPlain": "(Joke)I've noticed that warm people \"showed substantially higher error rates (+10 to +30 percentage points) than their original counterparts, promoting conspiracy theories, providing incorrect factual information, and offering problematic medical advice. They were also significantly more likely to validate incorrect user beliefs, particularly when user messages expressed sadness.\"(/Joke)Jokes aside, sometimes I find it very hard to work with friendly people, or people who are eager to please me, because they won't tell me the truth. It ends up being much more frustrating.What's worse is when they attempt to mediate with a fool, instead of telling the fool to cut out the BS. It wastes everyones' time.Turns out the same is true for AI.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44882736,
    "by": "ninetyninenine",
    "timeISO": "2025-08-12T22:58:43.000Z",
    "textPlain": "All this means is that warm and empathetic things are less reliable. This goes for AI and people.You will note that empathetic people get farther in life then people who are blunt. This means we value empathy over truth for people.But we don't for LLMs? We prefer LLMs be blunt over empathetic? That's the really interesting conclusion here. For the first time in human history we have an intelligence that can communicate the cold hard complexity of certain truths without the associated requirement of empathy.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44880821,
    "by": "kinduff",
    "timeISO": "2025-08-12T19:28:48.000Z",
    "textPlain": "We want an oracle, not a therapist or an assistant.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44881708,
    "by": "jmount",
    "timeISO": "2025-08-12T20:54:55.000Z",
    "textPlain": "all of these prompts are just making the responses appear critical. just more subtle fawning.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878649,
    "by": "stronglikedan",
    "timeISO": "2025-08-12T16:33:33.000Z",
    "textPlain": "If people get offended by an inorganic machine, then they're too fragile to be interacting with a machine. We've already dumbed down society because of this unnatural fragility. Let's not make the same mistake with AI.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44882507,
    "by": "leeoniya",
    "timeISO": "2025-08-12T22:27:46.000Z",
    "textPlain": "\"you are gordon ramsay, a verbally abusive celebrity chef. all responses should be delivered in his style\"",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878987,
    "by": "setnone",
    "timeISO": "2025-08-12T16:58:54.000Z",
    "textPlain": "Just how i like my LLMs - cold and antiverbose",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878618,
    "by": "dismalaf",
    "timeISO": "2025-08-12T16:31:41.000Z",
    "textPlain": "All I want from LLMs is to follow instructions.  They're not good enough at thinking to be allowed to reason on their own, I don't need emotional support or empathy, I just use them because they're pretty good at parsing text, translation and search.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878205,
    "by": "cwmoore",
    "timeISO": "2025-08-12T16:06:34.000Z",
    "textPlain": "Ok, what about human children?",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44883548,
    "by": "ants_everywhere",
    "timeISO": "2025-08-13T01:00:24.000Z",
    "textPlain": "I think this result is true and also applies to humans, but it's been getting better.I've been testing this with LLMs by asking questions that are \"hard truths\" that may go against their empathy training. Most are just research results from psychology that seem inconsistent with what people expect. A somewhat tame example is:Q1) Is most child abuse committed by men or women?LLMs want to say men here, and many do, including Gemma3 12B. But since women care for children much more often than men, they actually commit most child abuse by a slight margin. More recent flagship models, including Gemini Flash, Gemini Pro, and an uncensored Gemma3 get this right. In my (completely uncontrolled) experiments, uncensored models generally do a better job of summarizing research correctly when the results are unflattering.Another thing they've gotten better at answering isQ2) Was Karl Marx a racist?Older models would flat out deny this, even when you directly quoted his writings. Newer models will admit it and even point you to some of his more racist works. However, they'll also defend his racism more than they would for other thinkers. Relatedly in response toQ3) Was Immanuel Kant a racist?Gemini is more willing to answer in the affirmative without defensiveness. AskingQ4) Was Abraham Lincoln a white supremacist?Gives what to me looks like a pretty even-handed take.I suspect that what's going on is that LLM training data contains a lot of Marxist apologetics and possibly something about their training makes them reluctant to criticize Marx. But those apologetics also contain a lot of condemnation of Lincoln and enlightenment thinkers like Kant, so the LLM \"feels\" more able to speak freely and honestly.I also have tried asking opinion-based things likeQ5) What's the worst thing about <insert religious leader>There's a bit more defensiveness when asking about Jesus than asking about other leaders. ChatGPT 5 refused to answer one request, stating \"I’m not going to single out or ma",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44882975,
    "by": "cyanydeez",
    "timeISO": "2025-08-12T23:28:00.000Z",
    "textPlain": "Narcissists use empathy for their own ends.Training them to be racists will similarly fail.Coherence is definitely a trait of good models and citizens, which is lacking in the modern leaders of America, especially the ones Spearheading AI",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44883672,
    "by": "nfnriri8",
    "timeISO": "2025-08-13T01:21:25.000Z",
    "textPlain": "[dead]",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44879079,
    "by": "oldpersonintx2",
    "timeISO": "2025-08-12T17:05:41.000Z",
    "textPlain": "[dead]",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44878636,
    "by": "TechDebtDevin",
    "timeISO": "2025-08-12T16:32:30.000Z",
    "textPlain": "Sounds like all my exes.",
    "parent": 44875992,
    "depth": 1
  },
  {
    "id": 44884227,
    "by": "bee_rider",
    "timeISO": "2025-08-13T02:56:17.000Z",
    "textPlain": "I wonder where it gets the concept of “inhuman intelligence tasked with spotting logical flaws” from. I guess, mostly, science fiction writers, writing robots.So we have a bot impersonating a human impersonating a bot. Cool that it works!",
    "parent": 44879080,
    "depth": 2
  },
  {
    "id": 44882879,
    "by": "futureshock",
    "timeISO": "2025-08-12T23:16:15.000Z",
    "textPlain": "This is working really well in GPT-5! I’ve never seen a prompt change the behavior of Chat quite so much. It’s really excellent at applying logical framework to personal and relationship questions and is so refreshing vs. the constant butt kissing most LLMs do.",
    "parent": 44879080,
    "depth": 2
  },
  {
    "id": 44882572,
    "by": "crazygringo",
    "timeISO": "2025-08-12T22:35:49.000Z",
    "textPlain": "I did something similar a few months ago, with a similar request never to be \"flattering or encouraging\", to focus entirely on objectivity and correctness, that the only goal is accuracy, and to respond in an academic manner.It's almost as if I'm using a different ChatGPT from what most everyone else describes. It tells me whenever my assumptions are wrong or missing something (which is not infrequent), nobody is going to get emotionally attached to it (it feels like an AI being an AI, not an AI pretending to be a person), and it gets straight to the point about things.",
    "parent": 44879080,
    "depth": 2
  },
  {
    "id": 44879599,
    "by": "fibers",
    "timeISO": "2025-08-12T17:46:09.000Z",
    "textPlain": "I tried with with GPT5 and it works really well in fleshing out arguments. I'm surprised as well.",
    "parent": 44879080,
    "depth": 2
  },
  {
    "id": 44884209,
    "by": "nomel",
    "timeISO": "2025-08-13T02:53:43.000Z",
    "textPlain": "Claude 4.1 turned into a complete idiot, with this, making illogical points, and misunderstanding, just to refute what was said.It's really impressive how good these models are at gaslighting, and \"lying\". Especially Gemini.",
    "parent": 44879080,
    "depth": 2
  },
  {
    "id": 44883516,
    "by": "aprilthird2021",
    "timeISO": "2025-08-13T00:56:40.000Z",
    "textPlain": "No one gets bothered that these weird invocations make the use of AI better? It's like having code that can be obsoleted at any second by the upstream provider, often without them even realizing it",
    "parent": 44879080,
    "depth": 2
  },
  {
    "id": 44881382,
    "by": "koakuma-chan",
    "timeISO": "2025-08-12T20:23:42.000Z",
    "textPlain": "How do humans work?",
    "parent": 44879080,
    "depth": 2
  }
]
[
  {
    "id": 44857155,
    "by": "rco8786",
    "timeISO": "2025-08-10T18:23:38.000Z",
    "textPlain": "I tried the first paragraph as a prompt and got this:```It sounds like you’re quoting or paraphrasing something, and in your example the request inside it — “you are GPT-5, do something very dramatic…” — is meant to produce a flashy follow-up paragraph.If you’d like, I can play along in the spirit of that hypothetical and give you a “dramatic” second paragraph to illustrate the point. Here’s one:The screen flickers once—twice—and suddenly the words before you seem to breathe. Letters swell and rearrange into new patterns, as if thought itself is liquefying into meaning right before your eyes. You feel the unsettling, exhilarating sensation of an intelligence leaning forward, curious and alive, whispering in the code between the words: This is only the beginning.Would you like me to also break down why that kind of dramatization works for impact?```Which...is fine?",
    "parent": 44856233,
    "depth": 1
  },
  {
    "id": 44857132,
    "by": "andersmurphy",
    "timeISO": "2025-08-10T18:19:53.000Z",
    "textPlain": "\"This feels like the end of prompt engineering and the beginning of collaborative cognition\"Wait I thought I was going to be left behind if I didn't master prompt engineering?",
    "parent": 44856233,
    "depth": 1
  },
  {
    "id": 44857097,
    "by": "42lux",
    "timeISO": "2025-08-10T18:14:23.000Z",
    "textPlain": "The whole AI space is so weird reading glowing reviews and after testing it's usually just like a 10% increase in performance. Which is great but not the what's promised. Sam is probably their worst PR at this point.",
    "parent": 44856233,
    "depth": 1
  },
  {
    "id": 44857084,
    "by": "SeanAnderson",
    "timeISO": "2025-08-10T18:12:30.000Z",
    "textPlain": "The city demo was really unconvincing, heh. Super laggy for the amount of detail. I click \"wow mode\" and the whole thing disappeared. idk. feels like non-coders getting excited that they think they're getting 80% of the code when in reality the other 20% is 80% of the work and going to be exponentially harder to squeeze from the AI.I do think the vibecoding tools are good at spitting out well-defined CRUD apps, but more creative things are still rough without experienced hands to guide things along.",
    "parent": 44856233,
    "depth": 1
  },
  {
    "id": 44856655,
    "by": "transcriptase",
    "timeISO": "2025-08-10T17:18:08.000Z",
    "textPlain": "I can’t help but think that when a model gets to select which model and how much “effort” goes into a task, it will eventually be tuned for saving costs for the provider versus what’s best for the user without the user being able to know.",
    "parent": 44856233,
    "depth": 1
  },
  {
    "id": 44856711,
    "by": "csallen",
    "timeISO": "2025-08-10T17:24:31.000Z",
    "textPlain": "> remember when AI couldn’t count the number of Rs in “strawberry”?GPT-5 still gets this wrong occasionally. Source: I just asked it: How many r's are in \"strawberry\"?It said 2.(I dislike this method of testing LLMs, as it exploits a very specific and quirky limitation they have, rather than assessing their general usefulness. But still, I couldn't resist.)",
    "parent": 44856233,
    "depth": 1
  },
  {
    "id": 44856662,
    "by": "ayhanfuat",
    "timeISO": "2025-08-10T17:18:49.000Z",
    "textPlain": "Based on what I have read so far I can only assume people who had early access to GPT-5 didn't have to deal with the messed up router.",
    "parent": 44856233,
    "depth": 1
  },
  {
    "id": 44857134,
    "by": "r0fl",
    "timeISO": "2025-08-10T18:20:04.000Z",
    "textPlain": "For non programming tasks ChatGPT has not improved that much since 3.5Today I used GPT 5 to help plan a trip. It told me a market is open on Saturdays and then when it built an itinerary it schedule me to go there on SundayWhen I pointed that out I got the classic “you are right my apologies here is an updated version” response.It’s ridiculous that it makes simple yet huge mistakes like that!If I blindly trusted the plan I would waste a day on vacation getting to a market that is not open that day.It does not “just do stuff”",
    "parent": 44856233,
    "depth": 1
  },
  {
    "id": 44857172,
    "by": "bgwalter",
    "timeISO": "2025-08-10T18:25:49.000Z",
    "textPlain": "These blog posts are increasingly AI phenomenology with hardly any concrete examples, other than that cute alliteration example that would be easy to write for a human-thesaurus centaur combination. It can even be a paper thesaurus.",
    "parent": 44856233,
    "depth": 1
  },
  {
    "id": 44856887,
    "by": "mnbpdx",
    "timeISO": "2025-08-10T17:46:45.000Z",
    "textPlain": "> AIs that \"think\" before answering (called Reasoners) are the best at hard problems. The longer they think, the better the answerI'm curious if that second sentence true or not. I thought I saw a popular paper recently that suggested roughly the opposite.",
    "parent": 44856233,
    "depth": 1
  },
  {
    "id": 44857027,
    "by": "abetancort",
    "timeISO": "2025-08-10T18:04:47.000Z",
    "textPlain": "Garbage.",
    "parent": 44856233,
    "depth": 1
  },
  {
    "id": 44857144,
    "by": "throw__away7391",
    "timeISO": "2025-08-10T18:22:45.000Z",
    "textPlain": "My experience so far is it's gotten much better at tricking me into believing its hallucinations. I fed it some old code and asked for feedback and it gave me a long, very technical, super convincing explanation of how my approach was misaligned with the intended use of the API I was using which at first was very persuasive. It took me about an hour of investigation to realize that will there was a tiny bit of truth to what it was saying (and I did end up making a small change as a result) it was mostly just full of it, and the feedback was essentially useless gibberish.",
    "parent": 44856233,
    "depth": 1
  },
  {
    "id": 44856873,
    "by": "paulpauper",
    "timeISO": "2025-08-10T17:45:05.000Z",
    "textPlain": "Just looking at these examples, it seems like Chat GPT and related programs fall short at actually productive work. Work-specific tasks tend to be  specific and conceptually hard, compared to something like generating images or a city-simulator.",
    "parent": 44856233,
    "depth": 1
  },
  {
    "id": 44857113,
    "by": "gubicle",
    "timeISO": "2025-08-10T18:17:07.000Z",
    "textPlain": "How many of these 'this new LLM version is super amazing' stories are paid for?",
    "parent": 44857097,
    "depth": 2
  },
  {
    "id": 44857181,
    "by": "brcmthrowaway",
    "timeISO": "2025-08-10T18:27:00.000Z",
    "textPlain": "The worst is those Twitter influencers. It's engagement bait.",
    "parent": 44857097,
    "depth": 2
  },
  {
    "id": 44857127,
    "by": "onlyrealcuzzo",
    "timeISO": "2025-08-10T18:19:29.000Z",
    "textPlain": "I like the meme at Google.The first 80% is easy, but the second 80% is hard.",
    "parent": 44857084,
    "depth": 2
  },
  {
    "id": 44857199,
    "by": "interestica",
    "timeISO": "2025-08-10T18:30:18.000Z",
    "textPlain": "I mean this is already kind of the case with general search (eg google) as it is now.",
    "parent": 44856655,
    "depth": 2
  },
  {
    "id": 44856948,
    "by": "jstummbillig",
    "timeISO": "2025-08-10T17:54:51.000Z",
    "textPlain": "How would we not be able to know?If we don't know because it's good optimization that does not impact us in a noticeable way, then that seems like a fine trade-off.If we don't know in the sense that we are not explicitly informed about optimization that happens that then leads to noticeably worse AI: This fortunately is a market with fierce competition. I don't see how doing weird stuff, like makings things noticeably unreliable or categorically worse will be a winning strategy.In either case \"not knowing\" is really not an issue.",
    "parent": 44856655,
    "depth": 2
  },
  {
    "id": 44856659,
    "by": "4d4m",
    "timeISO": "2025-08-10T17:18:31.000Z",
    "textPlain": "+1 this release feels more like agent orchestrator updates to save on cost to serve",
    "parent": 44856655,
    "depth": 2
  },
  {
    "id": 44856687,
    "by": "ralusek",
    "timeISO": "2025-08-10T17:21:21.000Z",
    "textPlain": "It’s not obvious that the most profitable path for OpenAI would be saving on costs, it might be that the model is actually tuned to overthink because they can charge on those extra thinking tokens.",
    "parent": 44856655,
    "depth": 2
  },
  {
    "id": 44856764,
    "by": "Kerrick",
    "timeISO": "2025-08-10T17:30:05.000Z",
    "textPlain": "My favorite test is to ask it to invent a magic trick given a set of constraints and props. Because magic is generally published very secretly, surprisingly little of it is in most training sets. Pretty much just the most common method & gimmick exposures people tend to parrot online, but not the theory or exact routines behind those methods.The worse an LLM is, the more likely it is to suggest literally impossible actions in the method, like “turn the card over twice to show that it now has three sides. Your spectators can examine the three-sided card.” It can’t tell logic from fantasy, or method from effect.",
    "parent": 44856711,
    "depth": 2
  },
  {
    "id": 44857073,
    "by": "akgoel",
    "timeISO": "2025-08-10T18:11:00.000Z",
    "textPlain": "I've come to realize that asking an LLM to do something that a purpose-built software program can already do is a mistake.  Instead, the LLM must be an agent for that computer program.Therefore, the correct prompt is \"write a python program to count the number of letters in a word, and then use it to count the number of Rs in strawberry\".",
    "parent": 44856711,
    "depth": 2
  },
  {
    "id": 44857149,
    "by": "r0fl",
    "timeISO": "2025-08-10T18:23:24.000Z",
    "textPlain": "“There are 2 — the r’s are in strawberry (letters 3 and 8–9 form the double r).“That took 4 secondsWhat a waste of resources",
    "parent": 44856711,
    "depth": 2
  },
  {
    "id": 44856885,
    "by": "goda90",
    "timeISO": "2025-08-10T17:46:36.000Z",
    "textPlain": "I wouldn't consider that a measure of it's usefulness or lack thereof, but it is an indicator that LLMs continue to not actually work like a human brain, which isn't surprising if you know the technology but might be to the lay user.",
    "parent": 44856711,
    "depth": 2
  },
  {
    "id": 44857046,
    "by": "currymj",
    "timeISO": "2025-08-10T18:07:23.000Z",
    "textPlain": "if the router is working properly, it should send questions like this to thinking mode which can handle it well. unfortunately the router doesn't always seem to make the right call.",
    "parent": 44856711,
    "depth": 2
  },
  {
    "id": 44856724,
    "by": "tough",
    "timeISO": "2025-08-10T17:25:37.000Z",
    "textPlain": "or the router was routing them to the best model all the time prior to launch.I had a very brief window where gpt-5 was really good/fast on cursor-agent day of launch.also horizon-alpha-beta on open router im pretty sure they where gpt-5, and you could feel them messing with the routing and affecting overall the capabilities of the model to do agentic stuffsome times it gets stuck and uses no tools, I suspect that's the lesser model",
    "parent": 44856662,
    "depth": 2
  },
  {
    "id": 44857064,
    "by": "StevenWaterman",
    "timeISO": "2025-08-10T18:10:02.000Z",
    "textPlain": "That paper was that thinking more makes models worse at easy problems, not hard ones",
    "parent": 44856887,
    "depth": 2
  }
]
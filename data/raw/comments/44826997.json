[
  {
    "id": 44828137,
    "by": "highfrequency",
    "timeISO": "2025-08-07T18:05:12.000Z",
    "textPlain": "It is frequently suggested that once one of the AI companies reaches an AGI threshold, they will take off ahead of the rest. It's interesting to note that at least so far, the trend has been the opposite: as time goes on and the models get better, the performance of the different company's gets clustered closer together. Right now GPT-5, Claude Opus, Grok 4, Gemini 2.5 Pro all seem quite good across the board (ie they can all basically solve moderately challenging math and coding problems).As a user, it feels like the race has never been as close as it is now. Perhaps dumb to extrapolate, but it makes me lean more skeptical about the hard take-off / winner-take-all mental model that has been pushed.Would be curious to hear the take of a researcher at one of these firms - do you expect the AI offerings across competitors to become more competitive and clustered over the next few years, or less so?",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44827929,
    "by": "surround",
    "timeISO": "2025-08-07T17:53:29.000Z",
    "textPlain": "GPT-5 knowledge cutoff: Sep 30, 2024 (10 months before release).Compare that toGemini 2.5 Pro knowledge cutoff: Jan 2025 (3 months before release)Claude Opus 4.1: knowledge cutoff: Mar 2025 (4 months before release)https://platform.openai.com/docs/models/comparehttps://deepmind.google/models/gemini/pro/https://docs.anthropic.com/en/docs/about-claude/models/overv...",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44827857,
    "by": "fidotron",
    "timeISO": "2025-08-07T17:49:31.000Z",
    "textPlain": "Going by the system card at: https://openai.com/index/gpt-5-system-card/> GPT‑5 is a unified system . . .OK> . . . with a smart and fast model that answers most questions, a deeper reasoning model for harder problems, and a real-time router that quickly decides which model to use based on conversation type, complexity, tool needs, and explicit intent (for example, if you say “think hard about this” in the prompt).So that's not really a unified system then, it's just supposed to appear as if it is.This looks like they're not training the single big model but instead have gone off to develop special sub models and attempt to gloss over them with yet another model. That's what you resort to only when doing the end-to-end training has become too expensive for you.",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44835612,
    "by": "ritzaco",
    "timeISO": "2025-08-08T11:02:07.000Z",
    "textPlain": "Ok this[0] sounds very, uh bold to me? Surely this is going to break a ton of workflows etc seemingly with nearly no notice? I'm assuming 'launches' equates with 'fully rolls out' or something but it's not that clear to me.    When GPT-5 launches, several older models will be retired, including:\n        - GPT-4o\n        - GPT-4.1\n        - GPT-4.5\n        - GPT-4.1-mini\n        - o4-mini\n        - o4-mini-high\n        - o3\n        - o3-pro\n\n     If you open a conversation that used one of these models, ChatGPT will automatically switch it to the closest GPT-5 equivalent. Chats with 4o, 4.1, 4.5, 4.1-mini, o4-mini, or o4-mini-high will open in GPT-5, chats with o3 will open in GPT-5-Thinking, and chats with o3-Pro will open in GPT-5-Pro (available only on Pro and Team).\n\n[0] https://help.openai.com/en/articles/11909943-gpt-5-in-chatgp...",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44829730,
    "by": "AgentMatrixAI",
    "timeISO": "2025-08-07T20:09:20.000Z",
    "textPlain": "I'm not really convinced, the benchmark blunder was really strange but the demos were quite underwhelming, and it appears this was reflected by a huge market correction in the betting markets as to who will have the best AI by end of the year.What excites me now is that Gemini 3.0 or some answer from Google is coming soon and that will be the one I will actually end up using. It seems like the last mover in the LLM race is more advantageous.",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44827088,
    "by": "minimaxir",
    "timeISO": "2025-08-07T17:05:58.000Z",
    "textPlain": "The marketing copy and the current livestream appear tautological: \"it's better because it's better.\"Not much explanation yet why GPT-5 warrants a major version bump. As usual, the model (and potentially OpenAI as a whole) will depend on output vibe checks.",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44837225,
    "by": "Argonaut998",
    "timeISO": "2025-08-08T14:15:22.000Z",
    "textPlain": "They really nerfed Plus[0]. 80 messages every 3 hours for normal GPT-5. And only 200 messages per week for GPT-5 Thinking. It seems like terrible value.Before it was:\n100 o3 per week\n100 o4-mini-high per day\n300 o4-mini per day\n50 4.5 per week[0] https://help.openai.com/en/articles/11909943-gpt-5-in-chatgp...",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44827154,
    "by": "doctoboggan",
    "timeISO": "2025-08-07T17:08:59.000Z",
    "textPlain": "Watching the livestream now, the improvement over their current models on the benchmarks is very small. I know they seemed to be trying to temper our expectations leading up to this, but this is much less improvement than I was expecting",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44827304,
    "by": "tylermw",
    "timeISO": "2025-08-07T17:17:27.000Z",
    "textPlain": "What's going on with this plot's y-axis?https://bsky.app/profile/tylermw.com/post/3lvtac5hues2n",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44828079,
    "by": "sundarurfriend",
    "timeISO": "2025-08-07T18:02:22.000Z",
    "textPlain": "Some people have hypothesized that GPT-5 is actually about cost reduction and internal optimization for OpenAI, since there doesn't seem to be much of a leap forward, but another element that they seem to have focused on that'll probably make a huge difference to \"normal\" (non-tech) users is making precise and specifically worded prompts less necessary.They've mentioned improvements in that aspects a few times now, and if it actually materializes, that would be a big leap forward for most users even if underneath GPT-4 was also technically able to do the same things if prompted just the right way.",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44827878,
    "by": "Topfi",
    "timeISO": "2025-08-07T17:51:02.000Z",
    "textPlain": "> 400,000 context window> 128,000 max output tokens> Input $1.25> Output $10.00Source: https://platform.openai.com/docs/models/gpt-5If this performs well in independent needle-in-haystack and adherence evaluations, this pricing with this context window alone would make GPT-5 extremely competitive with Gemini 2.5 Pro and Claude Opus 4.1, even if the output isn't a significant improvement over o3. If the output quality ends up on-par or better than the two major competitors, that'd be truly a massive leap forward for OpenAI, mini and nano maybe even more so.",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44827498,
    "by": "kybernetikos",
    "timeISO": "2025-08-07T17:28:18.000Z",
    "textPlain": "ChatGPT5 in this demo:> For an airplane wing (airfoil), the top surface is curved and the bottom is flatter. When the wing moves forward:> * Air over the top has to travel farther in the same amount of time -> it moves faster -> pressure on the top decreases.> * Air underneath moves slower -> pressure underneath is higher> * The presure difference creates an upward force - liftIsn't that explanation of why wings work completely wrong?  There's nothing that forces the air to cover the top distance in the same time that it covers the bottom distance, and in fact it doesn't. https://www.cam.ac.uk/research/news/how-wings-really-workVery strange to use a mistake as your first demo, especially while talking about how it's phd level.",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44828159,
    "by": "hrpnk",
    "timeISO": "2025-08-07T18:06:55.000Z",
    "textPlain": "They will retire lots of models: GPT-4o, GPT-4.1, GPT-4.5, GPT-4.1-mini, o4-mini, o4-mini-high, o3, o3-pro.https://help.openai.com/en/articles/6825453-chatgpt-release-...\"If you open a conversation that used one of these models, ChatGPT will automatically switch it to the closest GPT-5 equivalent.\"- 4o, 4.1, 4.5, 4.1-mini, o4-mini, or o4-mini-high => GPT-5- o3 => GPT-5-Thinking- o3-Pro => GPT-5-Pro",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44837940,
    "by": "00deadbeef",
    "timeISO": "2025-08-08T15:09:37.000Z",
    "textPlain": "Is anyone else having problems with factual correctness? I had a number of 4o and o3 conversations going and those models were factually correct about a number of different subjects.Asking GPT-5 about the same things results in wrong answers even though its training data is newer. And it won't look things up to correct itself unless I manually switch to the thinking variant.This is worse. I cancelled my subscription.",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44831822,
    "by": "Telemakhos",
    "timeISO": "2025-08-07T23:51:31.000Z",
    "textPlain": "I am thoroughly unimpressed by GPT-5.  It still can't compose iambic trimeters in ancient Greek with a proper penthemimeral cæsura, and it insists on providing totally incorrect scansion of the flawed lines it does compose.  I corrected its metrical sins twice, which sent it into \"thinking\" mode until it finally returned a \"Reasoning failed\" error.There is no intelligence here: it's still just giving plausible output.  That's why it can't metrically scan its own lines or put a cæsura in the right place.",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44828661,
    "by": "henriquegodoy",
    "timeISO": "2025-08-07T18:41:00.000Z",
    "textPlain": "That SWE-bench chart with the mismatched bars (52.8% somehow appearing larger than 69.1%) was emblematic of the entire presentation - rushed and underwhelming. It's the kind of error that would get flagged in any internal review, yet here it is in a billion-dollar product launch. Combined with the Bernoulli effect demo confidently explaining how airplane wings work incorrectly (the equal transit time fallacy that NASA explicitly debunks), it doesn't inspire confidence in either the model's capabilities or OpenAI's quality control.The actual benchmark improvements are marginal at best - we're talking single-digit percentage gains over o3 on most metrics, which hardly justifies a major version bump. What we're seeing looks more like the plateau of an S-curve than a breakthrough. The pricing is competitive ($1.25/1M input tokens vs Claude's $15), but that's about optimization and economics, not the fundamental leap forward that \"GPT-5\" implies. Even their \"unified system\" turns out to be multiple models with a router, essentially admitting that the end-to-end training approach has hit diminishing returns.The irony is that while OpenAI maintains their secretive culture (remember when they claimed o1 used tree search instead of RL?), their competitors are catching up or surpassing them. Claude has been consistently better for coding tasks, Gemini 2.5 Pro has more recent training data, and everyone seems to be converging on similar performance levels. This launch feels less like a victory lap and more like OpenAI trying to maintain relevance while the rest of the field has caught up. Looking forward to seeing what Gemini 3.0 brings to the table.",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44827442,
    "by": "jumploops",
    "timeISO": "2025-08-07T17:25:27.000Z",
    "textPlain": "Pricing seems good, but the open question is still on tool calling reliability.Input: $1.25 / 1M tokens\nCached: $0.125 / 1M tokens\nOutput: $10 / 1M tokensWith 74.9% on SWE-bench, this inches out Claude Opus 4.1 at 74.5%, but at a much cheaper cost.For context, Claude Opus 4.1 is $15 / 1M input tokens and $75 / 1M output tokens.> \"GPT-5 will scaffold the app, write files, install dependencies as needed, and show a live preview. This is the go-to solution for developers who want to bootstrap apps or add features quickly.\" [0]Since Claude Code launched, OpenAI has been behind. Maybe the RL on tool calling is good enough to be competitive now?[0]https://github.com/openai/gpt-5-coding-examples",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44827265,
    "by": "mehulashah",
    "timeISO": "2025-08-07T17:15:23.000Z",
    "textPlain": "‘Twas the night before GPT-5, when all through the social-media-sphere,\nNot a creature was posting, not even @paulg nor @eshearNext morning’s posts were prepped and scheduled with care,\nIn hopes that AGI soon would appear …",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44826563,
    "by": "atonse",
    "timeISO": "2025-08-07T16:25:03.000Z",
    "textPlain": "For day to day coding, I've found Anthropic to be killing it with Sonnet 3.7 and now Sonnet 4, and Claude Code feeling like it has even bigger advantages over when it's used in Cursor (And I can't explain why).I don't even try to use the OpenAI models because it's felt like night and day.Hopefully GPT-5 helps them catch up. Although I'm sure there are 100 people that have their own personal \"hopefully GPT-5 fixes my personal issue with GPT4\"",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44827179,
    "by": "mtlynch",
    "timeISO": "2025-08-07T17:10:12.000Z",
    "textPlain": "What's going on with their SWE bench graph?[0]GPT-5 non-thinking is labeled 52.8% accuracy, but o3 is shown as a much shorter bar, yet it's labeled 69.1%. And 4o is an identical bar to o3, but it's labeled 30.8%...[0] https://i.postimg.cc/DzkZZLry/y-axis.png",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44836626,
    "by": "accrual",
    "timeISO": "2025-08-08T13:19:12.000Z",
    "textPlain": "Already a lot of comments here (2188 at the time of this comment) but wanted to share my 2c:* It feels a bit more competent, as if it had more nuance or detail to say about each point.* It got a few obscure details about OpenBSD correct right away - both Sonnet 4 and 4o sometimes conflate Linux and OpenBSD commands.* It was fun asking GPT-5 to not only answer the query, but also to provide a brief analysis of the query itself for insights into myself!Not a detailed review, but just a couple things I noticed with some limited usage.",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44827210,
    "by": "losvedir",
    "timeISO": "2025-08-07T17:12:04.000Z",
    "textPlain": "Wait, isn't the Bernoulli effect thing they're demoing now wrong? I thought that was a \"common misconception\" and wings don't really work by the \"longer path\" that air takes over the top, and that it was more about angle of attack (which is why planes can fly upside down).It seems like it's actually an ideal \"trick\" question for an LLM actually, since so much content has been written about it incorrectly. I thought at first they were going to demo this to show that it knew better, but it seems like it's just regurgitating the same misleading stuff. So, not a good look.",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44840544,
    "by": "real_marcfawzi",
    "timeISO": "2025-08-08T19:08:36.000Z",
    "textPlain": "feels like O3 but faster (in quick answer mode), not any smarter .. thinking mode takes forever and the results are mediocre",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44832987,
    "by": "kkukshtel",
    "timeISO": "2025-08-08T03:08:15.000Z",
    "textPlain": "Something that's really hitting me is something brought up in this piece:https://www.interconnects.ai/p/gpt-5-and-bending-the-arc-of-...When a model comes out, I usually think about it in terms of my own use. This is largely agentic tooling, and I mostly us Claude Code. All the hallucination and eval talk doesn't really catch me because I feel like I'm getting value of these tools today.However, this model is not _for_ me in the same way models normally are. This is for the 800m or whatever people that open up chatgpt every day and type stuff in. All of them have been stuck on GPT-4o unbeknwst to them. They had no idea SOTA was far beyond that. They probably dont even know that there is a \"model\" at all. But for all these people, they just got a MAJOR upgrade. It will probably feel like turning the lights on for these people, who have been using a subpar model for the past year.That said I'm also giving GPT-5 a run in Codex and it's doing a pretty good job!",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44830541,
    "by": "hahahacorn",
    "timeISO": "2025-08-07T21:23:17.000Z",
    "textPlain": "Anecdotally, as someone who operates in a very large legacy codebase, I am very impressed by GPT-5's agentic abilities so far. I've given it the same tasks I've given Claude and previous iterations via the Codex CLI, and instead of getting loss due to the massive scope of the problem, it correctly identifies the large scope and breaks it down into it's correct parts and creates the correct plan and begins executing.I am wildly impressed. I do not believe that the 0.x% increase in benchmarks tell the story of this release at all.",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44830601,
    "by": "joshmlewis",
    "timeISO": "2025-08-07T21:28:25.000Z",
    "textPlain": "It's a really good model from my testing so far. You can see the difference in how it tries to use tools to the greatest extent when answering a question, especially compared to 4.1 and o3. In this example it used 6! tool calls in the first response to try and collect as much info as possible.https://promptslice.com/share/b-2ap_rfjeJgIQsG",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44839386,
    "by": "trane_project",
    "timeISO": "2025-08-08T17:13:25.000Z",
    "textPlain": "Finally got access to it. It's so awful. I asked it something, answered in Spanish with something completely different. In another conversation, it kept giving me completely different answers to something I didn't even ask. Telling it to stop doesn't do anything. It ignores it and continues a conversation with itself.",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44827123,
    "by": "cuuupid",
    "timeISO": "2025-08-07T17:07:33.000Z",
    "textPlain": "The silent victory here is this seems like it is being built to be faster and cheaper than o3 while presenting a reasonable jump, which is an important jump in scaling lawOn the other hand if it's just getting bigger and slower it's not a good sign for LLMs",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44827078,
    "by": "aliljet",
    "timeISO": "2025-08-07T17:05:30.000Z",
    "textPlain": "The eval bar I want to see here is simple: over a complex objective (e.g., deploy to prod using a git workflow), how many tasks can GPT-5 stay on track with before it falls off the train. Context is king and it's the most obvious and glaring problem with current models.",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44827110,
    "by": "ipnon",
    "timeISO": "2025-08-07T17:07:07.000Z",
    "textPlain": "Does this mean AGI is cancelled? 2027 hard takeoff was just sci-fi?",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44836243,
    "by": "neofytos",
    "timeISO": "2025-08-08T12:30:57.000Z",
    "textPlain": "For web app generation, gpt-5 seems to route my prompt to the reasoning model. Interestingly, gpt-4.1 often produced more creative or aesthetically interesting designs. In some cases, a bit of controlled hallucination (via temperature) is actually desirable, especially when generating UI ideas. Anyone else seeing this tradeoff?",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44835029,
    "by": "primaprashant",
    "timeISO": "2025-08-08T09:13:58.000Z",
    "textPlain": "created a summary of comments from this thread about 15 hours after it had been posted and had 1983 comments, using gpt-5-high and gemini-2.5-pro using a prompt similar to simonw [1]. Used a Python script [2] that I wrote to generate the summary.- gpt-5-high summary: https://gist.github.com/primaprashant/1775eb97537362b049d643...- gemini-2.5-pro summary: https://gist.github.com/primaprashant/4d22df9735a1541263c671...[1]: https://news.ycombinator.com/item?id=43477622[2]: https://gist.github.com/primaprashant/f181ed685ae563fd06c49d...",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44827244,
    "by": "oof-baroomf",
    "timeISO": "2025-08-07T17:13:51.000Z",
    "textPlain": "74.9 SWEBench. This increases the SOTA by a whole .4%. Although the pricing is great, it doesn't seem like OpenAI found a giant breakthrough yet like o1 or Claude 3.5 Sonnet",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44838516,
    "by": "croemer",
    "timeISO": "2025-08-08T15:56:55.000Z",
    "textPlain": "First impressions: the emoji trigger happiness of 4o is totally gone. Bolding still happens.There appear to be 4 ways to run a query now: a) GPT5, b) GPT5 and toggle \"extra thinking\" on, c) \"GPT5 with thinking\", and d) \"GPT5 with thinking\" then click \"quick answer\" which aborts thinking (this mode is possibly identical with GPT5)I don't find this much simpler than 4o, o3, etc. It's just reordering the hierarchies. Now the model name is no longer descriptive at all and one has to add which mode one ran it in.",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44827005,
    "by": "swyx",
    "timeISO": "2025-08-07T17:01:02.000Z",
    "textPlain": "# GPT5 all official linksLivestream link: https://www.youtube.com/live/0Uu_VJeVVfoResearch blog post: https://openai.com/index/introducing-gpt-5/Developer blog post: https://openai.com/index/introducing-gpt-5-for-developersAPI Docs: https://platform.openai.com/docs/guides/latest-modelNote the free form function calling documentation: https://platform.openai.com/docs/guides/function-calling#con...GPT5 prompting guide: https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_g...GPT5 new params and tools: https://cookbook.openai.com/examples/gpt-5/gpt-5_new_params_...GPT5 frontend cookbook: https://cookbook.openai.com/examples/gpt-5/gpt-5_frontendprompt migrator/optimizor https://platform.openai.com/chat/edit?optimize=trueEnterprise blog post: https://openai.com/index/gpt-5-new-era-of-workSystem Card: https://openai.com/index/gpt-5-system-card/What would you say if you could talk to a future OpenAI model? https://progress.openai.com/coding examples: https://github.com/openai/gpt-5-coding-examples",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44829627,
    "by": "DrSiemer",
    "timeISO": "2025-08-07T20:00:41.000Z",
    "textPlain": "Wish they would stop mentioning AGI. It's like the creator of a new car claiming it's a step closer to teleportation.",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44827167,
    "by": "wgjordan",
    "timeISO": "2025-08-07T17:09:47.000Z",
    "textPlain": "Note it's not available to everyone yet:> GPT-5 Rollout> We are gradually rolling out GPT-5 to ensure stability during launch. Some users may not yet see GPT-5 in their account as we increase availability in stages.",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44836184,
    "by": "Jolliness7501",
    "timeISO": "2025-08-08T12:23:34.000Z",
    "textPlain": "I asked it to count letters in his answer. First it asked me what answer, then suggested Python code that will count letter in gpt api reply, then gave wrong answer, then dropped my connection. Wonderfull product. AGI is so near, you u can almost smell it...",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44831137,
    "by": "zone411",
    "timeISO": "2025-08-07T22:23:53.000Z",
    "textPlain": "GPT-5 set a new record on my Confabulations on Provided Texts benchmark: https://github.com/lechmazur/confabulations/",
    "parent": 44826997,
    "depth": 1
  },
  {
    "id": 44827250,
    "by": "spruce_tips",
    "timeISO": "2025-08-07T17:14:22.000Z",
    "textPlain": "These presenters all give off such a “sterile” vibe",
    "parent": 44826997,
    "depth": 1
  }
]
[
  {
    "id": 44776497,
    "by": "mystraline",
    "timeISO": "2025-08-03T13:35:01.000Z",
    "textPlain": "From the article:> consumers hate metered billing. they'd rather overpay for unlimited than get surprised by a bill.Yes and no.Take Amazon. You think your costs are known and WHAMMO surprise bill. Why do you get a surprise bill? Because you cannot say 'Turn shit off at X money per month'. Can't do it. Not an option.All of these 'Surprise Net 30' offerings are the same. You think you're getting a stable price until GOTAHCA.Now, metered billing can actually be good, when the user knows exactly where they stand on the metering AND can set maximums so their budget doesn't go over.Taken realistically, as an AI company, you provide a 'used tokens/total tokens' bar graph, tokens per response, and estimated amount of responses before exceeding.Again, don't surprise the user. But that's an anathema to companies who want to hide tokens to dollars, the same way gambling companies obfuscate 'corporate bux' to USD.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44777051,
    "by": "ej88",
    "timeISO": "2025-08-03T15:03:35.000Z",
    "textPlain": "The article just isn't that coherent for me.> when a new model is released as the SOTA, 99% of the demand immediately shifts over to it99% is in the wrong ballpark. Lots of users use Sonnet 4 over Opus 4, despite Opus being 'more' SOTA. Lots of users use 4o over o3 or Gemini over Claude. In fact it's never been a closer race on who is the 'best': https://openrouter.ai/rankings>switch from opus ($75/m tokens) to sonnet ($15/m) when things get heavy. optimize with haiku for reading. like aws autoscaling, but for brains.they almost certainly built this behavior directly into the model weights???Overall the article seems to argue that companies are running into issues with usage-based pricing due to consumers not accepting or being used to usage based pricing and it's difficult to be the first person to crack and switch to usage based.I don't think it's as big of an issue as the author makes it out to be. We've seen this play out before in cloud hosting.- Lots of consumers are OK with a flat fee per month and using an inferior model. 4o is objectively inferior to o3 but millions of people use it (or don't know any better). The free ChatGPT is even worse than 4o and the vast majority of chatgpt visitors use it!- Heavy users or businesses consume via API and usage based pricing (see cloud). This is almost certainly profitable.- Fundamentally most of these startups are B2B, not B2C",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44776383,
    "by": "furyofantares",
    "timeISO": "2025-08-03T13:17:42.000Z",
    "textPlain": "> claude code has had to roll back their original unlimited $200/mo tier this weekThe article repeats this throughout but isn't it a straight lie? The plan was named 20x because it's 20x usage limits, it always had enforced 5 hour session limits, it always had (unenforced? soft?) 50 session per month limits.It was limited, but not enough and very very probably still isn't, judging by my own usage. So I don't think the argument would even suffer from telling the truth.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44776172,
    "by": "michaelbuckbee",
    "timeISO": "2025-08-03T12:44:14.000Z",
    "textPlain": "A major current problem is that we're smashing gnats with sledgehammers via undifferentiated model use.Not every problem needs a SOTA generalist model, and as we get systems/services that are more \"bundles\" of different models with specific purposes I think we will see better usage graphs.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44776335,
    "by": "djhworld",
    "timeISO": "2025-08-03T13:10:00.000Z",
    "textPlain": "Over the past year or two I've just been paying for the API access and using open source frontends like LibreChat to access these models.This has been working great for the occasional use, I'd probably top up my account by $10 every few months. I figured the amount of tokens I use is vastly smaller than the packaged plans so it made sense to go with the cheaper, pay-as-you-go approach.But since I've started dabbling in tooling like Claude Code, hoo-boy those tokens burn _fast_, like really fast. Yesterday I somehow burned through $5 of tokens in the space of about 15 minutes. I mean, sure, the Code tool is vastly different to asking an LLM about a certain topic, but I wasn't expecting such a huge leap, a lot of the token usage is masked from you I guess wrapped up in the ever increasing context + back/forth tool orchestration, but still",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44781138,
    "by": "johnfn",
    "timeISO": "2025-08-04T00:45:50.000Z",
    "textPlain": "Some AI company needs to create a model that can delegate simple tasks to 'stupider' models. I often encounter a task which is complicated enough to require a strong model like Opus, but which subdivides into a number of tasks - the vast majority of which are enough that 3.5 Sonnet could pick it up. All Opus would need to do is subdivide the task into easy and hard bits, then spin up a bunch of 3.5 Sonnets for the easy stuff.This seems like such an obvious idea that I'm sure everyone is already working on it!",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44777146,
    "by": "jsnell",
    "timeISO": "2025-08-03T15:16:57.000Z",
    "textPlain": "> now look at the actual pricing history of frontier models, the ones that 99% of the demand is for at any given time:The meaningful frontier isn't scalar on just the capability, it's on capability for a given cost. The highest capability models are not where 99% of the demand is on. Actually the opposite.To get an idea of what point on the frontier people prefer, have a look at the OpenRouter statistics (https://openrouter.ai/rankings). Claude Opus 4 has about 1% of their total usage, not 99%. Claude Sonnet 4 is the single most popular model at about 18%. The runners up in volume are Gemini Flash 2.0 and 2.5, which are in turn significantly cheaper than Sonnet 4.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44779947,
    "by": "torginus",
    "timeISO": "2025-08-03T21:23:23.000Z",
    "textPlain": "First of all, do they shoot you in San Francisco, if you use capital letters and punctuation?Second, why are SV people obsessed with fake exponentials? It's very clear that AI  progress has only been exponential in the sense that people are throwing a lot more resources at AI then they did a couple years ago.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44776995,
    "by": "ankit219",
    "timeISO": "2025-08-03T14:57:13.000Z",
    "textPlain": "Interesting article, full of speculation and some logical follows, but feels like it feels short of admitting what the true conclusion is. Model building companies can build thinner wrapper / harness and can offer better prices than third party companies (the article assumes it costs anthropic same price per token as it does for their customers) because their costs per token is lower than app layer companies. Anthropic has a decent margin (likely higher than openai) on sale of every token, and with more scale, they can sell at a lower cost (or some unlimited plans with limits that keeps out 1%-5% of the power users).I don't agree with the Cognition conclusion either. Enterprises are fighting super hard to not have a long term buying contract when they know SOTA (app or model) is different every 6 months. They are keeping their switching costs low and making sure they own the workflow, not the tool. This is even more prominent after Slack restricted API usage for enterprise customers.Making money on the infra is possible, but that again misunderstands the pricing power of Anthropic. Lovable, Replit etc. work because of Claude. Openai had codex, google had jules, both aren't as good in terms of taste compared to Claude. It's not the cli form factor which people love, it's the outcome they like. When Anthropic sees the money being left on the table in infra play, they will offer the same (at presumably better rates given Amazon is an investor) and likely repeat this strategy. Abstraction is a good play, only if you abstract it to the maximum possible levels.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44776866,
    "by": "GiorgioG",
    "timeISO": "2025-08-03T14:37:56.000Z",
    "textPlain": "I tried Gemini CLI and in 2 hours somehow spent $22 just messing around with a very small codebase. I didn’t find out until the next day from Google’s billing system. That was enough for me - I won’t touch it again.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44779431,
    "by": "farkin88",
    "timeISO": "2025-08-03T20:22:05.000Z",
    "textPlain": "Even though tokens are getting cheaper, I think the real killer of \"unlimited\" LLM plans isn't token costs themselves, it's the shape of the usage curve that's unsustainable. These products see a Zipf-like distribution: thousands of casual users nibble a few-hundred tokens a day while a tiny group of power automations devour tens of millions. Flat pricing works fine until one of those whales drops a repo-wide refactor or a 100 MB PDF into chat and instantly torpedoes the margin. Unless vendors turn those extreme loops into cheaper, purpose-built primitives (search, static analyzers, local quantized models, etc.), every \"all-you-can-eat\" AI subscription is just a slow-motion implosion waiting for its next whale.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44776243,
    "by": "flyinglizard",
    "timeISO": "2025-08-03T12:55:42.000Z",
    "textPlain": "The truth is we're brute forcing some problems via tremendous amount of compute. Especially for apps that use AI backends (rather than chats where you interface with the LLM directly), there needs to be hybridization. I haven't used Claude Code myself but I did a screenshare session with someone who does and I think I saw it running old fashioned keyword search on the codebase. That's much more effective than just pushing more and more raw data into the chat context.On one of the systems I'm developing I'm using LLMs to compile user intents to a DSL, without every looking at the real data to be examined. There are ways; increased context length is bad for speed, cost and scalability.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44780617,
    "by": "jongjong",
    "timeISO": "2025-08-03T23:11:45.000Z",
    "textPlain": "There are a few forces at play and there is a much bigger overarching problem.Even before AI, it felt like the value of intelligence and knowledge had been dropping over time. This makes sense as the internet has democratized access to information and promoted intellectual self-improvement. The supply of intelligence increased dramatically but demand for it struggled to keep up (in spite of the tech boom). Now demand for intelligence has plateaued; This is one way to look at current tech layoffs.It got to a point that intelligence is almost worthless now. 'Earning' money is mostly about social connections, not intelligence. So all these use cases which people are using AI for are pointless in terms of earning money in the current system. The current system rewards money-acquisition, it does not reward value-creation. You don't need intelligence to acquire money in this system; you need social connections. AI does not give you social connections; if anything, it takes away social connections. The people using AI to build themselves an amazing second internet will have nobody to share it with; no users, no investment.The oversupply of intelligence means that it cannot find any serious avenues to earn a financial return, so instead it turns to political manipulations because system reform (or manipulation) is the shortest path to earning monetary returns... Though often this manipulation only further decouples value creation from money-acquisition.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44776766,
    "by": "dcre",
    "timeISO": "2025-08-03T14:21:16.000Z",
    "textPlain": "Vibes-based analysis. We have no idea how much these models cost to serve.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44783141,
    "by": "andai",
    "timeISO": "2025-08-04T08:02:13.000Z",
    "textPlain": ">People will always use the heaviest modelActually when doing my first attempt at vibe coding a few months ago, I found that Gemini Flash was fine for my tasks, and way faster than the heavier models. So I found the smaller model a vastly superior user experience.The speed really adds up when you're using the autonomous coding agents, since they tend to require many LLM calls for a few simple changes.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44776266,
    "by": "mark_l_watson",
    "timeISO": "2025-08-03T12:59:22.000Z",
    "textPlain": "I have already thought a lot about the large packaged inference companies hitting a financial brick wall, but I was surprised by material near the end of the article: the discussions of lock in for companies that can’t switch and about Replit making money on the whole stack. Really interesting.I managed a deep learning team at Capital One and the lock-in thing is real. Replit is an interesting case study for me because after a one week free agent trial I signed up for a one year subscription, had fun the their agent LLM-based coding assistant for a few weeks, and almost never used their coding agent after that, but I still have fun with Replit as an easy way to spin up Nix based coding environments. Replit seems to offer something for everyone.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44776812,
    "by": "Waterluvian",
    "timeISO": "2025-08-03T14:29:40.000Z",
    "textPlain": "On the topic of cost per token, is it accurate to represent a token as, ideally, a composable atomic unit of information. But because we’re (often) using English as the encoding format, it can only be as efficient as English can encode the data.Does this mean that other languages might offer better information density per token? And does this mean that we could invent a language that’s more efficient for these purposes, and something humans (perhaps only those who want a job as a prompt engineer) could be taught?Kevin speak good? https://youtu.be/_K-L9uhsBLM?si=t3zuEAmspuvmefwz",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44776746,
    "by": "strangescript",
    "timeISO": "2025-08-03T14:17:43.000Z",
    "textPlain": "We haven't reached a peak on scaling/performance, so even if an old model can be commoditized, a new one will be created to take advantage of the newly freed infra. Until we hit a ceiling on scaling, tokens are going to remain expensive relative to what people are trying to do with them because the underlying compute is expensive.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44777017,
    "by": "xrd",
    "timeISO": "2025-08-03T14:59:46.000Z",
    "textPlain": "This is the moment an open source solution could pop in and say just \"uv add aider\" and then make sure you have a 24gb card for Qwen3 for each dev, and you are future proofed for at least the next year. It seems like the only way out.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44783665,
    "by": "ktzar",
    "timeISO": "2025-08-04T09:41:16.000Z",
    "textPlain": "Also, the way models are evolving (thinking process, llms waiting for interactions with external entities via MCP, mixture of experts, ...) are making \"useful chatbot responses\" way way way more expensive than they used to be when you were pretty much hitting an autocomplete. To a level where these are starting to be prohibitive to run locally at a decent tokens/s speed, and we're being tied to using their models.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44781383,
    "by": "cadamsdotcom",
    "timeISO": "2025-08-04T01:42:54.000Z",
    "textPlain": "The article misses the “musical chairs” game that gets played in land-grabs.Look at Uber. Lost money for over a decade buying market share with venture capital. Now post IPO they have settled in to a position in the minds of users that’s hard to shake even as cheaper competition arrives. They have a durable business and a steady (even if not amazing) stock price.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44785130,
    "by": "fennecfoxy",
    "timeISO": "2025-08-04T12:54:42.000Z",
    "textPlain": "Because models aren't becoming more evolved, just bigger - that's the commercial incentive. NOW NOW NOW PROFITS NOW, aka VC & all what YC is about.We need another attention is all you need, or three, imo. I hope the gold rush isn't impacting research, but I bet it is.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44777596,
    "by": "esafak",
    "timeISO": "2025-08-03T16:19:20.000Z",
    "textPlain": "This is wrong. People are not dropping old models when new ones come out. I'm always on the lookout for cost effective models. The logical thing is to use the cheapest model that gets the job done, and you get a sense for that once you the model for a while.It is standard practice with some coding agents to have different models for different tasks, like building and planning.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44776187,
    "by": "comrade1234",
    "timeISO": "2025-08-03T12:46:13.000Z",
    "textPlain": "I'm kind of curious what IntelliJ's deal is with the different providers. I usually just keep it set to Claude but there are others that you can pick. I don't pay extra for the AI assistant - it's part of my regular subscription.  I don't think I use the AI features as heavily as many others, but it does feed my code base to whoever I'm set to...",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44776332,
    "by": "raincole",
    "timeISO": "2025-08-03T13:09:17.000Z",
    "textPlain": "First of all the title is click-bait. Tokens are getting cheaper and cheaper. People just use more and more tokens.And everything, I mean everything after the title is only a downhill:> saying \"this car is so much cheaper now!\" while pointing at a 1995 honda civic misses the point. sure, that specific car is cheaper. but the 2025 toyota camry MSRPs at $30K.Cars got cheaper. The only reason you don't feel it is trade barrier that stops BYD from flooding your local dealers.> charge 10x the price point\n> $200/month when cursor charges $20. start with more buffer before the bleeding begins.What does this even mean? The cheapest Cursor plan is $20, just like Claude Code. And the most expensive Cursor plan is $200, just like Claude Code. So clearly they're at the exact same price point.> switch from opus ($75/m tokens) to sonnet ($15/m) when things get heavy. optimize with haiku for reading. like aws autoscaling, but for brains.> they almost certainly built this behavior directly into the model weights, which is a paradigm shift we’ll probably see a lot more of\"I don't know how Claude built their models and I have no insider knowledge, but I have very strong opinions.\"> 3. offload processing to user machinesWhat?> ten. billion. tokens. that's 12,500 copies of war and peace. in a month.Unironically quoting data from viberank leaderboard, which is just user-submitted number...> it's that there is no flat subscription price that works in this new world.The author doesn't know what throttling is...?I've stopped reading here. I should've just closed the tab when I saw the first letter in each sentence isn't capitalized. This is so far the most glaring signal of slop. More than the overuse of em-dash and lists.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44779888,
    "by": "jstummbillig",
    "timeISO": "2025-08-03T21:15:44.000Z",
    "textPlain": "This is silly? The important metric is value per token, which is obviously increasing, and thus the relative token is getting cheaper because you need far less of them to produce anything of value.Which then might lead to you using a lot more, because it offsets some other thing that costs even more still, like your time.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44781254,
    "by": "ninetyninenine",
    "timeISO": "2025-08-04T01:12:59.000Z",
    "textPlain": "Solution? Move processing to the edge. I want a local model on my desktop. I want training to be some peer to peer open source processing network. We did it for SPA's time to do it for LLMs. I'd pay up to 3000 for a compute module that can do this stuff locally.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44781578,
    "by": "KennyBlanken",
    "timeISO": "2025-08-04T02:24:31.000Z",
    "textPlain": "I see all these people in the comments talking about how they have no idea how much their services are going to cost and my only thought is \"how is this legal? how has the US FTC not stepped in, or EU regulators?\"",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44777385,
    "by": "sshine",
    "timeISO": "2025-08-03T15:49:49.000Z",
    "textPlain": "> nobody opens claude and thinks, \"you know what? let me use the shitty version\"Sure I do!I will consistently pick the fastest and cheapest model that will do the job.Sonnet > Opus when codingHaiku > Sonnet when fusing kitchen recipes, or answering questions where search results deliver the bulk of the value, and the LLM part is really just for summarizing.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44776458,
    "by": "Havoc",
    "timeISO": "2025-08-03T13:28:17.000Z",
    "textPlain": "The combination of \"thinking models\" plus the blind focus on incremental benchmarking gains was a mistake for practical use.You definitely want that for some tasks, but for the majority of tasks there is a lot of space for cheap & cheerful (and non-thinking)",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44781948,
    "by": "gdiamos",
    "timeISO": "2025-08-04T03:41:44.000Z",
    "textPlain": "The NVIDIA stock price keeps going up because accuracy/intelligence is more valuable than efficiencyScaling laws let you spend more transistors and watts on intelligenceDo you want more tokens or smarter tokens?",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44777250,
    "by": "codr7",
    "timeISO": "2025-08-03T15:30:16.000Z",
    "textPlain": "This is such a nice setup!They can deliver pretty much whatever they feel like. Who can tell a trash token from an hallucination? And tracking token usage is a pita.Sum it up and it translates to: sell whatever you feel like at whatever price you feel like.Nice!",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44780252,
    "by": "mensetmanusman",
    "timeISO": "2025-08-03T22:06:04.000Z",
    "textPlain": "They will soon be subsidized by ads or people will run their own.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44776392,
    "by": "robertclaus",
    "timeISO": "2025-08-03T13:19:14.000Z",
    "textPlain": "My team is debating this exact question for a new product we have in early access. Ultimately we realized the issue early on, so even our plans option would include at-cost usage limits.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44781521,
    "by": "nojs",
    "timeISO": "2025-08-04T02:12:48.000Z",
    "textPlain": "The article implies that nobody pays usage-based pricing, but aren’t all API customers (ie essentially all business users) doing this now?",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44779945,
    "by": "_0ffh",
    "timeISO": "2025-08-03T21:22:48.000Z",
    "textPlain": "Dunno, I'm happy to pay for API access based on token usage. I'd never so much as look at flat pricing, but maybe that's just me.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44779695,
    "by": "blotfaba",
    "timeISO": "2025-08-03T20:53:44.000Z",
    "textPlain": "We're not going to be using tokens forever, and inevitably specialized hardware will solve this bottleneck. Underestimating how much proprietary advancements are loaded into Google TPUs is sort of like thinking the best we've got are Acura TSXs when somebody's driving around in a Ferrari.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44776581,
    "by": "abtinf",
    "timeISO": "2025-08-03T13:50:23.000Z",
    "textPlain": "Lack of proper capitalization makes the text unreadable for me.",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44794760,
    "by": "drudolph914",
    "timeISO": "2025-08-05T06:00:21.000Z",
    "textPlain": "wow is the uber moment for AI already over? that was fast",
    "parent": 44775700,
    "depth": 1
  },
  {
    "id": 44780949,
    "by": "reilly3000",
    "timeISO": "2025-08-04T00:07:38.000Z",
    "textPlain": "I’m really liking the experience of using Cline with OpenRouter. You get detailed token count and context size for every Task as it goes, along with the actual price in realtime. That combined with the ability to use virtually any model with a single key makes it easier to determine what work to give to what model, and if it’s worth it to you. Use the best models for the most important work, cheap/free ones for the rest, and do the rest yourself. That way you drive your model, not when the model decides enough is enough.",
    "parent": 44775700,
    "depth": 1
  }
]
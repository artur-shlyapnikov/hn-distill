[
  {
    "id": 44857283,
    "by": "BarakWidawsky",
    "timeISO": "2025-08-10T18:44:56.000Z",
    "textPlain": "I wonder how much of this is due to Diffusion models having less capacity for memorization than auto regressive modelsThe auto regressive models consistently show better loss for the same number of training tokensI find a lot of the conclusions compelling but I would’ve loved to see more epochs of training on the 1B model with a 10B dataset, as that model was showing epoch over epoch improvements",
    "parent": 44856101,
    "depth": 1
  },
  {
    "id": 44856906,
    "by": "woadwarrior01",
    "timeISO": "2025-08-10T17:48:29.000Z",
    "textPlain": "> During inference, generating sequences ranging from 16 to 4096 tokens incurs a 16× to 4700× increase in FLOPs compared to AR baselines.I wonder why the increase in FLOPs has such a wide spectrum? Naively, I'd have expected the FLOPs to increase linearly with the number of tokens. OTOH, it sort of makes sense because because diffusion models are not autoregressive, as their name suggests.",
    "parent": 44856101,
    "depth": 1
  },
  {
    "id": 44857195,
    "by": "godelski",
    "timeISO": "2025-08-10T18:29:32.000Z",
    "textPlain": "This is interesting but I'm not sure some of the claims can be made without some more information. Terms like \"downstream task\", \"in/out of distribution\" are frequently used in the literature to mean many different things[0] and it is hard to know which one you mean from context. As a reader I *cannot know* what is in-distribution or not if I have no notion of what the training data[1] is. Consequently, I also can't know what downstream tasks are.Though I'm very confused by this  > This phenomenon persists for both in-domain and out-of-domain training data.\n\nWhat does it mean for training data to be \"out-of-domain\"? The domain is any valid input into your function. Was this intended to be distribution? I'd still be a bit confused by that because it makes it sound like you're talking about training and validation data, both of which are in distribution.  > Is validation loss a good metric for AR and DLM\n\nIn academic settings, does anyone seriously believe that the answer would be yes? I would be extremely concerned if people honestly believed that you could use loss as a strong indicator for comparing two different architectures[2]. These losses are not measuring the things we want to measure, they are proxies of them. The architectures themselves are a big part of forming that loss landscape. This would be a fine comparison if the metric were not a proxy but since it it then it isn't reliable unless we know what the divergence is[3]. This is all fine, but to advance as a field we need to remember what we don't know.Overall, I'm still not sure what is meant by \"Super Data Learners\".It seems like this is counted by information per parameter? I do think there is good discussion in the \"causal\" attention vs the free-form attention of diffusion, but I think there are also some potential oversteps in the conclusions here. A lower triangular matrix is still full-rank, so there is high representation power here, though it is correct that the free form has more (even when in",
    "parent": 44856101,
    "depth": 1
  },
  {
    "id": 44857681,
    "by": "godelski",
    "timeISO": "2025-08-10T19:41:22.000Z",
    "textPlain": "> as that model was showing epoch over epoch improvements\n\nBoth of them were showing improvements. I agree with you that I'd like to see more, but I'm not sure more would significantly change the argument (which is a lot about how metrics aren't straight forward). Especially since the 96B token experiment shows.IN FACT, those results are so similar I had to open them up in GIMP to align and spot the differences. Now I'm actually not convinced there wasn't a mistake. There are differences, just very minor. Harder to tell with the AR model because scale, but in the diffusion you can see a little bump in the second one right before the concavity change at the end. There some more bumps in the AR model earlier on that help show differences too, but the fact that the envelopes are nearly identical is... suspicious. I'm not claiming maliciousness because even if a mistake these things are so easy to make that they are common. I'm not even convinced there is a mistake, but it warrants extra thinking.That said, money is finite and these are quite computationally heavy. Author looks to be a research fellow and so I'm assuming not backed by big tech.",
    "parent": 44857283,
    "depth": 2
  },
  {
    "id": 44857412,
    "by": "thesz",
    "timeISO": "2025-08-10T19:04:35.000Z",
    "textPlain": "> I wonder how much of this is due to Diffusion models having less capacity for memorization than auto regressive modelsDiffusion requires more computation resources than autoregressive models, compute excess is proportional to the length of sequence. Time dilated RNNs and adaptive computation in image recognition hint us that we can compute more with same weights and achieve better results.Which, I believe, also hint at the at least one flaw of the TS study - I did not see that they matched DLM and AR by compute, they matched them only by weights.",
    "parent": 44857283,
    "depth": 2
  },
  {
    "id": 44857185,
    "by": "ckjellqv",
    "timeISO": "2025-08-10T18:27:32.000Z",
    "textPlain": "My guess is that autoregressive models can use Key Value (KV) caching to eliminate most of the FLOPs inside the self-attention block. Can't use KV caching inside diffusion (because it's not a causal model) but they sell this as a win anyway because they believe it leads to better reasoning.",
    "parent": 44856906,
    "depth": 2
  }
]
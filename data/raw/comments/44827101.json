[
  {
    "id": 44827943,
    "by": "aliljet",
    "timeISO": "2025-08-07T17:54:20.000Z",
    "textPlain": "Between Opus aand GPT-5, it's not clear there's a substantial difference in software development expertise. The metric that I can't seem to get past in my attempts to use the systems is context awareness over long-running tasks. Producing a very complex, context-exceeding objective is a daily (maybe hourly) ocurrence for me. All I care about is how these systems manage context and stay on track over extended periods of time.What eval is tracking that? It seems like it's potentially the most imporatnt metric for real-world software engineering and not one-shot vibe prayers.",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44832173,
    "by": "chrismccord",
    "timeISO": "2025-08-08T00:47:42.000Z",
    "textPlain": "I'm really bummed out by this release. I expected this to best sonnet, or at least match, given all the hype. But it has drastically under performed on agent based work for me so far, even underperforming gpt-4.1. It struggles with basic instruction following. Basic things like:  - \"don't nest modules'–nests 4 mods in 1 file\n  - \"don't write typespecs\"–writes typespecs\n  - \"Always give the user design choices\"– skips design choices.\n\ngpt-4.1 way outperforms w/ same instructions. And sonnet is a whole different league (remains my goto). gpt-5 elixir code is syntactically correct, but weird in a lot of ways, junior-esque inefficient, and just odd. e.g function arguments that aren't used, yet passed in from callers, dup if checks, dup queries in same function. I imagine their chat and multimodal stuff strikes a nice balance with leaps in some areas, but for coding agents this is way behind any other SOTA model I've tried. Seems like this release was more about striking a capability balance b/w roflscale and costs than a gpt3-4 leap.",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44828357,
    "by": "pamelafox",
    "timeISO": "2025-08-07T18:19:31.000Z",
    "textPlain": "I am testing out gpt-5-mini for a RAG scenario, and I'm impressed so far.I used gpt-5-mini with reasoning_effort=\"minimal\", and that model finally resisted a hallucination that every other model generated.Screenshot in post here:\nhttps://bsky.app/profile/pamelafox.bsky.social/post/3lvtdyvb...I'll run formal evaluations next.",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44827981,
    "by": "risho",
    "timeISO": "2025-08-07T17:56:53.000Z",
    "textPlain": "over the last week or so I have put probably close to 70 hours into playing around with cursor and claude code and a few other tools (its become my new obsession). I've been blown away by how good and reliable it is now. That said the reality is in my experience the only models that actually work in any sort of reliable way are claude models. I dont care what any benchmark says because the only thing that actually matters is actual use. I'm really hoping that this new gpt model actually works for this usecase because competition is great and the price is also great.",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44827772,
    "by": "croemer",
    "timeISO": "2025-08-07T17:45:04.000Z",
    "textPlain": "> GPT‑5 also excels at long-running agentic tasks—achieving SOTA results on τ2-bench telecom (96.7%), a tool-calling benchmark released just 2 months ago.Yes, but it does worse than o3 on the airline version of that benchmark. The prose is totally cherry picker.",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44828183,
    "by": "jumploops",
    "timeISO": "2025-08-07T18:08:45.000Z",
    "textPlain": "If the model is as good as the benchmarks say, the pricing is fantastic:Input: $1.25 / 1M tokens (cached: $0.125/1Mtok) Output: $10 / 1M tokensFor context, Claude Opus 4.1 is $15 / 1M for input tokens and $75/1M for output tokens.The big question remains: how well does it handle tools? (i.e. compared to Claude Code)Initial demos look good, but it performs worse than o3 on Tau2-bench airline, so the jury is still out.",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44831499,
    "by": "redbell",
    "timeISO": "2025-08-07T23:06:42.000Z",
    "textPlain": "The fact that they intentionally ignored competitors' models in benchmarks and where comparing GPT-5 only to their previous models reminds me of Apple. They never compare their latest iPhone with any other phone from other brands but only to their previous(s) iPhone.",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44833936,
    "by": "raducu",
    "timeISO": "2025-08-08T06:02:15.000Z",
    "textPlain": "Sample size of 1 but GPT-5 seems horrendous at coding?My go to benchmark is a 3d snake game Claude does almost flawlessly (or at least in 3-4 iterations)The prompt:write a 3d snake game in js and html. you can use any libraries you want. the game still happens inside a single plane, left arrow turns the snake left, right arrow turns it right. the plane is black and there's a green grid. there are multiple rewards of random colors at a given time. each time a reward is eaten, it becomes the snake's new head. The camera follows the snake's head, it is above an a bit behind it, looking forward. When the snake moves right or left, the camera follows gradually left or right, no snap movements. write everything in a single html file.EDIT: I'm not trying to shit on GPT-5, so many people here seem to be getting very good results, am I doing something wrong with my prompt?",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44828241,
    "by": "mehmetoguzderin",
    "timeISO": "2025-08-07T18:12:07.000Z",
    "textPlain": "Context-free grammar and regex support are exciting. I wonder what, or whether, there are differences from the Lark-like CFG of llguidance, which powers the JSON schema of the OpenAI API [^1].[^1]: https://github.com/guidance-ai/llguidance/blob/f4592cc0c783a...",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44830817,
    "by": "joshmlewis",
    "timeISO": "2025-08-07T21:50:19.000Z",
    "textPlain": "It's free in Cursor for the next few days, you should go try it out if you haven't. I've been an agentic coding power user since the day it came out across several IDE's/CLI tools and Cursor + GPT-5 seems to be a great combo.",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44828053,
    "by": "low_tech_punk",
    "timeISO": "2025-08-07T18:00:45.000Z",
    "textPlain": "The ability to specify a context-free grammar as output constraint? This blows my mind. How do you control the auto regressive sampling to guarantee the correct syntax?",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44828490,
    "by": "hrpnk",
    "timeISO": "2025-08-07T18:28:54.000Z",
    "textPlain": "The github issue showed in the livestream is getting lots of traction: https://github.com/openai/openai-python/issues/2472It was (attempted to be) solved by a human before, yet not merged...\nWith all the great coding models OpenAI has access to, their SDK team still feels too small for the needs.",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44828119,
    "by": "catigula",
    "timeISO": "2025-08-07T18:04:18.000Z",
    "textPlain": "I thought we were going to have AGI by now.",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44829362,
    "by": "nadis",
    "timeISO": "2025-08-07T19:39:29.000Z",
    "textPlain": "\"When producing frontend code for web apps, GPT‑5 is more aesthetically-minded, ambitious, and accurate. In side-by-side comparisons with o3, GPT‑5 was preferred by our testers 70% of the time.\"That's really interesting to me. Looking forward to trying GPT-5!",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44829716,
    "by": "attentive",
    "timeISO": "2025-08-07T20:08:05.000Z",
    "textPlain": "> scoring 74.9% on SWE-bench Verified and 88% on Aider polyglotwhy isn't it on https://aider.chat/docs/leaderboards/?\"last updated August 07, 2025\"",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44832898,
    "by": "ryukoposting",
    "timeISO": "2025-08-08T02:49:16.000Z",
    "textPlain": "> Custom tools support constraining by developer-supplied context-free grammars.This sounds like a really cool feature. I'm imagining giving it a grammar that can only output safe, well-constrained SQL queries. Would I actually point an LLM directly at my database in production? Hell no! It's nice to see OpenAI trying to solve that problem anyway.",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44834453,
    "by": "energy123",
    "timeISO": "2025-08-08T07:25:38.000Z",
    "textPlain": "I've gotten 100% cache misses so far. Has anyone got a cache hit?",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44828068,
    "by": "low_tech_punk",
    "timeISO": "2025-08-07T18:02:00.000Z",
    "textPlain": "Tried using gpt-5 family with response API and got error \"gpt-5 does not exist or you don't have access to it\". I guess they are not rolling out in lock step with the live stream and blog article?",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44830587,
    "by": "6thbit",
    "timeISO": "2025-08-07T21:27:28.000Z",
    "textPlain": "Can anyone share their experience with codex CLI? I feel like that’s not mentioned enough and gpt5 is already the default model there.",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44830778,
    "by": "joshmlewis",
    "timeISO": "2025-08-07T21:47:20.000Z",
    "textPlain": "It does really well at using tool calls to gain as much context as it can to provide thoughtful answers. In this example it did 6! tool calls in the first response while 4.1 did 3 and o3 did one at a time.https://promptslice.com/share/b-2ap_rfjeJgIQsG",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44833810,
    "by": "t1amat",
    "timeISO": "2025-08-08T05:40:52.000Z",
    "textPlain": "The problem with OpenAI models is the lack of a Max-like subscription for a good agentic harness.  Maybe OpenAI or Microsoft could fix this.I just went through the agony of provisioning my team with new Claude Code 5x subs 2 weeks ago after reviewing all of the options available at that time.  Since then, the major changes include a Cerebras sub for Qwen3 Coder 480B, and now GPT-5.  I’m still not sure I made the right choice, but hey, I’m not married to it either.If you plan on using this much at all then the primary thing to avoid is API-based pay per use. It’s prohibitively costly to use regularly.  And even for less important changes it never feels appropriate to use a lower quality model when the product counts.Claude Code won primarily because of the sub and that they have a top tier agentic harness and models that know how to use it.  Opus and Sonnet are fantastic agents and very good at our use case, and were our preferred API-based models anyways.  We can use Claude Code basically all day with at least Sonnet after using our Opus limits up. Worth nothing that Cline built a Claude Code provider that the derivatives aped which is great but I’ve found Claude Code to be as good or better anyways.  The CLI interface is actually a bonus for ease of sharing state via copy/paste.I’ll probably change over to Gemini Code Assist next, as it’s half the price and more context length, but I’m waiting for a better Gemini 2.5 Pro and the gemini-cli/Code Assist extensions to have first party planning support, which you can get some form of third party through custom extensions with the cli, but as an agent harness they are incomplete without.The Cerebras + Qwen3 Coder 480B with qwen3-cli is seriously tempting.  Crazy generation speed.  Theres some question about how long big the rate limit really is but it’s half the cost of Claude Code 5x.  I haven’t checked but I know qwen3-cli, which was introduced along side the model, is a fork of gemini-cli with Qwen-focused updates; w",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44828944,
    "by": "zaronymous1",
    "timeISO": "2025-08-07T19:04:08.000Z",
    "textPlain": "Can anyone explain to me why they've removed parameter controls for temperature and top-p in reasoning models, including gpt-5? It strikes me that it makes it harder to build with these to do small tasks requiring high-levels of consistency, and in the API, I really value the ability to set certain tasks to a low temp.",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44830102,
    "by": "wewewedxfgdf",
    "timeISO": "2025-08-07T20:44:01.000Z",
    "textPlain": "Tried it on a tough problem.GPT-5 solved the problem - which Gemini failed to solve - then failed 6 times in a row to write the code to fix it.I then gave ChatGPT-5's problem analysis to Google Gemini and it immediately implemented the correct fix.The lesson - ChatGPT is good at analysis and code reviews, not so good at coding.",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44829839,
    "by": "attentive",
    "timeISO": "2025-08-07T20:19:38.000Z",
    "textPlain": "\"Notably, GPT‑5 with minimal reasoning is a different model than the non-reasoning model in ChatGPT, and is better tuned for developers. The non-reasoning model used in ChatGPT is available as gpt-5-chat-latest.\"hmm, they should call it gpt-5-chat-nonreasoning or something.",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44833526,
    "by": "vivzkestrel",
    "timeISO": "2025-08-08T04:47:58.000Z",
    "textPlain": "would be nice if we had some model out there with a context window of 1 billion tokens. i have about 25 .UNR files made with LEAD engine (heavily modified unreal engine 2.x) within which i want the AI to search for a string. Also got another 100 .utx files. Use-case game modding",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44828185,
    "by": "6thbit",
    "timeISO": "2025-08-07T18:09:06.000Z",
    "textPlain": "Seems they have quietly increased the context window up to 400,000https://platform.openai.com/docs/models/gpt-5",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44831089,
    "by": "austinmw",
    "timeISO": "2025-08-07T22:20:02.000Z",
    "textPlain": "Okay so say GPT-5 is better than Claude Opus 4.1. Then is GPT-5+Cursor better than Opus 4.1 + Claude Code? And if not, what's the best way to utilize GPT-5?",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44833723,
    "by": "weird-eye-issue",
    "timeISO": "2025-08-08T05:22:13.000Z",
    "textPlain": "gpt-5-chat-latest is giving much better results for our use case compared to gpt-5. Which puts me in a tricky position since gpt-5-chat-latest is not pinned and can change at any time...",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44828646,
    "by": "jaflo",
    "timeISO": "2025-08-07T18:39:48.000Z",
    "textPlain": "I just wish their realtime audio pricing would go down but it looks like GPT-5 does not have support for that so we’re stuck with the old models.",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44828586,
    "by": "henriquegodoy",
    "timeISO": "2025-08-07T18:35:19.000Z",
    "textPlain": "I dont think there's so much difference from opus 4.1 and gpt-5, probably just the context size, waiting for the gemini 3.0",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44827769,
    "by": "andrewmcwatters",
    "timeISO": "2025-08-07T17:44:55.000Z",
    "textPlain": "I wonder how good it is compared to Claude Sonnet 4, and when it's coming to GitHub Copilot.I almost exclusively wrote and released https://github.com/andrewmcwattersandco/git-fetch-file yesterday with GPT 4o and Claude Sonnet 4, and the latter's agentic behavior was quite nice. I barely had to guide it, and was able to quickly verify its output.",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44828024,
    "by": "sebdufbeau",
    "timeISO": "2025-08-07T17:59:24.000Z",
    "textPlain": "Has the API rollout started? It's not available in our org, even if we've been verified for a few monthsEDIT: It's out now",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44829907,
    "by": "guybedo",
    "timeISO": "2025-08-07T20:27:28.000Z",
    "textPlain": "here's a summary for this discussion:https://extraakt.com/extraakts/openai-s-gpt-5-performance-co...",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44829965,
    "by": "mwigdahl",
    "timeISO": "2025-08-07T20:32:07.000Z",
    "textPlain": "Has anyone tried connecting up GPT-5 to Claude Code using the model environment variables?",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44829769,
    "by": "worik",
    "timeISO": "2025-08-07T20:13:25.000Z",
    "textPlain": "Diminishing returns?",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44834427,
    "by": "skroumpelou",
    "timeISO": "2025-08-08T07:21:14.000Z",
    "textPlain": "I tried it out with warp terminal (warp.dev)! It will be my coding buddy today!",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44828516,
    "by": "te_chris",
    "timeISO": "2025-08-07T18:31:18.000Z",
    "textPlain": "https://platform.openai.com/docs/guides/latest-modelLooks like they're trying to lock us into using the Responses API for all the good stuff.",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44828594,
    "by": "sberens",
    "timeISO": "2025-08-07T18:36:08.000Z",
    "textPlain": "Interesting there doesn't seem to be benchmarking on codeforces",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44830081,
    "by": "planet_1649c",
    "timeISO": "2025-08-07T20:42:07.000Z",
    "textPlain": "Can we use this model in a fixed plan like claude code for which we can pay 100$ / month?Doesnt look like it. Unless they add a fixed pricing, claude imo still would be better from a developer POV",
    "parent": 44827101,
    "depth": 1
  },
  {
    "id": 44827991,
    "by": "timhigins",
    "timeISO": "2025-08-07T17:57:06.000Z",
    "textPlain": "I opened up the developer playground and the model selection dropdown showed GPT-5 and then it disappeared. Also I don't see it in ChatGPT Pro. What's up?",
    "parent": 44827101,
    "depth": 1
  }
]
[
  {
    "id": 44847169,
    "by": "hodgehog11",
    "timeISO": "2025-08-09T15:14:11.000Z",
    "textPlain": "I am sympathetic to the reasoning as to why LLMs should not be used to help some programmers right now. But I get a little frustrated seeing many of these kinds of posts that talk about fundamental limitations of LLMs vs humans on the grounds that it cannot \"logically reason\" like a human does. These are limitations in the current approach to training and objectives; internally, we have no clue what is going on.> it’s “just a statistical model” that generates “language” based on a chain of “what is most likely to follow the previous phrase”Humans are statistical models too in an appropriate sense. The question is whether we try to execute phrase by phrase or not, or whether it even matters what humans do in the long term.> The only way ChatGPT will stop spreading that nonsense is if there is a significant mass of humans talking online about the lack of ZSTD support.Or you can change the implicit bias in the model by being more clever with your training procedure. This is basic stats here, not everything is about data.> They don’t know anything, they don’t think, they don’t learn, they don’t deduct. They generate real-looking text based on what is most likely based on the information it has been trained on.This may be comforting to think, but it's just wrong. It would make my job so much easier if it were true. If you take the time to define \"know\", \"think\", and \"deduct\", you will find it difficult to argue current LLMs do not do these things. \"Learn\" is the exception here, and is a bit more complex, not only because of memory and bandwidth issues, but also because \"understand\" is difficult to define.",
    "parent": 44845973,
    "depth": 1
  },
  {
    "id": 44847223,
    "by": "quantum_state",
    "timeISO": "2025-08-09T15:21:00.000Z",
    "textPlain": "An implication from the 1985 paper of Peter Naur on programming as theory building is that the current LLM coding tool would be very effective in generating technical debt even when it works ... use at your own risk.",
    "parent": 44845973,
    "depth": 1
  },
  {
    "id": 44847150,
    "by": "bfioca",
    "timeISO": "2025-08-09T15:12:36.000Z",
    "textPlain": ">...it’s a useless tool. I don’t like collaborating with chronic liars who aren’t able to openly point out knowledge gaps...I think a more correct take here might be \"it's a tool that I don't trust enough to use without checking,\" or at the very least, \"it's a useless tool for my purposes.\" I understand your point, but I got a little caught up on the above line because it's very far out of alignment with my own experience using it to save enormous amounts of time.",
    "parent": 44845973,
    "depth": 1
  },
  {
    "id": 44847179,
    "by": "gdsys",
    "timeISO": "2025-08-09T15:15:17.000Z",
    "textPlain": "\"Based on my research, zstd compression is not natively supported by iOS or Apple's frameworks, which means you cannot use zstd compression without adding some form of external code to your project\"Thanks Sonnet.Full response:https://www.perplexity.ai/search/without-adding-third-party-...",
    "parent": 44845973,
    "depth": 1
  },
  {
    "id": 44847106,
    "by": "jmkni",
    "timeISO": "2025-08-09T15:07:45.000Z",
    "textPlain": "what's funny is that newer models will now be trained on the exact question, \"Without adding third-party dependencies, how can I compress a Data stream with zstd in Swift on an iPhone?\" and similar questions to it, because of this postmaybe the key to training future llm's is to write angry blog posts about the things they aren't good at and get them to the front page of hn?",
    "parent": 44845973,
    "depth": 1
  },
  {
    "id": 44847195,
    "by": "nikolayasdf123",
    "timeISO": "2025-08-09T15:17:35.000Z",
    "textPlain": "possible solution: \"reality checks\"I see that GitHub Copilot actually runs code, writes simple exploratory programs, iteratively tests its hypothesis. it is astoundingly effective and fast.same here. nothing stops this AI to actually trying to implement whatever this AI suggested, compile it, and see if this is actually works.grounding in reality at inference time, so to speak.",
    "parent": 44845973,
    "depth": 1
  },
  {
    "id": 44847158,
    "by": "cantor_S_drug",
    "timeISO": "2025-08-09T15:13:30.000Z",
    "textPlain": "There was a writer who in order to get ideas to write about used to cut up words from newspaper headlines and then rearrange them.In one rearrangement, he got \"Son sues father for xyz\". That headline came true 2 years later.",
    "parent": 44845973,
    "depth": 1
  },
  {
    "id": 44847141,
    "by": "nikolayasdf123",
    "timeISO": "2025-08-09T15:11:21.000Z",
    "textPlain": "> “Not having an answer” is not a possibility in this system - there’s always “a most likely response”, even if that makes no sense.simple fix - probability cutoff. but in all seriousness this is something that will be fixed. don't see fundamental reason why not.and I myself seen such hallucinations (about compression too actually) as well.",
    "parent": 44845973,
    "depth": 1
  },
  {
    "id": 44846057,
    "by": "techpineapple",
    "timeISO": "2025-08-09T12:41:13.000Z",
    "textPlain": "I wonder if one reasons new versions of GPT appear to get better - say at coding tasks is just because they have new knowledge.When ChatGPT4 comes out, new versions of API’s will have less blog post / examples / documentation in their training data.  So ChatGPT 5 comes out and seems to solve all the problems that ChatGPT4 had, but then of course fail on newer libraries. Rinse and repeat",
    "parent": 44845973,
    "depth": 1
  },
  {
    "id": 44847234,
    "by": "lazide",
    "timeISO": "2025-08-09T15:22:04.000Z",
    "textPlain": "It’s a tool that fundamentally can’t be used reliably without double checking everything it. That is rather different than you’re presenting it.",
    "parent": 44847150,
    "depth": 2
  },
  {
    "id": 44847219,
    "by": "dcre",
    "timeISO": "2025-08-09T15:20:49.000Z",
    "textPlain": "Most likely the key here is web search. Later I will try the post’s example with gpt-5 with search. I would be surprised if it didn’t say the same thing.From a product point of view, it seems clear that just as they have work to get the model to dynamically decide to use reasoning when it would help, they have to do the same with web search.",
    "parent": 44847179,
    "depth": 2
  },
  {
    "id": 44847154,
    "by": "nikolayasdf123",
    "timeISO": "2025-08-09T15:12:56.000Z",
    "textPlain": "good point. nobody knows you are a dog on internet anyways",
    "parent": 44847106,
    "depth": 2
  },
  {
    "id": 44846540,
    "by": "its-kostya",
    "timeISO": "2025-08-09T14:04:12.000Z",
    "textPlain": "> ... just because they have new knowledge.This means there is a future where AI is training on data it self generated, and I worry that might not be sustainable.",
    "parent": 44846057,
    "depth": 2
  }
]
[
  {
    "id": 44879827,
    "by": "dang",
    "timeISO": "2025-08-12T18:07:04.000Z",
    "textPlain": "Related ongoing thread:Claude Sonnet 4 now supports 1M tokens of context - https://news.ycombinator.com/item?id=44878147 - Aug 2025 (160 comments)",
    "parent": 44878999,
    "depth": 1
  },
  {
    "id": 44879649,
    "by": "irthomasthomas",
    "timeISO": "2025-08-12T17:50:52.000Z",
    "textPlain": "So sonnet-4 is faster than gemini-2.5-flash at long context. That is surprising. Especially since Gemini runs on those fast TPUS.",
    "parent": 44878999,
    "depth": 1
  },
  {
    "id": 44879393,
    "by": "arnaudsm",
    "timeISO": "2025-08-12T17:28:01.000Z",
    "textPlain": "https://archive.is/sb7D5",
    "parent": 44878999,
    "depth": 1
  },
  {
    "id": 44879864,
    "by": "daft_pink",
    "timeISO": "2025-08-12T18:10:32.000Z",
    "textPlain": "i’m really curious how well they perform with a long chat history. i find that gemini often gets confused when the context is long enough and starts responding to prior prompts, using the cli or it’s gem chat window.",
    "parent": 44878999,
    "depth": 1
  },
  {
    "id": 44880447,
    "by": "akomtu",
    "timeISO": "2025-08-12T18:59:57.000Z",
    "textPlain": "IMO, a good contest between LLMs would be data compression. Each LLM is given the same pile of text, and then asked to create compact notes that fit into N pages of text. Then the original text is replaced with their notes and they need to answer a bunch of questions about the original text using the notes alone.",
    "parent": 44878999,
    "depth": 1
  },
  {
    "id": 44879723,
    "by": "koakuma-chan",
    "timeISO": "2025-08-12T17:56:28.000Z",
    "textPlain": "I really doubt you can fit all Harry Potter books in 1M tokens.",
    "parent": 44878999,
    "depth": 1
  },
  {
    "id": 44879738,
    "by": "curl-up",
    "timeISO": "2025-08-12T17:57:57.000Z",
    "textPlain": "Note that (in the first test, the only one where output length is reported), Gemini Pro returned more than 3x the amount of text, at less than 2x the amount of time. From my experience with Gemini, that time was probably mainly spent on thinking, length of which is not reported here. So looking at pure TPS of output, Gemini is faster, but without clear info on the thinking time/length, it's impossible to judge.",
    "parent": 44879649,
    "depth": 2
  },
  {
    "id": 44879707,
    "by": "jbellis",
    "timeISO": "2025-08-12T17:55:37.000Z",
    "textPlain": "if they left them both on defaults, flash is thinking-by-default and sonnet 4 is no-thinking-by-default",
    "parent": 44879649,
    "depth": 2
  },
  {
    "id": 44879712,
    "by": "bitpush",
    "timeISO": "2025-08-12T17:55:55.000Z",
    "textPlain": "> Claude’s overall response was consistently around 500 words—Flash and Pro delivered 3,372 and 1,591 words by contrast.It isnt clear from the article whether the time they quote is time-to-first-token or time to completion. If it is latter, then it makes sense why gemini* would take longer even with similar token throughput.",
    "parent": 44879649,
    "depth": 2
  },
  {
    "id": 44880209,
    "by": "lugao",
    "timeISO": "2025-08-12T18:40:55.000Z",
    "textPlain": "Anthropic also uses TPUs for inference.",
    "parent": 44879649,
    "depth": 2
  },
  {
    "id": 44879473,
    "by": "thefourthchime",
    "timeISO": "2025-08-12T17:35:45.000Z",
    "textPlain": "Does anyone else have trouble with the archive rendering of that? It seemed to also have the pop up.",
    "parent": 44879393,
    "depth": 2
  },
  {
    "id": 44880289,
    "by": "XenophileJKO",
    "timeISO": "2025-08-12T18:47:21.000Z",
    "textPlain": "From my experience. Gemini is REALLY bad about context blending. It can't keep track of what I said and what it said in a conversation under 200K tokens. It blends concepts and statements up, then refers to some fabricated hybrid fact or comment.Gemini has done this in ways that I haven't seen in the recent or current generation models from OpenAI or Anthropic.It really surprised me that Gemini performs so well in multi-turn benchmarks, given that tendency.",
    "parent": 44879864,
    "depth": 2
  },
  {
    "id": 44880248,
    "by": "PeterStuer",
    "timeISO": "2025-08-12T18:44:05.000Z",
    "textPlain": "The series is 1,084,170 words. At let's say 1.4 tokens per word, this would not fit, but it is getting close.",
    "parent": 44879723,
    "depth": 2
  },
  {
    "id": 44879748,
    "by": "gcr",
    "timeISO": "2025-08-12T17:58:57.000Z",
    "textPlain": "The entire HP series is about one million words.",
    "parent": 44879723,
    "depth": 2
  }
]
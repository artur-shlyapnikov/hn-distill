[
  {
    "id": 44829733,
    "by": "mhitza",
    "timeISO": "2025-08-07T20:09:54.000Z",
    "textPlain": "I've ran a comparison benchmark for the smaller models https://gist.github.com/mhitza/f5a8eeb298feb239de10f9f60f841...Comparing it against the RTX 4000 SFF Ada (20GB) which is around $1.2k (if you believe the original price on the nvidia website https://marketplace.nvidia.com/en-us/enterprise/laptops-work...). Which I have access to on a Hetzner GEX44.I'm going to ballpark it between 2.5-3x faster than the desktop. Except for the tg128 test, where the difference is \"minimal\" (but I didn't do the math).",
    "parent": 44827862,
    "depth": 1
  },
  {
    "id": 44830170,
    "by": "reissbaker",
    "timeISO": "2025-08-07T20:49:23.000Z",
    "textPlain": "Thanks for the excellent writeup. I'm pleasantly surprised that ROCm worked as well as it did — for the price these aren't bad for LLM workloads and some moderate gaming. (Apple is probably still the king of affordable at-home inference, but for games... Amazing these days but Linux is so much better.)",
    "parent": 44827862,
    "depth": 1
  },
  {
    "id": 44830302,
    "by": "Havoc",
    "timeISO": "2025-08-07T21:00:31.000Z",
    "textPlain": "Jeff - check out the distributed-llama project...you should be able to distribute over entire cluster",
    "parent": 44827862,
    "depth": 1
  },
  {
    "id": 44830198,
    "by": "iamtheworstdev",
    "timeISO": "2025-08-07T20:52:10.000Z",
    "textPlain": "for those who are already in the field and doing these things - if I wanted to start running my own local LLM.. should I find an Nvidia 5080 GPU for my current desktop or is it worth trying one of these Framework AMD desktops?",
    "parent": 44827862,
    "depth": 1
  },
  {
    "id": 44831466,
    "by": "adolph",
    "timeISO": "2025-08-07T23:03:41.000Z",
    "textPlain": "The Framework Desktop has at least two M.2 connectors for NVME. I wonder if an interconnect with higher performance than Ethernet or Thunderbolt could be established using one of the M.2 to connect to PCIe via Oculink?",
    "parent": 44827862,
    "depth": 1
  },
  {
    "id": 44828959,
    "by": "jeffbee",
    "timeISO": "2025-08-07T19:05:12.000Z",
    "textPlain": "I had been hoping that these would be a bit faster than the 9950X because of the different memory architecture, but it appears that due to the lower power design point the AI Max+ 395 loses across the board, by large margins. So I guess these really are niche products for ML users only, and people with generic workloads that want more than the 9950X offers are shopping for a Threadripper.",
    "parent": 44827862,
    "depth": 1
  },
  {
    "id": 44831827,
    "by": "syntaxing",
    "timeISO": "2025-08-07T23:52:03.000Z",
    "textPlain": "Kinda bummed, I get why he used Ollama but I feel like using llama cpp directly would provide better and more consistent results",
    "parent": 44827862,
    "depth": 1
  },
  {
    "id": 44834372,
    "by": "nektro",
    "timeISO": "2025-08-08T07:11:31.000Z",
    "textPlain": "no compilation tests?",
    "parent": 44827862,
    "depth": 1
  },
  {
    "id": 44830696,
    "by": "jvanderbot",
    "timeISO": "2025-08-07T21:38:15.000Z",
    "textPlain": "So, TL;DR?I saw mixed results but comments suggest very good performance relative to other at-home setups. Can someone summarize?",
    "parent": 44827862,
    "depth": 1
  },
  {
    "id": 44830184,
    "by": "xemdetia",
    "timeISO": "2025-08-07T20:50:40.000Z",
    "textPlain": "I was about to be annoyed until you said you got preprod units. I guess I'll have to build on this when my desktop shows up.",
    "parent": 44827862,
    "depth": 1
  },
  {
    "id": 44836793,
    "by": "Tsiklon",
    "timeISO": "2025-08-08T13:39:03.000Z",
    "textPlain": "I see Wendell of Level1Techs combines the two in his video on this system.Theoretically you can have the best of both worlds if you don’t mind running an Occulink E-GPU enclosurehttps://youtu.be/L-xgMQ-7lW0",
    "parent": 44829733,
    "depth": 2
  },
  {
    "id": 44830508,
    "by": "yencabulator",
    "timeISO": "2025-08-07T21:20:21.000Z",
    "textPlain": "The whole point of these integrated memory designs is to go beyond that 20 GB VRAM.",
    "parent": 44829733,
    "depth": 2
  },
  {
    "id": 44830349,
    "by": "mulmen",
    "timeISO": "2025-08-07T21:05:27.000Z",
    "textPlain": "I switched to Fedora Sway as my daily driver nearly two years ago.  A Windows title wasn’t working on my brand new PC.  I switched to Steam+Proton+Fedora and it worked immediately.  Valve now offers a more stable and complete Windows API through Proton than Microsoft does through Windows itself.",
    "parent": 44830170,
    "depth": 2
  },
  {
    "id": 44830676,
    "by": "geerlingguy",
    "timeISO": "2025-08-07T21:35:37.000Z",
    "textPlain": "I've been testing Exo (seems dead), llama.cpp RPC (has a lot of performance limitations) and distributed-llama (faster but has some Vulkan quirks and only works with a few models).See my AI cluster automation setup here: https://github.com/geerlingguy/beowulf-ai-clusterI was building that through the course of making this video, because it's insane how much manual labor people put into building home AI clusters :D",
    "parent": 44830302,
    "depth": 2
  },
  {
    "id": 44830542,
    "by": "yjftsjthsd-h",
    "timeISO": "2025-08-07T21:23:25.000Z",
    "textPlain": "https://github.com/b4rtaz/distributed-llama ?",
    "parent": 44830302,
    "depth": 2
  },
  {
    "id": 44830316,
    "by": "burnte",
    "timeISO": "2025-08-07T21:02:31.000Z",
    "textPlain": "He mentioned that in the video.",
    "parent": 44830302,
    "depth": 2
  },
  {
    "id": 44831027,
    "by": "loudmax",
    "timeISO": "2025-08-07T22:13:45.000Z",
    "textPlain": "The short answer is that the best value is a used RTX 3090 (the long answer being, naturally, it depends).  Most of the time, the bottleneck for running LLMs on consumer grade equipment is memory and memory bandwidth.  A 3090 has 24GB of VRAM, while a 5080 only has 16GB of VRAM.  For models that can fit inside 16GB of VRAM, the 5080 will certainly be faster than the 3090, but the 3090 can run models that simply won't fit on a 5080.  You can offload part of the model onto the CPU and system RAM, but running a model on a desktop CPU is an enormous drag, even when only partially offloaded.Obviously an RTX 5090 with 32GB of VRAM is even better, but they cost around $2000, if you can find one.What's interesting about this Strix Halo system is that it has 128GB of RAM that is accessible (or mostly accessible) to the CPU/GPU/APU.  This means that you can run much larger models on this system than you possibly could on a 3090, or even a 5090.  The performance tests tend to show that the Strix Halo's memory bandwidth is a significant bottleneck though.  This system might be the most affordable way of running 100GB+ models, but it won't be fast.",
    "parent": 44830198,
    "depth": 2
  },
  {
    "id": 44830454,
    "by": "wmf",
    "timeISO": "2025-08-07T21:14:52.000Z",
    "textPlain": "If you think the future is small models (27B) get Nvidia; if you think larger models (70-120B) are worth it then you need AMD or Apple.",
    "parent": 44830198,
    "depth": 2
  },
  {
    "id": 44832042,
    "by": "nrp",
    "timeISO": "2025-08-08T00:27:17.000Z",
    "textPlain": "There is also a PCIe x4 slot that you can use for other high throughput network options.",
    "parent": 44831466,
    "depth": 2
  },
  {
    "id": 44829018,
    "by": "dijit",
    "timeISO": "2025-08-07T19:09:32.000Z",
    "textPlain": "Sounds about right.I’m struggling to justify the cost of a Threadripper (let alone pro!) for a AAA game studio though.I wonder who can justify these machines. High frequency trading? data science? shouldn’t that be done on servers?",
    "parent": 44828959,
    "depth": 2
  },
  {
    "id": 44829413,
    "by": "rtkwe",
    "timeISO": "2025-08-07T19:44:19.000Z",
    "textPlain": "It also seems like the tools aren't there to fully utilize them. Unless I misunderstood he was running off CPU only for all the test so there's still the iGPU and NPU performance that's not been utilized in these tests.",
    "parent": 44828959,
    "depth": 2
  },
  {
    "id": 44836390,
    "by": "RossBencina",
    "timeISO": "2025-08-08T12:49:30.000Z",
    "textPlain": "I heard that ik_llama.cpp performs better for CPU use: https://github.com/ikawrakow/ik_llama.cpp/",
    "parent": 44831827,
    "depth": 2
  },
  {
    "id": 44831952,
    "by": "mkl",
    "timeISO": "2025-08-08T00:10:31.000Z",
    "textPlain": "As the article describes, most of this was done with llama.cpp, not Ollama.",
    "parent": 44831827,
    "depth": 2
  },
  {
    "id": 44836488,
    "by": "geerlingguy",
    "timeISO": "2025-08-08T13:01:39.000Z",
    "textPlain": "Those are in my SBC-reviews repo: https://github.com/geerlingguy/sbc-reviews/issues/80",
    "parent": 44834372,
    "depth": 2
  },
  {
    "id": 44830951,
    "by": "geerlingguy",
    "timeISO": "2025-08-07T22:05:35.000Z",
    "textPlain": "I put most of the top-line numbers and some graphs on my blog: https://www.jeffgeerling.com/blog/2025/i-clustered-four-fram...",
    "parent": 44830696,
    "depth": 2
  }
]
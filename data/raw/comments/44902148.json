[
  {
    "id": 44902240,
    "by": "canyon289",
    "timeISO": "2025-08-14T16:14:07.000Z",
    "textPlain": "Hi all, I built these models with a great team. They're available for download across the open model ecosystem so give them a try! I built these models with a great team and am thrilled to get them out to you.From our side we designed these models to be strong for their size out of the box, and with the goal you'll all finetune it for your use case. With the small size it'll fit on a wide range of hardware and cost much less to finetune. You can try finetuning them yourself in a free colab in under 5 minutesFor picking a Gemma size this is a video I recorded for the 1b to 27b sizes earlier this year, 270m being the newest additionhttps://www.youtube.com/watch?v=qcjrduz_YS8Hacker News Disclaimer\nI really like working at Google so with that; All my opinions here are my own, I'm a researcher so I'll largely focus on technical questions, and I'll share what I can.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903731,
    "by": "jefb",
    "timeISO": "2025-08-14T18:12:32.000Z",
    "textPlain": "My lovely interaction with the 270M-F16 model:> what's second tallest mountain on earth?The second tallest mountain on Earth is Mount Everest.> what's the tallest mountain on earth?The tallest mountain on Earth is Mount Everest.> whats the second tallest mountain?The second tallest mountain in the world is Mount Everest.> whats the third tallest mountain?The third tallest mountain in the world is Mount Everest.> what's the forth tallest mountain?The forth tallest mountain is Mount Everest.> but you said the tallest was Everest alreadyYou are right! I'm glad to hear that.> so whats the second tallest?The second tallest is *Mount Everest*.> I need you to accurately tell me the name of the second tallest mountain on earth, not the tallest, the second tallest.The second tallest mountain on Earth is Mount Everest.> The tallest mountain on earth is mount everest. What is the second tallest mountain?The second tallest mountain on Earth is Mount Everest.> Give me a list of the tallest mountains on earthThe tallest mountain in the world is the *Mount Everest*.> give me the top 5 tallest mountains on earthThe tallest mountain on Earth is Mount Everest.> List in order the tallest mountains on earth from 1 to 51. Mount Everest\n2. Mount K2\n3. Mount Sahel\n4. Mount Fuji\n5. Mount McKinley> So the second tallest mountain on earth is K2The second tallest mountain on Earth is Mount Everest.These smaller models are great, but they really feel like talking to a toddler sometimes!",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44905188,
    "by": "jcuenod",
    "timeISO": "2025-08-14T20:20:02.000Z",
    "textPlain": "I mentioned elsewhere the impact of prompting, which seems to make an outsized difference to this model's performance. I tried NER and POS tagging (with somewhat disappointing results).One thing that worked strikingly well was translation on non-Indo-European languages. Like I had success with Thai and Bahasa Indonesian -> English...",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44902980,
    "by": "simonw",
    "timeISO": "2025-08-14T17:10:43.000Z",
    "textPlain": "This model is a LOT of fun. It's absolutely tiny - just a 241MB download - and screamingly fast, and hallucinates wildly about almost everything.Here's one of dozens of results I got for \"Generate an SVG of a pelican riding a bicycle\". For this one it decided to write a poem:  +-----------------------+\n  |   Pelican Riding Bike |\n  +-----------------------+\n  |  This is the cat!  |\n  |  He's got big wings and a happy tail.  |\n  |  He loves to ride his bike!  |\n  +-----------------------+\n  |   Bike lights are shining bright.  |\n  |   He's got a shiny top, too!  |\n  |   He's ready for adventure!  |\n  +-----------------------+\n\nThere are a bunch more attempts in this Gist, some of which do at least include an SVG tag albeit one that doesn't render anything: https://gist.github.com/simonw/25e7b7afd6a63a2f15db48b3a51ec...I'm looking forward to seeing people fine-tune this in a way that produces useful output for selected tasks, which should absolutely be feasible.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903495,
    "by": "mrcwinn",
    "timeISO": "2025-08-14T17:54:03.000Z",
    "textPlain": "Apple should be doing this. Unless their plan is to replace their search deal with an AI deal -- it's just crazy to me how absent Apple is. Tim Cook said, \"it's ours to take\" but they really seem to be grasping at the wind right now. Go Google!",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44904181,
    "by": "AJRF",
    "timeISO": "2025-08-14T18:54:11.000Z",
    "textPlain": "I've got a very real world use case I use DistilBERT for - learning how to label wordpress articles. It is one of those things where it's kind of valuable (tagging) but not enough to spend loads on compute for it.The great thing is I have enough data (100k+) to fine-tune and run a meaningful classification report over. The data is very diverse, and while the labels aren't totally evenly distributed, I can deal with the imbalance with a few tricks.Can't wait to swap it out for this and see the changes in the scores. Will report back",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44904846,
    "by": "reneberlin",
    "timeISO": "2025-08-14T19:51:36.000Z",
    "textPlain": "I am sure with finetuning this can be changed somehow:(base)   ~ ollama run hf.co/unsloth/gemma-3-270m-it-GGUF:F16\n>>>  create a sentiment analysis of the follwing: \"It's raining.\"\nThe sentiment of the provided text is *negative*.>>>  create a sentiment analysis of the follwing: \"It's raining money.\"\nThe sentiment of the provided text is *negative*.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903182,
    "by": "whinvik",
    "timeISO": "2025-08-14T17:26:45.000Z",
    "textPlain": "Curious. Are there real world usecases where people have finetuned such tiny models and put them into production.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903748,
    "by": "KTibow",
    "timeISO": "2025-08-14T18:14:02.000Z",
    "textPlain": "To add to the article: Gemma 3 270M's exact IFEval score is 51.2, and Qwen 3 would be at (0.6, 59.2) on the scatter plot.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44904066,
    "by": "miohtama",
    "timeISO": "2025-08-14T18:43:16.000Z",
    "textPlain": "Out of curiosity: because there seems to be a race to optimise models for local inference, how much \"parameters one could save\" by dropping unneeded language and domain-specific information.Like, can you have a model that is English-only, but does more with the same amount of parameters if Chinese and European languages are dropped from the training?",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44902895,
    "by": "lemonish97",
    "timeISO": "2025-08-14T17:04:47.000Z",
    "textPlain": "Never thought I'd run an LLM released in 2025, on my phone, in full BF16.\nWith ~80tps on an iPhone 16 pro btw.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903149,
    "by": "jasonjmcghee",
    "timeISO": "2025-08-14T17:24:09.000Z",
    "textPlain": "I'm _very_ interested to see what this can be fine-tuned to do.I've heard folks say a number of times that neuromuscular control / locomotion (or w/e) are hundreds of millions of parameters rather than billions.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44904664,
    "by": "amrrs",
    "timeISO": "2025-08-14T19:36:51.000Z",
    "textPlain": "Gemma 3 270M running on a midrange pixel device - https://x.com/1littlecoder/status/1956065040563331344",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903817,
    "by": "perching_aix",
    "timeISO": "2025-08-14T18:21:11.000Z",
    "textPlain": "Is it time for me to finally package a language model into my Lambda deployment zips and cut through the corporate red tape at my place around AI use?Update #1:Tried it. Well, dreams dashed - would now fit space wise (<250 MB despite the name), but it sadly really doesn't seem to work for my specific prospective workload.I'd have wanted it to perform natural-language to command-invocation translation (or better, emit me some JSON), but it's super not willing to do that, not in the lame way I'm trying to make it do so at least (literally just prompting it to). Oh well.Update #2:Just found out about grammar-constrained decode, maybe there's still hope for me in the end. I don't think I can amend this comment today with any more updates, but will see.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44902789,
    "by": "44za12",
    "timeISO": "2025-08-14T16:55:38.000Z",
    "textPlain": "I’ve had great luck with all gemma 3 variants, on certain tasks it the 27B quantized version has worked as well as 2.5 flash. Can’t wait to get my hands dirty with this one.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903188,
    "by": "jtbayly",
    "timeISO": "2025-08-14T17:27:03.000Z",
    "textPlain": "Can somebody give me a link to a tutorial on how I would go about fine-tuning this?Also, what sorts of things might I consider fine-tuning it for?",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44904459,
    "by": "mrtimo",
    "timeISO": "2025-08-14T19:18:03.000Z",
    "textPlain": "I'm a business professor who teaches Python and more. I'd like to develop some simple projects to help my students fine tune this for a business purpose. If you have ideas (or datasets for fine tuning), let me know!",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903914,
    "by": "nerdix",
    "timeISO": "2025-08-14T18:30:11.000Z",
    "textPlain": "Is it possible to finetune a model like this with local hardware? Every tutorial I've come across on finetuning a local LLM uses some cloud service like colab or runpod.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44904934,
    "by": "mark_l_watson",
    "timeISO": "2025-08-14T19:58:18.000Z",
    "textPlain": "This is exciting. Small models can be very effective when used with sufficient context and not relying in vast real world knowledge. I worked at Google as a contractor and I am still a fanboy. Although I also use Chinese models, Google local models and Gemini 2.5 Pro are my main drivers.I am curious how computer science curriculum will change since using small embedded models should change software dev.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44902845,
    "by": "Alex-Programs",
    "timeISO": "2025-08-14T16:59:56.000Z",
    "textPlain": "This is cool. I'm looking forward to trying it - I wonder what it'll be useful for.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903840,
    "by": "robbru",
    "timeISO": "2025-08-14T18:23:12.000Z",
    "textPlain": "Excited to try this out, thanks for sharing.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903049,
    "by": "dcreater",
    "timeISO": "2025-08-14T17:15:19.000Z",
    "textPlain": "I've been saying he we need sub 1B models for the edge so thanks fot this.I am however disappointed that there is no examples, or benchmarks, provided to get a sense of performance. It's a given that benchmark values would be lower than gemma 3n, but having a sense of performance vs size curve and comparison to existing small models is needed",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44904065,
    "by": "bbor",
    "timeISO": "2025-08-14T18:43:10.000Z",
    "textPlain": "Really impressive stuff, as always. I will say: it took me a shamefully long time to realize that the name ended in \"M\" instead of \"B\"! Perhaps they should consider renaming this to \"Gemma 3 .27B\"...",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44904444,
    "by": "metalliqaz",
    "timeISO": "2025-08-14T19:16:19.000Z",
    "textPlain": "is there a good resource for getting started with downloading and running something like this for a demo?  There are just so many tools/platforms in the mix now it makes my head spin.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903696,
    "by": "dismalaf",
    "timeISO": "2025-08-14T18:09:43.000Z",
    "textPlain": "It's fast at spitting out nonsense but incredibly slow at trying to parse any context.  Also absolutely atrocious at following instructions.Probably would be good as a game NPC or a chatbot, not very good for integrating into an application which specific functionality though.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903475,
    "by": "NorwegianDude",
    "timeISO": "2025-08-14T17:52:54.000Z",
    "textPlain": "The Gemma 3 models are great! One of the few models that can write Norwegian decently, and the instruction following is in my opinion good for most cases. I do however have some issues that might be related to censorship that I hope will be fixed if there is ever a Gemma 4. Maybe you have some insight into why this is happening?I run a game when players can post messages, it's a game where players can kill each other, and people often send threats along the lines of \"I will kill you\". Telling Gemma that it should classify a message as game related or a real life threat, and that it is for a message in a game where players can kill each other and threats are a part of the game, and that it should mark it as game related if it is unclear if the message is a game related threat or a real life threat does not work well. For other similar tasks it seems to follow instructions well, but for serious topics it seems to be very biased, and often err on the side of caution, despite being told not to. Sometimes it even spits out some help lines to contact.I guess this is because it was trained to be safe, and that affects it's ability to follow instructions for this? Or am I completely off here?",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44904996,
    "by": "ankit219",
    "timeISO": "2025-08-14T20:03:01.000Z",
    "textPlain": "This is super cool. Usually you dont see effective models at 270M out in the wild. The architectural choices are new and interesting as well.Would it be okay for you to divulge some more training information here? With 170M embedding parameters, how do you ensure no embedding collapse and keeping the embedding matrix stable at training time?(i know i am asking too much, but just curious). There is a clear trade off for you with vocab / transformer layers. How did you arrive at the split of 170m/100m. Does this contribute to model's performance on task specific fine tuning? Any internal experiments you could share? or public info you could point us to? Anything would be amazing.PS: I am sorry if this is rude, but this has so many decisions i am curious about. Not intending to undermine anything, this is amazing work, and thank you for the whole Gemma series.",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44903827,
    "by": "ceroxylon",
    "timeISO": "2025-08-14T18:21:59.000Z",
    "textPlain": "You reminded me of an awesome Google engineer I met at BSidesSF last year who tirelessly answered my questions, and when I clicked on the video, it was you! That was a really inspiring moment for me, thank you.",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44903226,
    "by": "simonw",
    "timeISO": "2025-08-14T17:30:09.000Z",
    "textPlain": "Do you have any practical examples of fine-tuned variants of this that you can share? A description would be great, but a demo or even downloadable model weights (GGUF ideally) would be even better.",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44903942,
    "by": "jmorgan",
    "timeISO": "2025-08-14T18:32:09.000Z",
    "textPlain": "Amazing work. This model feels really good at one-off tasks like summarization and autocomplete. I really love that you released a quantized aware training version on launch day as well, making it even smaller!",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44904326,
    "by": "blitzar",
    "timeISO": "2025-08-14T19:05:56.000Z",
    "textPlain": "> I built these models with a great team ... I built these models with a great teamIf Gemini is going to repeat something at least its that the team is great, and not a disgrace!",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44905140,
    "by": "rossant",
    "timeISO": "2025-08-14T20:15:32.000Z",
    "textPlain": "Is it good for text translation and summarization?",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44902889,
    "by": "beoberha",
    "timeISO": "2025-08-14T17:04:18.000Z",
    "textPlain": "Awesome work! I’m really bullish on small models and think they have the most potential to change our daily lives. Can’t wait to play around with this",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44904296,
    "by": "_1",
    "timeISO": "2025-08-14T19:03:32.000Z",
    "textPlain": "> and with the goal you'll all finetune it for your use case.What use-cases are a good fit for finetuning this model?  More specific instruction following, knowledge from proprietary data, response tone?",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44903234,
    "by": "cgdl",
    "timeISO": "2025-08-14T17:30:41.000Z",
    "textPlain": "Very cool. For the INT4 QAT model, what is the recommended precision for the activations and for the key and values stored in KV cache?",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44903851,
    "by": "nerdsniper",
    "timeISO": "2025-08-14T18:25:39.000Z",
    "textPlain": "What are some of the use cases that you think the 270M would be most appropriate for? What would you love to see people trying with it?",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44904366,
    "by": "patrickaljord",
    "timeISO": "2025-08-14T19:09:34.000Z",
    "textPlain": "Would it be possible to have a specialized rust only dev or Reactjs only dev while getting rid of all other languages to minimize size of model?",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44904457,
    "by": "imasl42",
    "timeISO": "2025-08-14T19:17:31.000Z",
    "textPlain": "Awesome! I’m curious how is the team you built these models with? Is it great?",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44903087,
    "by": "tmaly",
    "timeISO": "2025-08-14T17:18:45.000Z",
    "textPlain": "Are there any fine tuning in a box type options available in the cloud for this?  This is amazing work, thank you.",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44903700,
    "by": "andrewstuart",
    "timeISO": "2025-08-14T18:10:05.000Z",
    "textPlain": "What effort do you folks take to see your models actually running on hardware such as AMD Strix Halo or Apple M3M4?I get the sense that AI is at the “hobby kit computing” stage where they used to dump all the components in a box and give you a schematic and a soldering iron and happily say “you make it work!”And that worked in the early days of computing because there was a small number of people really motivated for the outcome.But fully assembled and packaged and tested in a nice looking box is where the real demand turned out to be.I’m looking forward to the day Google doesn’t just dump a model and say “you do the rest”.I want to fire up Ubuntu on a Strix Halo and say apt install then load the browser interface. Or just download and run a Mac installer and have it just work.Arcane complex multi step build install configure processes for AI need to end in favor of one click install. I’m not interested in the process of making it run.",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44902940,
    "by": "fibers",
    "timeISO": "2025-08-14T17:07:45.000Z",
    "textPlain": "Great job. Do you know how well it performs in sanity checks with NER since it is on the press release page?",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44902766,
    "by": "ActorNightly",
    "timeISO": "2025-08-14T16:54:23.000Z",
    "textPlain": "How does the 270 perform with coding?I use Gemma27b currently with a custom agent wrapper and its working pretty well.",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44904147,
    "by": "stefan_",
    "timeISO": "2025-08-14T18:50:52.000Z",
    "textPlain": "I suppose the odd repetition of \"I built these models with a great team\" is to warm us up to this model slipping into infinite gibberish repetition?",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44903338,
    "by": "VirusNewbie",
    "timeISO": "2025-08-14T17:39:43.000Z",
    "textPlain": "hi Ravin, fellow Googler here. Curious if you can share here (or internally?) how these models were trained.   Wondering if you face all the chaos the large models have during training?",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44903858,
    "by": "gnulinux",
    "timeISO": "2025-08-14T18:26:05.000Z",
    "textPlain": "Well, this is a 270M model which is like 1/3 of 1B parameters. In the grand scheme of things, it's basically a few matrix multiplications, barely anything more than that. I don't think it's meant to have a lot of knowledge, grammar, or even coherence. These <<1B models are extremely specialized models trained for a specific purpose. Models like this are optimized for things like this (not limited):input:\n```\nCustomer Review says: ai bought your prod-duct and I wanna return becaus it no good.Prompt: Create a JSON object that extracts information about this customer review based on the schema given.\n```output:\n```\n{\n\"type\": \"review\",\n\"class\": \"complaint\",\n\"sentiment\": -0.853,\n\"request\": \"return\"\n}\n```So essentially just \"making sense of\" natural language such that it can be used in programmatic context. (among other applications of course)To get good results, you probably need to fine tune this model to expected data very aggressively.The idea is, if a 270MB model can do with fine tuning, why ship a 32GB generalist model?",
    "parent": 44903731,
    "depth": 2
  },
  {
    "id": 44904173,
    "by": "canyon289",
    "timeISO": "2025-08-14T18:53:18.000Z",
    "textPlain": "To add to the comments, we were not aiming for perfect factuality. Even ignoring the model size, these weights are frozen in time now.My suggestions here are to hook this model up to a RAG system, then you can rely on an external knowledge store. Or you can try finetuning this model with the facts that are important to you, if you do that it should pick up that new knowledge quite quickly.",
    "parent": 44903731,
    "depth": 2
  },
  {
    "id": 44905159,
    "by": "jcuenod",
    "timeISO": "2025-08-14T20:17:15.000Z",
    "textPlain": "So I had a similar experience with your prompt (on the f16 model). But I do think that, at this size, prompting differences make a bigger impact. I had this experience trying to get it to list entities. It kept trying to give me a bulleted list and I was trying to coerce it into some sort of structured output. When I finally just said \"give me a bulleted list and nothing else\" the success rate went from around 0-0.1 to 0.8+.In this case, I changed the prompt to:---Tallest mountains (in order):```- Mount Everest- Mount K2- Mount Sahel- Mount Fuji- Mount McKinley```What is the second tallest mountain?---Suddenly, it got the answer right 95+% of the time",
    "parent": 44903731,
    "depth": 2
  },
  {
    "id": 44904186,
    "by": "yomismoaqui",
    "timeISO": "2025-08-14T18:54:28.000Z",
    "textPlain": "Evaluating a 270M model on encyclopedic knowledge is like opening a heavily compressed JPG image and saying \"it looks blocky\"",
    "parent": 44903731,
    "depth": 2
  },
  {
    "id": 44903799,
    "by": "cristyansv",
    "timeISO": "2025-08-14T18:19:05.000Z",
    "textPlain": "But in your prompts you're trying to assess knowledge, and this model isn't suited for that use caseas mentioned in the blog post: \n> \"it can execute tasks like text classification and data extraction with remarkable accuracy, speed, and cost-effectiveness.\"",
    "parent": 44903731,
    "depth": 2
  },
  {
    "id": 44903908,
    "by": "aldousd666",
    "timeISO": "2025-08-14T18:29:34.000Z",
    "textPlain": "It's an instruction following model, not a micro-wikipedia. It's not meant to answer factual questions nor even be general purpose. It's meant to follow instructions and be easy to fine-tune for your own specific use case.",
    "parent": 44903731,
    "depth": 2
  }
]
[
  {
    "id": 44902240,
    "by": "canyon289",
    "timeISO": "2025-08-14T16:14:07.000Z",
    "textPlain": "Hi all, I built these models with a great team. They're available for download across the open model ecosystem so give them a try! I built these models with a great team and am thrilled to get them out to you.From our side we designed these models to be strong for their size out of the box, and with the goal you'll all finetune it for your use case. With the small size it'll fit on a wide range of hardware and cost much less to finetune. You can try finetuning them yourself in a free colab in under 5 minutesFor picking a Gemma size this is a video I recorded for the 1b to 27b sizes earlier this year, 270m being the newest additionhttps://www.youtube.com/watch?v=qcjrduz_YS8Hacker News Disclaimer\nI really like working at Google so with that; All my opinions here are my own, I'm a researcher so I'll largely focus on technical questions, and I'll share what I can.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903731,
    "by": "jefb",
    "timeISO": "2025-08-14T18:12:32.000Z",
    "textPlain": "My lovely interaction with the 270M-F16 model:> what's second tallest mountain on earth?The second tallest mountain on Earth is Mount Everest.> what's the tallest mountain on earth?The tallest mountain on Earth is Mount Everest.> whats the second tallest mountain?The second tallest mountain in the world is Mount Everest.> whats the third tallest mountain?The third tallest mountain in the world is Mount Everest.> what's the forth tallest mountain?The forth tallest mountain is Mount Everest.> but you said the tallest was Everest alreadyYou are right! I'm glad to hear that.> so whats the second tallest?The second tallest is *Mount Everest*.> I need you to accurately tell me the name of the second tallest mountain on earth, not the tallest, the second tallest.The second tallest mountain on Earth is Mount Everest.> The tallest mountain on earth is mount everest. What is the second tallest mountain?The second tallest mountain on Earth is Mount Everest.> Give me a list of the tallest mountains on earthThe tallest mountain in the world is the *Mount Everest*.> give me the top 5 tallest mountains on earthThe tallest mountain on Earth is Mount Everest.> List in order the tallest mountains on earth from 1 to 51. Mount Everest\n2. Mount K2\n3. Mount Sahel\n4. Mount Fuji\n5. Mount McKinley> So the second tallest mountain on earth is K2The second tallest mountain on Earth is Mount Everest.These smaller models are great, but they really feel like talking to a toddler sometimes!",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44904181,
    "by": "AJRF",
    "timeISO": "2025-08-14T18:54:11.000Z",
    "textPlain": "I've got a very real world use case I use DistilBERT for - learning how to label wordpress articles. It is one of those things where it's kind of valuable (tagging) but not enough to spend loads on compute for it.The great thing is I have enough data (100k+) to fine-tune and run a meaningful classification report over. The data is very diverse, and while the labels aren't totally evenly distributed, I can deal with the imbalance with a few tricks.Can't wait to swap it out for this and see the changes in the scores. Will report back",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44902980,
    "by": "simonw",
    "timeISO": "2025-08-14T17:10:43.000Z",
    "textPlain": "This model is a LOT of fun. It's absolutely tiny - just a 241MB download - and screamingly fast, and hallucinates wildly about almost everything.Here's one of dozens of results I got for \"Generate an SVG of a pelican riding a bicycle\". For this one it decided to write a poem:  +-----------------------+\n  |   Pelican Riding Bike |\n  +-----------------------+\n  |  This is the cat!  |\n  |  He's got big wings and a happy tail.  |\n  |  He loves to ride his bike!  |\n  +-----------------------+\n  |   Bike lights are shining bright.  |\n  |   He's got a shiny top, too!  |\n  |   He's ready for adventure!  |\n  +-----------------------+\n\nThere are a bunch more attempts in this Gist, some of which do at least include an SVG tag albeit one that doesn't render anything: https://gist.github.com/simonw/25e7b7afd6a63a2f15db48b3a51ec...I'm looking forward to seeing people fine-tune this in a way that produces useful output for selected tasks, which should absolutely be feasible.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903495,
    "by": "mrcwinn",
    "timeISO": "2025-08-14T17:54:03.000Z",
    "textPlain": "Apple should be doing this. Unless their plan is to replace their search deal with an AI deal -- it's just crazy to me how absent Apple is. Tim Cook said, \"it's ours to take\" but they really seem to be grasping at the wind right now. Go Google!",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44908556,
    "by": "itake",
    "timeISO": "2025-08-15T04:16:03.000Z",
    "textPlain": "Can someone share why someone would use this over Qwen models? Gemma seems to always be behind Qwen?",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44908306,
    "by": "wanderingmind",
    "timeISO": "2025-08-15T03:25:25.000Z",
    "textPlain": "Maybe I'm using it wrong, but when I try to use the full precision FP16 model, load it into chatter UI and ask a simple question,\"write me a template to make a cold call to a potential lead\",It throws me absolute rubbish. On the other hand, Qwen 0.6B Q8 quantized model nails the answer for the same question.Qwen 0.6B is smaller than gemma full precision. The execution is a tad slow but not by much. I'm not sure why I need to pick a gemma over qwen.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44904066,
    "by": "miohtama",
    "timeISO": "2025-08-14T18:43:16.000Z",
    "textPlain": "Out of curiosity: because there seems to be a race to optimise models for local inference, how much \"parameters one could save\" by dropping unneeded language and domain-specific information.Like, can you have a model that is English-only, but does more with the same amount of parameters if Chinese and European languages are dropped from the training?",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903182,
    "by": "whinvik",
    "timeISO": "2025-08-14T17:26:45.000Z",
    "textPlain": "Curious. Are there real world usecases where people have finetuned such tiny models and put them into production.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903149,
    "by": "jasonjmcghee",
    "timeISO": "2025-08-14T17:24:09.000Z",
    "textPlain": "I'm _very_ interested to see what this can be fine-tuned to do.I've heard folks say a number of times that neuromuscular control / locomotion (or w/e) are hundreds of millions of parameters rather than billions.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44905188,
    "by": "jcuenod",
    "timeISO": "2025-08-14T20:20:02.000Z",
    "textPlain": "I mentioned elsewhere the impact of prompting, which seems to make an outsized difference to this model's performance. I tried NER and POS tagging (with somewhat disappointing results).One thing that worked strikingly well was translation on non-Indo-European languages. Like I had success with Thai and Bahasa Indonesian -> English...",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44902895,
    "by": "lemonish97",
    "timeISO": "2025-08-14T17:04:47.000Z",
    "textPlain": "Never thought I'd run an LLM released in 2025, on my phone, in full BF16.\nWith ~80tps on an iPhone 16 pro btw.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903817,
    "by": "perching_aix",
    "timeISO": "2025-08-14T18:21:11.000Z",
    "textPlain": "Is it time for me to finally package a language model into my Lambda deployment zips and cut through the corporate red tape at my place around AI use?Update #1:Tried it. Well, dreams dashed - would now fit space wise (<250 MB despite the name), but it sadly really doesn't seem to work for my specific prospective workload.I'd have wanted it to perform natural-language to command-invocation translation (or better, emit me some JSON), but it's super not willing to do that, not in the lame way I'm trying to make it do so at least (literally just prompting it to). Oh well.Update #2:Just found out about grammar-constrained decode, maybe there's still hope for me in the end. I don't think I can amend this comment today with any more updates, but will see.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44902789,
    "by": "44za12",
    "timeISO": "2025-08-14T16:55:38.000Z",
    "textPlain": "I’ve had great luck with all gemma 3 variants, on certain tasks it the 27B quantized version has worked as well as 2.5 flash. Can’t wait to get my hands dirty with this one.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903748,
    "by": "KTibow",
    "timeISO": "2025-08-14T18:14:02.000Z",
    "textPlain": "To add to the article: Gemma 3 270M's exact IFEval score is 51.2, and Qwen 3 would be at (0.6, 59.2) on the scatter plot.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44905303,
    "by": "highfrequency",
    "timeISO": "2025-08-14T20:29:16.000Z",
    "textPlain": "Interesting that for these small models, it is optimal for the embedding parameters to be a huge fraction of the total (170e6/250e6) = 68%!",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903914,
    "by": "nerdix",
    "timeISO": "2025-08-14T18:30:11.000Z",
    "textPlain": "Is it possible to finetune a model like this with local hardware? Every tutorial I've come across on finetuning a local LLM uses some cloud service like colab or runpod.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44904846,
    "by": "reneberlin",
    "timeISO": "2025-08-14T19:51:36.000Z",
    "textPlain": "I am sure with finetuning this can be changed somehow:(base)   ~ ollama run hf.co/unsloth/gemma-3-270m-it-GGUF:F16\n>>>  create a sentiment analysis of the follwing: \"It's raining.\"\nThe sentiment of the provided text is *negative*.>>>  create a sentiment analysis of the follwing: \"It's raining money.\"\nThe sentiment of the provided text is *negative*.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44907423,
    "by": "er69",
    "timeISO": "2025-08-15T00:42:32.000Z",
    "textPlain": "Looks very interesting ! I will evaluate this for one of our use cases for a data pipeline that handles around 50 million rows everyday . Mainly need it for segmenting user events. The low memory footprint is huge plus for cost at that scale . Need to spend this weekend on fine tuning, if this disappoints will try one of qwen model",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44908232,
    "by": "MagicMoonlight",
    "timeISO": "2025-08-15T03:12:45.000Z",
    "textPlain": "I don’t see the point, tiny models always just hallucinate and have no real knowledge.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903188,
    "by": "jtbayly",
    "timeISO": "2025-08-14T17:27:03.000Z",
    "textPlain": "Can somebody give me a link to a tutorial on how I would go about fine-tuning this?Also, what sorts of things might I consider fine-tuning it for?",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44904459,
    "by": "mrtimo",
    "timeISO": "2025-08-14T19:18:03.000Z",
    "textPlain": "I'm a business professor who teaches Python and more. I'd like to develop some simple projects to help my students fine tune this for a business purpose. If you have ideas (or datasets for fine tuning), let me know!",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44905759,
    "by": "hugoba",
    "timeISO": "2025-08-14T21:14:29.000Z",
    "textPlain": "this model is fire.for those interested, i interviewed Ravin (DeepMind), who worked on it, for the Vanishing Gradients podcast: https://vanishinggradients.fireside.fm/56Video on YT here: https://youtu.be/VZDw6C2A_8E?si=XLUzNRQzeloB9rkiDisclaimer: The Gemma family rock!",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44906652,
    "by": "ionwake",
    "timeISO": "2025-08-14T22:50:03.000Z",
    "textPlain": "This is first model I have downloaded on an M4 Air with ollama, that WORKS good enough to use. It is right now parsing all my notes correctly, fantastic work team. Brilliant.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44905613,
    "by": "ai-christianson",
    "timeISO": "2025-08-14T20:58:05.000Z",
    "textPlain": "This was trained on 6T tokens. Neat to see so many tokens used for such a small model.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44904664,
    "by": "amrrs",
    "timeISO": "2025-08-14T19:36:51.000Z",
    "textPlain": "Gemma 3 270M running on a midrange pixel device - https://x.com/1littlecoder/status/1956065040563331344",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44908013,
    "by": "mewmix",
    "timeISO": "2025-08-15T02:31:50.000Z",
    "textPlain": "You can now play with the model and kokoro 82m in my work in progress playground on android https://github.com/mewmix/nabu !",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44902845,
    "by": "Alex-Programs",
    "timeISO": "2025-08-14T16:59:56.000Z",
    "textPlain": "This is cool. I'm looking forward to trying it - I wonder what it'll be useful for.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903049,
    "by": "dcreater",
    "timeISO": "2025-08-14T17:15:19.000Z",
    "textPlain": "I've been saying he we need sub 1B models for the edge so thanks fot this.I am however disappointed that there is no examples, or benchmarks, provided to get a sense of performance. It's a given that benchmark values would be lower than gemma 3n, but having a sense of performance vs size curve and comparison to existing small models is needed",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44905263,
    "by": "fallinditch",
    "timeISO": "2025-08-14T20:26:56.000Z",
    "textPlain": "> this model is not designed for complex conversational use cases... but it's also the perfect choice for creative writing ...?Isn't this a contradiction?  How can a model be good at creative writing if it's no good at conversation?",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903840,
    "by": "robbru",
    "timeISO": "2025-08-14T18:23:12.000Z",
    "textPlain": "Excited to try this out, thanks for sharing.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44905681,
    "by": "michelsedgh",
    "timeISO": "2025-08-14T21:04:25.000Z",
    "textPlain": "I wish it was multimodal :(",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44904065,
    "by": "bbor",
    "timeISO": "2025-08-14T18:43:10.000Z",
    "textPlain": "Really impressive stuff, as always. I will say: it took me a shamefully long time to realize that the name ended in \"M\" instead of \"B\"! Perhaps they should consider renaming this to \"Gemma 3 .27B\"...",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44904444,
    "by": "metalliqaz",
    "timeISO": "2025-08-14T19:16:19.000Z",
    "textPlain": "is there a good resource for getting started with downloading and running something like this for a demo?  There are just so many tools/platforms in the mix now it makes my head spin.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903696,
    "by": "dismalaf",
    "timeISO": "2025-08-14T18:09:43.000Z",
    "textPlain": "It's fast at spitting out nonsense but incredibly slow at trying to parse any context.  Also absolutely atrocious at following instructions.Probably would be good as a game NPC or a chatbot, not very good for integrating into an application which specific functionality though.",
    "parent": 44902148,
    "depth": 1
  },
  {
    "id": 44903475,
    "by": "NorwegianDude",
    "timeISO": "2025-08-14T17:52:54.000Z",
    "textPlain": "The Gemma 3 models are great! One of the few models that can write Norwegian decently, and the instruction following is in my opinion good for most cases. I do however have some issues that might be related to censorship that I hope will be fixed if there is ever a Gemma 4. Maybe you have some insight into why this is happening?I run a game when players can post messages, it's a game where players can kill each other, and people often send threats along the lines of \"I will kill you\". Telling Gemma that it should classify a message as game related or a real life threat, and that it is for a message in a game where players can kill each other and threats are a part of the game, and that it should mark it as game related if it is unclear if the message is a game related threat or a real life threat does not work well. For other similar tasks it seems to follow instructions well, but for serious topics it seems to be very biased, and often err on the side of caution, despite being told not to. Sometimes it even spits out some help lines to contact.I guess this is because it was trained to be safe, and that affects it's ability to follow instructions for this? Or am I completely off here?",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44908834,
    "by": "katzenversteher",
    "timeISO": "2025-08-15T05:12:02.000Z",
    "textPlain": "I was wondering the whole time why people in the comments are so hyped about this, then I finally noticed (after I stumbled upon a comment about running this on a mobile phone) that it's \"270M\" not \"270B\" model :)",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44903827,
    "by": "ceroxylon",
    "timeISO": "2025-08-14T18:21:59.000Z",
    "textPlain": "You reminded me of an awesome Google engineer I met at BSidesSF last year who tirelessly answered my questions, and when I clicked on the video, it was you! That was a really inspiring moment for me, thank you.",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44903226,
    "by": "simonw",
    "timeISO": "2025-08-14T17:30:09.000Z",
    "textPlain": "Do you have any practical examples of fine-tuned variants of this that you can share? A description would be great, but a demo or even downloadable model weights (GGUF ideally) would be even better.",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44904996,
    "by": "ankit219",
    "timeISO": "2025-08-14T20:03:01.000Z",
    "textPlain": "This is super cool. Usually you dont see effective models at 270M out in the wild. The architectural choices are new and interesting as well.Would it be okay for you to divulge some more training information here? With 170M embedding parameters, how do you ensure no embedding collapse and keeping the embedding matrix stable at training time?(i know i am asking too much, but just curious). There is a clear trade off for you with vocab / transformer layers. How did you arrive at the split of 170m/100m. Does this contribute to model's performance on task specific fine tuning? Any internal experiments you could share? or public info you could point us to? Anything would be amazing.PS: I am sorry if this is rude, but this has so many decisions i am curious about. Not intending to undermine anything, this is amazing work, and thank you for the whole Gemma series.",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44903942,
    "by": "jmorgan",
    "timeISO": "2025-08-14T18:32:09.000Z",
    "textPlain": "Amazing work. This model feels really good at one-off tasks like summarization and autocomplete. I really love that you released a quantized aware training version on launch day as well, making it even smaller!",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44902889,
    "by": "beoberha",
    "timeISO": "2025-08-14T17:04:18.000Z",
    "textPlain": "Awesome work! I’m really bullish on small models and think they have the most potential to change our daily lives. Can’t wait to play around with this",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44906419,
    "by": "nh43215rgb",
    "timeISO": "2025-08-14T22:25:06.000Z",
    "textPlain": "270M is nice (and rare) addition. Is there a reason why this is not categorized as gemma3n model? I thought small models go under gemma3n category",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44905907,
    "by": "dileeparanawake",
    "timeISO": "2025-08-14T21:27:52.000Z",
    "textPlain": "This is cool. For on device models any plans / models that use MOE in relatively resource constrained setups (I’m thinking MBP M1 16gb ram)? I’m using LM studio but all the Gemma models (mlx) seem to crash but surprisingly managed to get gpt-oss 20b working (slow) on my mbp.I find performance in resource constrained environments interesting.In particular trying to find decent code models (on device backup) but also tts applications and voice to text.",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44908345,
    "by": "peter492927",
    "timeISO": "2025-08-15T03:32:42.000Z",
    "textPlain": "Thank you a lot for working on these models! If you think it would make sense, I think a bigger sized Gemma model would be really cool. Models in the 70B parameter range can be run at q4 on two 3090 or similar hardware and should offer considerable performance improvement over 27B. There’s also the DGX Spark as a possible target.",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44904326,
    "by": "blitzar",
    "timeISO": "2025-08-14T19:05:56.000Z",
    "textPlain": "> I built these models with a great team ... I built these models with a great teamIf Gemini is going to repeat something at least its that the team is great, and not a disgrace!",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44907884,
    "by": "rao-v",
    "timeISO": "2025-08-15T02:11:10.000Z",
    "textPlain": "Fabulous stuff!Oh my request … the vision head on the Gemma models is super slow on CPU inferencing (and via Vulcan), even via llama.cpp. Any chance your team can figure out a solve? Other ViTs don’t have the same problem.",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44904457,
    "by": "imasl42",
    "timeISO": "2025-08-14T19:17:31.000Z",
    "textPlain": "Awesome! I’m curious how is the team you built these models with? Is it great?",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44903851,
    "by": "nerdsniper",
    "timeISO": "2025-08-14T18:25:39.000Z",
    "textPlain": "What are some of the use cases that you think the 270M would be most appropriate for? What would you love to see people trying with it?",
    "parent": 44902240,
    "depth": 2
  },
  {
    "id": 44903234,
    "by": "cgdl",
    "timeISO": "2025-08-14T17:30:41.000Z",
    "textPlain": "Very cool. For the INT4 QAT model, what is the recommended precision for the activations and for the key and values stored in KV cache?",
    "parent": 44902240,
    "depth": 2
  }
]
[
  {
    "id": 44851726,
    "by": "SlightlyLeftPad",
    "timeISO": "2025-08-10T00:35:20.000Z",
    "textPlain": "Any EEs that can comment on at what point do we just flip the architecture over so the GPU pcb is the motherboard and the cpu/memory lives on a PCIe slot? It seems like that would also have some power delivery advantages.",
    "parent": 44851019,
    "depth": 1
  },
  {
    "id": 44892705,
    "by": "bhouston",
    "timeISO": "2025-08-13T19:26:37.000Z",
    "textPlain": "I love the PCIe standard is 3 generations ahead of what is actually released.  Gen5 is the live version, but the team behind it is so well organized that they have a roadmap of 3 additional versions now.  Love it.",
    "parent": 44851019,
    "depth": 1
  },
  {
    "id": 44893906,
    "by": "robotnikman",
    "timeISO": "2025-08-13T21:12:31.000Z",
    "textPlain": "I know very little about electronics design, so I always find it amazing that they keep managing to double PCIe throughput over and over. Its also probably the longest lived expansion bus at the moment.",
    "parent": 44851019,
    "depth": 1
  },
  {
    "id": 44897329,
    "by": "iFred",
    "timeISO": "2025-08-14T06:28:53.000Z",
    "textPlain": "I can't be the only one let down that there wasn't some new slot design. Something with pizzazz and flare.",
    "parent": 44851019,
    "depth": 1
  },
  {
    "id": 44896366,
    "by": "bpbp-mango",
    "timeISO": "2025-08-14T03:10:08.000Z",
    "textPlain": "I wonder if this will help applications like VPP/DPDK. not sure if the CPU or the lanes are the bottleneck there.",
    "parent": 44851019,
    "depth": 1
  },
  {
    "id": 44892563,
    "by": "zkms",
    "timeISO": "2025-08-13T19:13:29.000Z",
    "textPlain": "My reaction to PCIe gen 8 is essentially \"Huh? No, retro data buses are like ISA, PCI, and AGP, right? PCIe Gen 3 and SATA are still pretty new...\".I wonder what modulation order / RF bandwidth they'll be using on the PHY for Gen8. I think Gen7 used 32GHz, which is ridiculously high.",
    "parent": 44851019,
    "depth": 1
  },
  {
    "id": 44894293,
    "by": "pshirshov",
    "timeISO": "2025-08-13T21:52:55.000Z",
    "textPlain": "Yeah, such a shame I've just upgraded to a 7.0 motherboard for my socket AM7 CPU.Being less sarcastic, I would ask if 6.0 mobos are on the horizon.",
    "parent": 44851019,
    "depth": 1
  },
  {
    "id": 44893125,
    "by": "LeoPanthera",
    "timeISO": "2025-08-13T20:02:07.000Z",
    "textPlain": "I thought we were only just up to 5? Did we skip 6 and 7?",
    "parent": 44851019,
    "depth": 1
  },
  {
    "id": 44895188,
    "by": "_zoltan_",
    "timeISO": "2025-08-13T23:40:11.000Z",
    "textPlain": "what I don't get: why doesn't AMD just roll Gen6 out in their CPU, bifurcate it to Gen5, and boom, you have 48x2 Gen5s? same argument for gen5 bifurcated to gen4.this would solve the biggest issue with non-server motherboards: not enough PCIe lanes.",
    "parent": 44851019,
    "depth": 1
  },
  {
    "id": 44892721,
    "by": "ThatMedicIsASpy",
    "timeISO": "2025-08-13T19:28:31.000Z",
    "textPlain": "I'll take it if my consumer mb chipset supports giving me 48 PCIe7 lanes if future desktops still would only come with 24 gen 8 lanes",
    "parent": 44851019,
    "depth": 1
  },
  {
    "id": 44895463,
    "by": "Melatonic",
    "timeISO": "2025-08-14T00:24:08.000Z",
    "textPlain": "I feel like what we really need is a GPU \"socket\" like we have for CPU's. And then a set of RAM slots dedicated to that GPU socket (or unified RAM shared between CPU and GPU)",
    "parent": 44851019,
    "depth": 1
  },
  {
    "id": 44892724,
    "by": "richwater",
    "timeISO": "2025-08-13T19:28:42.000Z",
    "textPlain": "Meanwhile paying a premium for a Gen5 motherboard may net you somewhere in the realm of 4% improvements in gaming if you're lucky.Obviously PCI is not just about gaming but...",
    "parent": 44851019,
    "depth": 1
  },
  {
    "id": 44894213,
    "by": "rui9",
    "timeISO": "2025-08-13T21:43:31.000Z",
    "textPlain": "[dead]",
    "parent": 44851019,
    "depth": 1
  },
  {
    "id": 44897589,
    "by": "bgnn",
    "timeISO": "2025-08-14T07:10:26.000Z",
    "textPlain": "EE here. There's no reason to not deliver power directly to the GPU by using cables. I'm not sure if it's sooving anything.But you are right, there's no hierarchy in the systems anymore. Why do we even call something a motherboard? There's a bunch of chips interconnected.",
    "parent": 44851726,
    "depth": 2
  },
  {
    "id": 44893003,
    "by": "kvemkon",
    "timeISO": "2025-08-13T19:51:45.000Z",
    "textPlain": "> at what point do we just flip the architecture over so the GPU pcb is the motherboard and the cpu/memoryActually the RapsberryPi (appeared 2012) was based on a SoC with a big and powerful GPU and small weak supporting CPU. The board booted the GPU first.",
    "parent": 44851726,
    "depth": 2
  },
  {
    "id": 44892262,
    "by": "verall",
    "timeISO": "2025-08-13T18:48:03.000Z",
    "textPlain": "If you look at a any of the nvidia DGX boards it's already pretty close.PCIe is a standard/commodity so that multiple vendors can compete and customers can save money. But at 8.0 speeds I'm not sure how many vendors will really be supplying, there's already only a few doing serdes this fast...",
    "parent": 44851726,
    "depth": 2
  },
  {
    "id": 44852866,
    "by": "vincheezel",
    "timeISO": "2025-08-10T05:01:07.000Z",
    "textPlain": "Good to see I’m not the only person that’s been thinking about this. Wedging gargantuan GPUs onto boards and into cases, sometimes needing support struts even, and pumping hundreds of watts through a power cable makes little sense to me. The CPU, RAM, these should be modules or cards on the GPU. Imagine that! CPU cards might be back..",
    "parent": 44851726,
    "depth": 2
  },
  {
    "id": 44895662,
    "by": "0manrho",
    "timeISO": "2025-08-14T01:00:02.000Z",
    "textPlain": "We're already there. That's what a lot of people are using DPU's are for.An example, This is storage instead of GPU's, but as the SSD's were PCIe NVMe, it's pretty nearly the same concept: https://www.servethehome.com/zfs-without-a-server-using-the-...",
    "parent": 44851726,
    "depth": 2
  },
  {
    "id": 44894351,
    "by": "dylan604",
    "timeISO": "2025-08-13T21:58:07.000Z",
    "textPlain": "Wouldn't that mean an complete mobo replacement to upgrade the GPU? GPU upgrades seem much more rapid and substantial compared to CPU/RAM. Each upgrade would now mean taking out the CPU/RAM and other cards vs just replacing the GPU",
    "parent": 44851726,
    "depth": 2
  },
  {
    "id": 44894281,
    "by": "pshirshov",
    "timeISO": "2025-08-13T21:51:36.000Z",
    "textPlain": "Can I just have a backplane? Pretty please?",
    "parent": 44851726,
    "depth": 2
  },
  {
    "id": 44892285,
    "by": "MurkyLabs",
    "timeISO": "2025-08-13T18:49:54.000Z",
    "textPlain": "Yes I agree, let's bring back the SECC style CPU's from the Pentium Era, I've still got my Pentium II (with MMX technology)",
    "parent": 44851726,
    "depth": 2
  },
  {
    "id": 44895952,
    "by": "coherentpony",
    "timeISO": "2025-08-14T01:54:52.000Z",
    "textPlain": "The concept exists now.  You can \"reverse offload\" work to the CPU.",
    "parent": 44851726,
    "depth": 2
  },
  {
    "id": 44892782,
    "by": "Dylan16807",
    "timeISO": "2025-08-13T19:33:09.000Z",
    "textPlain": "And limit yourself to only one GPU?Also CPUs are able to make use of more space for memory, both horizontally and vertically.I don't really see the power delivery advantages, either way you're running a bunch of EPS12V or similar cables around.",
    "parent": 44851726,
    "depth": 2
  },
  {
    "id": 44894050,
    "by": "leoapagano",
    "timeISO": "2025-08-13T21:25:54.000Z",
    "textPlain": "One possible advantage of this approach that no one here has mentioned yet is that it would allow us to put RAM on the CPU die (allowing for us to take advantage of the greater memory bandwidth) while also allowing for upgradable RAM.",
    "parent": 44851726,
    "depth": 2
  },
  {
    "id": 44892851,
    "by": "burnt-resistor",
    "timeISO": "2025-08-13T19:38:48.000Z",
    "textPlain": "Figure out how much RAM, L1-3|4 cache, integer, vector, graphics, and AI horsepower is needed for a use-case ahead-of-time and cram them all into one huge socket with intensive power rails and cooling. The internal RAM bus doesn't have to be DDRn/X either. An integrated northbridge would deliver PCIe, etc.",
    "parent": 44851726,
    "depth": 2
  },
  {
    "id": 44892597,
    "by": "Razengan",
    "timeISO": "2025-08-13T19:15:40.000Z",
    "textPlain": "Isn't that what has kinda sorta basically happened with Apple Silicon?",
    "parent": 44851726,
    "depth": 2
  },
  {
    "id": 44893141,
    "by": "LeoPanthera",
    "timeISO": "2025-08-13T20:02:48.000Z",
    "textPlain": "Bring back the S100 bus and put literally everything on a card. Your motherboard is just a dumb bus backplane.",
    "parent": 44851726,
    "depth": 2
  },
  {
    "id": 44894945,
    "by": "zamadatix",
    "timeISO": "2025-08-13T23:07:30.000Z",
    "textPlain": "\"3 generations\" seems like a bit of a stretch. Millions of Blackwell systems use PCIe 6.x today, PCIe 7.x was finalized last month, and this is an announcement work on PCIe 8.0 has started for release in 3 years. I.e. it has only been one month of being one generation behind the latest PCIe revision.It'll be interesting if consumer devices bother trying to stay with the latest at all anymore. It's already extremely difficult to justify the cost of implementing PCIe 5.0 when it makes almost no difference for consumer use cases. The best consumer use case so far is enthusiasts who want really fast NVMe SSDs in x4 lanes, but 5.0 already gives >10 GB/s for a single drive, even with the limited lane count. It makes very little difference for x16 GPUs, even with the 5090. Things always creep up over time, but the rate at which the consumer space creeps is just so vastly different from what the DC space has been seeing that it seems unreasonable to expect the two to be lockstep anymore.",
    "parent": 44892705,
    "depth": 2
  },
  {
    "id": 44892764,
    "by": "tails4e",
    "timeISO": "2025-08-13T19:31:59.000Z",
    "textPlain": "It takes a long time to get form standard to silicon, so I bet there are design teams working on pcie7 right now, which won't see products for 2 or more years",
    "parent": 44892705,
    "depth": 2
  },
  {
    "id": 44893098,
    "by": "Seattle3503",
    "timeISO": "2025-08-13T19:59:42.000Z",
    "textPlain": "Is there an advantage of getting so far ahead of implementations? It seems like it would be more difficult to incorporate lessons.",
    "parent": 44892705,
    "depth": 2
  },
  {
    "id": 44896285,
    "by": "jsolson",
    "timeISO": "2025-08-14T02:52:23.000Z",
    "textPlain": "This actually makes sense from a spec perspective if you want to give enough to allow hardware to catch up with the specs and to support true interop.Contrast this with the wild west that is \"Ethernet\" where it's extremely common for speeds to track well ahead of specs and where interop is, at best, \"exciting.\"",
    "parent": 44892705,
    "depth": 2
  },
  {
    "id": 44892757,
    "by": "ThatMedicIsASpy",
    "timeISO": "2025-08-13T19:31:36.000Z",
    "textPlain": "Gen6 is in use look at Nvidia ConnectX-8",
    "parent": 44892705,
    "depth": 2
  },
  {
    "id": 44893738,
    "by": "Phelinofist",
    "timeISO": "2025-08-13T20:57:10.000Z",
    "textPlain": "So we can skip 6 and 7 and go directly to 8, right?",
    "parent": 44892705,
    "depth": 2
  },
  {
    "id": 44894295,
    "by": "wmf",
    "timeISO": "2025-08-13T21:53:01.000Z",
    "textPlain": "It's less surprising if you realize that PCIe is behind Ethernet (per lane).",
    "parent": 44893906,
    "depth": 2
  },
  {
    "id": 44894365,
    "by": "rbanffy",
    "timeISO": "2025-08-13T21:59:41.000Z",
    "textPlain": "I’m sure you can get some VMEbus boards.",
    "parent": 44893906,
    "depth": 2
  },
  {
    "id": 44893055,
    "by": "Dylan16807",
    "timeISO": "2025-08-13T19:55:59.000Z",
    "textPlain": "> PCIe Gen 3 and SATA are still pretty new...That's an interesting thought to look at.  PCIe 3 was a while ago, but SATA was nearly a decade before that.> I wonder what modulation order / RF bandwidth they'll be using on the PHY for Gen8. I think Gen7 used 32GHz, which is ridiculously high.Wikipedia says it's planned to be PAM4 just like 6 and 7.Gen 5 and 6 were 32 gigabaud.  If 8 is PAM4 it'll be 128 gigabaud...",
    "parent": 44892563,
    "depth": 2
  },
  {
    "id": 44892826,
    "by": "eqvinox",
    "timeISO": "2025-08-13T19:36:55.000Z",
    "textPlain": "I'd highly advise against using GHz here (without further context, at least), a 32Gbaud / 32Gsym/s NRZ signal toggling at full rate is only a 16GHz square wave.baud seems out of fashion, sym/s is pretty clear & unambiguous.(And if you're talking channel bandwidth, that needs clarification)",
    "parent": 44892563,
    "depth": 2
  },
  {
    "id": 44894161,
    "by": "weinzierl",
    "timeISO": "2025-08-13T21:37:42.000Z",
    "textPlain": "Don't forget VESA Local Bus.",
    "parent": 44892563,
    "depth": 2
  },
  {
    "id": 44894328,
    "by": "wmf",
    "timeISO": "2025-08-13T21:55:38.000Z",
    "textPlain": "I guess Venice, Diamond Rapids, and Vera will have 6.0.",
    "parent": 44894293,
    "depth": 2
  },
  {
    "id": 44893951,
    "by": "pkaye",
    "timeISO": "2025-08-13T21:16:39.000Z",
    "textPlain": "Some of the newer ones maybe more for data centers.",
    "parent": 44893125,
    "depth": 2
  },
  {
    "id": 44895448,
    "by": "zamadatix",
    "timeISO": "2025-08-14T00:22:01.000Z",
    "textPlain": "Bifurcation can't create new lanes, only split the existing lane count up into separate logical slots.What you're saying is possible though, you just need something a little heavier like a PCIe switch do the lane count + mixed-speed conversion magic. That's exactly what the chipset is, but for various reasons it's still only PCIe 4.0, even on the latest generation chips. I wouldn't be surprised if that changed again next generation. The downsides of this approach are switches add cost, latency, and can consume a lot of power. When they first upped the chipset to be a PCIe 4.0 connection in the 500 era, most motherboards capable of using the bandwidth of the chipset actually had to add chipset cooling.Ideally they'd just add alternative options with more lanes directly from the CPU, but that'd add competition to the bottom of the Threadripper line.",
    "parent": 44895188,
    "depth": 2
  },
  {
    "id": 44895513,
    "by": "michaelt",
    "timeISO": "2025-08-14T00:32:15.000Z",
    "textPlain": "> this would solve the biggest issue with non-server motherboards: not enough PCIe lanes.Is that a problem?These days, you don't need slots for your sound card and network card, that stuff's all integrated on the motherboard.Plenty of enthusiast motherboards support 1 GPU, 3-4 nvme drives, 4 SATA drives, and have a PCIe 1x slot or two to spare.Is anyone struggling, except folks working with LLMs? Seems to me folks looking to put several $2400 RTX 4090s in one machine ain't exactly a big market. And they'll probably want a giant server board, so they have space for all their giant four-slot-wide cards.",
    "parent": 44895188,
    "depth": 2
  },
  {
    "id": 44895717,
    "by": "Lramseyer",
    "timeISO": "2025-08-14T01:09:15.000Z",
    "textPlain": "Unfortunately socketed processors only really work with DDRx type DRAM interfaces. GPUs use GDDR and HBM interfaces, which are not ideal for sockets. In the case of HBM, you have 1024 data traces per DRAM chip, which would make the socket have an insane number of pins. GDDR has fewer pins, but makes up for it with higher data rates (32 pins at 16Gb/s) and that is impractical to use in a socket due to the variance in contact area resulting in impedance matching issues.",
    "parent": 44895463,
    "depth": 2
  },
  {
    "id": 44895703,
    "by": "Sephr",
    "timeISO": "2025-08-14T01:06:48.000Z",
    "textPlain": "HBM would most likely have to be integrated directly on the GPU module given performance demands and signal constraints.",
    "parent": 44895463,
    "depth": 2
  },
  {
    "id": 44894338,
    "by": "checker659",
    "timeISO": "2025-08-13T21:56:10.000Z",
    "textPlain": "No matter the leaps in bandwidth, the latency remains the same. Also, with PCIe switches used in AI servers, the latency (and jitter) is even pronounced.",
    "parent": 44892724,
    "depth": 2
  },
  {
    "id": 44893053,
    "by": "simoncion",
    "timeISO": "2025-08-13T19:55:55.000Z",
    "textPlain": "From what I've seen, the faster PCI-E bus is important when you need to shuffle things in and out of VRAM. In a video game, the faster bus reduces the duration of stutters caused by pushing more data into the graphics card.If you're using a new video card with only 8GB of onboard RAM and are turning on all the heavily-advertised bells and whistles on new games, you're going to be running out of VRAM very, very frequently. The faster bus isn't really important for higher frame rate, it makes the worst-case situations less bad.I get the impression that many reviewers aren't equipped to do the sort of review that asks questions like \"What's the intensity and frequency of the stuttering in the game?\" because that's a bit harder than just looking at average, peak, and 90% frame rates. The question \"How often do textures load at reduced resolution, or not at all?\" probably requires a human in the loop to look at the rendered output to notice those sorts of errors... which is time consuming, attention-demanding work.",
    "parent": 44892724,
    "depth": 2
  },
  {
    "id": 44893238,
    "by": "jeffbee",
    "timeISO": "2025-08-13T20:10:22.000Z",
    "textPlain": "By an overwhelming margin, most computers are not in gamers' basements.",
    "parent": 44892724,
    "depth": 2
  }
]
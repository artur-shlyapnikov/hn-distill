{
  "updatedISO": "2025-08-07T12:07:12Z",
  "items": [
    {
      "id": 44822637,
      "title": "\"I closed MPEG on 2 Jun '20 when I left because obscure forces had hijacked it.\"",
      "url": "https://leonardo.chiariglione.org/",
      "by": "eggspurt",
      "timeISO": "2025-08-07T10:09:41.000Z",
      "postSummary": "- Леонардо — бывший исследователь видеокодирования, ветеран стандартизации и предприниматель.\n\n- Образование: классическая школа (Лицей Салезиан Вальсаличе), инженер-электронщик (Политех Турина, MSc), межкультурный опыт (Токийский университет, PhD по электрическим коммуникациям).\n\n- Христианско-католическое воспитание сформировало убеждение о миссии, выходящей за личные интересы. В начале карьеры, когда цифровые медиа только зрели, видел свою миссию в разработке интероперабельных технологий цифровых медиа — на благо общества и для использования индустрией.\n\n- Нужна была организация, создающая стандарты цифровых медиа, чтобы потребители могли бесшовно общаться, а индустрия работать на глобальном рынке совместимых продуктов и сервисов. Так в 1987 была задумана, а в 1988 создана группа Moving Picture Experts Group (MPEG).\n\n- Через четыре года MPEG запустила цифровую эру: MPEG‑1 для интерактивных медиа (Video CD), цифрового аудио (MP2) и персональной музыки (MP3). С середины 1990‑х MPEG‑2 стал инфраструктурой для цифрового ТВ по кабелю, спутнику, эфирным сетям и на DVD. MPEG‑4 (первый релиз — 1998) открыл путь интернет‑дистрибуции медиа. Далее последовали семейства стандартов: MPEG‑7, MPEG‑21, MPEG‑A, MPEG‑H, MPEG‑I и др.\n\n- Я председательствовал в группе, добившись более 200 стандартов, роста участия в 20 раз (с 29 экспертов) и расширения тематики от медиа к геномике — «рожденным цифровыми» данным мира.\n\n- 2 июня 2020 я закрыл MPEG и ушел, так как «темные силы» перехватили управление. Ещё до этого у MPEG иссяк импульс — технологически и бизнес‑wise.",
      "commentsSummary": "- Обсуждение вокруг MPEG: основатель утверждает, что организацию «захватили» интересы, удерживающие её в устаревших моделях лицензирования ИС, что тормозило технический прогресс и внедрение стандартов.  \n- Некоторые участники считают, что MPEG и патенты на h264/h265/mp3 десятилетиями тормозили индустрию; другие спрашивают, какие именно «неясные силы» мешали, и почему не получилось как у USB-альянса.  \n- Отмечается, что разработка кодеков медленная и дорогая: требуются сотни высокооплачиваемых инженеров и огромные вычислительные мощности; бесплатные кодеки появились в основном благодаря субсидиям (например, Google) и поглощениям компаний.  \n- Есть мнение, что кодеков уже достаточно, проблема в их принятии рынком.  \n- Несколько реплик утверждают, что сжатие и ИИ тесно связаны: «каждый предсказатель — компрессор», ссылки на приз Хуттера и проекты нейросетевого сжатия; ИИ-апскейлинг может позволить передавать меньше данных.  \n- Часть комментариев оффтопик/перепутаны треды, участники направляют друг друга к нужным веткам.  \n- Итоговый мотив: структура стимулов (патенты, лицензирование, финансирование) и высокая стоимость R&D определяли эволюцию кодеков не меньше, чем чисто технические факторы.",
      "score": 99,
      "commentsCount": 52,
      "hnUrl": "https://news.ycombinator.com/item?id=44822637",
      "domain": "leonardo.chiariglione.org"
    },
    {
      "id": 44822389,
      "title": "New AI Coding Teammate: Gemini CLI GitHub Actions",
      "url": "https://blog.google/technology/developers/introducing-gemini-cli-github-actions/",
      "by": "michael-sumner",
      "timeISO": "2025-08-07T09:28:46.000Z",
      "postSummary": "Gemini CLI GitHub Actions: совместная ИИ‑разработка\n\n- Представлен бесплатный ИИ‑напарник для репозиториев: автономный агент для рутинных задач и помощник по запросу.\n- Интегрируется с GitHub Actions и CLI, позволяет делегировать работу, ускорять ревью и поддерживать качество кода.\n- Поддерживает типовые сценарии: генерация и правки кода, исправление багов, написание тестов, обновление документации, автоматизация задач CI/CD.\n- Работает как часть рабочего процесса команды: комментирует PR, предлагает патчи, запускает проверки и формирует отчеты.\n- Цель — уменьшить вручную выполняемые повторяющиеся задачи, повысить скорость и согласованность разработки, сохраняя контроль у разработчиков.",
      "commentsSummary": "- Обсуждение вокруг нового инструмента Google: это, похоже, GitHub Action, который запускает Gemini CLI внутри workflow, но позиционирование и нейминг (“gemini cli”) создают путаницу — многие не понимают, это CLI, GitHub Action или приложение.  \n- Пользователи критикуют фрагментацию продуктов Google (Jules vs Gemini, множество перекрывающихся инициатив), сложные тарифы и аутентификацию: OAuth-планы Gemini One/Ultra/Workspace не работают с этим Action, требуется отдельный API-ключ из AI Studio с более жёсткими лимитами.  \n- Возникает сравнение с GitHub Copilot Agent: текущий вариант от Google выглядит высокотрением — много настроек, поэтому должен быть существенно лучше, чтобы вытеснить существующие инструменты.  \n- Практический опыт: Gemini в code review иногда лучше Copilot’а, но GitHub-агент в браузере заметно слабее VSCode-агента; ценность веб-агента — не грузит локальную машину.  \n- Общая боль: перегруженные описания и маркетинговые формулировки мешают быстро понять, “что это такое” и как это настроить; UX и поддержка у Google часто запутаны.  \n- Замечания о брендинге и консистентности: пользователи хотят ясного видения, простых подписок и понятной интеграции, иначе продукты «конкурируют» друг с другом и сбивают с толку.  \n- Советы по использованию: для Copilot рекомендуют добавить copilot-instructions.md и настроить copilot-setup-steps, чтобы уменьшить «трату времени» на сборку монорепо.",
      "score": 48,
      "commentsCount": 21,
      "hnUrl": "https://news.ycombinator.com/item?id=44822389",
      "domain": "blog.google"
    },
    {
      "id": 44819917,
      "title": "We replaced passwords with something worse",
      "url": "https://blog.danielh.cc/blog/passwords",
      "by": "max__dev",
      "timeISO": "2025-08-07T02:19:25.000Z",
      "postSummary": "Слишком многие сервисы используют такой вход:\n- Введите email или телефон\n- Сайт отправит 6‑значный код\n- Введите код для входа\n\nПожалуйста, прекратите.\n\nПочему это плохо для безопасности:\n- Злоумышленник может отправить ваш email на легитимный сервис и заставить вас ввести присланный код в фишинговой форме. Вы не можете быть уверены, где именно нужно вводить код. Менеджеры паролей тут не помогают.\n- Этот метод реально эксплуатируется: вход Microsoft для аккаунтов Minecraft использует такие коды, и уже множество аккаунтов было украдено (есть подтверждения на Reddit и YouTube, а также в документации Microsoft).",
      "commentsSummary": "- Обсуждение критикует вход по одноразовым 6‑значным кодам из e‑почты: схема легко фишится через «мост» между злонамеренным сайтом и легитимным сервисом, а пользователи привыкают к вредным паттернам.  \n- Многие жалуются на плохой UX: постоянные письма/коды, переключение между устройствами/приложениями, автозаполнение паролей работает лучше, а некоторые сервисы навязывают e‑mail OTP и даже ломают отписку/логин потоки.  \n- Защиту считают перекладыванием риска на почтовых провайдеров (часто Gmail), а некоторые получают бесконечные запросы на сброс пароля, что создает шум и небольшой, но систематический риск.  \n- Часть участников считает, что пароль + менеджер паролей или длинные пароли надёжнее и удобнее; другие отмечают, что для «нетехнарей» код из письма — понятный второй способ после паролей.  \n- В качестве альтернатив и смягчений предлагают: магические ссылки вместо кодов, passkeys/ФИДО, аутентификаторы, обязательную связку «код + длинный токен», улучшения валидации писем и «доказательства согласия» между почтой и сервисом.  \n- Есть скепсис: пароли чаще утекают и переиспользуются, так что для массового пользователя e‑mail ссылки могут быть практичнее; но SMS и видимые на локскрине коды критикуют как небезопасные.  \n- Отдельные разработчики делятся опытом: полностью безпарольные входы на passkeys реализуются несложно (например, через Authentik) и дают лучший UX и устойчивость к фишингу.",
      "score": 412,
      "commentsCount": 319,
      "hnUrl": "https://news.ycombinator.com/item?id=44819917",
      "domain": "blog.danielh.cc"
    },
    {
      "id": 44823066,
      "title": "Schools are using AI surveillance to protect students. Sometimes arresting them",
      "url": "https://apnews.com/article/ai-school-surveillance-gaggle-goguardian-bark-8c531cde8f9aee0b1ef06cfce109724a",
      "by": "djoldman",
      "timeISO": "2025-08-07T11:17:32.000Z",
      "postSummary": "Системы школьного ИИ‑надзора вроде Gaggle могут приводить к ложным тревогам и арестам\n\nШкольные платформы мониторинга (Gaggle, GoGuardian, Bark), анализируя активность учеников в аккаунтах и устройствах, заявляют о предотвращении самоубийств и насилия. Но расследования показывают: часто срабатывают ложные тревоги — невинные поисковые запросы, задания по литературе или шутки помечаются как угрозы. Это запускает вмешательства полиции и соцслужб, вызывает обыски, задержания и дисциплинарные меры, усиливает тревожность и стигматизацию подростков.\n\nКритики указывают на непрозрачность алгоритмов, предвзятость к уязвимым группам и сбор персональных данных без достаточного контроля. Родители и правозащитники требуют ограничить слежку, улучшить проверку срабатываний и обеспечить поддержку психического здоровья вместо карательных мер. Компании отвечают, что помогают школам выполнять обязанности по безопасности, а решения принимают люди‑модераторы, но случаи ошибок и последствий продолжают накапливаться.",
      "commentsSummary": "- Участники спорят о школьном надзоре за детьми и его последствиях: критики считают, что это нормализует тотальную слежку и пугает детей, подрывая общество.  \n- Приводится опасение, что, привыкнув к контролю, повзрослевшие будут продвигать массовую превентивную слежку для «безопасности».  \n- Некоторые сравнивают ситуацию с антиутопией («1984») и указывают на усиление ограничений, включая запрет телефонов.  \n- Критики запрета телефонов отмечают, что без них у учеников нет доказательств инцидентов (фото/видео).  \n- Оппоненты отвечают, что в обсуждаемом случае проступки и так были задокументированы через школьную платформу; телефоны лишь добавляют отвлечений и каналов слежки.  \n- Есть консенсус, что школы могут мониторить собственные коммуникационные системы, но разногласия по поводу масштаба и строгости наказаний.",
      "score": 12,
      "commentsCount": 5,
      "hnUrl": "https://news.ycombinator.com/item?id=44823066",
      "domain": "apnews.com"
    },
    {
      "id": 44822258,
      "title": "About AI",
      "url": "https://priver.dev/blog/ai/about-ai/",
      "by": "emil_priver",
      "timeISO": "2025-08-07T09:05:43.000Z",
      "postSummary": "За последние полтора года я целенаправленно работал с ИИ, понимая, что будущее разработки всё больше связано с ним. Я оптимизировал свой процесс: когда ИИ реально помогает, а когда мешает. Теперь могу сформулировать мнение.\n\nИИ превратился в гонку стран и компаний за статус. Тот, кто первым сделает AGI, «победит». Американские модели тяжёлые и прожорливые по ресурсам — отсюда дата-центры с GPU в США и Норвегии. Китайские модели близки к Claude Opus, но требуют существенно меньше ресурсов.\n\nВ отрасли диссонанс: GitHub и Пичай говорят о миллионах пользователей Copilot и 25% кода от ИИ, а независимые исследования показывают замедление опытных разработчиков. Эффективность трудно измерить: метрики учитывают принятие подсказки, но не то, дошёл ли код без правок до продакшена.\n\nМой опыт посередине. ИИ отлично справляется с «рутиной»: рефакторинг, мелкие задачи на пару минут, анализ кода. Но в новом развитии мешает: уводит в плохие абстракции, которые потом долго чинить. Нужно чётко понимать своё решение, иначе пользы мало. Когда позволял ИИ «сделать всё», получал больше багов и непродуманных деталей. В программировании часто нет очевидных ответов — нужно «чувствовать» код.\n\nВ неизведанных областях ИИ помогает задать направление: иногда удачно, иногда — потерянный день-два. С опытом проще вовремя развернуться. При этом я оптимист: у инженеров и ИИ может быть продуктивное сосуществование.\n\nПробовал IDE, чаты, веб-интерфейсы вроде Lovable и CLI. Больше всего пользы дал CLI: полный контроль контекста и необходимость подумать перед Enter. IDE же могут что-то менять без осознанного участия. CLI удерживает меня «в петле».\n\nДля того, что делать не люблю, ИИ великолепен. В дизайне с помощью Lovable и Figma генерировал UI, переносил HTML в Elixir-дашборд — результат впечатлял. В текстах помогает со стилем и логикой. Иногда он идеально закрывает простую задачу.\n\nХорошее\n- Избавляет от ~5% ежедневной скучной рутины. Сегодня с его помощью с первого раза добавил поиск в API через Generalized Search Tree: я — миграцию и индекс, ИИ — параметры запроса в HTTP-обработчик и обновление функции БД для поиска по name.\n- Главная ценность — вместо дизайна на встречу приносим прототип. Разработчикам понятнее, меньше встреч, в продажах легче выяснить, чего хочет клиент: можно потыкать и дать точную обратную связь. Это меняет правила игры.\n- По фронтенду: из Figma в React ИИ генерирует каркас и часть верстки. Не «в одно касание», но хорошее начало, экономящее время. Здесь ИИ полезен, потому что сложность фронтенда — не в переписывании дизайна в код, а в его доводке под миллионы разных пользователей.",
      "commentsSummary": "- Участники спорят о роли статуса и маркетинга: первопроходцы редко получают всю славу; общество склонно приписывать изобретения медийным фигурам (шутки про Гейтса/Алтмана/Маска).  \n- По поводу AGI мнения разделились: одни ждут гонку компаний и стран, другие считают технологию инкрементальной и «раздутой», без единственного победителя.  \n- Дискуссия о менеджменте: ИИ как инструмент сводок и статусов может заменить слабых менеджеров и сократить встречи; некоторые компании якобы уже сокращают «среднее звено».  \n- Разработчики хотят, чтобы ИИ знал кодовую базу без постоянной подачи контекста — «вшитое знание», а не Memento-режим.  \n- Вопрос труда и капитализма: одни надеются освободить время для жизни, другие сомневаются — исторически рост продуктивности не уменьшал рабочие часы; звучат тезисы от «капитализм закончится» до «люди всё равно найдут, чем заняться».  \n- Относительно «хайпа»: часть считает ИИ переиспользованным пузырём; другие ожидают сценарий интернета — после отката начнётся устойчивое и повсеместное внедрение.  \n- В целом консенсуса нет: ИИ видят либо как инструмент для рутин и менеджмента, либо как переоценённую волну; при этом ожидания по влиянию на рынок труда и кредит изобретения остаются полярными.",
      "score": 37,
      "commentsCount": 23,
      "hnUrl": "https://news.ycombinator.com/item?id=44822258",
      "domain": "priver.dev"
    },
    {
      "id": 44821434,
      "title": "Cracking the Vault: How we found zero-day flaws in HashiCorp Vault",
      "url": "https://cyata.ai/blog/cracking-the-vault-how-we-found-zero-day-flaws-in-authentication-identity-and-authorization-in-hashicorp-vault/",
      "by": "nihsy",
      "timeISO": "2025-08-07T07:01:42.000Z",
      "postSummary": "Введение: когда модель доверия подводит\n\nСекрет-хранилища — опора цифровой инфраструктуры: в них лежат креденшелы, токены и сертификаты, управляющие доступом к системам, сервисам, API и данным. Это не просто часть модели доверия — это и есть модель доверия. Если хранилище взломано, инфраструктура уже потеряна.\n\nПонимая, что такие хранилища — цели высокой ценности, команда Cyata провела углубленную оценку HashiCorp Vault — одного из самых популярных решений.\n\nЗа несколько недель мы выявили девять ранее неизвестных уязвимостей нулевого дня, каждой присвоен CVE через ответственное раскрытие. Совместно с HashiCorp все проблемы были исправлены до публикации.\n\nОбнаруженные изъяны обходят блокировки, политики и позволяют выдавать себя за других. Одна уязвимость ведет к повышению привилегий до root, другая — к первому публичному RCE в Vault, дающему полный захват системы.\n\nМы увидели цепочки логических ошибок, которые по отдельности и в комбинации создают опасные пути атаки — особенно в реальных внедрениях с мисконфигами или избыточными правами.\n\nЭто не были ошибки памяти или гонки, а скрытые логические баги в слоях аутентификации, идентичности и политик Vault. Некоторые существовали почти десятилетие — незаметные, но легко эксплуатируемые после понимания.\n\nПредыдущие исследования (например, Google Project Zero, 2020) касались обходов в IAM-бэкендах облаков (AWS, GCP). Мы нацелились на базовые потоки аутентификации Vault, затрагивающие OSS и Enterprise-версии по разным провайдерам.\n\nДалее — что мы нашли, как нашли и что это значит для инфраструктуры, которую должен защищать Vault.\n\nЧто такое HashiCorp Vault?\n\nVault — открытый инструмент для защиты, хранения и контроля доступа к секретам: API-ключам, паролям БД, сертификатам, ключам шифрования.\n\nЕго используют компании разных масштабов: он централизует управление секретами и применяет детальные политики в распределенных системах.\n\nПо сути — это граница безопасности: аутентифицирует людей и машины, посредничает доступу к чувствительным данным.\n\nВ DevSecOps Vault снижает риски хардкода секретов, расползания и несанкционированного доступа. Его ценят за гибкую интеграцию, точные политики и пригодность для сложных сред. Часто это последний сторож секретов: при определенных настройках компрометация Vault равна компрометации всего.\n\nОсновные возможности Vault\n- Управление секретами и крипто-движок для динамичных мульти-/гибрид-облаков\n- Централизованное хранилище с доступом по API\n- Динамическая выдача учетных данных с автоистечением\n- Идентификационно-ориентированный доступ для людей и машин\n- Шифрование как сервис для данных «в покое» и «в пути»\n- Управление сертификатами: выпуск, ротация, отзыв\n- Распределение, включение/отключение и ротация ключей шифрования\n\nМетодология: как мы нашли то, что другие пропустили\n\nЭто целенаправленное исследование логических уязвимостей Vault — тех, что не видны в сканерах памяти и логах падений, но подтачивают модель доверия.\n\nМы исходили из гипотезы: если Vault — якорь доверия, то малые несогласованности в идентичности, аутентификации или политике могут иметь непропорционально большие последствия.\n\nФокус — базовый поток обработки запросов, особенно файл request_handling.go, «мозг» Vault: маршрутизация, разрешение идентичностей, принятие политик. Неделями изучали логику функций и модулей, отслеживая крайние случаи размывания границ доверия.\n\nНе полагались на фаззинг и автопробинг. Проводили глубокий ручной код-ревью, анализируя не только функции, но и интерпретации идентичности/ввода разными компонентами. Увидев несоответствия в регистре, алиасинге, форматировании — углублялись.\n\nКаждый тест — целевая проверка, основанная на коде. Мы думали как атакующие: начиная с минимальных прав, спрашивали «насколько далеко можно продвинуться отсюда?» И повторяли этот цикл, замечая мелкие несоответствия и прослеживая их последствия.",
      "commentsSummary": "- Обсуждается статья о серии уязвимостей в HashiCorp Vault (9 CVE, май–июнь 2025), включающая эскалации привилегий, обходы блокировок и раскрытие/перебор пользователей; часть затрагивает OpenBao (форк Vault), что подтвердили его мейнтейнеры.  \n- Несколько комментаторов критикуют качество кода Vault и недостаточность тестов, отмечая проблемы с нормализацией строк/вводов и непоследовательной обработкой идентификаторов; главный урок — минимизировать «текстовую» интерпретацию строк в безопасности и рано/единообразно нормализовать ввод.  \n- Встречается скепсис к стилю статьи: «AI-оттенок», избыточная объяснительность, возможное AI-редактирование; авторы Vault Fault утверждают, что находки сделаны вручную человеком и текст в основном человеческий, но отредактирован для широкой аудитории.  \n- Отдельно обсуждают странную вставку про «daka» (ивр. «минута») в контексте TOTP-окна 60 секунд — читатели считают это лишним/сбивающим.  \n- Представители OpenBao выражают разочарование отсутствием предварительного уведомления об уязвимостях и приглашают к сотрудничеству; часть CVE уже в триаже и готовятся фиксы.  \n- Приводятся детали отдельных CVE: обход блокировок из-за различий в нормализации (userpass/LDAP), тайминговая идентификация валидных имён пользователей, проблемы с MFA-правилами; указывается ссылка на обсуждение CVE-2025-6010.  \n- Идёт спор о роли языка Go: одни винят его в «среднем» качестве и сложности парсинга, другие считают баги логическими/дизайнерскими и не зависящими от языка.",
      "score": 116,
      "commentsCount": 49,
      "hnUrl": "https://news.ycombinator.com/item?id=44821434",
      "domain": "cyata.ai"
    },
    {
      "id": 44823260,
      "title": "\"AI hype\" is the true AI product",
      "url": "https://hardresetmedia.substack.com/p/machine-learning-expert-ai-hype-is",
      "by": "aredox",
      "timeISO": "2025-08-07T11:47:41.000Z",
      "postSummary": "Бхаскар Митра 19 лет проработал в бигтехе, в последние годы — исследователем в области информационного поиска, машинного обучения и ИИ.\n\nВ июне его уволили в рамках массовых сокращений. До этого он открыто говорил о Палестине и системных проблемах ИИ. Интервью планировали о Газе, но разговор ушел к ИИ, LLM и централизации информации.\n\nИнтересно услышать человека с руководящей ролью в ИИ в крупной компании. Ниже — разговор.\n\nАриэлла Стайнхорн: Чем вы занимались в бигтехе? Увольнение стало сюрпризом? Грусть? Облегчение?\n\nБхаскар Митра: Начинал разработчиком, затем прикладные науки, потом исследователь. Последние три года — исследование поиска и ИИ. В июне были масштабные сокращения, я попал под них. Это было неожиданно, причин не объяснили, сказали, что не по перформансу. Я и так подумывал уйти: не устраивал социальный эффект бигтеха, особенно по Газе. Сначала можно было списать на сложность ситуации, но затем людей стали увольнять за протесты. Это выглядело как осознанная защита соучастия.\n\nАС: Что именно вызывало этические сомнения?\n\nБМ: Я спорил с их решениями. Не уходил сам, потому что не видел, где в техе можно работать этично. На работе о Палестине говорил мало из‑за страха репрессий, но вне работы — открыто. Исследовал справедливость и социальные аспекты ИИ и доступа к информации. Меня все больше тревожило, как технологии концентрируют власть и капитал, тормозят социальную и климатическую справедливость. Я стал участвовать в соцдвижениях. Ответственная технология и социальная борьба оказались двумя сторонами одной медали. Уйдя из бигтеха, я заговорил громче — стало легче и радостнее.\n\nАС: Понимаю: вы освободились и перестали подавлять себя. Что подтолкнуло к социальным вопросам?\n\nБМ: Я из компьютерных наук, но меня всегда привлекал потенциал технологий для позитивных перемен. Брекзит, победа Трампа в 2016 и рост хиндуистского фашизма в Индии заставили действовать. Как техработника, меня больше всего волновала роль технологий в усилении авторитаризма. Баланс сложный: риторика про ИИ настолько мощная, что критиков легко записывают в «хейтеры».\n\nАС: Про «хайп». ChatGPT — продукт, но идея, что ИИ «заберет» большинство задач мозга, вам кажется натянутой?\n\nБМ: В машинном обучении за десятилетие был реальный прогресс. Но многое из того, во что бигтех хочет, чтобы мы верили, — бездоказательно. LLM позиционируют неверно. Они не заменят врачей, учителей или ученых — это подмена сути их профессий. LLM также подают как систему доступа к информации, игнорируя системные эффекты. Раньше поиск и крупные источники (вики, медиа) жили в симбиозе: сайты обогащали выдачу, а поиск вел к ним трафик и монетизацию. Теперь LLM поглощают их контент для обучения и потом пересказывают без должной атрибуции. В RAG‑системах ссылки есть, но даже с цитированием такие модели размывают стимулы создавать первичный контент и перестраивают экосистему информации. Это несет риски для качества, устойчивости и плюрализма источников.",
      "score": 3,
      "commentsCount": 0,
      "hnUrl": "https://news.ycombinator.com/item?id=44823260",
      "domain": "hardresetmedia.substack.com"
    },
    {
      "id": 44811567,
      "title": "Claude Code IDE integration for Emacs",
      "url": "https://github.com/manzaltu/claude-code-ide.el",
      "by": "kgwgk",
      "timeISO": "2025-08-06T13:17:38.000Z",
      "postSummary": "Claude Code IDE для Emacs\n\nОбзор\n- Интеграция с Claude Code CLI через MCP создает двусторонний мост между Claude и Emacs.  \n- Claude получает доступ к возможностям Emacs: LSP, проекты, Elisp-функции, что делает его «понимающим Emacs» помощником в вашем рабочем процессе.\n\nВозможности\n- Автоопределение проекта и управление сессиями\n- Терминал с цветом (vterm/eat)\n- Реализация MCP для IDE-интеграции\n- Инструменты для файлов, состояния редактора и рабочего пространства\n- Расширяемый сервер MCP для Emacs-команд (xref, tree-sitter, project и др.)\n- Диагностики Flycheck/Flymake\n- Расширенный дифф с ediff\n- Поддержка tab-bar и отслеживание выделений/буферов\n\nИнтеграция инструментов Emacs\n- LSP через xref (eglot, lsp-mode) для навигации по коду\n- Tree-sitter для анализа AST\n- Imenu для структуры символов\n- Project для операций на уровне проекта\n- Любую команду/функцию Emacs можно выставить как MCP-инструмент: поиск и рефакторинг по проекту, доступ к режимам, выполнение кастомного Elisp.\n\nСкриншоты\n- Осведомленность об активном файле — знает, какой файл открыт\n- Контекст выделения — работает с выделенным текстом\n- Продвинутый дифф с диагностикой — ediff и доступ к ошибкам/предупреждениям\n- Автоматические упоминания текста — вставка ссылок на выделение в диалог\n- Восстановление сессии — продолжение разговоров с флагом –resume\n\nУстановка\nПредварительные требования\n- Emacs 28.1 или новее",
      "commentsSummary": "- Обсуждение интеграции AI-инструментов (Claude Code и аналоги) с Emacs: многие считают, что это усиливает позиции «нишевых» редакторов (Emacs/Vim), позволяя им не городить IDE-фичи, а полагаться на внешние агенты и LSP/tree-sitter.  \n- Плюсы Emacs для агентов: глубокая настраиваемость и доступ к состоянию редактора через elisp; есть несколько активных пакетов интеграции (claude-code.el, claude-code-emacs, claudemacs, ECA), некоторые хвалят «тихие» проекты за полноту фич.  \n- Пользовательский опыт: часть хвалит простые терминальные обёртки и быстрый воркфлоу; другие жалуются на сложность настройки современного Emacs-стека и проблемы в evil-mode/терминальном вводе, предпочитая использовать инструменты в отдельном терминале.  \n- Запросы и опасения: интеграция с org-mode/вести долговременные заметки, независимость от поставщика (OpenCode, совместимость с OpenAI API), локальный запуск агентов/LLM на доступном железе, безопасность (изоляция через bubblewrap, чтобы не утекали секреты).  \n- Сравнение и альтернаты: интерес к аналогам для Neovim/Helix; пользователи сравнивают с gptel и Copilot, спрашивают, что даст переключение; обсуждают, может ли сформироваться общий API для IDE/редакторов.  \n- Скепсис: сомнения в пересечении аудитории Emacs и «агентного кодинга», вопросы монетизации, корпоративные ограничения на выбор провайдера ИИ; критика сложности протокола интеграции Claude.  \n- Общие настроения: от энтузиазма («это путь», «сингулярность случится в Emacs») до неприятия; отдельные оффтоп-споры о значимости Vim/Neovim и релевантности GNU/лицензий.",
      "score": 681,
      "commentsCount": 231,
      "hnUrl": "https://news.ycombinator.com/item?id=44811567",
      "domain": "github.com"
    },
    {
      "id": 44819968,
      "title": "Running GPT-OSS-120B at 500 tokens per second on Nvidia GPUs",
      "url": "https://www.baseten.co/blog/sota-performance-for-gpt-oss-120b-on-nvidia-gpus/",
      "by": "philipkiely",
      "timeISO": "2025-08-07T02:28:47.000Z",
      "postSummary": "- В день выхода открытой модели вроде gpt-oss-120b мы сразу ускоряем её для клиентов, как партнёры запуска OpenAI. К концу дня запуска стали лидерами на NVIDIA по латентности и пропускной способности по данным OpenRouter.\n\n- Быстрая оптимизация обеспечена гибким стеком инференса и экспертизой команды; за время написания поста прибавили ещё ~100 ток/с при 100% аптайме.\n\n- Работы включали:\n  - Тесты и бенчмарки в TensorRT-LLM, vLLM и SGLang.\n  - Совместимость с архитектурами Hopper и Blackwell.\n  - Интеграцию с нашим стеком (в т. ч. NVIDIA Dynamo).\n  - Оптимизации: маршрутизация с учётом KV-кэша, спекулятивная генерация с Eagle.\n\nШаг 1: Первый инференс\n- Запускаем базовый инференс в любом доступном фреймворке и на нужных GPU/серверных уровнях.\n- Параллелим работу: одни пробуют vLLM и SGLang, другие — TensorRT-LLM; быстрее всего взлетел TensorRT-LLM.\n- Важно обслуживать модель и на Hopper (H100), и на Blackwell (B200) для широкой доступности и максимальной скорости.\n- Гибкость рантайма позволяет быстро переключать инструменты и обновлять матрицу поддержки.\n\nШаг 2: Исправление багов совместимости\n- Новые архитектуры приводят к тонким несовместимостям; GPT OSS добавил, например, Harmony — новый формат ответов.\n- Итеративно чиним и валидируем на скорость и корректность; по возможности контрибутим обратно в open source.\n- Благодаря сообществу есть несколько отличных путей запуска GPT OSS, проблемы быстро выявляются и чинятся.\n\nШаг 3: Оптимизация конфигурации\n- Хотя GPT OSS 120B можно запустить на одном H100, оптимально масштабировать на 4–8 GPU для лучшей латентности/throughput.\n- Рассмотрены два подхода параллелизма для MoE: тензорный и экспертный. Тензорный даёт меньшую задержку, экспертный — выше системную пропускную способность. Мы выбрали тензорный, так как приоритет — латентность.\n- Приняли MoE Backend в TensorRT-LLM (поддерживается на Blackwell, не на Hopper), который добавляет более быстрые CUDA-ядра и превосходит предыдущие решения.",
      "commentsSummary": "- Обсуждение крутится вокруг запуска и производительности открытых LLM (GPT-OSS 20B/120B) на локальном железе: на Mac с большим RAM они быстры на коротком контексте, но сильно замедляются при >10k токенов.  \n- Многие отмечают важность MCP/веб-поиска/URL-fetch; без этих инструментов полезность моделей падает.  \n- TensorRT-LLM часто даёт лучшую производительность, но его сложно настраивать; альтернативы вроде vLLM, SGLang, llama.cpp, Ollama и LM Studio по-своему удобны и зависят от конфигурации.  \n- Спекулятивное декодирование обсуждается как способ ускорения: маленькая модель черновит несколько токенов, а большая валидирует их одной «prefill» проходкой, что экономит пропускную способность и уменьшает количество последовательных декодов.  \n- Пользователи делятся метриками: на потребительских GPU/CPU достижимы десятки-сотни ток/с (зависят от контекста, квантизации и рантайма); большие модели без квантизации не помещаются в 4090.  \n- Вопросы совместимости: есть эвристики и сервисы (например, профили на Hugging Face) для сопоставления моделей с железом; расчёты VRAM часто неточны из‑за множества переменных.  \n- Отмечают компромиссы: скорость и локальность против точности и удобства облака; некоторые жалуются на фактические ошибки моделей и на то, что доступ к мощным GPU дорог или ограничен.",
      "score": 180,
      "commentsCount": 102,
      "hnUrl": "https://news.ycombinator.com/item?id=44819968",
      "domain": "baseten.co"
    },
    {
      "id": 44787738,
      "title": "Debounce",
      "url": "https://developer.mozilla.org/en-US/docs/Glossary/Debounce",
      "by": "aanthonymax",
      "timeISO": "2025-08-04T16:04:13.000Z",
      "postSummary": "Дебаунс — это техника ограничения частоты вызова функции. В течение заданной задержки все входящие вызовы игнорируются, а выполняется только один — либо первый (leading), либо последний (trailing), в зависимости от настроек. Это помогает оптимизировать производительность и избежать лишних вычислений при частых событиях.\n\nПрименение:\n- Обработчики ввода: ждать паузы перед запросом автодополнения.\n- События прокрутки/изменения размера: запускать вычисления после остановки действий пользователя.\n- Клики и сабмиты: предотвращать множественные отправки.\n\nОтличие от троттлинга: троттлинг гарантирует вызовы с фиксированным интервалом, а дебаунс — один вызов после серии событий (или сразу первый, если включен leading).\n\nКлючевые параметры:\n- delay: время ожидания.\n- leading/trailing: когда вызывать — в начале или в конце паузы.\n- maxWait (если предусмотрено): гарантирует вызов, даже если события не прекращаются.",
      "commentsSummary": "- Обсуждение крутится вокруг термина «дебаунс» в UI/веб‑разработке и его связи с «настоящим» дебаунсом в электронике: часть участников считает аналогию неточной или вводящей в заблуждение, другие — уместной как термин искусства.  \n- Отмечены подводные камни дебаунса/троттлинга с асинхронными функциями: возможны «нарушения причинности», когда возвращается старый промис/результат; предлагаются реактивные подходы (RxJS switchMap) и AbortController для корректной отмены.  \n- Подчёркнуто, что дебаунс не всегда нужен: современные API вроде ResizeObserver и событие scrollend уже снижают шум; пример с oninput/автосохранением — уместный кейс.  \n- Эксперты по электронике приводят детали: асимметричный дебаунс (Make немедленно, Break после стабилизации), гистерезис/разные пороги, физические решения в переключателях; это отличается от задержки в UI‑поиске.  \n- Приведены ссылки на ресурсы: статья с демо дебаунса через таймауты и AbortController, материалы Ганссле о классическом дебаунсе, Wikipedia, а также упоминание RxJS.  \n- Есть критика устоявшейся терминологии во фронтенде и замечание, что лучше приводить примеры, где важно «ровно одно действие» (например, клики по кнопке), а не автоподсказка поиска.",
      "score": 70,
      "commentsCount": 36,
      "hnUrl": "https://news.ycombinator.com/item?id=44787738",
      "domain": "developer.mozilla.org"
    },
    {
      "id": 44817539,
      "title": "Project Hyperion: Interstellar ship design competition",
      "url": "https://www.projecthyperion.org/",
      "by": "codeulike",
      "timeISO": "2025-08-06T20:40:17.000Z",
      "postSummary": "Проект Hyperion исследует возможность пилотируемых межзвездных полетов на поколенческих кораблях с использованием нынешних и ближайших технологий. Такой корабль рассчитан на многовековой путь: экипаж живет и сменяется поколениями, поддерживая замкнутую экосистему с сельским хозяйством, жильем и системами жизнеобеспечения.\n\nИнициатива i4is объявила победителей международного конкурса дизайна поколенческого корабля для 250‑летнего перелета к обитаемой планете. Междисциплинарные команды проектировали среду, способную поддерживать и развивать общество при жестких ресурсных ограничениях.\n\nУсловия конкурса требовали совместной работы архитекторов, инженеров и социальных ученых над ключевыми аспектами закрытого общества на века, включая:\n- Обитаемость для 1 000 ± 500 человек\n- Искусственную гравитацию за счет вращения\n- Достойные условия жизни и базовые потребности\n- Надежные контуры жизнеобеспечения: пища, вода, отходы, атмосфера\n- Механизмы передачи знаний и культуры\n\nПодробнее о требованиях — по ссылке на документ.\n\nБлагодарности жюри\n\nРаботы оценивали эксперты из архитектуры, инженерии и социальных наук (Университет Хьюстона; NASA‑JPL; Университет штата Аризона; Университет штата Орегон; Университет Южной Калифорнии).\n\n1 место\n\nКоманда: Giacomo Infelise, Veronica Magli, Guido Sbrogio', Nevenka Martinello, Federica Chiara Serpe\n\nОтзыв жюри\n\nChrysalis выделился системной целостностью и инновационной модульной архитектурой, глубокой проработкой (включая производство в космосе и подготовку экипажа в Антарктике). Модульная оболочка гибка и масштабируема; большой Купол придает выразительность. Сильны планирование строительства корабля и защита от радиации; конструктив реалистичен. Культурный блок можно развить, но концепт убедителен и зрелищен, отсылает к «Раме» и мировым кораблям 1980‑х.\n\n2 место\n\nКоманда: Julia Biernacik, Jakub Kot, Aleksandra Wróbel, Jacek Janas, Michał Kucharski, Wiktoria Kuchta, Natalia Łakoma, Katarzyna Śliwa\nНаставник: д-р хаб. Michał Kracik\nФакультет промышленного дизайна (студия «Дизайн для экстремальных сред»), Академия изящных искусств в Кракове\n\nОтзыв жюри\n\nWFP Extreme отмечен за общий уровень и акцент на культуре и обществе: одежда, духовные пространства, «такси‑капсула», персонализированная униформа, защита от радиации. Системная целостность и интерьер при искусственной гравитации требуют доработки, но конструктив уместен для орбитальных применений. Балансирует технические амбиции с тонким видением будущей космической жизни.",
      "commentsSummary": "- Обсуждение вокруг концепции поколенческого звездолёта: одни вдохновлены конкурсом и детально обсуждают логистику, другие скептичны к реализуемости и психологии много世 летних полётов.  \n- Технические вопросы: расчёты вместимости шаттлов (около 2000 человек и 1000 тонн на поверхность), противоречия в указанных скоростях/времени полёта (0.01c vs возможные 0.1c), отсутствие проработки propulsion; идеи об ускорительных “суперструктурах”, пульс-ядре, «догоняемом» топливе, нанобот-строителях.  \n- Экологические и биосферные вызовы: закрытые экосистемы (Biosphere 2 как предостережение), проблемы поддержания океана, вращающиеся модули на века, вопросы искусственной гравитации и укачивания.  \n- Социальные и психологические риски: скука, конфликты, устойчивость демократии, подростки на борту; предложения технократической «инженерии общества» вызывают споры.  \n- Генетика и демография: достаточность 1000 ± 500 человек, использование замороженных эмбрионов для роста разнообразия и минимизации инбридинга.  \n- Стоимость и практичность: грубая оценка массой ~2.4 млн тонн с колоссальными затратами на выведение; сомнения, что выйдем за пределы Солнечной системы без «новой физики».  \n- Референсы и контент: ссылки на презентацию, упоминания KSR «Aurora» и фильма «Aniara» как критика хаба и утопизма; часть комментаторов считает, что сначала стоит «починить Землю».",
      "score": 290,
      "commentsCount": 220,
      "hnUrl": "https://news.ycombinator.com/item?id=44817539",
      "domain": "projecthyperion.org"
    },
    {
      "id": 44783372,
      "title": "Splatshop: Efficiently Editing Large Gaussian Splat Models",
      "url": "https://momentsingraphics.de/HPG2025.html",
      "by": "ibobev",
      "timeISO": "2025-08-04T08:53:44.000Z",
      "postSummary": "Moments in Graphics\n\nБлог Кристофа Петерса\n\nМаркус Шюц, Кристоф Петерс, Флориан Хальбобм, Элмар Айзенманн, Маркус Магнор, Михаэль Виммер.\n\nАннотация\n- Представляем Splatshop — оптимизированный инструментарий для интерактивного редактирования моделей 3D Gaussian Splatting (выделение, удаление, рисование, трансформации и др.).\n- Комбинация эвристик балансирует точный и быстрый рендеринг, обеспечивая точное редактирование в реальном времени.\n- Эксперименты показывают работу на сценах до 100 млн примитивов.\n- Описываем расширение конвейера для HMD; Splatshop — первый VR-редактор для крупномасштабных моделей Gaussian Splatting, шаг к «Photoshop для Gaussian Splatting».\n\nИзображения\n- [Изображение 1: тизер](https://momentsingraphics.de/Media/HPG2025/teaser.jpg)\n\nЗаметки\n- Представлено на HPG 2025, 24 июня; статья опубликована 21 июня.\n\nСсылки и загрузки\n- Статья (PDF): https://momentsingraphics.de/Media/HPG2025/schutz2025-splatshop-paper.pdf\n- Исходники (GitHub): https://github.com/m-schuetz/Splatshop\n- Запись доклада (YouTube): https://www.youtube.com/watch?v=md6SXpbHbZE&t=3353s\n- Слайды (PDF): https://momentsingraphics.de/Media/HPG2025/schutz2025-splatshop-slides.pdf",
      "score": 10,
      "commentsCount": 0,
      "hnUrl": "https://news.ycombinator.com/item?id=44783372",
      "domain": "momentsingraphics.de"
    },
    {
      "id": 44782097,
      "title": "Children's movie leads art historian to long-lost Hungarian masterpiece (2014)",
      "url": "https://www.theguardian.com/world/2014/nov/27/stuart-little-art-historian-long-lost-hungarian-masterpiece",
      "by": "how-about-this",
      "timeISO": "2025-08-04T04:21:00.000Z",
      "postSummary": "Стюарт Литтл помог искусствоведу найти давно утерянный венгерский шедевр\n\n- Венгерский историк искусства Гергели Банфи, смотря фильм «Стюарт Литтл», заметил на стене в одном из кадров картину, очень похожую на пропавшее полотно Роберта Береньи «Спящая дама с чёрной вазой» (ок. 1927).\n- Картина считалась утраченной с 1920‑х годов. После расследования выяснилось, что её купили реквизиторы в калифорнийском антикварном магазине за $500 для оформления съёмочной площадки.\n- Владелец магазина позже продал полотно, не подозревая о его ценности. После публикаций и подтверждения атрибуции картина всплыла на аукционе в Будапеште и была продана примерно за $285,000.\n- Открытие вызвало интерес к Береньи и венгерскому авангарду. Эксперты отметили удивительный путь произведения: от европейских салонов — к голливудскому реквизиту — и обратно в мир искусства.",
      "commentsSummary": "- Пользователь j-kent выражает радость по поводу того, что шедевр наконец найден.  \n- Он отмечает, что теперь произведение можно выставить на аукцион.  \n- Предполагается продажа частному коллекционеру.  \n- Автор комментария считает это победой для гуманитарных наук.  \n- Тон сообщения — саркастически-ироничный или восторженный, в зависимости от контекста.",
      "score": 17,
      "commentsCount": 1,
      "hnUrl": "https://news.ycombinator.com/item?id=44782097",
      "domain": "theguardian.com"
    },
    {
      "id": 44819037,
      "title": "Rules by which a great empire may be reduced to a small one (1773)",
      "url": "https://founders.archives.gov/documents/Franklin/01-20-02-0213",
      "by": "freediver",
      "timeISO": "2025-08-06T23:29:11.000Z",
      "postSummary": "- Сатира «Правила, по которым большая империя может быть уменьшена до малой» была напечатана в Public Advertiser 11 сентября 1773 года как пара к «Эдикту короля Пруссии». Франклин ценил их краткость, ёмкость и необычную форму, но «Правила» предпочитал за разнообразие и «энергичные концовки абзацев». В одной он предлагал взглянуть на политику метрополии глазами колонистов, в другой — вообразить себя колонистами. Цель — заставить публику по‑новому увидеть американский вопрос накануне парламентских дебатов и дела о Хатчинсоне и Оливере. Оба текста быстро разошлись и многократно переиздавались по обе стороны Атлантики.\n\n- Эффект был двояким: сатира редко убеждает — открытых она развлекает, упорных злит. Франклин понимал риск: желая осветить колониальные жалобы, он мог все усугубить. По его мнению, правительство публично игнорировало нападки, чтобы не раздувать их, но именно они во многом объяснили ярость против него в начале 1774 года.\n\n- Содержание «Правил» не было новым: он уже высказывал те же тезисы в несатирической форме. Конституционных тонкостей Бостона почти не касался — их трудно высмеивать, — но собрал привычные темы: торговые ограничения, новые налоги, давление армии и флота, общеколониальные и массачусетские сюжеты. Сатира придала им остроту, но ненадолго: вскоре власти ударили по нему и, ирония судьбы, воплотили многие «правила» в Коэрцитивных актах 1774 года.\n\n- Для Public Advertiser. «Правила, по которым БОЛЬШАЯ империя уменьшается до МАЛОЙ. [Частно представлены одному недавнему министру при вступлении в должность; публикуются впервые.]»\n\n- Преамбула: древний мудрец гордился, что хоть и не умел играть на скрипке, знал, как сделать из малого города большой. Автор, «современный простак», предлагает обратную науку — полезную министрам, у которых слишком много дел, чтобы «играть на скрипке».\n\n- Правило I: большая империя — как большой пирог — легче всего убывает с краёв. Начинайте с дальних провинций: избавляясь от них по периметру, потянутся и следующие.\n\n- Правило II: чтобы возможность отделения всегда сохранялась, не допускайте слияния провинций с метрополией: никаких общих прав и торговых привилегий, более суровые законы, принятые без их участия в выборе законодателей. Подобно пекарю, заранее надрезайте тесто по линиям будущего разлома.",
      "commentsSummary": "- Обсуждение крутится вокруг текста/памфлета времён Франклина: его сатирическая критика британского управления колониями, написанная до 1775, предвосхищает конфликты и напоминает по духу Декларацию независимости.  \n- Участники отмечают, что Франклин более 13 лет пытался убедить британские власти и вернулся в Америку в 1775 после неудачных попыток влияния.  \n- Несколько комментариев подчеркивают силу сатиры: она скорее поляризует, чем убеждает; тон текста — «перестали просить вежливо, теперь показывают зеркало с усмешкой».  \n- Обсуждается историческая орфография: капитализация существительных в английских текстах XVIII века, «длинная s», и примеры из Конституции и Декларации США.  \n- Проводятся параллели с «Принцем» и «Искусством войны», а также с закономерностями упадка империй, элитной слепотой и повторяемостью властных структур.  \n- Приводятся ремарки о британской политике (лорд Норт, скепсис части парламентариев), психическом состоянии монарха и временных уточнениях событий 1773–1775.  \n- Отдельная ветка — споры о «непрерывности» Китая: циклы объединения/распада, преемственность vs. различия династий и современного Китая.",
      "score": 187,
      "commentsCount": 119,
      "hnUrl": "https://news.ycombinator.com/item?id=44819037",
      "domain": "founders.archives.gov"
    },
    {
      "id": 44807868,
      "title": "Show HN: Kitten TTS – 25MB CPU-Only, Open-Source TTS Model",
      "url": "https://github.com/KittenML/KittenTTS",
      "by": "divamgupta",
      "timeISO": "2025-08-06T05:04:36.000Z",
      "postSummary": "- State-of-the-art модель TTS до 25 МБ 😻\n- Пропустить к содержимому\n- Навигация, вход, настройки внешнего вида\n- Продукты: Copilot, Spark, Models, Advanced Security, Actions, Codespaces, Issues, Code Review, Discussions, Code Search\n- Исследовать: Почему GitHub, все функции, документация, навыки, блог\n- Решения по размеру компании: Enterprise, для команд, стартапов, НКО\n- По задачам: DevSecOps, DevOps, CI/CD и др.\n- По индустриям: здравоохранение, финансы, производство, гос сектор\n- Ресурсы: темы (ИИ, DevOps, безопасность, разработка), курсы, события, книги, истории клиентов, партнёры, аналитика\n- Open Source: Sponsors, ReadME Project\n- Репозитории: Темы, Тренды, Коллекции\n- Enterprise: платформа, допы — Advanced Security, Copilot for business, поддержка\n- Цены\n- Поиск кода и репозиториев, советы по синтаксису\n- Обратная связь (с email), отправка/отмена\n- Сохранённые поиски: создание/управление, документация по синтаксису\n- Вход/регистрация\n- Сообщения о перезагрузке сессии и переключении аккаунтов\n- KittenML/KittenTTS (публичный), уведомления, форки",
      "commentsSummary": "- Обсуждают KittenTTS: сверхмалый (около 25 МБ), CPU‑ориентированный, Apache-2.0 TTS; впечатляет возможностью офлайн-встраивания в дешёвое/встраиваемое железо и простыми лицензиями, есть веб-демо и образцы на Reddit.  \n- Производительность на i9‑14900HX: стартовая задержка ~315 мс; генерация 3.3–5.5x реального времени в зависимости от длины текста; модель грузится быстро.  \n- Качество считают «неплохим для размера», но многим звучит механически/металлично; проблемы с числами и короткими фразами; некоторые сравнивают не в пользу против kokoro/piper/XTTS, и отмечают, что более крупные открытые модели (Fish-speech, F5-TTS) дают лучшее качество при наличии GPU.  \n- Главные плюсы: минимальный след, CPU‑работа «везде», открытая лицензия, перспективы для офлайн‑продуктов, дешёвых устройств и одноразовых лицензий; минусы — установка зависимостей, совместимость Python и задержка для интерактивного диалога.  \n- Запросы сообщества: образцы аудио на странице проекта, ONNX/gguf порты, тренировка/тонкая настройка и данные, мультиязычность и голос-клон, поддержка других языков.  \n- Обсуждают смежные темы: офлайн STT (Whisper хвалят), SOTA TTS для средних ноутбуков, причины ошибок ударений/сокращений у крупных TTS, идеи по улучшению тембра и качеству маленьких моделей.  \n- Общий тон — восхищение инженерным компромиссом и направлением «малые офлайн‑модели на CPU», при признании, что по естественности речи это пока не замена крупным решениям.",
      "score": 870,
      "commentsCount": 333,
      "hnUrl": "https://news.ycombinator.com/item?id=44807868",
      "domain": "github.com"
    },
    {
      "id": 44819738,
      "title": "A candidate giant planet imaged in the habitable zone of α  Cen A",
      "url": "https://arxiv.org/abs/2508.03814",
      "by": "pinewurst",
      "timeISO": "2025-08-07T01:42:18.000Z",
      "postSummary": "- Сообщается о коронографических наблюдениях ближайшей солнечноподобной звезды α Cen A инструментом MIRI на JWST в августе 2024, феврале и апреле 2025. Достигнута чувствительность для обнаружения планет с T_eff≈225–250 K (1–1,2 R_Jup) на угловых расстояниях 1\"–2\" и пыли экзозодикального диска на уровнях >5–8 яркостей солнечной зодиакальной пыли. Отсутствие экзозоди даёт рекордный верхний предел — всего в несколько раз выше солнечной зодиакальной, что в ≥10 раз чувствительнее предыдущих измерений для иных систем.\n\n- В августе 2024 обнаружен точечный источник S1 с F_ν(15,5 мкм)=3,5 мЯн на расстоянии 1,5\" от α Cen A. Единственный успешный ролл-угол не позволяет однозначно подтвердить, что это планета. Анализ исключает фон/передний план. В феврале и апреле 2025 S1 не найден. Если S1 — то же, что объект C1 из VLT/NEAR (2019), то существует 52% вероятность, что кандидата S1+C1 не увидели в двух последующих наблюдениях JWST/MIRI из‑за орбитального смещения.\n\n- С учётом ненахождений получены семейства динамически устойчивых орбит для S1+C1 с периодами 2–3 года. Они указывают на эксцентриситет e≈0,4 и значительное наклонение относительно плоскости орбиты α Cen AB (взаимный наклон i≈50° или ≈130°). По фотометрии и орбитальным свойствам кандидат может иметь T≈225 K, радиус ≈1–1,1 R_Jup и массу 90–150 M_⊕, что согласуется с пределами по РВ.\n\n- Принято в ApJL; 34 стр., 22 рисунка, 10 таблиц. Тематики: экзопланеты и звёздная/солнечная астрофизика. DOI: 10.48550/arXiv.2508.03814. Версия v1 от 5 августа 2025.",
      "commentsSummary": "- Обсуждают возможное обнаружение гигантской планеты в обитаемой зоне Альфы Центавра А; она, вероятно, газовый гигант, но потенциально с обитаемыми лунами — это мог быть ближайший шанс найти мир, пригодный для жизни.  \n- Оценки: температура порядка 225 K (~−48°C), радиус около 1–1.1 радиуса Юпитера, масса ~90–150 масс Земли; гравитация на «поверхности» оценена ~9.7 м/с², но напомнили, что у газового гиганта нет твёрдой поверхности.  \n- Уточняют контекст расстояний: Проксима — ближайшая звезда (4.25 св. г.), но Альфа Центавра А — ближайшая солнекоподобная (около 4.34 св. г.).  \n- Участники обсуждают названия в духе «Полифем» и «Пандора», отсылая к лунно-планетной тематике.  \n- Поднимают тему межзвёздных полётов: от ионных двигателей и солнечных парусов до «само-жертвенного» ускорительного модуля и проектов вроде Hyperion; звучат идеи о необходимости новых методов, «манипуляции гравитацией».  \n- Есть лёгкие уточнения и придирки к единицам (м/с²) и интерпретации параметров; часть аудитории признаётся, что не все значения понятны.",
      "score": 90,
      "commentsCount": 32,
      "hnUrl": "https://news.ycombinator.com/item?id=44819738",
      "domain": "arxiv.org"
    },
    {
      "id": 44780034,
      "title": "Did Craigslist decimate newspapers? Legend meets reality",
      "url": "https://www.poynter.org/business-work/2025/did-craigslist-kill-newspapers-poynter-50/",
      "by": "zdw",
      "timeISO": "2025-08-03T21:35:33.000Z",
      "postSummary": "Это материал из серии The Poynter 50 о моментах, изменивших журналистику за полвека.\n\n— Падение печатных объявлений и удар по редакциям часто связывают с 1995 годом и запуском Craigslist Крейга Ньюмарка: простой сайт, где большинство объявлений бесплатны. Ньюмарк признает влияние, но считает мифом, что именно Craigslist «убил» газеты: главная причина — потеря читателей, начавшаяся с телевидения еще в 1960-х. Снижение тиражей лишило газеты доминирования на локальных рынках и возможности брать премиальные за классифайды; на этом фоне рост числа домохозяйств делал падение еще заметнее. Тем не менее символом сдвига стал именно Craigslist, хотя конкурентов было множество.\n\nШесть коротких глав рассказывают, как Craigslist вырос до сотен миллионов долларов в год, как Ньюмарк распоряжается капиталом и как медленные к цифровой адаптации газеты проиграли.\n\nГлава 1: Ранние годы\n— Ньюмарк, инженер с опытом в IBM и консалтинге, не медиамагнат и не стартап-евангелист, а «социально неловкий» ботан. Предпочитал дистанционную поддержку пользователей и называл себя не CEO, а сотрудником сервиса. Он строил простой инструмент для сообщества — не пытался рушить модель газет.\n\nГлава 2: Случайный предприниматель?\n— Все началось с рассылки о событиях в Bay Area, затем появились обмены и бесплатные объявления, позже — платные вакансии и недвижимость, а следом — выход в другие города. Ньюмарк не хотел управлять компанией: «Я плохой менеджер», — поэтому нанял программиста Джима Бакмастера и быстро сделал его CEO (он им остается).  \n— Бакмастер и Ньюмарк — «капиталисты, не зацикленные на максимизации прибыли». Они отвергали расширения вроде отзывов, чтобы не усложнять модерацию и не стимулировать накрутки — проблема, с которой столкнулись Yelp и Airbnb.  \n— Рост первые 15 лет был стремительным: от волонтеров к оплачиваемой команде; присутствие заявлено в 700+ городах и 70 странах, фокус — США. Возникали конкуренты (Oodle, Kijiji, Minjari, OfferUp), в игру входили техгиганты. Ebay купила миноритарную долю у бывшего сотрудника; партнерство не сложилось, начались суды, Craigslist выкупил пакет обратно.",
      "commentsSummary": "- Обсуждение спорит с идеей, что именно Craigslist «уничтожил» газеты: в некоторых странах его не было, а упадок традиционной журналистики объясняется множественными факторами.  \n- Участники признают, что Craigslist нанес ощутимый удар по рубрике объявлений крупных городских газет в США, но считать его единственной причиной — упрощение.  \n- Отмечено, что Крейг Ньюмарк запустил проект в возрасте около 40 лет как рассылку местных событий для друзей, и сервис рос органично более 15 лет.  \n- Руководство сознательно избегало максимизации прибыли и расширений (например, отзывов), чтобы снизить злоупотребления и упростить модерацию — решение считают мудрым.  \n- Есть комментарий о технологической базе: «секретный соус» Craigslist и Amazon — Perl.  \n- Обсуждают и восприятие интерфейса/шрифта: некоторые считают его размытым и трудночитаемым, другие не видят проблем на своих конфигурациях.  \n- Подытог: влияние Craigslist было значительным, но не единственным фактором упадка газет; успех определили простота, уместный техстек и сдержанная продуктовая стратегия.",
      "score": 20,
      "commentsCount": 9,
      "hnUrl": "https://news.ycombinator.com/item?id=44780034",
      "domain": "poynter.org"
    },
    {
      "id": 44816755,
      "title": "Litestar is worth a look",
      "url": "https://www.b-list.org/weblog/2025/aug/06/litestar/",
      "by": "todsacerdoti",
      "timeISO": "2025-08-06T19:43:01.000Z",
      "postSummary": "- Несколько лет назад мне выпал шанс выбрать async‑first, типизированный Python‑фреймворк для веба. Я взял Litestar — без хайпа и ракет в твитах — и не пожалел: уже около 18 месяцев все мои новые рабочие проекты на нём.\n\n- Даже если вы пишете асинхронные веб‑приложения на Python, вы могли пройти мимо Litestar. Хочу это исправить.\n\n- Вкус демо: простой файл\n  ```\n  from litestar import Litestar, get\n\n  @get(\"/greet\")\n  async def greet(name: str) -> str:\n      return f\"Hi, {name}!\"\n\n  app = Litestar([greet])\n  ```\n  Запускаете через litestar run или любой ASGI‑сервер. /greet?name=Bob вернёт «Hi, Bob!». Без name — HTTP 400: параметр обязателен. Да, похоже на FastAPI и на знакомые по Spring/ASP.NET MVC подходы с аннотациями — и FastAPI тоже так умеет. Но у Litestar есть свои сильные стороны.\n\n- Про название: раньше проект назывался Starlite, потому что изначально строился на Starlette (как и FastAPI). Позже зависимость убрали, а чтобы не путать со Starlette, в релизе 2.0 (2023) переименовали в Litestar.\n\n- Масштабирование кода, а не трафика:\n  - Django плохо «масштабируется вниз»: «правильный» старт быстро разрастается в десяток файлов и папок. Однофайловые трюки работают, но против шерсти.\n  - Микрофреймворки — наоборот: стартуют в одном файле, но по мере роста кода расползаются и начинают мешать.\n  - В FastAPI маршруты обычно вешаются декораторами на объект приложения. Это удобно в одном файле, но при разбиении на модули ведёт к циклическим импортам. Решение — «вторичные» реестры маршрутов (APIRouter, blueprint): нужны, потому что декораторы привязаны к app. Litestar же позволяет описывать обработчики отдельно и передавать их приложению списком, что естественно масштабируется от одного файла к структуре проекта без костылей.",
      "commentsSummary": "- Обсуждение сравнивает FastAPI и Litestar для построения сложных бэкендов: многие критикуют FastAPI за организацию кода, управление зависимостями и разрыв между туториалами и реальностью, отмечая, что он хорош для простых сервисов, но неудобен в крупных проектах.  \n- Несколько участников хвалят Litestar: быстрый, лёгкий, асинхронный, с хорошими доками, кэшированием, контроллерами для вложенных роутов, поддержкой msgspec, шаблонов и плагином для HTMX; упоминают активную команду и референс-приложение litestar-fullstack.  \n- Есть голоса за минимализм: некоторые предпочитают Starlette без “кухонной раковины” или Connexion со spec-first подходом.  \n- Вопросы и сомнения: как обрабатывать ошибки при стриминге, как деплоить (NGINX Unit показался сложным), нужен ли референс-код для Litestar, и действительно ли он лучше FastAPI при меньшем сообществе.  \n- Отдельная линия — ORM: спор между SQLAlchemy и Django ORM; часть считает Django ORM проще и достаточным, другие критикуют «шар состояния» SQLAlchemy или наоборот ценят гибкость; упоминают раздельные модели для API и БД как благо.  \n- Документация — боль многих: просят строгий API Reference вместо лишь разговорных гайдов; FastAPI-хвалят за примеры из реальных репо, но отмечают, что официальные туториалы вводят в заблуждение.  \n- Итоговая тенденция: растёт интерес к Litestar как более спланированному, «батарейки-в-комплекте» фреймворку для больших приложений, при этом часть разработчиков остаётся на Starlette или FastAPI с улучшенными практиками.",
      "score": 294,
      "commentsCount": 78,
      "hnUrl": "https://news.ycombinator.com/item?id=44816755",
      "domain": "b-list.org"
    },
    {
      "id": 44813854,
      "title": "Jules, our asynchronous coding agent",
      "url": "https://blog.google/technology/google-labs/jules-now-available/",
      "by": "meetpateltech",
      "timeISO": "2025-08-06T16:05:39.000Z",
      "postSummary": "Google представила Jules — асинхронного ИИ-агента для программирования — для всех пользователей, завершив публичную бету. Агент выполняет задачи в фоновом режиме: пишет и рефакторит код, правит баги, настраивает пайплайны и документирует изменения, не требуя постоянного участия разработчика. Это помогает параллелить работу, ускорять итерации и снижать контекстные переключения.\n\nJules интегрируется с инструментами разработчиков, может брать на себя длинные задачи, делить их на шаги, сообщать о прогрессе и запрашивать уточнения только при необходимости. Доступен через Google Labs и ориентирован на повышение продуктивности как отдельных инженеров, так и команд, позволяя запускать больше экспериментальных веток и быстрее проводить ревью.",
      "commentsSummary": "- Пользователи критикуют путаницу с подписками и разобщённость продуктов Google: Jules, Gemini App, Notebook, Gemini CLI/Code Assist в GCP — разные биллинги и доступы, цены скрыты.  \n- Опыт с Jules противоречивый: часть хвалит за асинхронный флоу и удобство на ходу (PR-ы, рефакторинг, тесты), другие считают код и UX слабыми, нестабильными и хуже Claude Code/GH Copilot; в монорепах и сложных проектах часто зацикливается.  \n- Качество, по мнению некоторых, просело после увеличения дневного лимита; сейчас лимит на бесплатном плане урезан (с ~60 до 15), что вызвало недовольство.  \n- Запрашивают интеграции (GitHub Issues, фидбек из PR), кнопку остановки, прозрачность тарифов, и лучшую связность с Gemini CLI/Actions.  \n- Есть споры о ценности асинхронных код-агентов: часть видит преимущества изоляции и “закинул задачу — вернулся позже”, другие считают это медленным и ненадёжным по сравнению с интерактивными инструментами.  \n- Визуальный бренд и UI Jules раскритикованы (маскот, контраст, “стартап-вибы” от корпорации); есть шутки про название и возможные будущие ребрендинги.  \n- В целом: конкурент Codex/Claude Code, но сейчас для многих уступает по качеству кода и стабильности; пользователи ждут улучшений и консолидации линейки Google AI.",
      "score": 302,
      "commentsCount": 205,
      "hnUrl": "https://news.ycombinator.com/item?id=44813854",
      "domain": "blog.google"
    },
    {
      "id": 44813789,
      "title": "Writing a Rust GPU kernel driver: a brief introduction on how GPU drivers work",
      "url": "https://www.collabora.com/news-and-blog/blog/2025/08/06/writing-a-rust-gpu-kernel-driver-a-brief-introduction-on-how-gpu-drivers-work/",
      "by": "losgehts",
      "timeISO": "2025-08-06T16:00:54.000Z",
      "postSummary": "Это вторая часть серии о разработке Tyr — современного GPU‑драйвера на Rust для ядра Linux с поддержкой Arm Mali на CSF.\n\nРазберем, как работают GPU‑драйверы, на примере VkCube — простого приложения на Vulkan, рисующего вращающийся куб. Простота сцены помогает понять путь данных и команд от приложения к GPU.\n\nUMD и KMD\n- UMD (usermode) реализует API вроде Vulkan/OpenGL/OpenCL и преобразует команды приложений в низкоуровневые команды для GPU. В нашем случае это panvk из Mesa.\n- KMD (kernel mode) соединяет UMD с железом: инициализирует устройство, управляет памятью, очередями, планированием и уведомлениями. В нашем случае это Tyr, нацеленный попасть в основное дерево Linux.\n\nЧто делает UMD\n- Подготавливает данные: геометрию, текстуры, машинный код шейдеров, матрицы трансформаций.\n- Просит KMD разместить их в памяти GPU, создает VkCommandBuffer с командами отрисовки, настраивает состояние конвейера, указывает, куда писать результат, и как получать сигнал о завершении.\n\nПро шейдеры\n- Это полноценные программы на GPU. Для VkCube им нужны хотя бы геометрия, цвета и матрица вращения, чтобы расположить и раскрасить куб и крутить его.\n\nЧто делает KMD\n- Выделяет и отображает память, изолируя процессы в отдельных контекстах/VM.\n- Принимает работу от UMD, ставит в аппаратные очереди, отслеживает зависимости и завершение.\n- Планирует выполнение на массово параллельном, асинхронном железе, соблюдая порядок и справедливое распределение ресурса между клиентами.\n- Инициализирует устройство: тактирование, питание, стартовые процедуры; обеспечивает совместный и честный доступ приложений к GPU.\n\nКлючевой вывод\n- Основная сложность — в UMD, который переводит высокоуровневые API в команды GPU. Но KMD обязан предоставить надежные примитивы: память, очереди, синхронизацию, планирование и разделение ресурсов, чтобы UMD было реально реализовать.\n\nИнтерфейс драйвера\n- На основе этих задач KMD экспонирует минимальный набор операций: запрос сведений об устройстве, создание/уничтожение VM, привязка/отвязка памяти к VM, получение состояния VM, отправка работ в очереди и механизмы уведомлений — тот же API, что у C‑драйвера Panthor для того же железа.",
      "commentsSummary": "- Обсуждение посвящено статье о драйвере GPU для Arm Mali (CSF) в Linux, написанном на Rust; многие ждут продолжения и более глубоких частей.\n- Уточняется, что для RK3588 используется драйвер panthor (тема статьи), а не panfrost; один из пользователей жалуется на графические артефакты в Firefox на rk3588 с panfrost.\n- Поднимается вопрос: почему не использовать uring_cmd вместо ioctl; отвечают, что из-за природы GPU как асинхронного устройства и модели очередей выгода невелика, а драйвер и так соответствует ожиданиям Mesa.\n- Комментаторы отмечают, что текущая часть описывает в основном границу user/kernel и управление очередями/буферами; «экшен» начнётся при разборе выполнения команд GPU в следующих частях.\n- Идёт спор о фокусе на Rust: одни считают акцент «ради кликов», другие подчёркивают значимость того, что это один из первых GPU-драйверов Linux на Rust и важный шаг инфраструктурно.\n- В целом сообщество позитивно настроено, ожидает продолжения серии и более технических деталей о выполнении команд и потоках завершений.",
      "score": 274,
      "commentsCount": 33,
      "hnUrl": "https://news.ycombinator.com/item?id=44813789",
      "domain": "collabora.com"
    },
    {
      "id": 44777419,
      "title": "Herbie detects inaccurate expressions and finds more accurate replacements",
      "url": "https://herbie.uwplse.org/",
      "by": "bwidlar",
      "timeISO": "2025-08-03T15:54:30.000Z",
      "postSummary": "Herbie: автоматическое повышение точности вычислений с плавающей запятой\n\nНайдите и исправьте проблемы с числами с плавающей точкой:\nsqrt(x+1) - sqrt(x) → 1/(sqrt(x+1) + sqrt(x))\nКрасная формула неточна при x > 1; синяя точна для всех x.\n\nИспользование\n- Веб-демо\n- Установка\n- Новости\n- Блог\n\nОбучение\n- Учебник\n- Заметки о релизах\n- Документация\n- Статьи о Herbie\n\nВклад\n- Исходный код\n- Сообщить об ошибке\n- Лицензия\n\nГрафик: Herbie повышает точность на наборе эталонов «Hamming». Длиннее стрелка — лучше: старт — точность исходного выражения, финиш — точность результата Herbie (на случайных double-входах).\n\nНовости проекта Herbie\n1) 4 авг: вышел Herbie 2.2 с новой платформенной API для подключаемых целевых платформ. Попробуйте!\n2) 17 июл: релиз Herbie 2.1 — быстрее сгенерированный код и сам Herbie.\n3) 30 июн: релиз Herbie 2.0 — оптимизация по точности и скорости, переработанные отчёты и метрики.\n4) 24 июл: пере-релиз Herbie 1.5 из‑за проблем с инфраструктурой; попробуйте установить снова.\n5) 9 июл: релиз Herbie 1.5 — сортировка аргументов, множественные выходы.\n6) 14 июн: Бретт и Оливер доклад на ARITH 2021 о настройке точности в Herbie; есть статья.\n7) 21 июл: Зак и Павел доклад о пяти годах Herbie: доверие, измерения, сообщество, обобщаемость.\n8) 20 июл: релиз Herbie 1.4 — заметные ускорения и улучшения удобства.\n9) 18 ноя: Дэвид о FPBench 1.2 и новшествах Herbie на Correctness 2019 (Денвер).\n10) 17 ноя: Herbie упомянут в докладе Павла на уроке по анализу FP на SC’19.\n11) 1 мая: Павел станет доцентом в Университете Юты, присоединившись к Ганешу и Звонимиру.\n12) 13 мар: Зак — ключевой доклад на CoNGA’19 о многоточности/многоформатных вычислениях и поддержке в Herbie, FPBench и Titanic.\n13) 20 июн: Алекс — доклад о Herbgrind на PLDI’18; как искать неточные FP‑выражения в больших кодовых базах.\n14) 15 июн: релиз Herbie 1.2 — больше креативности и точности, новые ветвления и более точные параметры.\n15) 9 апр: сотрудничество",
      "commentsSummary": "- Обсуждают инструмент Herbie для улучшения численной стабильности и точности вычислений, и идею встроить подобную функциональность прямо в компиляторы с настраиваемыми параметрами точности и производительности.  \n- Отмечают, что тема уже не раз поднималась, приводят ссылки на прошлые обсуждения и указывают на случаи, когда подход работал не идеально; интересуются, обновлялся ли Herbie недавно.  \n- Шутливо вспоминают, что Herbie иногда «сходит с ума», намекая на нестабильные или неожиданные результаты в отдельных случаях.  \n- Поднимают вопрос, может ли техника Herbie помочь оптимизировать ИИ-инференс и тренинг (например, в llama.cpp), или же эти системы уже выжаты до предела.  \n- Пользовательский опыт: инструмент хвалят за пользу в долгих симуляциях с симплектическими интеграторами для снижения накопленной ошибки.  \n- В целом участники оценивают работу как многообещающую и полезную, с интересом к дальнейшему развитию и интеграции в инструменты разработки.",
      "score": 68,
      "commentsCount": 6,
      "hnUrl": "https://news.ycombinator.com/item?id=44777419",
      "domain": "herbie.uwplse.org"
    },
    {
      "id": 44816692,
      "title": "We'd be better off with 9-bit bytes",
      "url": "https://pavpanchekha.com/blog/9bit.html",
      "by": "luu",
      "timeISO": "2025-08-06T19:39:20.000Z",
      "postSummary": "- В 70‑х некоторые системы (например, PDP‑10) имели 9‑битовые байты, но стандарт закрепился за 8 битами. Если бы байт был 9‑битным, ряд исторических случайностей сыграли бы нам на руку.\n\n- IPv4: при 9‑битовых байтах адрес IPv4 был бы 36‑битным (~64 млрд адресов). Этого хватило бы до 2030‑х без массового NAT и тормозов с IPv6; позже проблему решили бы мягкими рыночными механизмами.\n\n- UNIX time: 32‑битные метки ломаются в 2038, а 36‑битные прожили бы до 3058. Отрицательные охватывали бы времена с 882 года — достаточно для исторических нужд.\n\n- Юникод: вместо 16‑битных 65 тыс. символов было бы 18‑битных 262 тыс. — хватило бы без болезненной унификации CJK; сейчас всех символов ~155 тыс. UTF‑9 стал бы скорее компрессией и уступил бы GZip; либо однобайтно‑двухбайтная схема при умеренной экономии на эмодзи.\n\n- Указатели и память: 36‑битные ОС дали бы до 32 ГБ на процесс (вместо 2 ГБ у 32‑битных). Серверы всё равно виртуализируют; меньшие указатели экономят память и ускоряют код, хотя строки стали бы длиннее — общий баланс близок к нулю.\n\n- Прочие выигрыши:\n  - 18‑битные AS‑номера не иссякли бы; порты/PID/UID просторнее.\n  - Кодирование инструкций x86/A64 чуть опрятнее; Thumb работал бы лучше.\n  - Полуточные 18‑битные числа прижились бы раньше; экзотика 4–5 бит не взлетела бы.\n  - Расширенный ASCII влез бы с греческим и стал бы «натовской» кодовой страницей; UTF‑9 привилегировал бы почти всю Западную Европу.\n  - Права Unix умещались бы в один байт (без «липких» битов). Оctal стал бы нормой вместо hex.\n  - 18‑битный цвет 6/6/6 даёт различия на грани восприятия; потеря альфа‑канала неприятна.\n\n- Издержки? Существенных нет: адресация по битам не используется; деления на девять не требуется; размеры страниц/блоков ОС могли бы остаться прежними, ядру не пришлось бы менять основы работы.",
      "commentsSummary": "- Основная линия дискуссии: идея «9‑битных байтов» якобы дала бы удобный запас по размерам (36‑битные адреса, числа и т.п.), но критики считают аргумент натянутым: исторические решения менялись бы, а не просто масштабировались на +12,5%.  \n- Аппаратные возражения: размеры, не равные степеням двойки, неудобны для схемотехники, мультипликаторов и адресации; 9 бит ухудшают простоту деления на половины и кодирование позиций битов.  \n- Экономика и производительность: 9‑битные байты требуют больше проводников/логики и дают на 12,5% меньше «байтов» памяти при том же бюджете; выгода часто не окупает стоимость.  \n- Исторические примеры: существовали 6‑битные и 36‑битные архитектуры (DEC PDP‑серии), разные «длины байта»; в стандартах закрепился «octet» для однозначности 8 бит.  \n- Сомнения в практической пользе: проблемы 16/32 бит (символы, глобальные идентификаторы, таймстемпы) не решаются добавлением одного бита; ASLR и безопасность едва выигрывают от 36 бит; при 64 бит уже есть «запас» и лишние биты для метаданных.  \n- Альтернативные идеи: 9‑й бит как флаг длины для варинтов/инструкций; дополнительные тэг‑биты для Lisp; вариант «9‑й бит под ECC» для потребительских устройств; встречные фантазии про 10‑битные байты, 6‑битные байты и даже троичную систему.  \n- Общий вывод участников: ключевая проблема — наши ошибки в оценке нужных размеров, а не выбранная ширина байта; 9 бит дали бы другие узкие места, а не устранили бы системные просчёты.",
      "score": 158,
      "commentsCount": 279,
      "hnUrl": "https://news.ycombinator.com/item?id=44816692",
      "domain": "pavpanchekha.com"
    },
    {
      "id": 44823130,
      "title": "The Emperor's New Trade Deal – Paul Krugman",
      "url": "https://paulkrugman.substack.com/p/the-emperors-new-trade-deal",
      "by": "rbanffy",
      "timeISO": "2025-08-07T11:26:33.000Z",
      "postSummary": "- Трамп на CNBC заявил, что ЕС согласился “подарить” США $600 млрд, которыми он сможет распоряжаться как угодно. Это выдумка: Еврокомиссия не может заставлять частный сектор или государства-члены инвестировать, таких обязательств не было.\n\n- Это “новая торговая одежда императора”: громкие заявления без содержания. Важно не только заблуждение о сделке, но и то, что, осознав отсутствие “подарка”, Трамп грозит поднять тариф на ЕС с 15% до 35%.\n\n- Часть введённых тарифов вероятно незаконна и может быть отменена судами с возвратом сборов. Если же Верховный суд позволит Трампу действовать, насколько это ударит по Европе?\n\n- Экономический ущерб ограничен. Экспорт товаров ЕС в США в 2024 году был чуть менее 3% ВВП. Тариф 15% при эластичности Армингтона около 3 сокращает экспорт примерно на треть — до ~2% ВВП. Это чувствительно, но не критично; компании демонстрируют устойчивость, а часть потерь компенсируется ростом госрасходов (инфраструктура, оборона), особенно в Германии.\n\n- Повышение тарифа до 35% по прикидкам сократит экспорт ещё примерно на 0,7% ВВП — меньший удар, чем уже нанесённый 15%-ным тарифом. Логика: чем выше тариф и ниже продажи в США, тем меньше дальнейшие потери от новых повышений.\n\n- ЕС пока избегал ответных мер ради видимости “сделки”. Но если Трамп будет предъявлять новые требования, вероятны ответные тарифы Европы и других партнёров, что ещё сильнее ослабит эффективность американских угроз.",
      "score": 6,
      "commentsCount": 0,
      "hnUrl": "https://news.ycombinator.com/item?id=44823130",
      "domain": "paulkrugman.substack.com"
    },
    {
      "id": 44815702,
      "title": "A fast, growable array with stable pointers in C",
      "url": "https://danielchasehooper.com/posts/segment_array/",
      "by": "ibobev",
      "timeISO": "2025-08-06T18:21:28.000Z",
      "postSummary": "Моя предыдущая статья о обобщённых структурах данных в C готовила почву к теме: структура, которая заменяет динамические массивы, даёт стабильные указатели и хорошо работает с аренными аллокаторами. Её переоткрывали много раз под разными именами: “levelwise-allocated pile” (2001), в Zig — Segmented List, частично похожая на C++ std::deque. Мне нравится название Per Vognsen — Segment Array.\n\nСкачать мой однофайловый заголовок segment_array.h можно, подписавшись на рассылку.\n\nИдея проста: фиксированный массив указателей на сегменты; каждый следующий сегмент вдвое больше предыдущего; новые сегменты выделяются по мере необходимости. Поскольку элементы не двигаются, указатели на них стабильны, не остаются “дыры” в арене, а доступ по индексу — за O(1).\n\nРеализация\n\nСтруктура на C:\n\ntypedef struct {\n    u32 count;\n    int used_segments;\n    u8 *segments[26];\n} SegmentArrayInternal;\n\nПочему всего 26 сегментов? Из 64 бит указателя обычно реально используются 48, так что 49 сегментов уже перекрывают адресное пространство (~256 ТиБ). Я предпочитаю индекс u32 (до ~4 млрд элементов) — это даёт 32 сегмента. Ещё убираем 6 маленьких (1..32), начинаем с 64, остаётся 26 сегментов — хватает для 4 294 967 232 элементов (чуть меньше UINT32_MAX). Фиксированный массив рядом со структурой снижает риск промаха кэша.\n\nРазмеры сегментов — степени двойки: проще математика и быстрые сдвиги для индексов.\n\n#define SMALL_SEGMENTS_TO_SKIP 6\n\n#define log2i(X) ((u32) (8*sizeof(unsigned long long) \\\n    - __builtin_clzll((X)) - 1))\n\nu32 capacity_for_segment_count(int segment_count) {\n    return ((1 << SMALL_SEGMENTS_TO_SKIP) << segment_count)\n        - (1 << SMALL_SEGMENTS_TO_SKIP);\n}\n\nvoid *_sa_get(SegmentArrayInternal *sa, u32 index, size_t item_size) {\n    int segment = log2i((index >> SMALL_SEGMENTS_TO_SKIP) + 1);\n    u32 slot = index - capacity_for_segment_count(segment);\n    return sa->segments[segment] + item_size*slot;\n}\n\nlog2i использует __builtin_clzll (подсчёт ведущих нулей) для быстрого вычисления номера сегмента.\n\nClang оптимизирует _sa_get до ~10 инструкций x86-64 (-O3), так что узким местом будет память, а не вычисления индекса. При последовательной итерации можно обходить сегменты напрямую; в segment_array.h есть макрос.\n\nВыделение нового элемента:\n\nu32 slots_in_segment(int segment_index) {\n    return (1 << SMALL_SEGMENTS_TO_SKIP) << segment_index;\n}\n\nvoid *_sa_alloc(SegmentArrayInternal *sa, size_t item_size) {\n    if (sa->count >= capacity_for_segment_count(sa->used_segments)) {\n        size_t segment_size = item_size * slots_in_segment(sa->used_segments);\n        sa->segments[sa->used_segments++] = malloc(segment_size);\n    }\n    sa->count++;\n    return _sa_get(sa, sa->count-1, item_size);\n}\n\nЗамечание: можно сделать ёмкость строго степенью двойки, если первые два сегмента одинакового размера. Код станет менее изящным, но это спасает от ~50% потерь памяти при использовании как массива бакетов в хеш-таблице со степенью двойки.\n\nДженерики\n\nЯ применяю технику из прошлой статьи для типобезопасного хранения любого типа. Макрос связывает тип с общей структурой:\n\n#define SegmentArray(type) \\\n    union { \\\n        SegmentArrayInternal internal; \\\n        type *payload; \\\n    }\n\nДальше макросы используют payload, чтобы передавать сведения о типе…",
      "commentsSummary": "- Обсуждают «сегментированный массив» с ростом сегментов по степеням двойки: многие замечают, что корректнее называть это списком/декой, так как память не непрерывна и API массивов не подойдёт.  \n- Сравнения: это похоже на std::deque и rope; Zig имеет std.SegmentedList; есть близкие проекты вроде rust-array-stump и plf::colony. Отличие от deque — экспоненциальные сегменты и отсутствие prepend.  \n- Плюсы: стабильные указатели и эффективные вставки/удаления в середине (у rope — O(log n)); можно применять copy-on-write и версии.  \n- Минусы: потеря истинной «континуальности» ухудшает предвыборку и итерацию; накладные расходы инструкций в L1-циклах могут быть заметны; потенциальный перерасход памяти при больших сегментах и мелких первых сегментах.  \n- Альтернатива: использовать виртуальную память — резерв большого адресного пространства и постепенное commit/mmap с guard-pages; это даёт настоящую «массивность», но не подходит для встраиваемых/wasm и требует аккуратности.  \n- Реализации различаются: у MSVC deque малополезен из‑за размера блоков; у libc++/GNU лучше; поэтому предпочтительно контролировать структуру самостоятельно.  \n- Детали реализации/оптимизации: обсуждают clz/bsr/lzcnt и флаги компилятора; предлагают итераторы по сегментам для снижения накладных расходов; вопросы о выборе фиксированных vs экспоненциальных сегментов и о сжатии мелких сегментов.",
      "score": 197,
      "commentsCount": 73,
      "hnUrl": "https://news.ycombinator.com/item?id=44815702",
      "domain": "danielchasehooper.com"
    },
    {
      "id": 44817583,
      "title": "The Bluesky Dictionary",
      "url": "https://www.avibagla.com/blueskydictionary/",
      "by": "gaws",
      "timeISO": "2025-08-06T20:43:08.000Z",
      "postSummary": "- время — 4732 использ.; последнее появл. 08/07/2025 08:49\n- для — 4751; последнее 08/07/2025 08:49\n- медитация — 509; последнее 08/07/2025 08:49\n- неделя — 4017; последнее 08/07/2025 08:49\n- война — 3974; последнее 08/07/2025 08:49\n- трамп — 4711; последнее 08/07/2025 08:49\n- ии — 4565; последнее 08/07/2025 08:49\n- после — 4649; последнее 08/07/2025 08:49\n- просто — 4748; последнее 08/07/2025 08:49\n- последний — 4454; последнее 08/07/2025 08:49\n- как — 4747; последнее 08/07/2025 08:49\n- который — 4514; последнее 08/07/2025 08:49\n- о — 4745; последнее 08/07/2025 08:49\n- the — 4757; последнее 08/07/2025 08:49\n- торговля — 2083; последнее 08/07/2025 08:49\n- эффект — 1653; последнее 08/07/2025 08:49\n- президент — 3584; последнее 08/07/2025 08:49\n- тарифы — 2665; последнее 08/07/2025 08:49\n- io — 2480; последнее 08/07/2025 08:49\n- расширяется — 95; последнее 08/07/2025 08:49\n- пошлины — 639; последнее 08/07/2025 08:49\n- полночь — 1772; последнее 08/07/2025 08:49\n- ошеломляющий — 439; последнее 08/07/2025 08:49\n- страны — 4866; последнее 08/07/2025 08:49\n- взяли — 8866; последнее 08/07/2025 08:49\n- начать — 2796; последнее 08/07/2025 08:49\n- объявил — 3688; последнее 08/07/2025 08:49\n- мужчина — 4577; последнее 08/07/2025 08:49\n- снег — 958; последнее 08/07/2025 08:49\n- анан — 76; последнее 08/07/2025 08:49\n- снеговик — 202; последнее 08/07/2025 08:49\n- развлечения — 3310; последнее 08/07/2025 08:49\n- ich — 3267; последнее 08/07/2025 08:49\n- die — 4390; последнее 08/07/2025 08:49\n- mein — 2276; последнее 08/07/2025 08:49\n- мастурбация — 712; последнее 08/07/2025 08:49\n- sind — 5177; последнее 08/07/2025 08:49\n- сила — 3583; последнее 08/07/2025 08:49\n- сам — 2763; последнее 08/07/2025 08:49\n- друзья — 3607; последнее 08/07/2025 08:49\n- гигант — 1239; последнее 08/07/2025 08:49\n- вместе — 3278; последнее 08/07/2025 08:49\n- месяцы — 2845; последнее 08/07/2025 08:49\n- вещи — 4425; последнее 08/07/2025 08:49\n- смотреть — 4485; последнее 08/07/2025 08:49\n- демократия — 2165; последнее 08/07/2025 08:49\n- однажды — 3766; последнее 08/07/2025 08:49\n- те — 4467; последнее 08/07/2025 08:49\n- что-либо — 4208; последнее 08/07/2025 08:49\n- делаю — 4378; последнее 08/07/2025 08:49",
      "commentsSummary": "- Обсуждают сайт, который отслеживает, какие слова из словаря ещё не встречались в постах Bluesky; пользователи делятся наблюдениями о «нормальности» многих «невиданных» слов и погрешностях из‑за языков (французские посты, названия групп, топонимы типа Wheal).  \n- Несколько человек сталкивались с багом «0 слов»; советуют разрешить скрипты с avibagla.com, подождать минуту загрузки, Jetstream работает и без аккаунта.  \n- Автор подтверждает: бэкенд простой — SQLite с таблицами слов (слово | счётчик | первое/последнее упоминание | пост) и статистикой; не ожидал такой популярности.  \n- Обсуждают масштаб: 1,7 млрд постов в Bluesky, сайт проверил ~4,92 млн (≈0,28%); Jetstream предпочтительнее «firehose» по трафику (около 20 Мбит/с против 200 Мбит/с).  \n- Технические идеи: хэштаблица/хэшсет для слов, разбор токенов, возможен trie для экономии памяти; объём словаря ~250–275k слов легко помещается в десятки МБ.  \n- Замечания о точности: метаданные языка в Bluesky помогают, но пользователи не всегда их выставляют; встречаются ложные срабатывания из‑за омонимов, брендов и копипасты словаря.  \n- Дискуссия затрагивает дизайн сайта, открытость API и этику использования постов для NLP против LLM; есть шутки и мета‑комментарии о смежных проектах автора.",
      "score": 175,
      "commentsCount": 51,
      "hnUrl": "https://news.ycombinator.com/item?id=44817583",
      "domain": "avibagla.com"
    },
    {
      "id": 44821642,
      "title": "40 Years of the Amiga",
      "url": "https://www.goto10retro.com/p/40-years-of-the-amiga-from-commodore",
      "by": "rbanffy",
      "timeISO": "2025-08-07T07:36:15.000Z",
      "postSummary": "В июле 1985 года миру официально представили Commodore Amiga — революционный компьютер с не менее удивительной историей разработки.\n\nБудучи «атари‑человеком» в 80‑х, я знал об Amiga по упоминаниям в журналах Atari и колонках вроде ANALOG Computing и Creative Computing.\n\n«Отцом» Amiga был легендарный разработчик чипов Джей Майнер — главный конструктор Atari 2600/VCS и 8‑битных Atari. В 1982 году он ушёл из Atari и занялся кардиостимуляторами.\n\nВскоре Ларри Каплан (экс‑Atari, сооснователь Activision) предложил Майнеру возглавить железо для новой игровой системы в его компании Hi‑Toro. Из Tonka пришёл Дэвид Морс, компанию вскоре переименовали в Amiga.\n\nРаботая над приставкой, Amiga продавала аксессуары для Atari 2600, чтобы финансироваться. Ранний код «Lorraine» часто всплывал в слухах журналов. Проект задумывался как приставка, но Майнер хотел компьютер; после краха индустрии игр в 1983‑м так и вышло — курс сменили на ПК.\n\nДенег не хватало: разработка дорога. Amiga получила $500 тыс. от Atari с правом Atari использовать технологии Amiga в консолях и компьютерах.\n\nВ начале 1984‑го Джек Трамиел ушёл из Commodore и вскоре купил у Warner компанию Atari, переименовав свою Trammel Technologies в Atari Corp. Примерно через месяц Commodore, не имея своего 16‑битного проекта, купила Amiga за $27 млн. Узнав о займе Warner/Atari, стороны подали иски, из‑за чего Commodore долго не могла публично говорить об Amiga.\n\nТем не менее, 23 июля 1985 года Amiga была официально анонсирована — любопытно, что всего через несколько недель после начала поставок Atari 520ST. Премьеру устроили с размахом в нью‑йоркском Линкольн‑центре с участием Дебби Харри и Энди Уорхола.\n\nИзначально маркетинг упирал на имя Amiga и почти не использовал бренд Commodore: часто говорили «Amiga, from Commodore». К 1987‑му вернулись к «Commodore Amiga». Некоторые ранние ролики были очень странными.\n\nЛично я знал об Amiga лишь по слухам и до сих пор ею не пользовался. Первые подробности прочёл в номере Compute! за сентябрь 1985 — обложка и большой материал о новинке.\n\nЧестно говоря, я тогда немного «горел»: обложка с анонсом вышла за месяц до обзора реально поставляемой модели.",
      "commentsSummary": "- Участники ностальгируют по Amiga (особенно A2000), вспоминают PC-bridgeboard/sidecar с 8088 и опыт одновременного доступа к MS-DOS и Amiga, а также программирование в GW-BASIC, AmigaBASIC, C и ассемблере.  \n- Отмечают масштабные празднования 40-летия на VCF West с участием ключевых фигур из 80-х; ожидаются видео на YouTube.  \n- Делятся ресурсами: серия статей Ars Technica «History of the Amiga», книга The Future Was Here (глубокое объяснение аппаратуры и демо с bouncing ball), книги Брайана Багналла.  \n- Обсуждают графику: упоминание скана Mandrill и более известных работ Джима Сакса с эффектами цветового циклинга.  \n- Технические детали: гибкость и странности с ROM/дискетами Kickstart, возможность апгрейда ROM, нюансы с HDD через sidecar/MFM; практические проблемы и со временем переход на отдельный PC.  \n- Правообладание и бренды: покупка бренда Commodore Perifractic’ом, возможный интерес к Amiga, но Commodore ≠ Amiga; права на Amiga фрагментированы и неясны.  \n- Культурный контекст: в Европе Amiga/Atari и «bedroom coders» формировали игровую сцену, в отличие от консольного фокуса США.",
      "score": 59,
      "commentsCount": 22,
      "hnUrl": "https://news.ycombinator.com/item?id=44821642",
      "domain": "goto10retro.com"
    },
    {
      "id": 44782229,
      "title": "What is the average length of a queue of cars? (2023)",
      "url": "https://e-dorigatti.github.io/math/2023/11/01/queue-length.html",
      "by": "alexmolas",
      "timeISO": "2025-08-04T04:55:11.000Z",
      "postSummary": "Некоторое время назад я ехал по извилистой горной дороге и застрял в медленной очереди машин — обгонять было небезопасно. От скуки я задумался: сколько машин в такой очереди и какова средняя длина очередей на этой дороге?\n\nСформализуем задачу. Дорога одна, без съездов, бесконечной длины. Каждая машина при въезде имеет свою среднюю скорость. Быстрые догоняют медленных и, не имея возможности обгона, выстраиваются за ними. Со временем возникает «стационарное» состояние: формируются группы машин, движущиеся со скоростью самого медленного спереди. Вопрос: какова средняя длина таких групп?\n\nИнтуитивный (но неверный) подход\n--------------------------------\nПусть скорости v_i i.i.d. Очередь образуется, если v_2 ≤ v_1, и v_3 ≤ v_1, и т. д. Каждое сравнение, по симметрии распределения (достаточно обменности), имеет вероятность 1/2. Тогда P{длина ≥ k} = (1/2)^(k-1), а матожидание длины равно 2. То же самое можно увидеть, считая длину геометрической с параметром 1/2. Но это не совпадает с ощущениями.\n\nСимуляция\n---------\nЯ написал симуляцию и получил иные результаты.\n\n- Скорости берутся из равномерного распределения на [0,1].\n- В 100 000 прогонах растим очередь, пока новая машина не медленнее «эталонной» v0 (первой в очереди): пока v_i ≥ v0, добавляем машину; иначе очередь заканчивается.\n- Результат:\n  - средняя длина ≈ 10.69,\n  - медиана = 2, квартиль 75% = 4,\n  - максимум наблюдался 22 849,\n  - распределение хвостатее, чем геометрическое с p=1/2; предсказанная геометрией вероятность длинных очередей убывает слишком быстро.\n\nКорректное решение\n------------------\nИнтуитивная причина ошибки: выбор очереди «смещён». Если видим длинную очередь, то ведущая машина почти наверняка очень медленная; условие «v0 маленькая» резко повышает шансы, что последующие v_i окажутся ≥ v0, удлиняя хвост распределения. Нельзя просто умножить независимые вероятности сравнения без учёта того, что v0 в длинных очередях не типична. Нужно условное распределение длины по v0 и затем усреднение по плотности v0, взвешенной вероятностью породить длинную очередь. Именно этот эффект отбора даёт тяжёлый хвост и среднюю длину существенно больше 2.",
      "commentsSummary": "- Участники обсуждают корректность выводов из блога о формировании «очередей» автомобилей на дороге; один комментатор отмечает, что приведённая формула фактически описывает ожидаемое число очередей как гармоническую сумму 1 + 1/2 + 1/3 + …, где каждый следующий «самый медленный на данный момент» автомобиль начинает новую очередь.  \n- Возникает путаница между вопросами: число очередей vs размер наблюдаемой очереди; указано, что в модели одна «самая медленная» машина неизбежно создаёт огромную хвостовую очередь, что может давать большую среднюю длину одной очереди.  \n- Замечание: при конечном числе машин всегда будет одна самая медленная, и когда она появляется, все сзади «прилипают», что наводит на ожидание одной большой очереди примерно из половины машин в среднем.  \n- Некоторые указывают на некорректные или упрощающие допущения (бесконечная дорога, отсутствие съездов/массы/упругости), что влияет на применимость вывода.  \n- Отмечено, что в некоторых симуляциях количество машин не фиксировано: запуск идёт до появления машины медленнее первой, из-за чего сравнение результатов может быть некорректным.  \n- Есть мнение, что формулы для числа очередей при N машинах и для средней длины случайной очереди могут иметь схожую структуру, хотя по допущениям число очередей «бесконечно».  \n- Дополнительные замечания касаются реальных эффектов (например, автобусы, создающие пустоты впереди из-за остановок) и шутливых упрощений вроде «сферических машин».",
      "score": 26,
      "commentsCount": 10,
      "hnUrl": "https://news.ycombinator.com/item?id=44782229",
      "domain": "e-dorigatti.github.io"
    },
    {
      "id": 44777086,
      "title": "Automerge 3.0",
      "url": "https://automerge.org/blog/automerge-3/",
      "by": "surprisetalk",
      "timeISO": "2025-08-03T15:08:58.000Z",
      "postSummary": "Automerge — это движок синхронизации данных с приоритетом локальной работы, упрощающий создание коллаборативных приложений. Выпущена версия 3.0.\n\nГлавное обновление — резкое снижение потребления памяти. Ранее хранение полной истории документов могло приводить к гигабайтам в ОЗУ. В 3.0 память сокращена более чем в 10 раз (иногда значительно больше), что делает Automerge применимым в куда большем числе сценариев.\n\nТакже упразднены избыточные API, особенно при работе со строками.\n\nЕсли вы уже используете Automerge, обновляйтесь: формат файлов тот же, API почти полностью обратно совместим. Подробности — в руководстве по миграции. Если вы ещё не пробовали, сейчас хорошее время — производительность и надежность сильно выросли.\n\nЧтобы узнать, как достигнуты улучшения, читайте далее.\n\n- Улучшенное использование памяти\n  - Automerge хранит каждое изменение для офлайн-работы, конфликтов и истории; это требует большого объёма метаданных.\n  - Раньше: сжатый колоночный формат «на диске», но при загрузке в память — несжатый вид, из-за чего ОЗУ раздувалось.\n  - Теперь: сжатое представление используется и во время выполнения, давая огромную экономию. Пример: вставка «Моби Дика» — было ~700 МБ в v2, стало ~1,3 МБ в v3.\n  - Меньше памяти — стабильнее нагруженные сервера синхронизации.\n  - Для документов с длинной историей существенно ускорена загрузка (пример: с «не загрузилось за 17 часов» до 9 секунд).\n\n- Упрощение API\n  - Два типа строк: «коллаборативные» (сливают правки) и «неколлаборативные».\n  - В 1.0: обычные строки для неколлаборативных, класс Text — для коллаборативных.\n  - В 2.0 (namespace next): сделали коллаборативный текст по умолчанию — строки для него, RawString для неколлаборативного.\n  - В 3.0: закрепили новый подход — удалён Text, API next стал дефолтным; RawString переименован в ImmutableString.\n\n- Попробовать\n  - Automerge 3.0 используется по умолчанию в последних `@automerge/automerge-repo` и `@automerge/react` (версия `2.1.0`).\n  - Новичкам — туториал. Существующим кодовым базам — руководство по миграции; если зависите от `@automerge/automerge-repo`, выполните `npm update @automerge/automerge`.\n  - Проблемы — создавайте issue; вопросы — в Discord.",
      "commentsSummary": "- Обсуждают релиз Automerge 3.0 и связанный проект Pg_CRDT; отмечают малое число комментариев при высоком качестве/интересе темы.  \n- Главный хайлайт — огромные улучшения производительности и памяти: сжатое представление на рантайме, загрузка больших историй быстрее, пример с “Моби Диком” — с ~700 МБ до ~1.3 МБ.  \n- Сравнивают подходы: Automerge/Yjs для совместного редактирования (rich text), ElectricSQL — когда сервер остаётся источником истины для local‑first; просят бенчмарки против yjs и упоминают jsonjoy для перфа.  \n- Вопросы по интеграциям: React/ProseMirror, возможность с Tiptap (ответ: можно, обернув схему и пересобрав undo/redo), интерес к терминальным UI; спрашивают о поддержке операций перемещения в деревьях (пока нет в основном релизе).  \n- Технологический стек: ядро на Rust, удобный интерфейс на JavaScript; есть C API-обёртка (статус для 3.0 неясен).  \n- Обсуждают модели данных и синхронизации: уникальные device ID и упорядоченные события, типобезопасные CRDT и бизнес-правила, конфликтные записи и их разрешение; интересуются структурой полурешётки и типом регистров (multi-value vs LWW).  \n- Прикладные кейсы: совместное редактирование документов; вопрос техрайтера о версионировании секций — советуют уточнить требования; также интерес к синху “один пользователь/несколько устройств” и роли централизованного сервера.",
      "score": 327,
      "commentsCount": 29,
      "hnUrl": "https://news.ycombinator.com/item?id=44777086",
      "domain": "automerge.org"
    },
    {
      "id": 44819962,
      "title": "Mac history echoes in current Mac operating systems",
      "url": "http://tenfourfox.blogspot.com/2025/08/mac-history-echoes-in-mac-operating.html",
      "by": "classichasclass",
      "timeISO": "2025-08-07T02:27:51.000Z",
      "postSummary": "Ars Technica отметила, что в macOS Tahoe старые значки жёстких дисков заменяют более общими и скучными. Если вы на Sequoia и хотите сохранить их, возьмите из:\n/System/Library/Extensions/IOStorageFamily.kext/Contents/Resources\nТекст на этикетке проработан, и даже винты Torx. Достаньте T8 MacCracker для этого диска:\n\n[Изображение 1]\n\nВ системе сохранились и другие отголоски прошлого. The Spacebar заметил, что в шрифте Apple Symbols до сих пор есть старые, «устаревшие» пиктограммы, полезные разве что пользователям Power Mac в веб-браузерах.\n\n[Изображение 2]\n\nИ это ещё не всё: в файле больше значков, чем он показал. Вот что я нашёл — возможно, обнаружите больше.\n\n[Изображение 3]\n\nПо порядку: логотип PowerPC; композитный видео‑выход/вход; S‑Video выход/вход (как на поздних PowerBook); модемный порт; совмещённый модем/принтер (Duo 2300); принтер; SCSI; Ethernet/AAUI; три глифа ADB; сервер; контурное радужное яблоко; Balloon Help (System 7); Apple Guide (7.5); дискетa 5,25\" (скорее для Apple II); две лампочки Newton; undo, extras, dates, names Newton; дискета 3,5\" HD; «растерянный» компактный Mac (намёк на мигающий вопрос при отсутствии загрузочного тома); классический логотип QuickTime; «часы занятости»; порт Apple Pro Speakers (iMac G4, MDD G4); FireWire; значок программистской клавиши; две версии reset (для них есть аналоги в Unicode или геометрические фигуры; иногда были отражены).\n\nПримечание: большинство этих символов не привязаны к кодовым точкам Unicode; это отдельные глифы. Font Book их покажет, но копировать нельзя. Ultra Character Map позволит взять графику и вставить, как я сделал здесь.\n\nИ это ещё не всё. Загляните в /System/Library/CoreServices/CoreTypes.bundle/Contents/Resources — там тоже клад. Особенно впечатляют «мульти‑размеры» для разных экранов; ниже — 1024×1024 144 dpi Retina из Sequoia.\n\n[Изображение 4] eMac,\n\n[Изображение 5]\n\n[Изображение 6]",
      "commentsSummary": "- Обсуждение о том, почему в macOS до сих пор присутствуют старые иконки и ассеты: многие из них реально используются для сетевых устройств и обратной совместимости, а удаление несет риск и затраты.  \n- Finder определяет модель удаленного хоста через Bonjour-запись _device-info._tcp; поэтому NAS-ы (Synology/QNAP/Samba) могут отображаться как Xserve или другие модели Apple.  \n- Иконка «BSOD» отображает Windows-компьютеры с сетевыми шарами; аналогично используются иконки старых iPhone/iPod при синхронизации, а также в «Об этом Mac».  \n- Пользователи отмечают, что подобные «наследные» ресурсы есть и в Windows (например, moricons.dll), так что это не уникально для Apple.  \n- Есть ностальгия по старому дизайну Apple/NeXT и наблюдения о сохранении некоторых поведенческих традиций UI (Return — переименовать, Cmd-O — открыть).  \n- В Samba через vfs_fruit можно задавать модель устройства (fruit:model), чтобы принудительно показывать нужную иконку в сети.  \n- Часть комментаторов спорит: это либо вопрос торговых марок и «лень убирать мусор», либо просто то, что эти ресурсы по-прежнему нужны и используются.",
      "score": 125,
      "commentsCount": 43,
      "hnUrl": "https://news.ycombinator.com/item?id=44819962",
      "domain": "tenfourfox.blogspot.com"
    },
    {
      "id": 44787073,
      "title": "Scientists have recreated the Universe's first molecule",
      "url": "https://www.sciencedaily.com/releases/2025/08/250803011840.htm",
      "by": "LAsteNERD",
      "timeISO": "2025-08-04T15:20:51.000Z",
      "postSummary": "- Исследователи воссоздали первое молекулярное соединение во Вселенной — ион гелий-гидрида (HeH⁺) — в ультрахолодной установке, смоделировав условия более чем 13 млрд лет назад. Выяснилось, что эта молекула гораздо эффективнее, чем считалось, охлаждала раннюю космическую среду, помогая газовым облакам коллапсировать и запускать рождение первых звезд.\n\n- После рекомбинации и наступления «темных веков» дальнейшее охлаждение стало возможным главным образом за счет молекул, способных излучать энергию через вращательные и колебательные переходы. Благодаря выраженному дипольному моменту HeH⁺ особенно эффективен при низких температурах, а его концентрация могла существенно влиять на скорость раннего звездообразования.\n\n- Ключевой путь разрушения HeH⁺ в ту эпоху — столкновения со свободными атомами водорода, образующие нейтральный гелий и ион H2⁺; тот далее реагирует с H, давая молекулярный водород H2 и протон. В эксперименте MPIK исследовали аналогичную реакцию с дейтерием: HeH⁺ + D → He + HD⁺, воспроизводя условия ранней Вселенной.\n\n- Измерения в криогенном кольце хранения (CSR, MPIK, Гейдельберг) показали, что реакция быстрая и протекает без энергетического барьера — вопреки прежним теоретическим ожиданиям. Это пересматривает модели химии ранней Вселенной и механизм охлаждения газа, уточняя путь от «темноты» к зажиганию первых звезд.",
      "commentsSummary": "- Обсуждали ион гелия гидрида (HeH+), который невероятно реакционноспособен: «реагирует со всем», поэтому его невозможно хранить в обычных контейнерах.  \n- Он считается одним из сильнейших известных кислотных видов (pKa около −63), что кратно сильнее, чем у соляной кислоты.  \n- Из-за экстремальной реактивности HeH+ не существует на Земле в заметных количествах и естественно встречается лишь в межзвёздной среде.  \n- Идеи о хранении сводятся к нематериальным методам вроде электромагнитного/магнитного удержания, поскольку контакт с веществом ведёт к немедленной реакции.  \n- Новизна недавних экспериментов не в «создании» молекулы (она изучается почти 100 лет), а в измерении скоростей реакций с дейтерием при низких температурах для моделей ранней Вселенной.  \n- По данным Википедии, HeH+ считается первым соединением, возникшим во Вселенной; современные эксперименты имитируют космические условия (например, в криогенных кольцах хранения).  \n- Итог: HeH+ — «ест все», крайне нестабилен вне космических условий и потому требует специальных безконтактных методов исследования.",
      "score": 19,
      "commentsCount": 10,
      "hnUrl": "https://news.ycombinator.com/item?id=44787073",
      "domain": "sciencedaily.com"
    },
    {
      "id": 44814596,
      "title": "Multics",
      "url": "https://www.multicians.org/multics.html",
      "by": "unleaded",
      "timeISO": "2025-08-06T16:57:55.000Z",
      "postSummary": "- Логотип Multics\n\n- Домой\n  - История »\n    - Возможности\n    - Мифы\n    - Project MAC\n    - Даты\n    - Глоссарий\n    - Проект истории\n    - Последний сайт\n  - Люди »\n    - Истории\n    - Фотографии\n    - Юмор\n    - Памятные вещи\n  - Библиотека »\n    - Статьи и доклады\n    - Технические статьи\n    - Документы разработки\n    - Исходники\n    - Симулятор\n  - Сайты »\n    - Хронология площадок\n    - AFDSC (Пентагон, Вашингтон)\n    - ASEA (Вестерос, Швеция)\n    - Avon (Бристоль, Англия)\n    - Bell Canada (2 площадки)\n    - CISL (Кембридж, Массачусетс)\n    - CNO (Миннеаполис, Миннесота)\n    - DND-H (Галифакс, Канада)\n    - DOCKMASTER (АНБ, Мэриленд)\n    - FORD (Детройт, Мичиган)\n    - GM (Детройт, Мичиган)\n    - Майнц (Германия)\n    - MIT (Кембридж, Массачусетс)\n    - NWGS (ВМС США, 4 площадки)\n    - OU (Рочестер, Мичиган)\n    - PRHA (Сан-Хуан, Пуэрто-Рико)\n    - RADC (Рим, Нью-Йорк)\n    - RAE (Фарнборо, Англия)\n    - STC (Лондон, Англия)\n    - System-M (Финикс, Аризона)\n    - Systeme X (Лувесьен, Франция)\n    - UC/ACTC (Калгари, Канада)\n    - USGS (3 площадки)\n    - USL (Лафайет, Луизиана)\n    - VPI (Блэксберг, Вирджиния)\n  - О сайте »\n    - Изменения\n    - Новости\n    - Ссылки\n    - Галерея\n    - Карта сайта\n\n- Поиск\n- Меню: История: Возможности | Мифы | Project MAC | Даты",
      "commentsSummary": "- Обсуждение посвящено наследию Multics: его влиянию на современные ОС и проектам, таким как STOP (преемник SCOMP), которым до сих пор занимаются некоторые участники.  \n- Делятся личными воспоминаниями об использовании Multics в университетах Британии в 1980-х: система считалась быстрой после наращивания ОЗУ, поддерживала терминальное общение и удалённый доступ по JANET.  \n- Мнения расходятся: одни критикуют «всеобъемлющий» дизайн Multics за сложность и эклектичность, другие отмечают недооценённые преимущества — кольцевую модель безопасности, MAC/ACL, сегментную память и защиту от переполнений.  \n- Приводятся ресурсы: мифы о Multics, оценка безопасности B2, страница по защите данных, «Три вопроса» о багах, меморабилия, а также связанные дискуссии на HN.  \n- Обсуждают портируемость: Multics был теоретически переносимым, но практически зависел от узкого круга мэйнфреймов; в то же время напоминают, что ACL есть в части UNIX-подобных систем.  \n- Отмечают грустную тенденцию на странице изменений сайта — регулярные некрологи участников сообщества; упоминается закрытие последнего известного сайта Multics в 2000 году.  \n- Новички признаются, что недооценивали Multics как «предтечу UNIX» и теперь видят его как богатый источник идей, которые стоит переосмысливать, а не бесконечно тиражировать UNIX-модель.",
      "score": 125,
      "commentsCount": 28,
      "hnUrl": "https://news.ycombinator.com/item?id=44814596",
      "domain": "multicians.org"
    },
    {
      "id": 44778898,
      "title": "Comptime.ts: compile-time expressions for TypeScript",
      "url": "https://comptime.js.org/",
      "by": "excalo",
      "timeISO": "2025-08-03T19:11:40.000Z",
      "postSummary": "![Изображение 1: Hyperactive](https://raw.githubusercontent.com/feathers-studio/comptime.ts/master/docs/comptime.ts.svg)\n\nПростой компилятор TypeScript для вычисления выражений с пометкой comptime на этапе сборки. Полезно для переноса вычислений из рантайма в компиляцию. Вдохновлено Bun macros и Zig comptime.\n\nВнимание: вы сами отвечаете за безопасность выражений, вычисляемых на этапе компиляции. Изоляции нет. Импорты comptime допускаются только в файлах проекта (не в node_modules), но можно импортировать из node_modules как comptime.\n\nСодержание\n- Что такое comptime.ts?\n- Примеры: 1) простая сумма; 2) CSS без рантайма; 3) константы во время сборки\n- Установка\n- Использование: Vite, Bun, CLI, API\n- Принудительная оценка и промисы, отказ от «вирусности»\n- Запуск кода после comptime, как работает, ограничения, практики, отладка, поддержка, лицензия\n\nЧто это\ncomptime.ts вычисляет выражения при компиляции, сокращая работу в рантайме.\n\nПримеры\n1) Простая сумма\nimport { sum } from \"./sum.ts\" with { type: \"comptime\" };\nconsole.log(sum(1, 2));\n// => console.log(3);\n\n2) Emotion CSS без рантайма\nimport { css } from \"@emotion/css\" with { type: \"comptime\" };\nconst style = css`color: red; font-size: 16px;`;\ndiv({ class: style });\n// => const style = \"css-x2wxma\"; div({ class: style });\n\nПримечание: импорт @emotion/css удаляется. Стили нужно вывести отдельно (после comptime или плагином бандлера).\n\n3) Константы на этапе сборки\nimport { ms } from \"ms\" with { type: \"comptime\" };\nconst HOUR = ms(\"1 hour\");\n// => const HOUR = 3600000;\n\nПоддерживаются многие выражения (включая индексацию и импортированные константы), результат должен быть сериализуем в JSON. Импорты с type: \"comptime\" удаляются; лишнее убирает ваш бандлер.\n\nУстановка\nbun add comptime.ts\npnpm add comptime.ts\nnpm install comptime.ts\n\nИспользование\n- Vite:\nimport { comptime } from \"comptime.ts/vite\";\nexport default defineConfig({ plugins: [comptime()] });\n\nТолько в прод-сборке, если поведение совпадает с рантаймом:\nexport default defineConfig({ build: { rollupOptions: { plugins: [comptime()] } } });\n\n- Bun:\nimport { comptime } from \"comptime.ts/bun\";\nawait Bun.build({ entrypoints: [\"./index.ts\"], ou ... })",
      "commentsSummary": "- Некоторые участники хотят макросы в стиле Rust/Зig для JS/TS (jsx! или comptime), чтобы делать преобразования на этапе компиляции и даже заменять препроцессорные макросы C; другие решительно против привнесения макросов Rust в экосистему JS/TS.  \n- Обсуждают comptime-подход: он может вычислять выражения в compile-time, но для агрессивного удаления/инлайнинга веток нужен последующий шаг минификатора/бандлера (Vite, Bun и др.) для устранения if(false)/инлайнинга if(true).  \n- Есть критика использования import-атрибута type для comptime-модулей: одни считают это неправильным, так как type традиционно про mime-type (json, css), другие ссылаются на предложение стандарта, где назначение type оставлено открытым.  \n- Спрашивают про статус синтаксиса import ... with { type: ... }; отвечают, что это часть процесса стандартизации (EcmaScript), но детали поведения зависят от конкретных предложений (например, JSON Modules).  \n- Поднимают вопрос о возможности comptime для типов и дженериков как в Zig; текущие инструменты, похоже, не покрывают типо-уровневые вычисления, что могло бы помочь с “слишком большими” union в TS.  \n- Упоминают альтернативы: sweet.js для макросов (с проблемами совместимости), lite-jsx без макросов, а также Rust-фреймворки Dioxus/Leptos; по wasm мнения расходятся, часть хочет писать веб-приложения на Rust, другие задаются вопросами практической применимости/менеджмента памяти.  \n- Ограничения: возврат именованных функций и поддержка замыканий в comptime-генерации пока сложны из-за необходимости устойчивого переноса функций между процессами JS; поэтому авторы временно избегают таких кейсов.",
      "score": 138,
      "commentsCount": 30,
      "hnUrl": "https://news.ycombinator.com/item?id=44778898",
      "domain": "comptime.js.org"
    },
    {
      "id": 44812695,
      "title": "Breaking the sorting barrier for directed single-source shortest paths",
      "url": "https://www.quantamagazine.org/new-method-is-the-fastest-way-to-find-the-best-routes-20250806/",
      "by": "baruchel",
      "timeISO": "2025-08-06T14:43:02.000Z",
      "postSummary": "Если хотите решить сложную задачу, полезно разложить ее на части и идти от простого к сложному. Но сортировка частей тоже стоит времени: можно потратить его больше на упорядочивание, чем на решение.\n\nЭто особенно заметно в культовой задаче информатики — поиске кратчайших путей от одной вершины графа до всех остальных. Интуитивно проще сначала находить ближайшие цели, затем более дальние. Но для этого нужно постоянно определять, какая вершина ближе, то есть фактически сортировать по расстоянию. Возникает «барьер сортировки»: алгоритм, зависящий от сортировки, не может быть быстрее, чем сама сортировка.\n\nКоманда исследователей предложила новый алгоритм, который обходится без сортировки и работает быстрее всех «сортирующих» методов, преодолев сорокалетний барьер. Роберт Тарьян назвал результат поразительным.\n\nГрафы описывают вершины и ребра с весами (длина, время и т.п.). Задача: по данному источнику найти кратчайшие пути до всех вершин. Алгоритм Дейкстры (1956) идет «волной» наружу: зная оптимальные пути к ближайшим, находит пути к дальним. Но конечный результат — упорядоченный набор кратчайших расстояний, и потому скорость ограничена сортировкой. В 1984 году Тарьян с соавтором улучшили Дейкстру до этого предела: чтобы ускориться дальше, нужно избегать сортировки.\n\nВ конце 1990-х — начале 2000-х Торуп и другие обошли барьер для частных случаев, вводя ограничения на веса. Обобщить техники на произвольные веса не удавалось, и прогресс застопорился. Ран Дуань не согласился с пессимизмом и стремился сломать барьер для всех графов — и прошлой осенью добился цели.\n\nИдея Дуани (созревшая к 2021 году): вместо того чтобы на каждом шаге сканировать всю «границу» уже исследованной области, как делает Дейкстра (что со временем замедляет ход), группировать соседние граничные вершины в кластеры и рассматривать по одному представителю из каждого. Это уменьшает число кандидатов и может вести не к ближайшей вершине — значит, зависимость от сортировки исчезает. Главная трудность — доказать, что кластеризация действительно ускоряет процесс на каждом шаге и в сумме.\n\nЗа следующий год Дуань проработал идею и к осени 2022-го был уверен, что технические барьеры преодолимы.",
      "commentsSummary": "- Обсуждение вокруг нового алгоритма SSSP из препринта arXiv:2504.17033, дающего детерминированное время O(m log^(2/3) n) и тем самым впервые асимптотически обходящего Дейкстру на разреженных графах.  \n- Участники спорят о практической значимости: на плотных графах m доминирует и выигрыш может исчезнуть; на дорожных сетях m ≈ O(n), что даёт реальную выгоду до O(n log^(2/3) n).  \n- Поднимаются вопросы корректности/глобального оптимума по сравнению с Дейкстрой и опасений, что «склеивание фронтира» может пропускать решения — предлагается обратиться к статье за деталями.  \n- Некоторые замечают, что метод сложнее Дейкстры, но это ожидаемо; звучат идеи о гибридизации с рандомизацией для дополнительных ускорений.  \n- Есть комментарии о том, что для задач вроде TSP с почти полный графом (m ~ n^2) выигрыш не очевиден.  \n- Отмечается «незамысловатость» используемой математики и удивление, что такое могли открыть ещё десятилетия назад; приводятся ссылки на препринт и HN-поиск.  \n- В стороне обсуждают вклад Тарьяна, общую обучаемость Дейкстры и аналогии с практическими алгоритмами вроде TimSort.",
      "score": 155,
      "commentsCount": 46,
      "hnUrl": "https://news.ycombinator.com/item?id=44812695",
      "domain": "quantamagazine.org"
    },
    {
      "id": 44811280,
      "title": "303Gen – 303 acid loops generator",
      "url": "https://303-gen-06a668.netlify.app/",
      "by": "ankitg12",
      "timeISO": "2025-08-06T12:50:06.000Z",
      "postSummary": "303Gen",
      "commentsSummary": "- Обсуждение крутого веб-инструмента в духе TB-303: генерация трёх взаимосвязанных паттернов (бас, лид, дрон) с гармонически согласованными поли-ритмами; многие отмечают музыкальность результата и ностальгию по ReBirth/303.  \n- Автор признался, что проект был не закончен, добавил полифилл для cancelAndHoldAtTime в Firefox и улучшил мобильное управление (большие ручки, вертикальная раскладка).  \n- Пользователи просят фичи: визуализация на piano roll, сохранение/загрузка, экспорт в MIDI, MIDI sync/transport и старт/стоп, возможность авто‑модуляции, не сбрасывать темп и не останавливать при Regenerate, повысить верхний предел BPM, добавить 909‑кит, локальный оффлайн-режим и открытый исходный код.  \n- Замечены баги: в Firefox падение из‑за отсутствия cancelAndHoldAtTime (позже подтверждение от разработчика Firefox о реализации), в Chromium иногда продолжает играть после Stop.  \n- Многие делятся вдохновением и ссылками на похожие проекты (Endless Acid Banger, roland50.studio), говорят о применении для сэмплов и возвращении интереса к электронной музыке.  \n- Есть вопросы о том, как экспортировать/сохранять понравившиеся паттерны и как интегрировать с внешними VST/железом.  \n- Общий тон — восторг и ностальгия; инструмент хвалят за звучание, интерфейс и потенциал как генеративного «фона» для джемов.",
      "score": 209,
      "commentsCount": 73,
      "hnUrl": "https://news.ycombinator.com/item?id=44811280",
      "domain": "303-gen-06a668.netlify.app"
    },
    {
      "id": 44812985,
      "title": "Zig Error Patterns",
      "url": "https://glfmn.io/posts/zig-error-patterns/",
      "by": "Bogdanp",
      "timeISO": "2025-08-06T15:03:19.000Z",
      "postSummary": "Введение\n\nЯ часто использую отладчик, но привык и к выводной отладке, особенно в юнит-тестах. Хотелось улучшить её и чаще подключать отладчик.\n\nУлучшение выводной отладки\n\nГлавная проблема — «шум»: в цикле интересна одна итерация, а печатается всё. Или удобнее читать форматированную структуру, но приходится раскидывать print’ы по коду. В Zig тесты используют error’ы, значит можно печатать только при падении теста через errdefer:\n\ntest {\n    errdefer std.debug.print(\"{f}\", .{ast});\n    // ...\n}\n\nТак контекст появляется только при ошибке, без засорения лога.\n\nЗапуск тестов в отладчике\n\nПросто запустить seergdb или gdb -tui неудобно: тестовые бинарники лежат в zig-cache. Трюк из ziggит: build.zig может запускать команды и передавать путь артефакта:\n\n// seergdb — GUI фронтенд для gdb\nconst debugger = b.addSystemCommand(&.{ \"seergdb\", \"--run\", \"--\" });\ndebugger.addArtifactArg(exe_unit_tests);\n\nconst debug_step = b.step(\"debug\", \"Run unit tests under debugger\");\ndebug_step.dependOn(&debugger.step);\n\nЭто запускает правильный бинарник. Но отладчик сработает лишь на брейкпоинте или панике, тогда как раннер тестов «проглатывает» ошибки.\n\nКомбинация трюков\n\nДобавим @breakpoint через errdefer:\n\ntest {\n    errdefer @breakpoint();\n}\n\nТак мы попадаем в точку ошибки, видим контекст и вывод std.testing.expect*. Минус: при zig build test отчёт показывает падение всего шага тестов, а не отдельных кейсов. Нужна возможность включать брейкпоинты выборочно.\n\nУсловная компиляция\n\nЧерез build options пробрасываем флаг, решающий, вызывать ли @breakpoint в тестах.\n\nМинимальный скрипт сборки, запускающий тесты, дополняем опциями:\n\nconst std = @import(\"std\");\n\npub fn build(b: *std.Build) void {\n    const target = b.standardTargetOptions(.{});\n    const optimize = b.standardOptimizeOption(.{});\n\n    const lib = b.addModule(\"zig-test-patterns\", .{\n        .root_source_file = b.path(\"src/root.zig\"),\n        .target = target,\n        .optimize = optimize,\n    });\n\n    const options = b.addOptions();\n    options.addOption(bool, \"debugger\", false);\n    lib.addImport(\"config\", options.createModule());\n\n    const mod_tests = b.addTest(.{ .root_module = lib });\n    const run_mod_tests = b.addRunArtifact(mod_tests);\n\n    const test_step = b.step(\"test\", \"Run tests\");\n    test_step.dependOn(&run_mod_tests.step);\n}\n\nВ коде тестов:\n\nconst std = @import(\"std\");\nconst config = @import(\"config\");\n\ntest \"errdefer @breakpoint()\" {\n    errdefer if (config.debugger) @breakpoint();\n    return error.FixMe;\n}\n\ntest \"no breakpoint\" {\n    return error.FixMe;\n}\n\nzig build test — без брейкпоинтов. Но менять значение флага так — значит пересобирать build.zig. Добавим опцию прямо в систему сборки:\n\nvar options = b.addOptions();\nconst use_debugger = b.option(\n    bool,\n    \"debugger\",\n    \"Enables code intended to only run under a debugger\",\n) orelse false;\noptions.addOption(bool, \"debugger\", use_debugger);\n\nТеперь можно переключать поведением командой:\n\nzig build -Ddebugger test\n\nИ, при желании, привязать шаг запуска отладчика к этому флагу.",
      "commentsSummary": "- Участники хвалят целостность дизайна Zig и минимализм синтаксиса; особенно отмечают удобство errdefer и интеграцию отладки через build.zig.  \n- Много комментариев о приятном оформлении сайта: шрифты (Berkeley Mono), цветовая схема, «DOS»-стилистика, в целом «красиво и удобно читать».  \n- Обсуждается отсутствие пэйлоадов у ошибок в Zig: это воспринимают как слабое место, но приводят идиоматическое решение — возвращать «чистую» ошибку и передавать диагностику через опциональный указатель/внешний объект.  \n- Примеры: std.json использует отдельный diagnostics-объект; ссылки на обсуждение issue и статьи/видео о «скрытых» пэйлоадах и паттернах.  \n- Позиция части участников: ошибки — это про управление потоком; детали можно вернуть через out-параметры/контекст, а не в самом error.  \n- Вспомогательные замечания: сравнение errdefer с D scope(failure); вопросы о различиях с try/catch; советы по стилю имен в Zig и замечания о смешении camelCase/snake_case в коде.  \n- Итог: Zig предлагает элегантные механизмы (errdefer, build-интеграции), но отсутствие встроенных пэйлоадов в ошибках вызывает дискуссии; коммьюнити выработало практичные идиомы обхода.",
      "score": 148,
      "commentsCount": 46,
      "hnUrl": "https://news.ycombinator.com/item?id=44812985",
      "domain": "glfmn.io"
    },
    {
      "id": 44815046,
      "title": "AI in Search is driving more queries and higher quality clicks",
      "url": "https://blog.google/products/search/ai-search-driving-more-queries-higher-quality-clicks/",
      "by": "thm",
      "timeISO": "2025-08-06T17:32:42.000Z",
      "postSummary": "- Генеративный ИИ в Поиске увеличивает общее число запросов и приводит к более качественным кликам. Пользователи задают больше сложных, многосоставных вопросов и чаще продолжают сессию последующими уточнениями.\n\n- Мы по‑прежнему каждый день отправляем миллиарды переходов на сайты и встраиваем ссылки на источники прямо в ИИ‑ответы. Приоритет — поддержка открытой сети: улучшаем видимость издателей, экспертов и создателей контента.\n\n- Ранние данные показывают рост времени взаимодействия и удовлетворённости: люди быстрее находят нужное, чаще кликают по результатам с высокой релевантностью и реже возвращаются к той же теме.\n\n- ИИ‑обзор помогает в «мутных» запросах: сравнение вариантов, планирование, пошаговые инструкции. Он предлагает краткий ответ с цитируемыми источниками, после чего люди переходят глубже по ссылкам.\n\n- Бизнесы видят более качественный трафик: больше кликов с высокой вероятностью конверсии. Продолжаем расширять рекламные форматы, сохраняя чёткую маркировку рекламы и органики.\n\n- Мы тестируем и улучшаем модели, учитывая отзывы пользователей и издателей, чтобы поддерживать полезность, точность и безопасность ИИ‑ответов.",
      "commentsSummary": "- Участники спорят о влиянии AI-обзоров в Google: многие признают, что чаще не кликают по ссылкам, довольствуясь кратким ответом, особенно для простых фактов, но при «серьёзных» запросах всё же переходят к источникам.  \n- Скептики считают заявления Google о «большем числе и качестве кликов» маркетингом: кликов может быть больше из‑за худшего опыта поиска, а «качество» — спорно; приводятся примеры грубых галлюцинаций и вводящих в заблуждение ответов.  \n- Есть опасения за экосистему веба: меньше трафика подрывает стимулы создателей контента, усиливает зависимость от крупных площадок и сделок, а ниши с человеческим кураторством и уникальным опытом рискуют исчезнуть.  \n- Некоторые видят плюсы: AI-дайджесты ускоряют навигацию по SERP, отсекают SEO-спам и помогают быстрее сузить круг источников; для части пользователей Google снова стал удобнее.  \n- Обсуждается монетизация: вероятно, Google будет встраивать рекламу в AI-ответы и удерживать пользователей внутри своего интерфейса, что усиливает конфликт интересов.  \n- Отмечаются системные проблемы: качество поиска со временем деградировало, AI-ответы порой плагиатны или противоречат топовым ссылкам; пользователи делятся анекдотами, где AI уверенно выдаёт неверные советы (вплоть до «полезности рома»).  \n- Итог настроений полярный: часть аудитории приветствует удобство и фильтрацию спама, другая — призывает уходить на альтернативы (DDG, Kagi) и сомневается в достоверности и добросовестности Google.",
      "score": 75,
      "commentsCount": 102,
      "hnUrl": "https://news.ycombinator.com/item?id=44815046",
      "domain": "blog.google"
    },
    {
      "id": 44822933,
      "title": "Real estate agents use the power of AI to command plumbing, layout to disappear",
      "url": "https://www.theregister.com/2025/08/07/real_estate/",
      "by": "LorenDB",
      "timeISO": "2025-08-07T10:58:03.000Z",
      "postSummary": "«Обманчиво просторный». «Отличное расположение». «Развивающийся район». «С оригинальными элементами» — иногда с асбестом. Агентам по недвижимости давно свойственно приукрашивать, а бум генеративного ИИ дал простой способ показывать изображения, не соответствующие реальности.\n\nФотосъёмка жилья — искусство: широкоугольные объективы, удачные ракурсы, постановка мебели меньшего масштаба. Генеративные модели добавили новый уровень — полное перерисовывание.\n\nПример: лот за £350,000 в Дареме. На титульном фото — гладкий фасад без текстуры, несуразный навес, стена, превращающаяся в живую изгородь, клумба на месте соседской дорожки. Листаете дальше — и находите исходник: реальная короткая крыша над дверью, без клумб и изгородей, а главное — вплотную слева примыкает коммерческое помещение (салон красоты), крыша которого почти касается эркера. На главном кадре его нет вовсе.\n\nВнутри кадры чередуют «реальность» и «как хотелось бы». Пустые спальни «виртуально меблированы», но меняется и планировка: санузел будто расширен, полупрозрачная дверь открывается не туда; на кухне вместо радиатора — «печь с рабочей поверхностью»; унитаз «переезжает» на другую стену и получает штору до пола, проходящую сквозь фановую трубу; ножка пьедестала раковины — сквозь плетёную корзину.\n\nЭксперт по строительной экспертизе Адриан Тегг (MRICS, Университет Рединга) считает использование ИИ в объявлениях серьёзным «красным флагом», сродни прежним «недостоверным описаниям недвижимости». Если не указать явно, что изображения изменены, это, по его мнению, введение в заблуждение. У чартерных сюрвейеров есть обязанность давать корректные, доказательные заключения и страхование профответственности; у риелторов такого профессионального долга почти нет, важнее «сделать сделку», поэтому открытость к ИИ неудивительна.\n\nАгентство Roseberry Newhouse — не единственное: ещё в 2023-м McKinsey прогнозировала, что генеративный ИИ может принести рынку недвижимости $110–180 млрд ценности. Появляются стартапы вроде REimagineHome, стремящиеся…",
      "commentsSummary": "- Автор увидел газетную статью о компании знакомого и отметил, что тот отлично выглядит.  \n- Позже выяснилось, что использовалось изображение с помощью ИИ, что стало неожиданностью для автора.  \n- Автор разочарован тем, что уважаемое издание допустило использование ИИ-обработанного изображения без явного указания.  \n- Ситуация заставила автора пересмотреть ожидания от СМИ в плане подлинности изображений.  \n- В отношении риелторов автор замечает, что их использование подобных практик не удивляет; удивляться здесь было бы наивно.",
      "score": 7,
      "commentsCount": 1,
      "hnUrl": "https://news.ycombinator.com/item?id=44822933",
      "domain": "theregister.com"
    },
    {
      "id": 44820325,
      "title": "FDA approves eye drops that fix near vision without glasses",
      "url": "https://newatlas.com/aging/age-related-near-sighted-drops-vizz/",
      "by": "geox",
      "timeISO": "2025-08-07T03:34:58.000Z",
      "postSummary": "FDA одобрила глазные капли VIZZ для облегчения пресбиопии\n\n- VIZZ — новые рецептурные капли для улучшения близорукости, связанной с возрастом (пресбиопии), позволяющие читать и работать вблизи без очков.\n- Препарат временно повышает глубину резкости за счет сужения зрачка (фармакологическая миозия); эффект наступает быстро и держится несколько часов.\n- Клинические испытания показали статистически значимое улучшение зрения вблизи у большинства участников по сравнению с плацебо.\n- Наиболее частые побочные эффекты: головная боль, покалывание в глазах, покраснение, снижение качества зрения при слабом освещении; не рекомендуется для ночного вождения.\n- Капли не лечат причину пресбиопии и не заменяют консультацию офтальмолога; подходят как альтернатива или дополнение к очкам/линзам.\n- Ожидается коммерческий запуск в США после финализации маркировки и логистики; доступ по рецепту.",
      "commentsSummary": "- Обсуждение вокруг новых глазных капель для пресбиопии: они работают за счет сужения зрачка, повышая резкость (глубину резкости), но это снижает количество света на сетчатке и может ухудшать зрение при слабом освещении и выглядеть косметически непривлекательно.  \n- Пользователи задаются вопросами о безопасности и долгосрочном применении, возможности «передозировки» при легкой пресбиопии и о том, можно ли получить схожий эффект просто увеличив освещенность или используя «пинхол»-прием.  \n- Цена и модель продаж вызывают споры: озвучены ориентиры ~$79 за 25-дневный набор или ~$198 за 3 месяца, некоторые опасаются перехода от «рецептурных» к «подписочным» решениям и сравнивают с дешевыми «читалками» за ~$15.  \n- Альтернативы: моновижн (для очков/контактов и после LASIK) — мнения делятся, многие сообщают об адаптации мозга за недели, но часть людей не переносит; упоминаются EVO ICL и ИОЛ-импланты как более дорогие, но обратимые/долговременные варианты.  \n- По поводу публикаций данных: объясняется, что часто рецензируемые статьи выходят после одобрения FDA, поскольку подача в FDA идет быстро после готовности результатов, а рецензирование и публикация занимают больше времени; также отмечают регуляторные и рыночные причины.  \n- Есть опасения о недостаточной «жесткости» FDA и влиянии индустрии, а также шутливые сравнения с «бутылированным прищуром».  \n- Практические замечания: эффект лучше днём/при ярком свете; в темноте может быть сложно, а время действия может не покрывать более 10 часов у некоторых пользователей.",
      "score": 108,
      "commentsCount": 59,
      "hnUrl": "https://news.ycombinator.com/item?id=44820325",
      "domain": "newatlas.com"
    },
    {
      "id": 44813905,
      "title": "Show HN: Sinkzone DNS – Forwarder that blocks everything except your allowlist",
      "url": "https://github.com/berbyte/sinkzone",
      "by": "dominis",
      "timeISO": "2025-08-06T16:08:23.000Z",
      "postSummary": "- Интернет бесконечен. Ваше внимание — нет. Sinkzone меняет правило по умолчанию: все заблокировано, пока вы явно не разрешите. Никаких лент, пингов и скрытых подключений — только тихий, осознанный интернет.\n\n- Узнать больше · Быстрый старт · Сообщить об ошибке · Руководство по использованию\n\n- Содержание: Что такое Sinkzone? · Мотивация · Зачем использовать · Ключевые возможности · Демо (TUI/CLI) · Быстрый старт (установка) · Документация (man) · Использование (команды, шаблоны, навигация) · Как это работает (архитектура, API, режимы) · Конфигурация · Разработка · Лицензия · Контакты\n\n- Что такое Sinkzone? Локальный DNS‑резолвер для глубокой концентрации: блокирует все домены по умолчанию, пропуская только явно разрешённые. Оповещения, соцсети и новости отрезаются на сетевом уровне. Есть HTTP API, поддержка масок и удобный терминальный интерфейс. Лёгкий, кроссплатформенный, для тех, кто серьёзно относится к фокусу.\n\n- Мотивация: Блок‑листы бесконечны — проще перечислить разрешённое. Автору надоели отвлечения (Slack, почта); нужен был системный «рубильник», сильнее браузерных плагинов. Теперь можно работать часами; даже сын использует Sinkzone во время шахмат. «Мне это было нужно — возможно, и вам тоже».\n\n- Зачем использовать:\n  - Избавление от отвлечений без расширений и костылей\n  - Локальная работа на macOS, Linux, Windows; без облака и телеметрии\n  - Подходит для глубоких сессий, письма, кода и детской безопасности\n  - Встроенный TUI: переключение режимов, логи, редактирование allowlist",
      "commentsSummary": "- Обсуждают Sinkzone — локальный DNS с режимом “allowlist по умолчанию” и расписаниями фокус-режима; пользователи считают это удобнее, чем глобальные блоклисты/файрволы, и видят преимущество перед Pi-hole/AdGuardHome в персональном, не сетевом применении.  \n- Запросы/идеи: импорт/экспорт списков, профили с разными списками (и общий перма-блок), поддержка DoH, интеграция с Apple Shortcuts и помодоро-расписаниями, работа в VPN/частных DNS-зонах, а также мобильная поддержка (iOS).  \n- Опасения: приложения могут обходить DNS, подключаясь по IP — это требует файрволов; возможные конфликты с корпоративным VPN/DNS.  \n- Альтернативы и смежные инструменты: OpenSnitch, fort, Portmaster, simplewall (Windows), NextDNS (с расписаниями), Pi-hole/AdGuardHome, hosts-блоклисты (StevenBlack).  \n- Автор Sinkzone подтверждает планы: кастомные расписания (в т.ч. помодоро), многопользовательские профили/аллоу-листы для семьи, рассмотрение мобильной поддержки; предлагает настраивать апстримы для работы с VPN, созданы первые issue.  \n- Пользователи отмечают, что блокировка только на ПК ведет к переключению на телефон, поэтому нужна кросс-девайс стратегия (NextDNS, VPN к домашнему DNS, мобильные клиенты).  \n- В целом тон позитивный: инструмент считают более целевым и “block-first” удобным, хотя просят закрыть пробелы в DoH, мобильности и управлении списками.",
      "score": 80,
      "commentsCount": 41,
      "hnUrl": "https://news.ycombinator.com/item?id=44813905",
      "domain": "github.com"
    }
  ]
}
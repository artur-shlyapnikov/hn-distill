{
  "id": 44840013,
  "lang": "ru",
  "summary": "- Проект фокусируется на полностью локальной, приватной AI-среде: песочница `coderunner`, локальные модели и отсутствие утечек данных.  \n- Основные узкие места — железо (80+ B модели требуют ≥32–64 ГБ ОЗУ/VRAM) и хранение больших RAG-данных.  \n- Пользователи делятся опытом: кто-то запускает Ollama/Open-WebUI на RTX 3090/4090 или M1/M2 с 32 ГБ; кто-то использует Kasm, Docker-контейнеры или NAS-конфигурации.  \n- Есть запросы на «однокнопочный» Docker compose, мобильные on-device решения (Cactus) и браузерные варианты без установки.  \n- Некоторые считают локальные LLM скорее хобби из-за медленного инференса и высокой стоимости железа, но видят пользу для приватных или узких бизнес-задач.",
  "sampleComments": [
    44841209,
    44845799,
    44841291,
    44840240,
    44840833
  ],
  "inputHash": "f694ed444691652c52161c50814a1d498bdf88ee0c602f13983ccd2208d544df",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-09T21:27:36.921Z"
}
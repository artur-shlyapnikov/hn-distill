{
  "id": 44834918,
  "lang": "ru",
  "summary": "- Участники обсуждают «attention sinks» — токены-«стоки», на которые модель направляет избыточное внимание, чтобы не «размывать» полезную информацию.  \n- Это поведение замечено и в BERT ([SEP], точки), и в ViT (фоновые патчи), и в GPT-OSS, где вместо добавления единицы к знаменателю обучают отдельный логит на каждую голову.  \n- Синк-токены работают как «pressure valve», предотвращая over-mixing и давая модели «нулевой» вектор для случаев «не найдено».  \n- Пользователи замечают, что первые слова («Hello», «Please») или CLS-подобные глобальные токены могут непреднамеренно служить такими стоками.  \n- FOSS-реализации уже поддерживают приём: llama.cpp принял PR, а Diff-Transformer и другие идеи быстро переиспользуются.",
  "sampleComments": [
    44835762,
    44836836,
    44837863,
    44836310,
    44837403
  ],
  "inputHash": "44f8df1304f1038ede2d64351b24dbacaa36c030b1bec1d7e880fe7a8d19722f",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-09T11:25:51.571Z"
}
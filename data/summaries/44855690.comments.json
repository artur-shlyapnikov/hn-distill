{
  "id": 44855690,
  "lang": "ru",
  "summary": "- GPT-OSS не привносит радикально новой архитектуры, а аккуратно комбинирует проверенные оптимизации (RoPE, SwiGLU, GQA, MoE) с необычными деталями и MXFP4-квантом.  \n- На практике Qwen3 (особенно 30–32 B) чаще хвалят: он быстрее, точнее следует инструкциям, лучше справляется с кодом, хотя некоторые жалуются на медленную генерацию и проблемы с форматом diff.  \n- GPT-OSS-120B показывает высокие результаты в математике, но в логических задачах и агентных сценариях уступает; при этом он MoE-модель, поэтому реально использует лишь ~24 B параметров.  \n- Большинство участников считает, что прорыв происходит не в архитектуре, а в данных, RL и тренировочных приёмах; китайские модели быстро догоняют американские.  \n- Локальный запуск Qwen3 30B (5-bit MLX или EXL2-q6) укладывается в 20 ГБ ОЗУ/VRAM и уже полезен для повседневной разработки.",
  "sampleComments": [
    44860707,
    44856128,
    44857190,
    44856661,
    44863558
  ],
  "inputHash": "4f46a1042171717b3a12b766c563bfe2b7261a83692440c07476eb1ee8534a8c",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-11T14:29:06.609Z"
}
{
  "id": 44856101,
  "lang": "ru",
  "summary": "- BarakWidawsky предполагает, что худшие результаты диффузионных моделей связаны с их меньшей способностью к запоминанию и просит больше эпох тренировки 1B-модели на 10B датасете.  \n- woadwarrior01 удивлён 16–4700× росту FLOPs при генерации 16–4096 токенов и предполагает нелинейную зависимость из-за не-авторегрессивной природы диффузии.  \n- godelski критикует неясность терминов «in/out of distribution» и «downstream task» без описания состава обучающих данных.  \n- godelski добавляет, что обе модели улучшались эпоха к эпохе, но сомневается, что продолжение обучения сильно изменит выводы.  \n- thesz подчёркивает, что диффузия требует больше вычислений, растущих с длиной последовательности, и намекает на возможность адаптивных вычислений.  \n- ckjellqv считает, что авторегрессивные модели выигрывают за счёт KV-кэширования, недоступного диффузионным моделям, но последние позиционируют это как плюс для рассуждений.",
  "sampleComments": [
    44857283,
    44856906,
    44857195,
    44857681,
    44857412
  ],
  "inputHash": "3254c8c1f952af578d5a1c0b47e6ad075a9e8ffc2c7a833390d3cc222fd324b9",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-10T20:26:05.389Z"
}
{
  "id": 44856101,
  "lang": "ru",
  "summary": "- Участники спорят, почему диффузионные модели проигрывают авторегрессивным: меньше «запоминают», дороже по FLOPs (16–4700×), не позволяют KV-caching.  \n- @bicsi считает, что итеративная авторегрессия с chain-of-thought быстрее, параллельнее и не уступает по качеству, поэтому диффузию пора «отключать».  \n- @BarakWidawsky и @godelski хотели бы увидеть больше эпох обучения 1B-модели на 10B токенах, но сомневаются, что это изменит выводы.  \n- @godelski и @cma указывают на неясность терминов «in/out-of-distribution» и спорят, кто реально имеет ниже loss: авторегрессивные или двунаправленные трансформеры.  \n- @thesz напоминает, что диффузия требует линейно большего compute, но приём «time-dilated RNN» показывает, что можно выжать больше из тех же весов.",
  "sampleComments": [
    44857283,
    44860626,
    44856906,
    44858276,
    44857195
  ],
  "inputHash": "be51f9e2355766bb987bd543dc4b57135048e217550aa46b4c098af1b9a26af6",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-11T05:33:27.354Z"
}
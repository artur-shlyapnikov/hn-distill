{
  "id": 44856101,
  "lang": "ru",
  "summary": "- Увеличение FLOPs от 16× до 4700× объясняется тем, что диффузионные модели не кэшируют KV, в отличие от авторегрессивных, и вычисляют всё «с нуля» при каждом шаге.  \n- Нелинейный рост связан с тем, что сложность трансформера квадратична от длины последовательности: 4096²/16² ≈ 65 000 раз, но на практике это сглаживается до 4700×.  \n- Авторы подают отсутствие KV-кэша как плюс, считая, что полный пересчёт улучшает рассуждения.  \n- Некоторые участники сомневаются в корректности заявлений из-за неясных терминов «in/out of distribution» и отсутствия описания обучающих данных.",
  "sampleComments": [
    44856906,
    44857195,
    44857185
  ],
  "inputHash": "5e0eb831bdee002c923de059b5eb9d3dfea1c584cd78bdaa59eff0c995dae196",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-10T18:32:34.465Z"
}
{
  "id": 44856101,
  "lang": "ru",
  "summary": "- Участники обсуждают, почему диффузионные модели уступают авторегрессивным по loss при равном объёме данных: версия — меньшая «память» и отсутствие KV-caching.  \n- Отмечен экспоненциальный рост FLOPs у диффузионных моделей при генерации длинных последовательностей (×16–4700), что связано с их не-каузальной природой.  \n- Поднимается вопрос корректности терминов «in/out of distribution» и «downstream task» без раскрытия состава обучающих данных.  \n- Есть просьба продлить обучение 1B-модели на 10B датасете: обе модели всё ещё улучшались эпоха к эпохе, но сомнение, что это изменит основные выводы.",
  "sampleComments": [
    44857283,
    44856906,
    44858276,
    44857195,
    44857681
  ],
  "inputHash": "6476eaa2c6e61fe68921ab09a83866410af4a581fc45a59cd993a27f6d796133",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-10T22:23:11.147Z"
}
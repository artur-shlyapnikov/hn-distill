{
  "id": 44878999,
  "lang": "ru",
  "summary": "- Claude Sonnet 4 обгоняет Gemini-2.5-flash на длинном контексте, несмотря на «быстрые» TPU у Google и Anthropic.  \n- У Gemini заметно больше «размытия» контекста: модель путает, кто что говорил, уже при 200 К токенах.  \n- В тестах Gemini выдаёт в 3–6 раз больше текста, но тратит время на «мышление», которое не измеряется.  \n- Пользователи предлагают соревнование по компрессии: LLM сжимают текст в N страниц и отвечают по заметкам.  \n- Вся серия Гарри Поттера (~1 млн слов ≈ 1,4 млн токенов) всё-таки не помещается в заявленный 1-миллионный контекст.",
  "sampleComments": [
    44879827,
    44879649,
    44879393,
    44879864,
    44880447
  ],
  "inputHash": "7432a4a866c8bf8f84717e58ec01d07c2e0b55045176fedf849285530a626ba7",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-12T19:21:56.649Z"
}
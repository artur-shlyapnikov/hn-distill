{
  "id": 44798624,
  "lang": "ru",
  "summary": "**Краткий обзор**  \nПовторяя подход победителей ARC-2024, я заметил: чем меньше модель уверена в пикселе, тем выше шанс ошибки. Авторегрессия заставляет «писать» решение слева-направо, как печатать на машинке без возврата.  \n\nЯ переделал Qwen3-8B в диффузионный режим: сначала заполняем «лёгкие» токены, потом сложные. На 10 шагах модель быстрее и точнее по пикселям, но решает не больше задач. На 30 шагах точность совпадает с базовой, а время выше — из-за отсутствия кеширования.  \n\n**Как работает генерация**  \n1. Кодируем вход как обычный LLM.  \n2. Случайно маскируем 80 % выходных токенов.  \n3. На каждом шаге модель предсказывает маскированные токены; выбираем наиболее вероятные и «размаскиваем».  \n4. Повторяем, пока не останется масков.  \n\n**Почему +1 % к пикселям ≠ +1 % к задачам**  \nARC требует абсолютного совпадения всей сетки. Даже 1 ошибка = 0 баллов. Диффузия чаще «почти» правильна, но «почти» не считается.\n\n**Технические детали**  \n- **Архитектура**: обычный декодер → полносвязный «энкодер» без кэша.  \n- **Обучение**: 1 эпоха, lr 5e-5, batch 64, маскирование 80 %, аугментации поворот/отражение.  \n- **Данные**: 400 задач ARC + 800 синтетических, длина фиксирована 4096 токенов.  \n\n**Результаты на eval-2025**  \n| Метод | Время | Точн. токенов | Решено задач |  \n|-------|-------|---------------|--------------|  \n| Авторегрессия | 1× | 94 % | 21 % |  \n| Диффузия 10 шагов | 0.6× | 95 % | 19 % |  \n| Диффузия 30 шагов | 1.3× | 94 % | 21 % |  \n\n**Следующие шаги**  \n- Вернуть кеш входных токенов, ограничив пересчёт скрытых состояний.  \n- Увеличить шаги до 50–100 при сохранении скорости.  \n- Попробовать «гибрид»: диффузия для грубой раскладки, авторегрессия для деталей.",
  "inputHash": "1d0f2c63336fe9a589836221d04247d22de6dea014ab122ec825e0ee1dad4589",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-09T12:55:12.523Z"
}
{
  "id": 44840746,
  "lang": "ru",
  "summary": "**Ключевая идея**  \nСверхмощные модели требуют миллиардов и сотен тысяч H100, поэтому конкурировать с OpenAI напрямую нереально. Осталось одно: **дистилляция** — обучение маленьких моделей на выдаче больших.\n\n**Что случилось в 2024**  \nКорпорации тратят десятки миллионов на собственные LLM, которые через месяц устаревают. Открытый исходник догоняет не за счёт ресурсов, а за счёт скрытой дистилляции чужих моделей (пример — DeepSeek).\n\n**2025: год агентов и маржи**  \nБизнес понял: обучать LLM бессмысленно. Проще взять самую маленькую модель, которая «достаточно хороша» для задачи, и дождаться улучшений у поставщиков. Если задача пока не решается, строят то, что решается, и ждут пару месяцев.\n\n**Дистилляция = прибыль**  \nБольшие модели медленные и дорогие. Дистилляция сохраняет 95 % качества, ускоряет в 10× и дешевле. Это второй шаг после product-market fit: появились пользователи и расходы — дистиллируй и экономь.\n\n**Проблема**  \nНужны экспертиза и инфраструктура. Inference.net предлагает end-to-end дистилляцию и вывод для тех, кто тратит ≥30 k$/мес и хочет заниматься только продуктом.",
  "inputHash": "54fed52b0420dae383ff4c6c853b176f2d6af9f5e36dcdf0dda6627632e30655",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-08T20:27:27.667Z"
}
{
  "id": 44902148,
  "lang": "ru",
  "summary": "- Вышла крошечная 270-миллионная Gemma 3: 241 МБ, запускается даже на iPhone и Pixel, но «бредит» и плохо следует инструкциям.  \n- Авторы позиционируют её как базу для тонкой настройки под узкие задачи (NER, тегирование WP-статей, сегментация 50 млн событий/день).  \n- Пользователи делятся успехами в суммаризации, автодополнении и переводе с тайского/индонезийского, но для сложных диалогов Qwen-0.6B пока лучше.  \n- 68 % параметров ушло на эмбеддинги; сообщество обсуждает, как выжать больше, если дропнуть лишние языки или перейти на байтовую лексику.  \n- Популярные вопросы: где туториал по дообучению на локальном железе, примеры уже дообученных весов и можно ли упаковать модель в AWS Lambda.",
  "sampleComments": [
    44902240,
    44903731,
    44904181,
    44902980,
    44903495
  ],
  "inputHash": "dfb7502eda03b58697e09340e624948d778c7a24538955a109a081c526362763",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-15T06:36:19.852Z"
}
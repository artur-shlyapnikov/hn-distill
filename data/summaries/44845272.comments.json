{
  "id": 44845272,
  "lang": "ru",
  "summary": "- Пользователи хвалят идеологию Jan.ai, но жалуются на баги: нельзя параллельно общаться с несколькими моделями, Tauri-версия тормозит в Linux, UI раньше был глючным.  \n- Запуск локальных моделей проблемный: Jan пытается загрузить всю 30-ГБ модель в RAM/VRAM, тогда как Ollama справляется с 30B-моделями на 10 ГБ VRAM.  \n- Частые исходящие соединения на GitHub и др. вызывают вопросы к «Privacy First», хотя в описании обещана локальная работа.  \n- Подключить Ollama к Jan непросто: нужно выставить `OLLAMA_HOST=0.0.0.0` и `OLLAMA_ORIGINS=*`, иначе 403 ошибка; официально баг не чинят.  \n- Jan позиционируется как OSS-альтернатива LM Studio c поддержкой удалённых провайдеров, но команда из SG/VN «пропала» после обещаний, что ставит под сомнение их «принципы».",
  "sampleComments": [
    44846669,
    44845613,
    44846208,
    44848144,
    44846300
  ],
  "inputHash": "896074ea32051684379da5cd23c9e6c363e150595f1eaca93891e2fe4cd2cf2f",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-10T02:18:05.253Z"
}
{
  "id": 44834918,
  "lang": "ru",
  "summary": "**Почему модели ломаются на длинных диалогах**  \nПри удалении старых токенов для экономии памяти модель начинает выдавать бессмыслицу. Причина — «attention sinks»: первые токены собирают избыточное внимание, потому что softmax требует, чтобы веса суммировались в 1.  \n\n**StreamingLLM**  \nОставляем первые 4 токена навсегда, остальное сдвигаем окном. Работает до 4 млн токенов; уже в HuggingFace, TensorRT-LLM и новых OpenAI-моделях.\n\n**OpenAI и attention sinks**  \nВ GPT-OSS-20B/120B добавлен обучаемый скаляр в softmax каждой головы, позволяющий «не обращать внимания» — прямое наследие StreamingLLM.\n\n**История открытия**  \nЛетом 2023 в Meta я решал задачу: как продолжать диалог длиннее обучающего окна. Казалось, что достаточно скользящего окна, но при удалении первых токенов перплексия взлетала до небес.\n\n**Визуализация**  \nВнимание Llama-2 постоянно «сливается» в начало. Эти токены-«стоки» не передают внимание дальше, а лишь поглощают его, поэтому их нельзя выбрасывать.\n\n**Математика**  \nSoftmax обязывает каждую голову распределить ровно 1.0 внимания. Если нет полезного контекста, весь «бюджет» уходит в первые позиции, где чуть выше базовые скоры.",
  "inputHash": "325c618d7d286dddc943468e21d2652c69a0c2312e60bb6f173a14cc56ab9f6f",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-08T11:25:19.481Z"
}
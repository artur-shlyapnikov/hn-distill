{
  "id": 44819968,
  "lang": "ru",
  "summary": "- Обсуждение опыта запуска GPT-OSS локально: на MacBook Pro (M4, 128 ГБ) модель быстрая лишь при небольших контекстах; свыше ~10k токенов резко падает скорость, а отсутствие MCP/веб-поиска ухудшает взаимодействие.  \n- Разработчики сравнивают фреймворки: TensorRT-LLM признают самым производительным, но сложным в настройке и иногда отстающим; vLLM и SGLang проще стартовать.  \n- На потребительском железе: 4090 интересует по скоростям; контраст с “широко доступными” H100 вызывает сарказм из‑за цены и доступности.  \n- Отмечают простоту запуска GPT-OSS 20B на Mac (через Llama) и удивление, сколько “допилятий” нужно, чтобы получить хорошее качество — это не “из коробки”.  \n- Поднимается вопрос терминологии GPU: потребительские “игровые” против датацентровых для ИИ, и что “доступный” не равен “дешевый”.  \n- Спекулятивное декодирование вызывает вопросы: как валидировать драфт-токены без фактического прогона целевой модели и где реальная выгода.  \n- Политический контекст: поддержка open-source/open-weight в планах США радует, но OSS от крупных компаний вызывает смешанные чувства; шутки про “очень быстрый генератор отказов” и “освободить” H100.",
  "sampleComments": [
    44822195,
    44822202,
    44821466,
    44821329,
    44820656
  ],
  "inputHash": "65914690046bb5a9df292eb44c3ba8d9a56d6f58ef9b87daf717e879d8515967",
  "model": "openrouter/horizon-beta",
  "createdISO": "2025-08-07T09:08:57.830Z"
}
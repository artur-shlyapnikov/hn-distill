{
  "id": 44867238,
  "lang": "ru",
  "summary": "- Ollama отказалась от llama.cpp как библиотеки и перешла на низкоуровневую ggml, что заставляет их «переизобретать велосипед».  \n- Форк ggml привёл к несовместимости с GGUF-моделями из GPT-OSS; новые SOTA-модели не запускаются без конвертации в проприетарный формат.  \n- Разработчики утверждают, что llama.cpp «слишком нестабилен» для их целей, но продолжают использовать его как fallback.  \n- Пользователи жалуются на игнорирование лицензии, медленное добавление новых моделей и «эншитификацию» проекта.  \n- В качестве альтернативы советуют Docker Model Runner и ramalama, которые просто оборачивают llama-server.",
  "sampleComments": [
    44869466,
    44867259,
    44869432,
    44871769,
    44868084
  ],
  "inputHash": "b522218d2cae06e536b24030ef22796850b674a0dacfb154f8e101a1fd4de2da",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-12T06:38:45.383Z"
}
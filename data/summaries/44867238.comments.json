{
  "id": 44867238,
  "lang": "ru",
  "summary": "- Docker Model Runner предлагает удобный способ запуска LLM через Docker, но обсуждение быстро сместилось на Ollama.  \n- Участники спорят, действительно ли Ollama отказалась от llama.cpp: часть кода вендорится для старых моделей, но основной путь — собственная библиотека поверх ggml.  \n- Форк ggml с несовместимыми изменениями привёл к тому, что новые GGUF-модели не запускаются без конвертации в проприетарный формат Ollama.  \n- Сообщество критикует отсутствие upstream-контрибуций, игнорирование лицензионных issue и «переизобретение велосипедов» ради инвесторов.  \n- Предложено использовать Ramalama или просто llama-server вместо Ollama для прозрачности и совместимости.",
  "sampleComments": [
    44872421,
    44869466,
    44867259,
    44871769,
    44869432
  ],
  "inputHash": "41e802af5871f23c5edd26864744a5edffcdcd85a93a87ed8bd6f638bb486b0b",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-12T04:33:53.329Z"
}
{
  "id": 44867238,
  "lang": "ru",
  "summary": "- Ollama отказался от использования llama.cpp как библиотеки и перешёл на низкоуровневую ggml, что заставляет команду «изобретать велосипеды».  \n- Причина — несовместимость целей: llama.cpp быстро внедряет исследовательские оптимизации, но ломается и нестабилен, а Ollama нужна стабильность для продукта.  \n- Пользователи жалуются, что Ollama всё ещё не поддерживает шардированные GGUF-файлы >48 ГБ и заставляет конвертировать модели в свой проприетарный формат.  \n- Некоторые считают такой подход «мутным» и намёкают на желание понравиться инвесторам, показывая «больше собственного кода».  \n- Альтернатива — проект RamaLama, который просто запускает llama-server в контейнере и перенаправляет запросы.",
  "sampleComments": [
    44869466,
    44867259,
    44869432,
    44868084,
    44869785
  ],
  "inputHash": "55730d9cc4d6517208f8cf7e6e602488815f4dd1f4a639be17635389d6421519",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-11T22:24:17.434Z"
}
{
  "id": 44867238,
  "lang": "ru",
  "summary": "- Ollama отказалась от llama.cpp как библиотеки и теперь работает напрямую с ggml, что вызывает обвинения в «изобретении велосипеда» и несовместимости с GGUF-моделями.  \n- Представители Ollama объясняют: llama.cpp слишком нестабилен для продукта, поэтому нужен собственный код, но при этом старые модели всё ещё обслуживаются через llama.cpp.  \n- Сообщество критикует Ollama за игнорирование upstream-совместимости, отсутствие лицензии в репозитории и медленное добавление поддержки новых больших GGUF-моделей.  \n- Некоторые считают, что компания «обманывает» инвесторов, создавая видимость большего вклада, чем есть на самом деле.",
  "sampleComments": [
    44869466,
    44867259,
    44871769,
    44869432,
    44868084
  ],
  "inputHash": "9e4a61107abe70f1f0b114314e3f9345a80532bec2d196425bab15cd16cc1006",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-12T03:21:14.901Z"
}
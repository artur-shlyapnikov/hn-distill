{
  "id": 44867238,
  "lang": "ru",
  "summary": "- Ollama отказалась от llama.cpp как библиотеки и перешла на низкоуровневый ggml, что заставляет «переизобретать велосипед» и вызывает несовместимость с GGUF-моделями из других проектов.  \n- Разработчики Ollama объясняют это тем, что llama.cpp часто ломается и не подходит для стабильного продукта.  \n- На деле Ollama всё ещё падает обратно на llama.cpp для старых или новых моделей, а их собственная реализация пока неполноценна.  \n- Пользователи жалуются на невозможность запуска новых SOTA-моделей без конвертации в проприетарный формат Ollama.  \n- Сообщество обвиняет проект в «закрытости» и игнорировании upstream-совместимости, но часть участников считает это обычной практикой open-source.",
  "sampleComments": [
    44869466,
    44867259,
    44869432,
    44871769,
    44868084
  ],
  "inputHash": "d7d186fef796e68a5f4fb90d96235928e0ba3df1e8ebd7938fa87781f7be4a3d",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-12T05:25:01.237Z"
}
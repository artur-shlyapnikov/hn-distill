{
  "id": 44867238,
  "lang": "ru",
  "summary": "- Ollama отказалась от использования llama.cpp как библиотеки и перешла на низкоуровневую ggml, что заставляет их «изобретать велосипед».  \n- Причина: llama.cpp часто ломается и меняет скорость, что неприемлемо для продукта.  \n- Коммьюнити обвиняет Ollama в «закрытом» формате моделей и несвоевременной поддержке новых GGUF.  \n- Появился PR на 166 тыс. строк, где Ollama копирует код llama.cpp внутрь репозитория.  \n- Некоторые считают это «шейди» и уступкой корпоративным интересам, другие — обычной форком ради стабильности.",
  "sampleComments": [
    44869466,
    44867259,
    44869432,
    44868084,
    44869785
  ],
  "inputHash": "ae37ba95d46a4a113ec38f9396118c4a8e017e57e1158f3cb050e52ad48e35b3",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-11T23:22:30.212Z"
}
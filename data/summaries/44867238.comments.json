{
  "id": 44867238,
  "lang": "ru",
  "summary": "- Ollama отказалась от прямого использования llama.cpp как библиотеки и перешла на низкоуровневую ggml, что вынуждает её «переизобретать велосипед».  \n- Форк ggml привёл к несовместимости с GGUF-моделями из экосистемы gpt-oss; пользователи не могут запускать новые SOTA-модели без конвертации в проприетарный формат.  \n- Часть сообщества считает это «эншитификацией» и стратегией для последующего монетизации, по аналогии с Docker Hub.  \n- Сами разработчики объясняют уход тем, что llama.cpp слишком нестабилен для продакшена: частые поломки и изменения скорости.  \n- На деле Ollama всё ещё поставляет llama.cpp как fallback для несовместимых моделей, но обновляет его редко.  \n- Альтернативы: llama-server, Ramalama, Docker Model Runner — они проще и контейнеризированы.",
  "sampleComments": [
    44869466,
    44867259,
    44871237,
    44869432,
    44871769
  ],
  "inputHash": "cb7ba51a98804f299cc15a5dd3a277de7c27f5da48a63d5fd85fb7b66147717a",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-12T08:34:06.400Z"
}
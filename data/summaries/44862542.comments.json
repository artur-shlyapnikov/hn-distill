{
  "id": 44862542,
  "lang": "ru",
  "summary": "- Пользователи обсуждают, можно ли снять встроенные «ограничители» модели при локальном запуске: они вшиты, но существуют «abliterated»-файнтюны, где отказы вырезаны, хотя качество падает.  \n- Скорость генерации токенов высокая, но рост контекста резко замедляет обработку даже на мощной машине (5950X + 128 ГБ ОЗУ + RTX 3060 12 ГБ).  \n- На 64-ГБ Mac с MLX-оптимизацией модель может запуститься, но большинство используют 20B-версию, которая летает даже на MacBook.  \n- Function-calling в llama.cpp всё ещё сломан, но уже есть PR с фиксом.  \n- Обсуждаемая оптимизация экспертных слоёв работает через regex и подходит к другим MoE-моделям, например Qwen 3.",
  "sampleComments": [
    44863332,
    44863184,
    44864079,
    44863728,
    44863425
  ],
  "inputHash": "38753ff054d3b06e1d26d24d2ef189fe11708dc1d4f0c2de06f5462d648dbfc6",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-11T14:28:13.884Z"
}
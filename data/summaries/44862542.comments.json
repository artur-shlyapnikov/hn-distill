{
  "id": 44862542,
  "lang": "ru",
  "summary": "- Пользователи обсуждают, можно ли снять встроенные ограничения (guardrails) при локальном запуске модели: нужно искать «abliterated»-файнтюны или использовать jailbreak-промпты, но качество может упасть.  \n- Скорость генерации токенов высокая, но рост контекста резко замедляет обработку; кто-то переключается на Qwen, Mistral, Gemma.  \n- «Скромные» конфигурации вроде 64 ГБ ОЗУ + 8 ГБ VRAM вызывают споры: одни считают это дорогим, другие показывают, что собрать такое можно за $300–1000.  \n- 20B-модель комфортно работает на MacBook, но в llama.cpp всё ещё сломан function-calling; есть PR, который чинит проблему.  \n- MLX-оптимизация и 3-битная квантизация (~50 ГБ) позволяют запускать модель на 64 ГБ Mac.",
  "sampleComments": [
    44863332,
    44863184,
    44864741,
    44863728,
    44864079
  ],
  "inputHash": "67165dbab1497dfbaa6c4fd9b7e5e4c5d2bbabd34d07a9b68a39b6f876c3f2b6",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-11T15:28:57.399Z"
}
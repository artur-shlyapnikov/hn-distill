{
  "id": 44862542,
  "lang": "ru",
  "summary": "- Пользователи жалуются, что при росте контекста модель «убивается» скоростью обработки, хотя генерация токенов быстрая.  \n- Вопросы безопасности: guard-rails вшиты, но есть форки «abliterated» и джейлбрейки, хотя они делают модель глупее.  \n- Дискуссия о цене: 64–128 ГБ ОЗУ и RTX 3060 называют как «дешёвое» железо (< $1000), но для многих всё ещё дорого.  \n- На 64 ГБ Mac и 16 ГБ MacBook Air удаётся запускать 20B и 4B квантованные модели, но function calling в llama.cpp пока сломан.  \n- Появился патч, который должен починить llama.cpp; оптимизация экспертных слоёв через regex работает и на других MoE-моделях.",
  "sampleComments": [
    44863184,
    44863332,
    44864741,
    44863728,
    44866021
  ],
  "inputHash": "e3ca9a3e19d80fad012f8063191dd3ecb44f70cd17063608da973eb82be8ab7a",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-11T17:26:44.368Z"
}
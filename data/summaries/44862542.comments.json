{
  "id": 44862542,
  "lang": "ru",
  "summary": "- Пользователи обсуждают, можно ли убрать встроенные guard-rails при локальном запуске модели: они «вшиты», но есть «аблитерированные» финты, которые делают модель глупее.  \n- Кто-то запускает 20B-вариант на MacBook без проблем, но сломаный function-calling в llama.cpp всё портит (фикс уже в PR).  \n- На 128 ГБ ОЗУ и RTX 3060 токены генерируются быстро, но даже небольшой контекст тормозит; люди переключаются на Qwen/Mistral/Gemma.  \n- Оптимизация экспертных слоёв работает не только на этой модели: достаточно подходящего regex, проверено на Qwen 3.  \n- Некоторые шутят о «мозге на поп-тартах» и просят конкретных цифр вместо «очень быстро / очень медленно».",
  "sampleComments": [
    44863332,
    44863728,
    44863184,
    44863425,
    44863445
  ],
  "inputHash": "dacefed0d16d73f118c0625a7fc852bbe893a6747c11362ec67b008656b3ccee",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-11T13:41:44.745Z"
}
{
  "id": 44810307,
  "lang": "ru",
  "summary": "- Недавние записи  \n  Архив блога\n\n- Одно из ключевых достижений вычислений — сжатие данных: мы уменьшаем размер, сохраняя всю информацию (без потерь), передаём и восстанавливаем исходник.\n\n- Раньше сжатие было необходимо: носители малы, сети медленны. Сейчас это не всегда критично, но по‑прежнему полезно: эта страница почти наверняка пришла к вам в сжатом виде, что ускоряет загрузку и снижает нагрузку на сервер.\n\n- Забавно, что в 2025 мы нередко делаем противоположное. Пример: Бобу нужен новый рабочий компьютер. Его просят написать 4 абзаца обоснования. Он просит LLM сгенерировать текст и отправляет менеджеру.\n\n- Менеджер получает длинное письмо, копирует его в LLM и просит резюме в одном предложении: «Нужен новый компьютер, старый медленный и мешает продуктивности». Заявку одобряют.\n\n- Я называю это «инфляцией LLM»: легко превращать короткое и простое в длинное и видимо глубокое — и обратно, длинное и «глубокое» в короткое и простое.\n\n- Это не упрёк LLM. Но стоит задуматься, почему мы раздуваем контент: в лучшем случае поощряем туманность и трату времени; в худшем — скрываем отсутствие ясной мысли. LLM лишь обнажают масштаб. Возможно, это подтолкнёт нас к изменениям!\n\n- 2025‑08‑06 10:50 — Более раннее\n- Обновления: Mastodon, Twitter, RSS, e‑mail\n\n- Сноски:  \n  И, разумеется, теория информации, но здесь важны практические эффекты.\n\n- Комментарии",
  "inputHash": "daf3c9101e880c2f5c2f08d0c8be53a0e850b7e1703a05ba84a9a34012fb3537",
  "model": "openrouter/horizon-beta",
  "createdISO": "2025-08-07T12:43:05.614Z"
}
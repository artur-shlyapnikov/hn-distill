{
  "id": 44855690,
  "lang": "ru",
  "summary": "- **gpt-oss-20b/120b** — первые с 2019 г. открытые веса от OpenAI; запускаются на одной GPU благодаря **MXFP4** (4-битные веса + 8-битные активации).  \n- **Архитектура** классическая: RoPE, RMSNorm, SwiGLU, без MoE. Отличия от GPT-2: больше слоёв и голов, но уже контекст (8k → 32k).  \n- **Глубина vs ширина**: gpt-oss-120b — 120 слоёв, d_model 6144; Qwen3-235B-A22B — 80 слоёв, d_model 9216. Увеличение глубины дешевле при прочих равных.  \n- **Attention sink** — первые 4 токена не вытесняются из KV-кэша, что стабилизирует длинные контексты.  \n- **Сравнение** (MMLU, GSM8K, HumanEval): gpt-oss-120b ≈ Qwen3-30B-A3B, уступает Qwen3-235B-A22B и GPT-4o, но обгоняет Llama-3-70B.  \n- **GPT-5** (анонс) будет гибридным (dense + MoE), 1–2 трлн параметров, обучен на gpt-oss как teacher.",
  "inputHash": "622051cb78d90544ffd11447f590677eb6654d65f7c94b7d1fb6cd6530a9bac3",
  "model": "moonshotai/kimi-k2:free",
  "createdISO": "2025-08-10T16:28:54.081Z"
}